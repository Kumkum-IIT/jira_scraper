{"id": "d76e02c0bd6c85384297b6dd5550163d", "issue_key": "SPARK-515", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make it easier to run external code on a Nexus cluster", "description": "Right now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.com/nexusproject/nexus/issues/18).", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-04-03T23:04:00.000+0000", "updated": "2020-09-14T19:26:26.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: I have a (partial) implementation of this feature. Please consider reviewing for future merging. https://github.com/tjhunter/spark/commit/8587497497dc825e0d0a312ff156f1853348aebf", "created": "2010-10-11T19:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Closing this now that it is possible to pass jars and sparkHome to SparkContext's constructor.", "created": "2010-10-16T16:01:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-1, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-515\nSummary: Make it easier to run external code on a Nexus cluster\nDescription: Right now, one must modify the Makefile and run script to point to external directories, which is ugly. This will get easier when a Nexus features is added to make it possible to pass the classpath to the Spark executor through environment variables (https://github.com/nexusproject/nexus/issues/18).\n\nComments (3):\n1. Patrick McFadin: Github comment from tjhunter: I have a (partial) implementation of this feature. Please consider reviewing for future merging. https://github.com/tjhunter/spark/commit/8587497497dc825e0d0a312ff156f1853348aebf\n2. Patrick McFadin: Github comment from mateiz: Closing this now that it is possible to pass jars and sparkHome to SparkContext's constructor.\n3. Patrick McFadin: Imported from Github issue spark-1, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.929776"}}
{"id": "e4e58c6edbff9c1f803c5834fb600c2f", "issue_key": "SPARK-514", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Merge fault tolerance code into master branch", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-04-03T23:05:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Done.", "created": "2010-06-17T17:07:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-2, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-514\nSummary: Merge fault tolerance code into master branch\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Done.\n2. Patrick McFadin: Imported from Github issue spark-2, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.929776"}}
{"id": "f7d1d488c5ef769d1dbb3d5ef979f3f6", "issue_key": "SPARK-513", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make NexusScheduler more efficient by keeping a list of tasks for each node", "description": "Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-04-03T23:06:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Fixed in master.", "created": "2010-10-15T15:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-3, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-513\nSummary: Make NexusScheduler more efficient by keeping a list of tasks for each node\nDescription: Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Fixed in master.\n2. Patrick McFadin: Imported from Github issue spark-3, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.929776"}}
{"id": "965ee1f312f3dfd5689e9e2432decae3", "issue_key": "SPARK-512", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Shuffle operation", "description": "This is a big issue... the shuffle operation needs to be designed first!", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-04-03T23:07:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed in master now.", "created": "2011-05-26T11:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-4, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-512\nSummary: Shuffle operation\nDescription: This is a big issue... the shuffle operation needs to be designed first!\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is fixed in master now.\n2. Patrick McFadin: Imported from Github issue spark-4, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.929776"}}
{"id": "ad8e138af827307478a840806a2f40b9", "issue_key": "SPARK-511", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Unify the API and implementation of ParallelArrays and HdfsFiles", "description": "Right now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files. In other words, implement the internal structure described in the Spark paper.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-04-03T23:09:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Done in master branch.", "created": "2010-06-17T17:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-5, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-511\nSummary: Unify the API and implementation of ParallelArrays and HdfsFiles\nDescription: Right now, ParallelArrays support fewer operations simply because HdfsFile was written later. It would be good to rewrite them to provide the same low-level API as files (maybe pulled into a trait called DistributedDataset) and to reuse classes such as MappedFile so that they work on both arrays and HDFS files. In other words, implement the internal structure described in the Spark paper.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Done in master branch.\n2. Patrick McFadin: Imported from Github issue spark-5, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "8488196a0359f9f04d3b0f6f9c82df2e", "issue_key": "SPARK-509", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Merge Mosharaf's broadcast code into master branch", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-06-17T17:07:00.000+0000", "updated": "2012-10-22T14:55:30.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-7, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-509\nSummary: Merge Mosharaf's broadcast code into master branch\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-7, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "130cc340f771249efa8053bb32504e35", "issue_key": "SPARK-510", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Save operation", "description": "RDDs should support a save() call that saves each partition to a distributed filesystem.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-06-17T17:07:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ijuma: This would be handy indeed. Also, it would be nice to support Hadoop's OutputFormat (similar to how InputFormat is supported as a source of data).", "created": "2011-05-31T05:16:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I've looked into this a bit in the past, and the annoying part about Hadoop OutputFormats is that they are tied pretty tightly to running inside a Hadoop task (or at least the file-based ones are). They need a \"task attempt context\" to figure out a temp name for their file, as well as a JobConf that passes various parameters around. I'll probably do this though because it seems useful to support the existing Hadoop formats. In the meantime, if you have a specific format you want (e.g. text or SequenceFile), you can probably use runJob() directly to do it. The quickest way would be to pick some output directory to write to and have each task create a file with a random name in there. However, this puts you in trouble if some task fails, as a retry attempt will choose a different filename (and in general, when we add speculative execution, there may be 2+ concurrent copies of each task active). The \"right\" way then is to have each task create a file in some other (temp) directory and return its filename when it completes, and then have the master rename the output file from each task to a location in the final directory when done.", "created": "2011-05-31T22:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I see. Thanks for the explanation. I had indeed figured out that runJob would allow me to do some of what I need now. It is why I filed issue #56 (I've patched the version I am using, of course).", "created": "2011-06-01T02:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: TD implemented this with pull request 64 (https://github.com/mesos/spark/pull/64), so I'm closing this issue. It should work for text files, SequenceFiles, and other Hadoop output formats of your choice.", "created": "2011-06-27T21:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-6, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-510\nSummary: Save operation\nDescription: RDDs should support a save() call that saves each partition to a distributed filesystem.\n\nComments (5):\n1. Patrick McFadin: Github comment from ijuma: This would be handy indeed. Also, it would be nice to support Hadoop's OutputFormat (similar to how InputFormat is supported as a source of data).\n2. Patrick McFadin: Github comment from mateiz: I've looked into this a bit in the past, and the annoying part about Hadoop OutputFormats is that they are tied pretty tightly to running inside a Hadoop task (or at least the file-based ones are). They need a \"task attempt context\" to figure out a temp name for their file, as well as a JobConf that passes various parameters around. I'll probably do this though because it seems useful to support the existing Hadoop formats. In the meantime, if you have a specific format you want (e.g. text or SequenceFile), you can probably use runJob() directly to do it. The quickest way would be to pick some output directory to write to and have each task create a file with a random name in there. However, this puts you in trouble if some task fails, as a retry attempt will choose a different filename (and in general, when we add speculative execution, there may be 2+ concurrent copies of each task active). The \"right\" way then is to have each task create a file in some other (temp) directory and return its filename when it completes, and then have the master rename the output file from each task to a location in the final directory when done.\n3. Patrick McFadin: Github comment from ijuma: I see. Thanks for the explanation. I had indeed figured out that runJob would allow me to do some of what I need now. It is why I filed issue #56 (I've patched the version I am using, of course).\n4. Patrick McFadin: Github comment from mateiz: TD implemented this with pull request 64 (https://github.com/mesos/spark/pull/64), so I'm closing this issue. It should work for text files, SequenceFiles, and other Hadoop output formats of your choice.\n5. Patrick McFadin: Imported from Github issue spark-6, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "be85065a3268f6fe6a2a921663a2b7bf", "issue_key": "SPARK-508", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Union operation on RDDs", "description": "It should be possible to \"union\" two RDDs with the same element type and run some operation (e.g. a reduce) on the combined dataset.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-06-18T09:10:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is mostly in already, except that the implementation is somewhat inefficient for large unions (it stores a chain of UnionRDD objects, rather than trying to coalesce them). Might want to fix that before closing this issue.", "created": "2010-09-28T23:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Closed in 0e2adecdabe4b1b8f5df21ec16cea182b72d5626.", "created": "2010-10-16T16:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-8, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-508\nSummary: Union operation on RDDs\nDescription: It should be possible to \"union\" two RDDs with the same element type and run some operation (e.g. a reduce) on the combined dataset.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: This is mostly in already, except that the implementation is somewhat inefficient for large unions (it stores a chain of UnionRDD objects, rather than trying to coalesce them). Might want to fix that before closing this issue.\n2. Patrick McFadin: Github comment from mateiz: Closed in 0e2adecdabe4b1b8f5df21ec16cea182b72d5626.\n3. Patrick McFadin: Imported from Github issue spark-8, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "100510f88c97877c9899950237f6915f", "issue_key": "SPARK-507", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add save() operation", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-06-27T10:41:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Duplicate of previous issue...", "created": "2010-06-27T10:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-9, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-507\nSummary: Add save() operation\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Duplicate of previous issue...\n2. Patrick McFadin: Imported from Github issue spark-9, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "57e0d5b7c44b3143be6fbb0cadf56838", "issue_key": "SPARK-506", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Load-balance tasks better across nodes", "description": "In the current scheduler, each node in a resource offer is filled up individually, rather than tasks being spread out. Therefore, some nodes might get many more tasks than others, increasing latency.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-06-27T10:42:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Justin fixed this in one of his scheduler commits.", "created": "2010-09-28T23:14:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-10, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-506\nSummary: Load-balance tasks better across nodes\nDescription: In the current scheduler, each node in a resource offer is filled up individually, rather than tasks being spread out. Therefore, some nodes might get many more tasks than others, increasing latency.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Justin fixed this in one of his scheduler commits.\n2. Patrick McFadin: Imported from Github issue spark-10, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "d6209d3d4db9117a3d12e39b6a7008b2", "issue_key": "SPARK-505", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Consider using a better build tool than make", "description": "Either Ant or SimpleBuildTool would be good. The latter is nicer but less likely to be installed on peoples' machines.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-06-27T14:00:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Closed by using SBT.", "created": "2011-02-10T12:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-11, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-505\nSummary: Consider using a better build tool than make\nDescription: Either Ant or SimpleBuildTool would be good. The latter is nicer but less likely to be installed on peoples' machines.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Closed by using SBT.\n2. Patrick McFadin: Imported from Github issue spark-11, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "0fbe7cccdd0bbb2fadb347343df36c0d", "issue_key": "SPARK-504", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add support for Hadoop InputFormats other than TextInputFormat", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-07-19T09:48:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "resolved": "2012-10-19T22:50:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Added support for generic Hadoop InputFormats and refactored textFile to use this. Closed by 74bbfa91c252150811ce9617424cdeea3711bdf9.", "created": "2010-10-16T18:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-12, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-504\nSummary: Add support for Hadoop InputFormats other than TextInputFormat\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Added support for generic Hadoop InputFormats and refactored textFile to use this. Closed by 74bbfa91c252150811ce9617424cdeea3711bdf9.\n2. Patrick McFadin: Imported from Github issue spark-12, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "894b8fa9527f76c7567b8cf2d5c5fa17", "issue_key": "SPARK-503", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make it possible to run spark-shell without a shared NFS", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-07-22T16:54:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Fixed now that the REPL uses HTTP to serve classes to executors (commit e5e9edeeb3612663b885643cccbc4aaeae8c00be merged this in).", "created": "2010-09-28T23:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-13, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-503\nSummary: Make it possible to run spark-shell without a shared NFS\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Fixed now that the REPL uses HTTP to serve classes to executors (commit e5e9edeeb3612663b885643cccbc4aaeae8c00be merged this in).\n2. Patrick McFadin: Imported from Github issue spark-13, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "cdd6dee4c2d5186008e558b6a55c56f9", "issue_key": "SPARK-502", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make per-task CPU and memory configurable", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-08-03T14:30:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Fixed in master in commit 28d6f231968a1ee7bf17c890fc7a4cc850ed55c7.", "created": "2010-10-15T15:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-14, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-502\nSummary: Make per-task CPU and memory configurable\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Fixed in master in commit 28d6f231968a1ee7bf17c890fc7a4cc850ed55c7.\n2. Patrick McFadin: Imported from Github issue spark-14, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "d2c6374d9eb58583dcfe5118a7aa7ada", "issue_key": "SPARK-501", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Don't keep trying to run tasks on a slave if they all fail", "description": "Justin ran into this on the R cluster when there was a misconfigured slave", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-08-05T13:26:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed now.", "created": "2012-06-09T13:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-15, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-501\nSummary: Don't keep trying to run tasks on a slave if they all fail\nDescription: Justin ran into this on the R cluster when there was a misconfigured slave\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is fixed now.\n2. Patrick McFadin: Imported from Github issue spark-15, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "88577c2a7df5a074a71e9fafe1e418e4", "issue_key": "SPARK-500", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add a single spark.shared.fs option to use for REPL classes, broadcast, etc", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-08-13T19:01:00.000+0000", "updated": "2012-10-22T14:55:30.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed now that the REPL uses HTTP to serve classes to the workers rather than relying on a file system -- this change means that spark.dfs is the only option used to specify a distributed file system to use. We could use a distributed FS for the REPL too if necessary, but won't do that yet (the generated classes will almost surely all be tiny).", "created": "2010-09-28T23:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-16, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-500\nSummary: Add a single spark.shared.fs option to use for REPL classes, broadcast, etc\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is fixed now that the REPL uses HTTP to serve classes to the workers rather than relying on a file system -- this change means that spark.dfs is the only option used to specify a distributed file system to use. We could use a distributed FS for the REPL too if necessary, but won't do that yet (the generated classes will almost surely all be tiny).\n2. Patrick McFadin: Imported from Github issue spark-16, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "cf875f76ee486ba330e3f2525b4a4adf", "issue_key": "SPARK-578", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fix interpreter code generation to only capture needed dependencies", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-08-16T13:02:00.000+0000", "updated": "2016-01-18T10:20:41.000+0000", "resolved": "2016-01-18T10:20:41.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-17, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matthew Farrellee", "body": "[~matei] is this related to slimming down he assembly?", "created": "2014-09-21T18:31:59.537+0000"}, {"author": "Sean R. Owen", "body": "This is so old that I think it's obsolete", "created": "2016-01-18T10:20:41.216+0000"}], "num_comments": 3, "text": "Issue: SPARK-578\nSummary: Fix interpreter code generation to only capture needed dependencies\n\nComments (3):\n1. Patrick McFadin: Imported from Github issue spark-17, originally reported by mateiz\n2. Matthew Farrellee: [~matei] is this related to slimming down he assembly?\n3. Sean R. Owen: This is so old that I think it's obsolete", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "7140d0cc0f3d73e395d07d6429cf651e", "issue_key": "SPARK-499", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Write up a Spark tutorial for wiki", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-09-01T22:46:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: I started a page on EC2 but I am blocked for now.", "created": "2010-10-08T18:51:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed now.", "created": "2012-06-09T13:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-18, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-499\nSummary: Write up a Spark tutorial for wiki\n\nComments (3):\n1. Patrick McFadin: Github comment from tjhunter: I started a page on EC2 but I am blocked for now.\n2. Patrick McFadin: Github comment from mateiz: This is fixed now.\n3. Patrick McFadin: Imported from Github issue spark-18, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "07c1825e3c532fb2c4010bd32f10988e", "issue_key": "SPARK-498", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make Spark use log4j / slf4j for logging", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-09-27T15:01:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This has been fixed in master.", "created": "2010-10-15T15:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-19, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-498\nSummary: Make Spark use log4j / slf4j for logging\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This has been fixed in master.\n2. Patrick McFadin: Imported from Github issue spark-19, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "8a946ca9c5adfb67fa9f55eab5f5e056", "issue_key": "SPARK-497", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Increase default locality wait from 1s to 3-5s", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-09-28T23:23:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Increase default locality wait to 3s. Closed by f50b23b825dfbaa7ad65a8c27db2cc7eb0b2ebfa.", "created": "2010-09-29T09:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-20, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-497\nSummary: Increase default locality wait from 1s to 3-5s\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Increase default locality wait to 3s. Closed by f50b23b825dfbaa7ad65a8c27db2cc7eb0b2ebfa.\n2. Patrick McFadin: Imported from Github issue spark-20, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.931783"}}
{"id": "5c8e4e405c5e2be3db141c2a2aef3848", "issue_key": "SPARK-496", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "java crashes when trying to run spark+EC2", "description": "Mesos and Spark compile fine, the EC2 computing nodes are launched without a problem, however when I want to run the following command: ./run SparkPi \"1@ec2-184-73-97-35.compute-1.amazonaws.com\" The program stops with the following error: java: ev.c:2210: ev_loop: Assertion `(\"libev: ev_loop recursion during release detected\", ((loop)->loop_done) != 0x80)' failed. Aborted Running the example locally has no issue: tjhunter@paris:~/mm/spark> ./run SparkPi \"local[10]\" 10/10/08 19:42:07 INFO SparkContext: Running 2 tasks in parallel 10/10/08 19:42:07 INFO LocalScheduler: Running task 0 10/10/08 19:42:07 INFO LocalScheduler: Running task 1 10/10/08 19:42:07 INFO LocalScheduler: Size of task 0 is 933 bytes 10/10/08 19:42:07 INFO LocalScheduler: Size of task 1 is 933 bytes 10/10/08 19:42:07 INFO ForeachTask: Processing spark.ParallelArraySplit@691 10/10/08 19:42:07 INFO ForeachTask: Processing spark.ParallelArraySplit@692 10/10/08 19:42:07 INFO LocalScheduler: Finished task 1 10/10/08 19:42:07 INFO LocalScheduler: Finished task 0 10/10/08 19:42:07 INFO SparkContext: Tasks finished in 0.1540033 s Pi is roughly 3.13712 Looking at the log of the mesos master, nothing happened. Do you have any advice? I can provide some logs and the specs of the machine.", "reporter": "tjhunter", "assignee": null, "created": "0010-10-08T18:44:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I think this might be because you didn't include the port in your address. That is, it should've been 1@ec2-184-73-97-35.compute-1.amazonaws.com:5050. This is a pretty horrible error message to get for that though. If you're using a relatively new version of the EC2 scripts, they will write the master URL in ~/mesos-ec2/cluster-url, so you can just cat that file to see the master.", "created": "2010-10-08T18:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: No difference :( I just tried to run cpp-test-framework from the mesos examples and it fails. I suggest we close this ticket and I reopen a new one on mesos.", "created": "2010-10-08T19:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I think this is because you're trying to submit jobs from your laptop to the EC2 cluster (see my email). Let me know if it also happens when you submit them from the cluster.", "created": "2010-10-08T19:34:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: Thanks Matei, I had a compilation problem with mesos (solved), I will try again later this week end. One point that is not clear to me is why I cannot send a spark job from my local computer to the cluster by passing the adress of the master. Is it a missing feature or simply a wrong way to use spark and mesos?", "created": "2010-10-08T19:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: It's just something we're not supporting yet. There are a few reasons for it: 1) For your computer to be able to submit jobs to the EC2 cluster, the cluster would need to have a port open to the outside world on which jobs can be submitted. This could be a security problem -- random people would be able to submit jobs to your EC2 cluster. Some kind of authentication is required to prevent this. 2) Spark also requires your worker nodes to be able to send messages to the master. This might require your computer to have a public IP address (i.e. a way to be accessible from behind a firewall). This will not be true for most users. 3) Right now, different versions (builds) of Mesos will not play well with each other. If you happen to have a different version on your machine than the one on the Amazon image, then the Spark built against your local version may not be able to talk to the one on EC2. This will be fixed later but it's not yet fixed. 4) In general the latency between your machine and EC2 might make it unattractive to run a job there. (I.e. the job will perform better if the master and workers are close). For now, the recommended approach is to log into your master EC2 node, check out any code you want, and build it and run it from there. With EBS-backed clusters, you can have your master, or your entire cluster, persist so that you can restart it later. I can show you how to use that in person sometime.", "created": "2010-10-08T19:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I'm going to close this issue now because it isn't something we're aiming to support at the moment.", "created": "2010-10-15T15:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-21, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-496\nSummary: java crashes when trying to run spark+EC2\nDescription: Mesos and Spark compile fine, the EC2 computing nodes are launched without a problem, however when I want to run the following command: ./run SparkPi \"1@ec2-184-73-97-35.compute-1.amazonaws.com\" The program stops with the following error: java: ev.c:2210: ev_loop: Assertion `(\"libev: ev_loop recursion during release detected\", ((loop)->loop_done) != 0x80)' failed. Aborted Running the example locally has no issue: tjhunter@paris:~/mm/spark> ./run SparkPi \"local[10]\" 10/10/08 19:42:07 INFO SparkContext: Running 2 tasks in parallel 10/10/08 19:42:07 INFO LocalScheduler: Running task 0 10/10/08 19:42:07 INFO LocalScheduler: Running task 1 10/10/08 19:42:07 INFO LocalScheduler: Size of task 0 is 933 bytes 10/10/08 19:42:07 INFO LocalScheduler: Size of task 1 is 933 bytes 10/10/08 19:42:07 INFO ForeachTask: Processing spark.ParallelArraySplit@691 10/10/08 19:42:07 INFO ForeachTask: Processing spark.ParallelArraySplit@692 10/10/08 19:42:07 INFO LocalScheduler: Finished task 1 10/10/08 19:42:07 INFO LocalScheduler: Finished task 0 10/10/08 19:42:07 INFO SparkContext: Tasks finished in 0.1540033 s Pi is roughly 3.13712 Looking at the log of the mesos master, nothing happened. Do you have any advice? I can provide some logs and the specs of the machine.\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: I think this might be because you didn't include the port in your address. That is, it should've been 1@ec2-184-73-97-35.compute-1.amazonaws.com:5050. This is a pretty horrible error message to get for that though. If you're using a relatively new version of the EC2 scripts, they will write the master URL in ~/mesos-ec2/cluster-url, so you can just cat that file to see the master.\n2. Patrick McFadin: Github comment from tjhunter: No difference :( I just tried to run cpp-test-framework from the mesos examples and it fails. I suggest we close this ticket and I reopen a new one on mesos.\n3. Patrick McFadin: Github comment from mateiz: I think this is because you're trying to submit jobs from your laptop to the EC2 cluster (see my email). Let me know if it also happens when you submit them from the cluster.\n4. Patrick McFadin: Github comment from tjhunter: Thanks Matei, I had a compilation problem with mesos (solved), I will try again later this week end. One point that is not clear to me is why I cannot send a spark job from my local computer to the cluster by passing the adress of the master. Is it a missing feature or simply a wrong way to use spark and mesos?\n5. Patrick McFadin: Github comment from mateiz: It's just something we're not supporting yet. There are a few reasons for it: 1) For your computer to be able to submit jobs to the EC2 cluster, the cluster would need to have a port open to the outside world on which jobs can be submitted. This could be a security problem -- random people would be able to submit jobs to your EC2 cluster. Some kind of authentication is required to prevent this. 2) Spark also requires your worker nodes to be able to send messages to the master. This might require your computer to have a public IP address (i.e. a way to be accessible from behind a firewall). This will not be true for most users. 3) Right now, different versions (builds) of Mesos will not play well with each other. If you happen to have a different version on your machine than the one on the Amazon image, then the Spark built against your local version may not be able to talk to the one on EC2. This will be fixed later but it's not yet fixed. 4) In general the latency between your machine and EC2 might make it unattractive to run a job there. (I.e. the job will perform better if the master and workers are close). For now, the recommended approach is to log into your master EC2 node, check out any code you want, and build it and run it from there. With EBS-backed clusters, you can have your master, or your entire cluster, persist so that you can restart it later. I can show you how to use that in person sometime.\n6. Patrick McFadin: Github comment from mateiz: I'm going to close this issue now because it isn't something we're aiming to support at the moment.\n7. Patrick McFadin: Imported from Github issue spark-21, originally reported by tjhunter", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "d4d6ccd5541bc6ef4c3b48f7ccca6a61", "issue_key": "SPARK-495", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark/Mesos crashes after completion of jobs", "description": "It used to work before I came back. So I am not really sure whether its due to the new version of Mesos, or Scala 2.8.0, or something else that changed in Spark itself. The following error message shows up in the console. <pre> ====================ERROR_MESSAGE==================== # # A fatal error has been detected by the Java Runtime Environment: # # SIGSEGV (0xb) at pc=0xb3fc70b0, pid=18408, tid=3029719920 # # JRE version: 6.0_20-b02 # Java VM: Java HotSpot(TM) Client VM (16.3-b01 mixed mode, sharing linux-x86 ) # Problematic frame: # C [libmesos.so+0x440b0] _ZNSt8_Rb_treeISsSt4pairIKSsSsESt10_Select1stIS2_ESt4lessISsESaIS2_EE8_M_eraseEPSt13_Rb_tree_nodeIS2_E+0x20 # # An error report file with more information is saved as: # /home/mosharaf/Work/spark/hs_err_pid18408.log # # If you would like to submit a bug report, please visit: # http://java.sun.com/webapps/bugreport/crash.jsp # The crash happened outside the Java Virtual Machine in native code. # See problematic frame for where to report the bug. # Aborted ====================ERROR_MESSAGE==================== </pre> Note that the problematic frame in most cases is something related to libmesos* (like this one), but it can be libstd* as well.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-10-12T14:51:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mosharaf: The problem has propagated to all the branches out of multi-tracker. Most probably it has something to do with threads that just hang around (to timeout after a default 60s) even after the job is over.", "created": "2010-10-28T10:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mosharaf: This has not yet been observed in Mesos on EC2. Most probably it is specific to Mesos on local machine.", "created": "2010-11-01T11:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Try updating Mesos.. it might be a bug that was fixed in there in the past. Also check whether there are any upgrades for your JVM. Some old versions on Ubuntu apparently crash with this type of error pretty often.", "created": "2010-11-08T23:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mosharaf: This actually happens when I am doing a small broadcast. If the broadcast takes long enough (say >20s) then it does not crash.", "created": "2010-11-08T23:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mosharaf: Similar things happen during LocalFileShuffle in local VM if there are more than 2 mesos slaves. Now I think all these are due to insufficient memory in my VM.", "created": "2010-12-17T18:47:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Seems like this was a memory problem, closing the issue.", "created": "2011-05-27T09:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-22, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-495\nSummary: Spark/Mesos crashes after completion of jobs\nDescription: It used to work before I came back. So I am not really sure whether its due to the new version of Mesos, or Scala 2.8.0, or something else that changed in Spark itself. The following error message shows up in the console. <pre> ====================ERROR_MESSAGE==================== # # A fatal error has been detected by the Java Runtime Environment: # # SIGSEGV (0xb) at pc=0xb3fc70b0, pid=18408, tid=3029719920 # # JRE version: 6.0_20-b02 # Java VM: Java HotSpot(TM) Client VM (16.3-b01 mixed mode, sharing linux-x86 ) # Problematic frame: # C [libmesos.so+0x440b0] _ZNSt8_Rb_treeISsSt4pairIKSsSsESt10_Select1stIS2_ESt4lessISsESaIS2_EE8_M_eraseEPSt13_Rb_tree_nodeIS2_E+0x20 # # An error report file with more information is saved as: # /home/mosharaf/Work/spark/hs_err_pid18408.log # # If you would like to submit a bug report, please visit: # http://java.sun.com/webapps/bugreport/crash.jsp # The crash happened outside the Java Virtual Machine in native code. # See problematic frame for where to report the bug. # Aborted ====================ERROR_MESSAGE==================== </pre> Note that the problematic frame in most cases is something related to libmesos* (like this one), but it can be libstd* as well.\n\nComments (7):\n1. Patrick McFadin: Github comment from mosharaf: The problem has propagated to all the branches out of multi-tracker. Most probably it has something to do with threads that just hang around (to timeout after a default 60s) even after the job is over.\n2. Patrick McFadin: Github comment from mosharaf: This has not yet been observed in Mesos on EC2. Most probably it is specific to Mesos on local machine.\n3. Patrick McFadin: Github comment from mateiz: Try updating Mesos.. it might be a bug that was fixed in there in the past. Also check whether there are any upgrades for your JVM. Some old versions on Ubuntu apparently crash with this type of error pretty often.\n4. Patrick McFadin: Github comment from mosharaf: This actually happens when I am doing a small broadcast. If the broadcast takes long enough (say >20s) then it does not crash.\n5. Patrick McFadin: Github comment from mosharaf: Similar things happen during LocalFileShuffle in local VM if there are more than 2 mesos slaves. Now I think all these are due to insufficient memory in my VM.\n6. Patrick McFadin: Github comment from mateiz: Seems like this was a memory problem, closing the issue.\n7. Patrick McFadin: Imported from Github issue spark-22, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "ede7a9574154ffd7b3ca39115c87641f", "issue_key": "SPARK-577", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Rename Split to Partition", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "0010-10-16T16:10:00.000+0000", "updated": "2014-02-03T07:18:55.000+0000", "resolved": "2013-04-05T20:25:08.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-23, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "This was done in 0.7, by https://github.com/mesos/spark/commit/06e5e6627f3856b5c6e3e60cbb167044de9ef6d4", "created": "2013-04-05T20:25:08.343+0000"}, {"author": "Stevo Slavić", "body": "\"split\" is still there, e.g. in RDD class' scaladoc and parameter names. (spark 0.9.0)", "created": "2014-02-03T07:18:39.725+0000"}], "num_comments": 3, "text": "Issue: SPARK-577\nSummary: Rename Split to Partition\n\nComments (3):\n1. Patrick McFadin: Imported from Github issue spark-23, originally reported by mateiz\n2. Josh Rosen: This was done in 0.7, by https://github.com/mesos/spark/commit/06e5e6627f3856b5c6e3e60cbb167044de9ef6d4\n3. Stevo Slavić: \"split\" is still there, e.g. in RDD class' scaladoc and parameter names. (spark 0.9.0)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "0c917387e1731f811723e5db3b54894f", "issue_key": "SPARK-576", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Design and develop a more precise progress estimator", "description": "In addition to <task_completed>/<total_tasks>, we need to have something that says <estimated_time_remaining>.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-10-17T12:06:00.000+0000", "updated": "2014-10-20T18:34:06.000+0000", "resolved": "2014-10-20T18:34:06.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-24, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Dev Lakhani", "body": "I've created a PR for this: https://github.com/apache/spark/pull/2837/", "created": "2014-10-17T12:51:28.204+0000"}, {"author": "Josh Rosen", "body": "Closing this as \"Won't Fix\"; see our discussion at https://github.com/apache/spark/pull/2837.  I'm not sure whether this is a good idea - since estimates like this are likely to be very inaccurate due to the presence of stragglers in most jobs. I think it's better to just give people a slider as we have now. If someone sees an estimate but we are off by more than 100%, it could end up frustrating users.", "created": "2014-10-20T18:34:06.180+0000"}], "num_comments": 3, "text": "Issue: SPARK-576\nSummary: Design and develop a more precise progress estimator\nDescription: In addition to <task_completed>/<total_tasks>, we need to have something that says <estimated_time_remaining>.\n\nComments (3):\n1. Patrick McFadin: Imported from Github issue spark-24, originally reported by mosharaf\n2. Dev Lakhani: I've created a PR for this: https://github.com/apache/spark/pull/2837/\n3. Josh Rosen: Closing this as \"Won't Fix\"; see our discussion at https://github.com/apache/spark/pull/2837.  I'm not sure whether this is a good idea - since estimates like this are likely to be very inaccurate due to the presence of stragglers in most jobs. I think it's better to just give people a slider as we have now. If someone sees an estimate but we are off by more than 100%, it could end up frustrating users.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "1ab0af7ba9130294f6c56963d0d2e97f", "issue_key": "SPARK-494", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Print milliseconds in the log timestamps", "description": "", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-10-18T09:59:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: You can get this by editing conf/log4j.properties. Lets not commit it as the default though because it can get annoying.", "created": "2010-10-18T12:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-25, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-494\nSummary: Print milliseconds in the log timestamps\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: You can get this by editing conf/log4j.properties. Lets not commit it as the default though because it can get annoying.\n2. Patrick McFadin: Imported from Github issue spark-25, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "7f9a37bf3d0bdf3172e718867c4f968f", "issue_key": "SPARK-493", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Organize broadcast implementations", "description": "Everything related to Broadcast right now is in the Broadcast.scala file, which is rapidly growing. We should create a separate broadcast directory and have separate source files for different implementations and shared classes. Also we need to put them inside spark.broadcast package (similar to spark.repl)", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-10-18T14:06:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-26, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-493\nSummary: Organize broadcast implementations\nDescription: Everything related to Broadcast right now is in the Broadcast.scala file, which is rapidly growing. We should create a separate broadcast directory and have separate source files for different implementations and shared classes. Also we need to put them inside spark.broadcast package (similar to spark.repl)\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-26, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "c446b700305ada9372fc15191b51fc19", "issue_key": "SPARK-575", "issue_type": "Improvement", "status": "Closed", "priority": null, "resolution": null, "summary": "Maintain a cache of JARs on each node to avoid unnecessary copying", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-10-18T17:27:00.000+0000", "updated": "2015-02-12T07:25:07.000+0000", "resolved": "2014-09-21T18:30:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-27, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "What was the actual issue here? I'm pretty sure that JARs are only downloaded once per machine during the lifetime of a SparkContext, so was the idea to maintain a cache of JARs that would be re-used across contexts? That could become non-trivial if different jobs use different versions of JAR dependencies that have the same filename (but this could be addressed by tracking files by SHA sums, for example).", "created": "2013-04-26T09:40:33.980+0000"}, {"author": "Matthew Farrellee", "body": "[~joshrosen] is quite correct. this issue looks inactive. i'm going to close it out, but as always feel free to re-open. i can think of a few ways this could be done, and not all need spark code to be changed.", "created": "2014-09-21T18:30:06.228+0000"}, {"author": "Apache Spark", "body": "User 'mengxr' has created a pull request for this issue: https://github.com/apache/spark/pull/4555", "created": "2015-02-12T07:25:07.399+0000"}], "num_comments": 4, "text": "Issue: SPARK-575\nSummary: Maintain a cache of JARs on each node to avoid unnecessary copying\n\nComments (4):\n1. Patrick McFadin: Imported from Github issue spark-27, originally reported by mateiz\n2. Josh Rosen: What was the actual issue here? I'm pretty sure that JARs are only downloaded once per machine during the lifetime of a SparkContext, so was the idea to maintain a cache of JARs that would be re-used across contexts? That could become non-trivial if different jobs use different versions of JAR dependencies that have the same filename (but this could be addressed by tracking files by SHA sums, for example).\n3. Matthew Farrellee: [~joshrosen] is quite correct. this issue looks inactive. i'm going to close it out, but as always feel free to re-open. i can think of a few ways this could be done, and not all need spark code to be changed.\n4. Apache Spark: User 'mengxr' has created a pull request for this issue: https://github.com/apache/spark/pull/4555", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "8cbe135efe006a8832ebe6369f32e7a7", "issue_key": "SPARK-574", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Put license notice and copyrights headers to all files", "description": "There is no license indication right now, and this may cause some potential issues regarding copyright and licensing rights of the code in the future. Also, what is the license? Apache? BSD? LGPL? Public domain? GPL?", "reporter": "tjhunter", "assignee": "Matei Alexandru Zaharia", "created": "0010-10-19T14:29:00.000+0000", "updated": "2013-09-01T15:24:45.000+0000", "resolved": "2013-09-01T15:24:45.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-28, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "0.8 adds Apache license headers", "created": "2013-09-01T15:24:45.900+0000"}], "num_comments": 2, "text": "Issue: SPARK-574\nSummary: Put license notice and copyrights headers to all files\nDescription: There is no license indication right now, and this may cause some potential issues regarding copyright and licensing rights of the code in the future. Also, what is the license? Apache? BSD? LGPL? Public domain? GPL?\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-28, originally reported by tjhunter\n2. Josh Rosen: 0.8 adds Apache license headers", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "3747eee9325fe9a46887b50fee3a1395", "issue_key": "SPARK-573", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Clarify semantics of the parallelized closures", "description": "I do not think there is any guideline about which features of scala are allowed/forbidden in the closure that gets sent to the remote nodes. Two examples I have are a return statement and updating mutable variables of singletons. Ideally, a compiler plugin could give an error at compile time, but a good error message at run time would be good also. Are there any other cases that should not be allowed?", "reporter": "tjhunter", "assignee": null, "created": "0010-10-28T09:56:00.000+0000", "updated": "2016-01-08T14:11:54.000+0000", "resolved": "2016-01-08T14:11:54.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-29, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-573\nSummary: Clarify semantics of the parallelized closures\nDescription: I do not think there is any guideline about which features of scala are allowed/forbidden in the closure that gets sent to the remote nodes. Two examples I have are a return statement and updating mutable variables of singletons. Ideally, a compiler plugin could give an error at compile time, but a good error message at run time would be good also. Are there any other cases that should not be allowed?\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-29, originally reported by tjhunter", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "f55c07c2a36d32d8a351a3c3f43872d1", "issue_key": "SPARK-572", "issue_type": "Improvement", "status": "Closed", "priority": null, "resolution": null, "summary": "Forbid update of static mutable variables", "description": "Consider the following piece of code:  object Foo { var xx = -1 def main() { xx = 1 val sc = new SparkContext(...) sc.broadcast(xx) sc.parallelize(0 to 10).map(i=>{ ... xx ...}) } }  Can you guess the value of xx? It is 1 when you use the local scheduler and -1 when you use the mesos scheduler. Given the complications, it should probably just be forbidden for now...", "reporter": "tjhunter", "assignee": null, "created": "0010-10-28T10:00:00.000+0000", "updated": "2015-07-29T11:55:41.000+0000", "resolved": "2014-11-11T08:29:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: Spark can't really tell what you are updating outside the closures. I think it is more important to document this behavior for global, static variables.", "created": "2012-05-02T12:09:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: +1 for documentation in spark I still think the best solution would be a compiler plugin that would detect a few dangerous patterns like these. With the new macro system coming in scala 2.1x, that should not be too hard.", "created": "2012-05-02T12:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-30, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Andrew Ash", "body": "Static mutable variables are now a standard way of having code run on a per-executor basis. To run per-entry, you can use map(), for per-partition you can use mapPartitions(), but for per-executor you need static variables or initializers. If for example you want to open a connection to another data storage system and write all of an executor's data into that system, a static connection object is the common way to do that. I would propose closing this ticket as \"Won't Fix\". Using this technique is confusing, but prohibiting it is difficult and introduces additional roadblocks to Spark power users. cc [~rxin]", "created": "2014-11-11T08:27:52.212+0000"}, {"author": "Reynold Xin", "body": "Closing this as won't fix since it is very hard to enforce and we do \"abuse\" it to run stateful computation.", "created": "2014-11-11T08:29:31.970+0000"}], "num_comments": 5, "text": "Issue: SPARK-572\nSummary: Forbid update of static mutable variables\nDescription: Consider the following piece of code:  object Foo { var xx = -1 def main() { xx = 1 val sc = new SparkContext(...) sc.broadcast(xx) sc.parallelize(0 to 10).map(i=>{ ... xx ...}) } }  Can you guess the value of xx? It is 1 when you use the local scheduler and -1 when you use the mesos scheduler. Given the complications, it should probably just be forbidden for now...\n\nComments (5):\n1. Patrick McFadin: Github comment from rxin: Spark can't really tell what you are updating outside the closures. I think it is more important to document this behavior for global, static variables.\n2. Patrick McFadin: Github comment from tjhunter: +1 for documentation in spark I still think the best solution would be a compiler plugin that would detect a few dangerous patterns like these. With the new macro system coming in scala 2.1x, that should not be too hard.\n3. Patrick McFadin: Imported from Github issue spark-30, originally reported by tjhunter\n4. Andrew Ash: Static mutable variables are now a standard way of having code run on a per-executor basis. To run per-entry, you can use map(), for per-partition you can use mapPartitions(), but for per-executor you need static variables or initializers. If for example you want to open a connection to another data storage system and write all of an executor's data into that system, a static connection object is the common way to do that. I would propose closing this ticket as \"Won't Fix\". Using this technique is confusing, but prohibiting it is difficult and introduces additional roadblocks to Spark power users. cc [~rxin]\n5. Reynold Xin: Closing this as won't fix since it is very hard to enforce and we do \"abuse\" it to run stateful computation.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "865797105fd65451d565852148820ea9", "issue_key": "SPARK-571", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Forbid return statements when cleaning closures", "description": "By mistake, I wrote some code like this:  object Foo { def main() { val sc = new SparkContext(...) sc.parallelize(0 to 10,10).map({ ... return 1 ... }).collect } }  This compiles fine and actually runs using the local scheduler. However, using the mesos scheduler throws a NotSerializableException in the CollectTask . I agree the result of the program above should be undefined or it should be an error. Would it be possible to have more explicit messages?", "reporter": "tjhunter", "assignee": "William Benton", "created": "0010-10-28T10:07:00.000+0000", "updated": "2014-05-13T20:45:32.000+0000", "resolved": "2014-05-13T20:45:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-31, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "William Benton", "body": "I have a patch for this and will submit a PR later today; can someone please assign this to me?", "created": "2014-05-09T21:11:20.298+0000"}], "num_comments": 2, "text": "Issue: SPARK-571\nSummary: Forbid return statements when cleaning closures\nDescription: By mistake, I wrote some code like this:  object Foo { def main() { val sc = new SparkContext(...) sc.parallelize(0 to 10,10).map({ ... return 1 ... }).collect } }  This compiles fine and actually runs using the local scheduler. However, using the mesos scheduler throws a NotSerializableException in the CollectTask . I agree the result of the program above should be undefined or it should be an error. Would it be possible to have more explicit messages?\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-31, originally reported by tjhunter\n2. William Benton: I have a patch for this and will submit a PR later today; can someone please assign this to me?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.933789"}}
{"id": "663090c11ff0983dce939d92e5b38ba3", "issue_key": "SPARK-570", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Easy way to set remote node memory", "description": "By default, spark accepts a number of java arguments (like for the repl, etc) For specifying the memory on the remote nodes, maybe some options like: spark.remote.Xmx spark.remote.Xms would seem more intuitive than SPARK_MEM?", "reporter": "tjhunter", "assignee": null, "created": "0010-10-28T10:15:00.000+0000", "updated": "2013-12-07T14:54:11.000+0000", "resolved": "2013-12-07T14:54:11.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-32, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "This is addressed by the {{spark.executor.memoy}} system property.", "created": "2013-12-07T14:54:11.868+0000"}], "num_comments": 2, "text": "Issue: SPARK-570\nSummary: Easy way to set remote node memory\nDescription: By default, spark accepts a number of java arguments (like for the repl, etc) For specifying the memory on the remote nodes, maybe some options like: spark.remote.Xmx spark.remote.Xms would seem more intuitive than SPARK_MEM?\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-32, originally reported by tjhunter\n2. Josh Rosen: This is addressed by the {{spark.executor.memoy}} system property.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "ec7a094ac1add718a2b917a2a0035df2", "issue_key": "SPARK-492", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add unit tests for shuffle operations", "description": "Need to add these in the shuffle branch.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-11-08T00:48:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Unit tests for shuffle operations. Closed by d0a99665551ed6ecbe29583983198e27beb7200e.", "created": "2010-11-12T16:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-33, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-492\nSummary: Add unit tests for shuffle operations\nDescription: Need to add these in the shuffle branch.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Unit tests for shuffle operations. Closed by d0a99665551ed6ecbe29583983198e27beb7200e.\n2. Patrick McFadin: Imported from Github issue spark-33, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "24bd6b1f7302da4a26f802660e542c81", "issue_key": "SPARK-491", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support other serialization mechanisms than Java Serialization", "description": "Currently looking at Kryo (http://code.google.com/p/kryo/).", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-11-08T23:55:00.000+0000", "updated": "2014-11-10T17:44:48.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Done in master.", "created": "2011-05-27T09:23:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-34, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Jeff Hammerbacher", "body": "[~matei] I can't track down the original GitHub issue with commentary where Kryo was first used in Spark. I'm trying to understand why it was perceived to be a good idea, and what advantages it has over other solutions. If you can point me to where the old GitHub issue can be found, I'd love to read through the discussion on the original pull request. Thanks!", "created": "2014-05-04T23:15:27.203+0000"}], "num_comments": 3, "text": "Issue: SPARK-491\nSummary: Support other serialization mechanisms than Java Serialization\nDescription: Currently looking at Kryo (http://code.google.com/p/kryo/).\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Done in master.\n2. Patrick McFadin: Imported from Github issue spark-34, originally reported by mateiz\n3. Jeff Hammerbacher: [~matei] I can't track down the original GitHub issue with commentary where Kryo was first used in Spark. I'm trying to understand why it was perceived to be a good idea, and what advantages it has over other solutions. If you can point me to where the old GitHub issue can be found, I'd love to read through the discussion on the original pull request. Thanks!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "faf4b160531eb35c52419157ee4bf1fc", "issue_key": "SPARK-490", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Clean up temporary files after LocalFileShuffle", "description": "Otherwise, disks run out of space and tasks start to fail resulting in job failure.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-11-28T11:29:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed now.", "created": "2012-06-09T13:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-35, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-490\nSummary: Clean up temporary files after LocalFileShuffle\nDescription: Otherwise, disks run out of space and tasks start to fail resulting in job failure.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is fixed now.\n2. Patrick McFadin: Imported from Github issue spark-35, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "c326288cb1a5e9cfac1a4a6bf58d4f88", "issue_key": "SPARK-489", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Pluggable broadcast implementations", "description": "Summary ======= Three different implementations of broadcast that pluggable through the config option \"spark.broadcast.Factory\": 1. DfsBroadcast: * Write to DFS from master and read back from everywhere else 2. ChainedBroadcast: * Divide the broadcast variable into blocks and form a O(n)-length chain from the master. * Can be and should be generalized to any d-ary tree instead of an 1-ary tree. 3. BitTorrentBroadcast: Stripped-down bit torrent with the master as the seed * Very fast and fairly distributed (upload:download=~1:~1) distribution * Special case with only a few leechers have been resolved Notes ====== * Code files as well broadcast related config files should be restructured * There are still lots of logging going on in Chained and BitTorrent implementations. Should be turned OFF at some point. * conf/* files are included in the 'mos-bt' branch. Should be ignored/removed before committing the merge with 'master'", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-11-30T19:01:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks pretty good overall, but I noticed a few small things: - Can you remove the conf files (git rm) to make the merge happen more easily? You're free to copy them and leave them around, just don't have them be under git's control. - I noticed that you changed the default shuffle implementation in RDD.scala (see the \"files changed\" view). Change it back to DfsShuffle.", "created": "2010-12-07T09:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mosharaf: Done. Couple of things about the config files: before using Streaming or BitTorrent, one must update spark.broadcast.MasterHostAddress with Master's IP address and will want to update spark.broadcast.BlockSize to their liking.", "created": "2010-12-07T10:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Noticed a few other things: - Why is sendBroadcast part of the public interface of trait Broadcast? It's only called internally, so it doesn't need to be in the Broadcast trait. - The default broadcast implementation should be the HDFS one, not the BitTorrent one, until we get some more testing of BT. This is especially true if BT requires you to configure those variables. - When initializing BitTorrent broadcast on the master, figure out your local IP address using code such as http://www.exampledepot.com/egs/java.net/Local.html and set the spark.broadcast.MasterHostAddress system property to that. It will automatically be transferred to the slaves when they get launched. This way the user doesn't need to configure it manually. - Your naming of system properties is a bit inconsistent with existing ones. You capitalize the first letter of the last word (e.g. spark.broadcast.BlockSize), whereas all the existing properties don't (e.g. spark.broadcast.blockSize). It would be good to uncapitalize those.", "created": "2010-12-13T10:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mosharaf: Done. Let me know if you find anything else.", "created": "2010-12-13T14:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-36, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-489\nSummary: Pluggable broadcast implementations\nDescription: Summary ======= Three different implementations of broadcast that pluggable through the config option \"spark.broadcast.Factory\": 1. DfsBroadcast: * Write to DFS from master and read back from everywhere else 2. ChainedBroadcast: * Divide the broadcast variable into blocks and form a O(n)-length chain from the master. * Can be and should be generalized to any d-ary tree instead of an 1-ary tree. 3. BitTorrentBroadcast: Stripped-down bit torrent with the master as the seed * Very fast and fairly distributed (upload:download=~1:~1) distribution * Special case with only a few leechers have been resolved Notes ====== * Code files as well broadcast related config files should be restructured * There are still lots of logging going on in Chained and BitTorrent implementations. Should be turned OFF at some point. * conf/* files are included in the 'mos-bt' branch. Should be ignored/removed before committing the merge with 'master'\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: This looks pretty good overall, but I noticed a few small things: - Can you remove the conf files (git rm) to make the merge happen more easily? You're free to copy them and leave them around, just don't have them be under git's control. - I noticed that you changed the default shuffle implementation in RDD.scala (see the \"files changed\" view). Change it back to DfsShuffle.\n2. Patrick McFadin: Github comment from mosharaf: Done. Couple of things about the config files: before using Streaming or BitTorrent, one must update spark.broadcast.MasterHostAddress with Master's IP address and will want to update spark.broadcast.BlockSize to their liking.\n3. Patrick McFadin: Github comment from mateiz: Noticed a few other things: - Why is sendBroadcast part of the public interface of trait Broadcast? It's only called internally, so it doesn't need to be in the Broadcast trait. - The default broadcast implementation should be the HDFS one, not the BitTorrent one, until we get some more testing of BT. This is especially true if BT requires you to configure those variables. - When initializing BitTorrent broadcast on the master, figure out your local IP address using code such as http://www.exampledepot.com/egs/java.net/Local.html and set the spark.broadcast.MasterHostAddress system property to that. It will automatically be transferred to the slaves when they get launched. This way the user doesn't need to configure it manually. - Your naming of system properties is a bit inconsistent with existing ones. You capitalize the first letter of the last word (e.g. spark.broadcast.BlockSize), whereas all the existing properties don't (e.g. spark.broadcast.blockSize). It would be good to uncapitalize those.\n4. Patrick McFadin: Github comment from mosharaf: Done. Let me know if you find anything else.\n5. Patrick McFadin: Imported from Github issue spark-36, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "41777f509956bcbc0fe21994c21e6fe0", "issue_key": "SPARK-569", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Consolidate/reorganize configuration options", "description": "Organize broadcast, shuffle, cache, and general Spark configuration options into separate files.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-12-01T10:54:00.000+0000", "updated": "2013-12-11T09:32:03.000+0000", "resolved": "2013-12-11T09:32:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-37, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Duplicates SPARK-544", "created": "2013-12-11T09:32:03.628+0000"}], "num_comments": 2, "text": "Issue: SPARK-569\nSummary: Consolidate/reorganize configuration options\nDescription: Organize broadcast, shuffle, cache, and general Spark configuration options into separate files.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-37, originally reported by mosharaf\n2. Patrick McFadin: Duplicates SPARK-544", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "5ffd432ce8b5de54f7e960db4028e5c6", "issue_key": "SPARK-568", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Document configuration options", "description": "Way too many configuration options already. Even though, in most cases, there names are descriptive enough, things can and will go out of control.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-12-01T10:55:00.000+0000", "updated": "2021-06-15T07:07:05.000+0000", "resolved": "2013-12-07T14:56:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: +1 . A list of all the command line variables / options would be appreciated.", "created": "2011-04-26T10:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-38, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "We now have docs for the configuration options: https://spark.incubator.apache.org/docs/latest/configuration.html", "created": "2013-12-07T14:56:20.150+0000"}], "num_comments": 3, "text": "Issue: SPARK-568\nSummary: Document configuration options\nDescription: Way too many configuration options already. Even though, in most cases, there names are descriptive enough, things can and will go out of control.\n\nComments (3):\n1. Patrick McFadin: Github comment from tjhunter: +1 . A list of all the command line variables / options would be appreciated.\n2. Patrick McFadin: Imported from Github issue spark-38, originally reported by mosharaf\n3. Josh Rosen: We now have docs for the configuration options: https://spark.incubator.apache.org/docs/latest/configuration.html", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "5e7b7585874e7741ee9acc8e090b73b1", "issue_key": "SPARK-488", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Replacing the native lzf compression code with ning's lzf library", "description": "Hi Matei (and others), I replaced the native code with ning's apache 2.0 compression library. This way, spark doesn't have to maintain the LZF code, lzf compression can be used in places (like corporate environments) where native code is a no-go for deployment issues, and the JNI performance penalty doesn't have to be paid. Code compiles and tests pass. - Josh", "reporter": "Joshua Hartman", "assignee": null, "created": "0010-12-05T21:31:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks great, but I have one minor request: Can you include the license for Ning's LZF library in its directory in third_party, just for future reference?", "created": "2010-12-06T18:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from jhartman: There you are. I'll merge this into the sbt branch soon as well.", "created": "2010-12-07T08:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Josh. I've merged this into master. Do you want to add another pull request for sbt or should I merge it myself?", "created": "2010-12-07T09:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from jhartman: I'll send a pull request for sbt. I'm not sure when it'll be ready - I really haven't started on it yet. By the way, it looks like Spark doesn't have an actual license. What is the license(s) you plan on using? - Josh", "created": "2010-12-07T10:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I'm going to put it under the BSD license. I'll add that soon. In the long term, after more core functionality is done, I'm also thinking of putting Spark in the Apache Incubator. (All the projects we work on at Berkeley get released under BSD or BSD-like licenses as part of our agreement with our industry sponsors.)", "created": "2010-12-07T10:31:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-39, originally reported by jhartman", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-488\nSummary: Replacing the native lzf compression code with ning's lzf library\nDescription: Hi Matei (and others), I replaced the native code with ning's apache 2.0 compression library. This way, spark doesn't have to maintain the LZF code, lzf compression can be used in places (like corporate environments) where native code is a no-go for deployment issues, and the JNI performance penalty doesn't have to be paid. Code compiles and tests pass. - Josh\n\nComments (6):\n1. Patrick McFadin: Github comment from mateiz: This looks great, but I have one minor request: Can you include the license for Ning's LZF library in its directory in third_party, just for future reference?\n2. Patrick McFadin: Github comment from jhartman: There you are. I'll merge this into the sbt branch soon as well.\n3. Patrick McFadin: Github comment from mateiz: Thanks Josh. I've merged this into master. Do you want to add another pull request for sbt or should I merge it myself?\n4. Patrick McFadin: Github comment from jhartman: I'll send a pull request for sbt. I'm not sure when it'll be ready - I really haven't started on it yet. By the way, it looks like Spark doesn't have an actual license. What is the license(s) you plan on using? - Josh\n5. Patrick McFadin: Github comment from mateiz: I'm going to put it under the BSD license. I'll add that soon. In the long term, after more core functionality is done, I'm also thinking of putting Spark in the Apache Incubator. (All the projects we work on at Berkeley get released under BSD or BSD-like licenses as part of our agreement with our industry sponsors.)\n6. Patrick McFadin: Imported from Github issue spark-39, originally reported by jhartman", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "601b2ce967b1747c56ff588776c3b8ad", "issue_key": "SPARK-487", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Serialization issue", "description": "The following code fails at deserialization: import spark.Utils @serializable class XXX val xx=new XXX val cc=Utils.serialize(xx) val xx2=Utils.deserialize[Any](cc) Reported error is below. In particular, it causes an exception to be thrown when the simple job class gets some results back to the master. I am looking at a workaround using my own class loader. java.lang.ClassNotFoundException: XXX at java.net.URLClassLoader$1.run(URLClassLoader.java:217) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:205) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$findClass(ScalaClassLoader.scala:86) at scala.tools.nsc.util.ScalaClassLoader$class.findClass(ScalaClassLoader.scala:51) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.findClass(ScalaClassLoader.scala:86) at java.lang.ClassLoader.loadClass(ClassLoader.java:321) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$loadClass(ScalaClassLoader.scala:86) at scala.tools.nsc.util.ScalaClassLoader$class.loadClass(S...", "reporter": "tjhunter", "assignee": null, "created": "0010-12-10T19:42:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: I have a patch in my branch that solves a related issue: using classes not used by spark (and thus not in spark's class loader). Eventually, the deserialization routines should be the same as the one used in the executor.", "created": "2010-12-10T21:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: That makes sense, thanks for the patch. I didn't realize that deserialize is also called here when I added support for JARs. Should I just cherry-pick this commit, or do you have other stuff in your branch that you want merged in? I noticed there's a synchronized statement but if I recall correctly that had to do with some behavior of MM code.", "created": "2010-12-10T21:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: BTW, if you want to use Utils.deserialize in your own code, the easiest thing to pass in is Thread.currentThread.getContextClassLoader. That will be the ClassLoader that loaded your code to begin with. Is this what you're trying to do? Utils.serialize and deserialize aren't actually meant to be part of the public Spark API, but they will probably work if you do this.", "created": "2010-12-10T21:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: Actually, I am not very concerned with the example I provided in the first message (I am not using the deserializer directly from the console, it was just for narrowing down the issue). I am closing this bug since it is solved in my branch. Since my branch has this synchronized statement, you should cherry-pick the relevant commits, I have to find a workaround in the MM code to remove it. Also, I committed another fix related to SPARK_MEM.", "created": "2010-12-11T23:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Tim, I merged in your SPARK_MEM fix, but for the other one, I'll check to see if there are cleaner ways to pass that classloader around throughout the code.", "created": "2010-12-12T13:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-40, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-487\nSummary: Serialization issue\nDescription: The following code fails at deserialization: import spark.Utils @serializable class XXX val xx=new XXX val cc=Utils.serialize(xx) val xx2=Utils.deserialize[Any](cc) Reported error is below. In particular, it causes an exception to be thrown when the simple job class gets some results back to the master. I am looking at a workaround using my own class loader. java.lang.ClassNotFoundException: XXX at java.net.URLClassLoader$1.run(URLClassLoader.java:217) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:205) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$findClass(ScalaClassLoader.scala:86) at scala.tools.nsc.util.ScalaClassLoader$class.findClass(ScalaClassLoader.scala:51) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.findClass(ScalaClassLoader.scala:86) at java.lang.ClassLoader.loadClass(ClassLoader.java:321) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.scala$tools$nsc$util$ScalaClassLoader$$super$loadClass(ScalaClassLoader.scala:86) at scala.tools.nsc.util.ScalaClassLoader$class.loadClass(S...\n\nComments (6):\n1. Patrick McFadin: Github comment from tjhunter: I have a patch in my branch that solves a related issue: using classes not used by spark (and thus not in spark's class loader). Eventually, the deserialization routines should be the same as the one used in the executor.\n2. Patrick McFadin: Github comment from mateiz: That makes sense, thanks for the patch. I didn't realize that deserialize is also called here when I added support for JARs. Should I just cherry-pick this commit, or do you have other stuff in your branch that you want merged in? I noticed there's a synchronized statement but if I recall correctly that had to do with some behavior of MM code.\n3. Patrick McFadin: Github comment from mateiz: BTW, if you want to use Utils.deserialize in your own code, the easiest thing to pass in is Thread.currentThread.getContextClassLoader. That will be the ClassLoader that loaded your code to begin with. Is this what you're trying to do? Utils.serialize and deserialize aren't actually meant to be part of the public Spark API, but they will probably work if you do this.\n4. Patrick McFadin: Github comment from tjhunter: Actually, I am not very concerned with the example I provided in the first message (I am not using the deserializer directly from the console, it was just for narrowing down the issue). I am closing this bug since it is solved in my branch. Since my branch has this synchronized statement, you should cherry-pick the relevant commits, I have to find a workaround in the MM code to remove it. Also, I committed another fix related to SPARK_MEM.\n5. Patrick McFadin: Github comment from mateiz: Hey Tim, I merged in your SPARK_MEM fix, but for the other one, I'll check to see if there are cleaner ways to pass that classloader around throughout the code.\n6. Patrick McFadin: Imported from Github issue spark-40, originally reported by tjhunter", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "9d68d5f71bb4245814795c3aa568e055", "issue_key": "SPARK-567", "issue_type": "Improvement", "status": "Closed", "priority": null, "resolution": null, "summary": "Unified directory structure for temporary data", "description": "Broadcast, shuffle, and unforeseen use cases should use the same directory structure.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-12-13T14:23:00.000+0000", "updated": "2014-09-21T15:45:09.000+0000", "resolved": "2014-09-21T15:45:09.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-41, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matthew Farrellee", "body": "please re-open with additional details for how this could be implemented", "created": "2014-09-21T15:45:09.387+0000"}], "num_comments": 2, "text": "Issue: SPARK-567\nSummary: Unified directory structure for temporary data\nDescription: Broadcast, shuffle, and unforeseen use cases should use the same directory structure.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-41, originally reported by mosharaf\n2. Matthew Farrellee: please re-open with additional details for how this could be implemented", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "14b919afc474279a56d0175da5d2b660", "issue_key": "SPARK-486", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Passing masterHostAddress to workers is not working", "description": "Both BitTorrent and Chained broadcast implementations are not working because the workers are not receiving masterHostAddress properly. Current code in place is obviously wrong. But using System.setProperty (\"spark.broadcast.masterHostAddress\", InetAddress.getLocalHost.getHostAddress) in master and reading it everywhere using System.setProperty (\"spark.broadcast.masterHostAddress\", \"127.0.0.1\") is also not working. <pre> if (isMaster) { System.setProperty(\"spark.broadcast.masterHostAddress\", InetAddress.getLocalHost.getHostAddress) } MasterHostAddress_ = System.getProperty(\"spark.broadcast.masterHostAddress\", \"127.0.0.1\") </pre>", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-12-17T12:12:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mosharaf: The main reason its not working is because in the SparkContext.scala, scheduler is started before the Broadcast is initialized; it already creates the args for the workers before Broadcast gets a chance to update masterHostAddress. Same is true for Shuffle codes. Straightforward solution is to initialize Broadcast and Shuffle subsystems before starting the scheduler. Is there any dependency to not change the order?", "created": "2011-02-09T15:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-42, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-486\nSummary: Passing masterHostAddress to workers is not working\nDescription: Both BitTorrent and Chained broadcast implementations are not working because the workers are not receiving masterHostAddress properly. Current code in place is obviously wrong. But using System.setProperty (\"spark.broadcast.masterHostAddress\", InetAddress.getLocalHost.getHostAddress) in master and reading it everywhere using System.setProperty (\"spark.broadcast.masterHostAddress\", \"127.0.0.1\") is also not working. <pre> if (isMaster) { System.setProperty(\"spark.broadcast.masterHostAddress\", InetAddress.getLocalHost.getHostAddress) } MasterHostAddress_ = System.getProperty(\"spark.broadcast.masterHostAddress\", \"127.0.0.1\") </pre>\n\nComments (2):\n1. Patrick McFadin: Github comment from mosharaf: The main reason its not working is because in the SparkContext.scala, scheduler is started before the Broadcast is initialized; it already creates the args for the workers before Broadcast gets a chance to update masterHostAddress. Same is true for Shuffle codes. Straightforward solution is to initialize Broadcast and Shuffle subsystems before starting the scheduler. Is there any dependency to not change the order?\n2. Patrick McFadin: Imported from Github issue spark-42, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "4cad3b788f8a7bdab7206fb106e97f9b", "issue_key": "SPARK-566", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Replace polling+sleeping with semaphores in broadcast and shuffle", "description": "All the FixedThreadPools are doing this right now. Must be fixed.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0010-12-21T19:19:00.000+0000", "updated": "2014-10-21T07:48:02.000+0000", "resolved": "2014-10-21T07:48:02.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-43, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Patrick Wendell", "body": "Thew shuffle and broadcast implementations have been re-written at least twice since this was created. I'm gonna close it as it's no longer relevant.", "created": "2014-10-21T07:47:50.994+0000"}], "num_comments": 2, "text": "Issue: SPARK-566\nSummary: Replace polling+sleeping with semaphores in broadcast and shuffle\nDescription: All the FixedThreadPools are doing this right now. Must be fixed.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-43, originally reported by mosharaf\n2. Patrick Wendell: Thew shuffle and broadcast implementations have been re-written at least twice since this was created. I'm gonna close it as it's no longer relevant.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "cb3ee20e34e7c9afa13c21a5695a0069", "issue_key": "SPARK-485", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Concurrency bug in BlockedShuffle implementations in mos-shuffle-tracked branch", "description": "Since BlockedShuffle implementations started reading multiple blocks per connection to the server/mapper instead of just one (Commit 1e26fb39534cc5b79c6980e0974bedbf72e19c69), a new bug has been introduced where they are reading more blocks than they are supposed to. We use hasBlocksInSplit(i) and totalBlocksInSplit(i) to keep track of how many blocks have been received and this reducer is supposed to receive from mapper i. A particular index i of these two variables are accessed by only one thread at a time which is ensured by hasSplitsBitVector and splitsInRequestBitVector. In short, when we access hasBlocksInSplit(i) and totalBlocksInSplit(i), we do so without any synchronize blocks and use them for if/while -related comparisons. While totalBlocksInSplit(i) is written only once, hasBlocksInSplit(i) is incremented after every block is received. This can possibly create concurrency bug if some other thread is accessing/incrementing some other index hasBlocksInSplit(j) (not sure if thats likely though). In any case, if we instead copy hasBlocksInSplit(i) and totalBlocksInSplit(i) to local variables in each thread and copy back at the end using synchronize block the problem should be away and it does. Reducers receiver exact number of blocks they are supposed to. But this creates a completely unrelated problem in ShuffleConsumer threads. Some of them just keep waiting on a take() call on a LinkedBlockingQueue even though they are not supposed to get to the take() call unless the size of the queue is non-zero. Essentially, they see that the size of queue is non-zero, they proceed, but find the queue to be empty. Since take() is a blocking call they just halt there.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0011-01-03T17:36:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mosharaf: splitsInRequestBitVector was not being properly updated. Fixed by 7eb334d97ce3731d6397136e1df4c2c989a665f3.", "created": "2011-01-03T19:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-44, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-485\nSummary: Concurrency bug in BlockedShuffle implementations in mos-shuffle-tracked branch\nDescription: Since BlockedShuffle implementations started reading multiple blocks per connection to the server/mapper instead of just one (Commit 1e26fb39534cc5b79c6980e0974bedbf72e19c69), a new bug has been introduced where they are reading more blocks than they are supposed to. We use hasBlocksInSplit(i) and totalBlocksInSplit(i) to keep track of how many blocks have been received and this reducer is supposed to receive from mapper i. A particular index i of these two variables are accessed by only one thread at a time which is ensured by hasSplitsBitVector and splitsInRequestBitVector. In short, when we access hasBlocksInSplit(i) and totalBlocksInSplit(i), we do so without any synchronize blocks and use them for if/while -related comparisons. While totalBlocksInSplit(i) is written only once, hasBlocksInSplit(i) is incremented after every block is received. This can possibly create concurrency bug if some other thread is accessing/incrementing some other index hasBlocksInSplit(j) (not sure if thats likely though). In any case, if we instead copy hasBlocksInSplit(i) and totalBlocksInSplit(i) to local variables in each thread and copy back at the end using synchronize block the problem should be away and it does. Reducers receiver exact number of blocks they are supposed to. But this creates a completely unrelated problem in ShuffleConsumer threads. Some of them just keep waiting on a take() call on a LinkedBlockingQueue even though they are not supposed to get to the take() call unless the size of the queue is non-zero. Essentially, they see that the size of queue is non-zero, they proceed, but find the queue to be empty. Since take() is a blocking call they just halt there.\n\nComments (2):\n1. Patrick McFadin: Github comment from mosharaf: splitsInRequestBitVector was not being properly updated. Fixed by 7eb334d97ce3731d6397136e1df4c2c989a665f3.\n2. Patrick McFadin: Imported from Github issue spark-44, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.935793"}}
{"id": "55a04c772aa3be1ae23c411eff1aa584", "issue_key": "SPARK-484", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make sure slf4j is initialized before any piece of code tries to use it", "description": "Sometimes important pieces of log messages get lost that cause trouble during log parsing. It does print warnings though.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0011-01-06T17:05:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed in commit 5166d76, so closing it.", "created": "2011-06-26T21:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-45, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-484\nSummary: Make sure slf4j is initialized before any piece of code tries to use it\nDescription: Sometimes important pieces of log messages get lost that cause trouble during log parsing. It does print warnings though.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is fixed in commit 5166d76, so closing it.\n2. Patrick McFadin: Imported from Github issue spark-45, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "7fdc49bb1a1f33db6f9b89ca2881e112", "issue_key": "SPARK-565", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Integrate spark in scala standard collection API", "description": "This is more a meta-bug / whish item than a real bug. Scala 2.0 provides some API for parallel collections which might be interesting to leverage, but mostly as a user, I would like to be able to write a function like: def contrived_example(xs:Seq[Int]) = xs.map(_ * 2).sum and not have to care if xs is an array, a scala parallel collection or a RDD. Given that RDDs already implement most of the API for Seq, it seems mostly a matter of standardization. I am probably missing some subtle details here?", "reporter": "tjhunter", "assignee": null, "created": "0011-04-26T10:44:00.000+0000", "updated": "2014-11-06T06:53:11.000+0000", "resolved": "2014-11-06T06:53:11.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-46, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "koert kuipers", "body": "RDD.flatMap being inconsistent with the collections API is an issue here. See: http://mail-archives.apache.org/mod_mbox/spark-user/201403.mbox/%3CCAKn3j0s7paRiWVjjweEYGHLsGwY269gADY2fzWjruMYSu3YpzQ@mail.gmail.com%3E", "created": "2014-03-20T22:01:17.265+0000"}, {"author": "Matei Alexandru Zaharia", "body": "FYI I'm going to close this because we've locked down the API for 1.X, and it's pretty clear that it can't fully fit into the Scala collections API (that has a lot of things we don't have, and vice versa). This is something we can investigate later but it's unlikely that we'll want to bind the API to Scala even if we change pieces of it in the future.", "created": "2014-11-06T06:53:11.064+0000"}], "num_comments": 3, "text": "Issue: SPARK-565\nSummary: Integrate spark in scala standard collection API\nDescription: This is more a meta-bug / whish item than a real bug. Scala 2.0 provides some API for parallel collections which might be interesting to leverage, but mostly as a user, I would like to be able to write a function like: def contrived_example(xs:Seq[Int]) = xs.map(_ * 2).sum and not have to care if xs is an array, a scala parallel collection or a RDD. Given that RDDs already implement most of the API for Seq, it seems mostly a matter of standardization. I am probably missing some subtle details here?\n\nComments (3):\n1. Patrick McFadin: Imported from Github issue spark-46, originally reported by tjhunter\n2. koert kuipers: RDD.flatMap being inconsistent with the collections API is an issue here. See: http://mail-archives.apache.org/mod_mbox/spark-user/201403.mbox/%3CCAKn3j0s7paRiWVjjweEYGHLsGwY269gADY2fzWjruMYSu3YpzQ@mail.gmail.com%3E\n3. Matei Alexandru Zaharia: FYI I'm going to close this because we've locked down the API for 1.X, and it's pretty clear that it can't fully fit into the Scala collections API (that has a lot of things we don't have, and vice versa). This is something we can investigate later but it's unlikely that we'll want to bind the API to Scala even if we change pieces of it in the future.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "ccfb92f874ecc7ac0b741c861a1985d0", "issue_key": "SPARK-483", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Cache to disk", "description": "DiskSpillingCache is a cache implementation derived from BoundedMemoryCache that spills entries to disk when it runs out of space. The location on disk where entries are stored is configurable through the spark.diskSpillingCache.cacheDir property.", "reporter": "Ankur Dave", "assignee": null, "created": "0011-04-28T09:22:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Ankur, I committed this.", "created": "2011-05-09T13:23:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-47, originally reported by ankurdave", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-483\nSummary: Cache to disk\nDescription: DiskSpillingCache is a cache implementation derived from BoundedMemoryCache that spills entries to disk when it runs out of space. The location on disk where entries are stored is configurable through the spark.diskSpillingCache.cacheDir property.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Ankur, I committed this.\n2. Patrick McFadin: Imported from Github issue spark-47, originally reported by ankurdave", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "7ed153815bc79559c6ee799c38ca5fec", "issue_key": "SPARK-482", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Bagel: Large-scale graph processing on Spark", "description": "Bagel is an implementation of the Pregel graph processing framework on Spark. Bagel currently supports basic graph computation, combiners, and aggregators. Future work includes support for mutating the graph topology. Tests exist but currently don't run due to a Spark bug.", "reporter": "Ankur Dave", "assignee": null, "created": "0011-05-03T14:49:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: I would recommend you refactor your code before merging, it is always harder / less tempting to do after.", "created": "2011-05-03T16:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks great, Ankur, except for two naming things: can you change the package name from bagel to spark.bagel, and can you rename the Pregel class to Bagel?", "created": "2011-05-09T13:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ankurdave: Sure, I've done so.", "created": "2011-05-09T14:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks great, thanks. The one thing I should add is that maybe you should write a README documenting the examples, or a wiki page (and put a comment in the code pointing to this location).", "created": "2011-05-12T20:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-48, originally reported by ankurdave", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-482\nSummary: Bagel: Large-scale graph processing on Spark\nDescription: Bagel is an implementation of the Pregel graph processing framework on Spark. Bagel currently supports basic graph computation, combiners, and aggregators. Future work includes support for mutating the graph topology. Tests exist but currently don't run due to a Spark bug.\n\nComments (5):\n1. Patrick McFadin: Github comment from tjhunter: I would recommend you refactor your code before merging, it is always harder / less tempting to do after.\n2. Patrick McFadin: Github comment from mateiz: This looks great, Ankur, except for two naming things: can you change the package name from bagel to spark.bagel, and can you rename the Pregel class to Bagel?\n3. Patrick McFadin: Github comment from ankurdave: Sure, I've done so.\n4. Patrick McFadin: Github comment from mateiz: Looks great, thanks. The one thing I should add is that maybe you should write a README documenting the examples, or a wiki page (and put a comment in the code pointing to this location).\n5. Patrick McFadin: Imported from Github issue spark-48, originally reported by ankurdave", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "926da546fa733c878ac41af636467947", "issue_key": "SPARK-481", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support Scala 2.9.x", "description": "I am opening an issue to track this. The FAQ says: \"Spark currently works with Scala 2.8.1. We're working on a port to 2.9.\" Is this work published somewhere? I would be interested in helping out. I looked at the branch names and there wasn't an obvious one for Scala 2.9.0.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-26T01:53:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Right now there isn't anything pushable done here. It seems that Spark will compile with 2.9, but the part I'd like to change is the interpreter. We currently include a modified version of the Scala 2.8 interpreter and I know the 2.9 one has some nice changes. Ideally the interpreter would've been modular enough that Spark doesn't have to ship a modified version (and I've been meaning to ask Scala guys about this) but for now this will have to do. That part is kind of hairy so it's probably easiest for me to do.", "created": "2011-05-26T08:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: After moving the repl classes to a separate module, it was trivial to build for 2.9.0 without repl support. I pushed the work to the following branch: https://github.com/ijuma/spark/tree/scala-2.9", "created": "2011-05-27T02:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I've rebased the scala-2.9 branch against master since it now contains the repl classes in a separate module.", "created": "2011-05-30T17:34:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I'm still working on supporting the 2.9 interpreter. Is anything else required to run on 2.9, other than updating the build file?", "created": "2011-05-30T17:36:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: The build file and the run shell script. The branch above helps so that you don't have to figure out what version you need for the Scala dependencies so that they work with 2.9.0. Quite trivial though.", "created": "2011-05-30T17:40:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I've merged all your changes (up to upgrading scalacheck to 2.9) and a bunch of work on the interpreter into branch scala-2.9 in the repo. Rebase against that one if you need to make more changes. I think this is working OK but I want to test it a bit more before merging it into master.", "created": "2011-05-31T22:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Excellent, thanks!", "created": "2011-06-01T00:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Matei merged the Scala 2.9 branch to master, so this is now done. Closing.", "created": "2011-08-02T00:47:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-49, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 9, "text": "Issue: SPARK-481\nSummary: Support Scala 2.9.x\nDescription: I am opening an issue to track this. The FAQ says: \"Spark currently works with Scala 2.8.1. We're working on a port to 2.9.\" Is this work published somewhere? I would be interested in helping out. I looked at the branch names and there wasn't an obvious one for Scala 2.9.0.\n\nComments (9):\n1. Patrick McFadin: Github comment from mateiz: Right now there isn't anything pushable done here. It seems that Spark will compile with 2.9, but the part I'd like to change is the interpreter. We currently include a modified version of the Scala 2.8 interpreter and I know the 2.9 one has some nice changes. Ideally the interpreter would've been modular enough that Spark doesn't have to ship a modified version (and I've been meaning to ask Scala guys about this) but for now this will have to do. That part is kind of hairy so it's probably easiest for me to do.\n2. Patrick McFadin: Github comment from ijuma: After moving the repl classes to a separate module, it was trivial to build for 2.9.0 without repl support. I pushed the work to the following branch: https://github.com/ijuma/spark/tree/scala-2.9\n3. Patrick McFadin: Github comment from ijuma: I've rebased the scala-2.9 branch against master since it now contains the repl classes in a separate module.\n4. Patrick McFadin: Github comment from mateiz: I'm still working on supporting the 2.9 interpreter. Is anything else required to run on 2.9, other than updating the build file?\n5. Patrick McFadin: Github comment from ijuma: The build file and the run shell script. The branch above helps so that you don't have to figure out what version you need for the Scala dependencies so that they work with 2.9.0. Quite trivial though.\n6. Patrick McFadin: Github comment from mateiz: I've merged all your changes (up to upgrading scalacheck to 2.9) and a bunch of work on the interpreter into branch scala-2.9 in the repo. Rebase against that one if you need to make more changes. I think this is working OK but I want to test it a bit more before merging it into master.\n7. Patrick McFadin: Github comment from ijuma: Excellent, thanks!\n8. Patrick McFadin: Github comment from ijuma: Matei merged the Scala 2.9 branch to master, so this is now done. Closing.\n9. Patrick McFadin: Imported from Github issue spark-49, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "d926c1585ce9b114c36a5dedd30e26ce", "issue_key": "SPARK-480", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fix deprecations when compiled with Scala 2.8.1", "description": "There are a lot of deprecations when compiling Spark with Scala 2.8.1. There are several instances of: - +' with mutable collections and then assigning it to a var. - Math instead of math - List.- - Iterator.fromArray - first instead of head - List.-- - Array.fromFunction Is there a reason to use the deprecated methods or is it OK for me to submit a pull request with the changes?", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-26T02:08:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Sounds great, thanks for taking a look at these.", "created": "2011-05-26T08:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Matei has merged these changes. Closing.", "created": "2011-05-30T17:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-50, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-480\nSummary: Fix deprecations when compiled with Scala 2.8.1\nDescription: There are a lot of deprecations when compiling Spark with Scala 2.8.1. There are several instances of: - +' with mutable collections and then assigning it to a var. - Math instead of math - List.- - Iterator.fromArray - first instead of head - List.-- - Array.fromFunction Is there a reason to use the deprecated methods or is it OK for me to submit a pull request with the changes?\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Sounds great, thanks for taking a look at these.\n2. Patrick McFadin: Github comment from ijuma: Matei has merged these changes. Closing.\n3. Patrick McFadin: Imported from Github issue spark-50, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "7192bf216dae0fd71730937c9a60776d", "issue_key": "SPARK-564", "issue_type": "New Feature", "status": "Closed", "priority": null, "resolution": null, "summary": "Publish to maven repository", "description": "Publishing the jar to a maven repository would make adoption easier. The first step towards that would be to include the dependencies via the SBT build file instead of having the libraries in lib. If there are no objections, I could have a go.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-26T03:05:00.000+0000", "updated": "2012-10-20T16:50:07.000+0000", "resolved": "2012-10-20T16:50:07.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: That sounds great. The only thing to watch out for will be kryo-1.04-mod.jar, which is a slightly modified version of the Kryo serialization library (http://code.google.com/p/kryo/). I could push that as a separate GitHub project too.", "created": "2011-05-26T08:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I started on this and I have the following questions: - Is there no way to avoid modifying the kryo jar? Modified third-party jars cause problems to clients that mix them with the unmodified ones. - What version of the mesos jar is being used? - What version of jline is being used? - Is the native liblzf-3.5 still being used? The work in progress can be seen here: https://github.com/ijuma/spark/tree/issue51", "created": "2011-05-26T16:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Unfortunately the Kryo changes are needed to have it work with common Scala objects that lack a default constructor (like Pair, List, tuples, etc). I reported some of the issues to the Kryo developer but they aren't fixed yet. We could include Kryo-mod as a separate project somehow since Kryo serialization is an optional feature. For the Mesos JAR, we are using the pre-protobuf branch at github.com/mesos/mesos. Would it be possible to give this a separate version number? We can call it something like 18039d (the latest commit in that branch). For jline, this is a good question and I'm not sure how to find the answer. I took the jline.jar out of SCALA_HOME/lib in order to support the Scala interpreter, but I don't see where its version is documented. Maybe just try the latest one? liblzf is no longer being used and could be deleted from the repo.", "created": "2011-05-26T22:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I see. I think we could leave Kryo in lib for now until upstream incorporates those features then. That would mean that someone who depends on the SBT build of Spark would not get that dependency automatically. OK. Maybe I'll leave this in lib too as this may cause conflicts with another jline in the user's classpath. Do you think it would make sense to move the REPL classes to a separate subproject? I'll delete it then. Thanks.", "created": "2011-05-26T23:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Moving the REPL to a separate subproject is a great idea. It shouldn't be too bad.. it will just depend on core, and all the code for it is in the spark/repl packages anyway.", "created": "2011-05-26T23:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: (But for that make sure that ./spark-shell continues to work.)", "created": "2011-05-26T23:19:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Hmm, when I try spark-shell from master I get a NoClassDefFoundError. It seems like the scala-compiler.jar needs to be added to the classpath: [ijuma@localhost]~/src/spark% ./spark-shell java.lang.NoClassDefFoundError: scala/tools/nsc/InterpreterResults$Result at spark.repl.Main$.main(Main.scala:13) Does it work for you? I've rebased the branch and moved the REPL to its own module. I had to use reflection in two places where the core module optionally uses REPL classes: https://github.com/ijuma/spark/tree/issue51 I don't have any other changes planned aside from the spark-shell issue (which I think was already there, but I don't mind fixing it by simply adding the scala-compiler.jar to the classpath if you agree that it's the way to go).", "created": "2011-05-27T02:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Oh, the other thing left to discuss is what public Maven repository to publish the binaries too. Once we decide that, we need to add that to the build configuration.", "created": "2011-05-27T05:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: That InterpreterResult class should be in Scala 2.8.1's scala-compiler.jar. I believe it gets added to the classpath when you just run scala. Has spark-shell ever worked for you without changes to Spark? When I try your branch, I believe I get past that problem, but I see: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/thread/ThreadPool at spark.repl.SparkInterpreter.<init>(SparkInterpreter.scala:111) at spark.repl.SparkInterpreterLoop$$anon$1.<init>(SparkInterpreterLoop.scala:131) ... I'll figure out what the right dependency is called for that. In terms of which Maven repository to push it to, I don't have a preference as I don't know much about Maven. What are the options?", "created": "2011-05-27T14:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I fixed this by not making the Jetty dependency \"provided\" and making it be on jetty-server rather than jetty-webapp. (We're using Jetty to run an HTTP server, not to embed Spark in a webapp.) Just remove the jettyWebapp and the val jetty = ... lines and add the following in BaseProject: ```val jettyServer = \"org.eclipse.jetty\" % \"jetty-server\" % \"7.4.1.v20110513\"``` Also, I'm not 100% sure about this, but shouldn't the REPL, examples, Bagel, etc projects just depend on BaseProject rather than being subclasses? Right now there is a separate lib_managed directory for each one with pretty much the same JARs in it.", "created": "2011-05-27T15:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: \"That InterpreterResult class should be in Scala 2.8.1's scala-compiler.jar. I believe it gets added to the classpath when you just run scala. Has spark-shell ever worked for you without changes to Spark?\" No, the scala-compiler doesn't get added to the runtime classpath by default (only scala-library). Have you modified your Scala installation in any way? I had never tried spark-shell previously. \"I fixed this by not making the Jetty dependency \"provided\" and making it be on jetty-server rather than jetty-webapp.\" Thanks for looking into the issue, makes sense. \"Also, I'm not 100% sure about this, but shouldn't the REPL, examples, Bagel, etc projects just depend on BaseProject rather than being subclasses? Right now there is a separate lib_managed directory for each one with pretty much the same JARs in it.\" BaseProject isn't an actual project, it's just there to avoid duplication (so you can't depend on it). Sadly, there's no way around the duplication of jars in lib_managed in SBT 0.7.x. SBT 0.9.x has additional options (you can use the ivy cache or you can use an improved version of lib_managed that includes a shared lib_managed for multi-module projects). Once SBT 0.10.0 is out (0.9.x is development while 0.10.x is stable) and the plugins have been ported, I can submit a patch to move to that as it requires incompatible changes. I've already done that for my company's project which has a build file that is way more complicated than Spark.", "created": "2011-05-28T03:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I always install Scala by unzipping the .tar.gz somewhere and setting SCALA_HOME to point to that. I've done this on multiple machines with different OSes and it always seems to have the compiler on the classpath. Are you running it some other way? For example, I believe the Eclipse plugin doesn't add the compiler JAR to the classpath. In any case, maybe there's a way to make SBT add it, or at least to make the run script look for it in SCALA_HOME.", "created": "2011-05-28T11:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Sorry, you are indeed right that scala-compiler is included by default when something is executed with scala. I had never realised this because I tend to run my applications with the java launcher and I add the scala-library manually (or deploy to a container). It seems like it's done that way for ease of implementation (the scala command can also be used for scripts and the compiler jar is needed for that). I'd say it's a bug, but whatever. :) With that out of the way, I now realise what was the issue I was having. My installation of scala 2.9.0 was being used instead of the 2.8.1 one. Mystery solved. :) I'll fix the jetty thing and I'll try to configure publishing to a private repository. Later, we can look into publishing to scala-tools.org or the central Maven repository.", "created": "2011-05-29T04:16:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I pushed the jetty-server change and spark-shell works for me when I set SCALA_HOME to my 2.8.1 install. In terms of publishing, it seems like the easiest option is to use scala-tools.org. At the bottom of their home page[1], there are instructions on what is required to publish there. It looks like it has to be done by someone that can represent the project. I suggest you email them when you have a chance. Meanwhile, the work that's been done here makes it easy for someone to publish spark to their internal Maven repositories (I've done it to the one used by my company), so I believe it should be merged to master. Is there anything you'd like me to do before that is done? [1] http://scala-tools.org/", "created": "2011-05-29T11:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: The branch looks good, so I've merged it in. I fixed one bug in the Executor code though (it was passing the wrong classes when looking for a constructor for ExecutorClassLoader). Thanks for spending the time to do this. I'll look into hosting on scala-tools.org; that seems like a good solution. Maybe before that we should also change the package name from spark to something like edu.berkeley.spark throughout.", "created": "2011-05-29T15:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Thanks for merging and catching the bug. Have you considered org.spark_project?", "created": "2011-05-29T16:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from patelh: Has this been pushed to maven yet?", "created": "2012-01-16T12:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Not to the official Maven repo, but you can do sbt publish-local to publish it to your own local repository. I'm actually thinking of submitting Spark as an Apache incubator project soon. If that gets in we'd have a clear home and we'd change the packages to org.apache.spark and publish.", "created": "2012-01-16T14:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-51, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "From the latest releases:  Spark is now available in Maven Central, making it easier to link into your programs without having to build it as a JAR. Use the following Maven identifiers to add it to a project: groupId: org.spark-project artifactId: spark-core_2.9.2 version: 0.5.1 groupId: org.spark-project artifactId: spark-core_2.9.2 version: 0.6.0", "created": "2012-10-20T16:50:07.832+0000"}], "num_comments": 20, "text": "Issue: SPARK-564\nSummary: Publish to maven repository\nDescription: Publishing the jar to a maven repository would make adoption easier. The first step towards that would be to include the dependencies via the SBT build file instead of having the libraries in lib. If there are no objections, I could have a go.\n\nComments (20):\n1. Patrick McFadin: Github comment from mateiz: That sounds great. The only thing to watch out for will be kryo-1.04-mod.jar, which is a slightly modified version of the Kryo serialization library (http://code.google.com/p/kryo/). I could push that as a separate GitHub project too.\n2. Patrick McFadin: Github comment from ijuma: I started on this and I have the following questions: - Is there no way to avoid modifying the kryo jar? Modified third-party jars cause problems to clients that mix them with the unmodified ones. - What version of the mesos jar is being used? - What version of jline is being used? - Is the native liblzf-3.5 still being used? The work in progress can be seen here: https://github.com/ijuma/spark/tree/issue51\n3. Patrick McFadin: Github comment from mateiz: Unfortunately the Kryo changes are needed to have it work with common Scala objects that lack a default constructor (like Pair, List, tuples, etc). I reported some of the issues to the Kryo developer but they aren't fixed yet. We could include Kryo-mod as a separate project somehow since Kryo serialization is an optional feature. For the Mesos JAR, we are using the pre-protobuf branch at github.com/mesos/mesos. Would it be possible to give this a separate version number? We can call it something like 18039d (the latest commit in that branch). For jline, this is a good question and I'm not sure how to find the answer. I took the jline.jar out of SCALA_HOME/lib in order to support the Scala interpreter, but I don't see where its version is documented. Maybe just try the latest one? liblzf is no longer being used and could be deleted from the repo.\n4. Patrick McFadin: Github comment from ijuma: I see. I think we could leave Kryo in lib for now until upstream incorporates those features then. That would mean that someone who depends on the SBT build of Spark would not get that dependency automatically. OK. Maybe I'll leave this in lib too as this may cause conflicts with another jline in the user's classpath. Do you think it would make sense to move the REPL classes to a separate subproject? I'll delete it then. Thanks.\n5. Patrick McFadin: Github comment from mateiz: Moving the REPL to a separate subproject is a great idea. It shouldn't be too bad.. it will just depend on core, and all the code for it is in the spark/repl packages anyway.\n6. Patrick McFadin: Github comment from mateiz: (But for that make sure that ./spark-shell continues to work.)\n7. Patrick McFadin: Github comment from ijuma: Hmm, when I try spark-shell from master I get a NoClassDefFoundError. It seems like the scala-compiler.jar needs to be added to the classpath: [ijuma@localhost]~/src/spark% ./spark-shell java.lang.NoClassDefFoundError: scala/tools/nsc/InterpreterResults$Result at spark.repl.Main$.main(Main.scala:13) Does it work for you? I've rebased the branch and moved the REPL to its own module. I had to use reflection in two places where the core module optionally uses REPL classes: https://github.com/ijuma/spark/tree/issue51 I don't have any other changes planned aside from the spark-shell issue (which I think was already there, but I don't mind fixing it by simply adding the scala-compiler.jar to the classpath if you agree that it's the way to go).\n8. Patrick McFadin: Github comment from ijuma: Oh, the other thing left to discuss is what public Maven repository to publish the binaries too. Once we decide that, we need to add that to the build configuration.\n9. Patrick McFadin: Github comment from mateiz: That InterpreterResult class should be in Scala 2.8.1's scala-compiler.jar. I believe it gets added to the classpath when you just run scala. Has spark-shell ever worked for you without changes to Spark? When I try your branch, I believe I get past that problem, but I see: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/thread/ThreadPool at spark.repl.SparkInterpreter.<init>(SparkInterpreter.scala:111) at spark.repl.SparkInterpreterLoop$$anon$1.<init>(SparkInterpreterLoop.scala:131) ... I'll figure out what the right dependency is called for that. In terms of which Maven repository to push it to, I don't have a preference as I don't know much about Maven. What are the options?\n10. Patrick McFadin: Github comment from mateiz: I fixed this by not making the Jetty dependency \"provided\" and making it be on jetty-server rather than jetty-webapp. (We're using Jetty to run an HTTP server, not to embed Spark in a webapp.) Just remove the jettyWebapp and the val jetty = ... lines and add the following in BaseProject: ```val jettyServer = \"org.eclipse.jetty\" % \"jetty-server\" % \"7.4.1.v20110513\"``` Also, I'm not 100% sure about this, but shouldn't the REPL, examples, Bagel, etc projects just depend on BaseProject rather than being subclasses? Right now there is a separate lib_managed directory for each one with pretty much the same JARs in it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "6adb6c92782ab11f4f2ba31d3e1c8777", "issue_key": "SPARK-479", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fix deprecations when compiled with Scala 2.8.1", "description": "See issue #50. The change for `--` and `-` are the only controversial ones in my opinion and there's no better alternative as far as I know (it's the suggested approach in the deprecated message).", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-26T13:25:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ijuma: Oh, I also upgraded the sbt-idea plugin to version 0.4.0.", "created": "2011-05-26T13:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks, looks great!", "created": "2011-05-26T22:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-52, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-479\nSummary: Fix deprecations when compiled with Scala 2.8.1\nDescription: See issue #50. The change for `--` and `-` are the only controversial ones in my opinion and there's no better alternative as far as I know (it's the suggested approach in the deprecated message).\n\nComments (3):\n1. Patrick McFadin: Github comment from ijuma: Oh, I also upgraded the sbt-idea plugin to version 0.4.0.\n2. Patrick McFadin: Github comment from mateiz: Thanks, looks great!\n3. Patrick McFadin: Imported from Github issue spark-52, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.937797"}}
{"id": "43abb12d51875a9266eccecb1657ed10", "issue_key": "SPARK-478", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Use explicit asInstanceOf instead of misleading unchecked pattern matching.", "description": "This makes the code a bit uglier, but lets someone easily spot where dangerous things are being done. It also silences the compiler so that real \"unchecked\" warnings can be spotted. Also enabled -unchecked warnings in SBT build file and restructured it a bit to make that easier.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-26T23:03:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good. It looks like your editor also made some formatting changes in the build file, but we can keep those. Is there any widely used Scala style guide out there? We've tried to adhere to http://www.codecommit.com/scala-style-guide.pdf but it doesn't cover everything.", "created": "2011-05-27T09:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Thanks for merging. Sorry for the formatting changes in the build file, it seemed like two styles were being used in the same file so I changed it to be consistent with typical Scala style, but forgot to mention it. I think http://davetron5000.github.com/scala-style/ScalaStyleGuide.pdf is a more recent version of the style guide that Daniel started. In terms of what's used out there, it varies quite a lot and this variation happens even inside the Scala standard library and compiler. I'll have a look at the Style Guide once again (it's been a while since I looked at it) and try to stick to that for Spark then.", "created": "2011-05-28T04:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-53, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-478\nSummary: Use explicit asInstanceOf instead of misleading unchecked pattern matching.\nDescription: This makes the code a bit uglier, but lets someone easily spot where dangerous things are being done. It also silences the compiler so that real \"unchecked\" warnings can be spotted. Also enabled -unchecked warnings in SBT build file and restructured it a bit to make that easier.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Looks good. It looks like your editor also made some formatting changes in the build file, but we can keep those. Is there any widely used Scala style guide out there? We've tried to adhere to http://www.codecommit.com/scala-style-guide.pdf but it doesn't cover everything.\n2. Patrick McFadin: Github comment from ijuma: Thanks for merging. Sorry for the formatting changes in the build file, it seemed like two styles were being used in the same file so I changed it to be consistent with typical Scala style, but forgot to mention it. I think http://davetron5000.github.com/scala-style/ScalaStyleGuide.pdf is a more recent version of the style guide that Daniel started. In terms of what's used out there, it varies quite a lot and this variation happens even inside the Scala standard library and compiler. I'll have a look at the Style Guide once again (it's been a while since I looked at it) and try to stick to that for Spark then.\n3. Patrick McFadin: Imported from Github issue spark-53, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "f9a514a4b5a79cbbc7de68888aee7fcd", "issue_key": "SPARK-477", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Remove merged branches", "description": "It would be nice if branches that have already been merged were removed from the repository. This would help new contributors know which branches are worth looking at. Tags could also be created before deletion where relevant.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-30T20:38:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I've removed some old ones that I worked on now, but there are still 16 that I know are active or I need to ask people about. For people who want to check out recent activity, another way is to view https://github.com/mesos/spark/network.", "created": "2011-07-13T20:39:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Thanks!", "created": "2011-07-13T21:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-54, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-477\nSummary: Remove merged branches\nDescription: It would be nice if branches that have already been merged were removed from the repository. This would help new contributors know which branches are worth looking at. Tags could also be created before deletion where relevant.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: I've removed some old ones that I worked on now, but there are still 16 that I know are active or I need to ask people about. For people who want to check out recent activity, another way is to view https://github.com/mesos/spark/network.\n2. Patrick McFadin: Github comment from ijuma: Thanks!\n3. Patrick McFadin: Imported from Github issue spark-54, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "e07d430fcfd8e02d85e7de7db21db766", "issue_key": "SPARK-563", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Run findBugs and IDEA inspections in the codebase", "description": "I ran into a few instances of unused local variables and unnecessary usage of the 'return' keyword (the recommended practice is to avoid 'return' if possible) and thought it would be good to run findBugs and IDEA inspections to clean-up the code. I am willing to do this, but first would like to know whether you agree that this is a good idea and whether this is the right time to do it. These changes tend to affect many source files and can cause issues if there is major work ongoing in separate branches.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-31T03:46:00.000+0000", "updated": "2014-09-03T11:39:44.000+0000", "resolved": "2014-09-03T11:39:44.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Let's wait a bit on this. In the near future, I want to merge in some changes to make Spark run on top of the latest Mesos version (branch new-rdds-protobuf), finish the 2.9 changes, and merge in some improvements to broadcast and shuffle (branch mos-bt). I think this will all be done sometime next week. By the way, are you using IDEA to browse/develop Spark? Does it work OK? I might try it out.", "created": "2011-05-31T09:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Sounds good. Yes, I am using IDEA with SBT for all my development at the moment. Compilation is handled by SBT. I also use sbt-idea and idea-sbt-plugin. The former creates the project and the latter provides a console for hyperlinked errors inside IDEA. I think it works pretty well as far as Scala IDEs go.", "created": "2011-05-31T09:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I won't have time to do this for a couple of weeks, but I was curious about the status. Have the important branches been merged now?", "created": "2011-08-03T00:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-55, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Sean R. Owen", "body": "This appears to be obsolete/stale too.", "created": "2014-09-03T11:39:44.731+0000"}], "num_comments": 5, "text": "Issue: SPARK-563\nSummary: Run findBugs and IDEA inspections in the codebase\nDescription: I ran into a few instances of unused local variables and unnecessary usage of the 'return' keyword (the recommended practice is to avoid 'return' if possible) and thought it would be good to run findBugs and IDEA inspections to clean-up the code. I am willing to do this, but first would like to know whether you agree that this is a good idea and whether this is the right time to do it. These changes tend to affect many source files and can cause issues if there is major work ongoing in separate branches.\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Let's wait a bit on this. In the near future, I want to merge in some changes to make Spark run on top of the latest Mesos version (branch new-rdds-protobuf), finish the 2.9 changes, and merge in some improvements to broadcast and shuffle (branch mos-bt). I think this will all be done sometime next week. By the way, are you using IDEA to browse/develop Spark? Does it work OK? I might try it out.\n2. Patrick McFadin: Github comment from ijuma: Sounds good. Yes, I am using IDEA with SBT for all my development at the moment. Compilation is handled by SBT. I also use sbt-idea and idea-sbt-plugin. The former creates the project and the latter provides a console for hyperlinked errors inside IDEA. I think it works pretty well as far as Scala IDEs go.\n3. Patrick McFadin: Github comment from ijuma: I won't have time to do this for a couple of weeks, but I was curious about the status. Have the important branches been merged now?\n4. Patrick McFadin: Imported from Github issue spark-55, originally reported by ijuma\n5. Sean R. Owen: This appears to be obsolete/stale too.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "15c6504467c1c224dce485a014d94bde", "issue_key": "SPARK-476", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make SparkContext.runJob public", "description": "Is there a good reason to prevent clients from using it? It's useful to be able to implement methods that can schedule operations and work on the result provided by each split.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-05-31T10:03:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Sounds good, as a means to let people write their own parallel operations. I've committed it.", "created": "2011-06-01T10:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-56, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-476\nSummary: Make SparkContext.runJob public\nDescription: Is there a good reason to prevent clients from using it? It's useful to be able to implement methods that can schedule operations and work on the result provided by each split.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Sounds good, as a means to let people write their own parallel operations. I've committed it.\n2. Patrick McFadin: Imported from Github issue spark-56, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "a90b9e2701859193148b33127bdb5b53", "issue_key": "SPARK-475", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "LocalScheduler should catch Throwable to avoid silent failures", "description": "I was testing a job with the LocalScheduler and it consistently got stuck. After spending some time with the debugger, it became clear that tasks were being sent to LocalScheduler, but they would never finish. It turns out that Throwable subclasses (at one point NoClassDefFoundError and in another OutOfMemoryError) were being thrown and the thread would die silently. I suggest simply catching Throwable instead of Exception in this situation, since we just report the problem and then call System.exit. I also think we should remove the System.exit call, but that can be discussed in a separate issue.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-06-01T01:00:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-57, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-475\nSummary: LocalScheduler should catch Throwable to avoid silent failures\nDescription: I was testing a job with the LocalScheduler and it consistently got stuck. After spending some time with the debugger, it became clear that tasks were being sent to LocalScheduler, but they would never finish. It turns out that Throwable subclasses (at one point NoClassDefFoundError and in another OutOfMemoryError) were being thrown and the thread would die silently. I suggest simply catching Throwable instead of Exception in this situation, since we just report the problem and then call System.exit. I also think we should remove the System.exit call, but that can be discussed in a separate issue.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-57, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "56154680f751530e5fbb2eb164b0d9d7", "issue_key": "SPARK-474", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Remove unnecessary toStream calls", "description": "It could be that I am missing something, but it looks to me like the toStream calls are unnecessary. Used the scala-2.9 branch as I believe that will become master soon.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-06-01T07:23:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I'm pretty sure those were needed in 2.8, but it looks like they aren't in 2.9.", "created": "2011-06-01T10:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-58, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-474\nSummary: Remove unnecessary toStream calls\nDescription: It could be that I am missing something, but it looks to me like the toStream calls are unnecessary. Used the scala-2.9 branch as I believe that will become master soon.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: I'm pretty sure those were needed in 2.8, but it looks like they aren't in 2.9.\n2. Patrick McFadin: Imported from Github issue spark-58, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "ec238cc54d0f5e6d7cbf129c1f9600fb", "issue_key": "SPARK-473", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Move managedStyle to SparkProject", "description": "I had added it to DepJar by mistake. This is not urgent and it could be merged after the other work you have planned for the coming week. I just put it here as a heads-up.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-06-02T13:24:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for the fix.", "created": "2011-06-06T23:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-59, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-473\nSummary: Move managedStyle to SparkProject\nDescription: I had added it to DepJar by mistake. This is not urgent and it could be merged after the other work you have planned for the coming week. I just put it here as a heads-up.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks for the fix.\n2. Patrick McFadin: Imported from Github issue spark-59, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "feb445ee323641f183a0066c211b5572", "issue_key": "SPARK-562", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add a way to ship files with as Spark job", "description": "The new pipe() operation allows users to pass an RDD through an external shell command to enable the use of Python scripts, existing binaries, etc in Spark, but there should also be a way to ship the program to the working directory of the slave nodes as in Hadoop Streaming.", "reporter": "Matei Alexandru Zaharia", "assignee": "Denny Britz", "created": "0011-06-20T14:26:00.000+0000", "updated": "2013-01-20T12:38:36.000+0000", "resolved": "2013-01-20T12:38:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: Shark's issue number 7 ( https://github.com/amplab/shark/issues/7 ) actually depends on this. Hive uses Hadoop distributed cache to distribute external files (e.g. scripts) across the cluster.", "created": "2012-05-02T11:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-60, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "This was added in 0.6.0 via addFile() / addJar().", "created": "2013-01-20T12:38:36.839+0000"}], "num_comments": 3, "text": "Issue: SPARK-562\nSummary: Add a way to ship files with as Spark job\nDescription: The new pipe() operation allows users to pass an RDD through an external shell command to enable the use of Python scripts, existing binaries, etc in Spark, but there should also be a way to ship the program to the working directory of the slave nodes as in Hadoop Streaming.\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: Shark's issue number 7 ( https://github.com/amplab/shark/issues/7 ) actually depends on this. Hive uses Hadoop distributed cache to distribute external files (e.g. scripts) across the cluster.\n2. Patrick McFadin: Imported from Github issue spark-60, originally reported by mateiz\n3. Josh Rosen: This was added in 0.6.0 via addFile() / addJar().", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "f03507e1b3b199ae3bff619316cc373f", "issue_key": "SPARK-472", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Better readme", "description": "Various improvements to the README file to make it look better on github.", "reporter": "Olivier Grisel", "assignee": null, "created": "0011-06-22T16:29:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Olivier!", "created": "2011-06-23T09:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-61, originally reported by ogrisel", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-472\nSummary: Better readme\nDescription: Various improvements to the README file to make it look better on github.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Olivier!\n2. Patrick McFadin: Imported from Github issue spark-61, originally reported by ogrisel", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "2cdd81991b3d90820bb79fa45803d5e7", "issue_key": "SPARK-471", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add missing test for RDD.groupWith", "description": "Test for the CoGroup operation.", "reporter": "Olivier Grisel", "assignee": null, "created": "0011-06-22T16:30:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks!", "created": "2011-06-23T09:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-62, originally reported by ogrisel", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-471\nSummary: Add missing test for RDD.groupWith\nDescription: Test for the CoGroup operation.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks!\n2. Patrick McFadin: Imported from Github issue spark-62, originally reported by ogrisel", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "c3e9ab67ef040607a2ef64079e047025", "issue_key": "SPARK-470", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Implemented RDD.leftOuterJoin and RDD.rightOuterJoin", "description": "I decided to implement them from scratch on the `join` method model rather than filtering out the output of `groupWith` to avoid emitting intermediate results that are dropped right afterward. I think it's better to use the scala Option type to handle missing values rather that null values as in SQL or Pig, tell be if you don't aggree. *Note*: There is no scaladoc on those methods as they all lack such documentation in the RDD class. I really think this is missing though. Maybe the RDD should be fully documented at once in another pull request?", "reporter": "Olivier Grisel", "assignee": null, "created": "0011-06-24T01:06:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks. A docs patch for RDD would also be appreciated.", "created": "2011-06-24T11:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-63, originally reported by ogrisel", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-470\nSummary: Implemented RDD.leftOuterJoin and RDD.rightOuterJoin\nDescription: I decided to implement them from scratch on the `join` method model rather than filtering out the output of `groupWith` to avoid emitting intermediate results that are dropped right afterward. I think it's better to use the scala Option type to handle missing values rather that null values as in SQL or Pig, tell be if you don't aggree. *Note*: There is no scaladoc on those methods as they all lack such documentation in the RDD class. I really think this is missing though. Maybe the RDD should be fully documented at once in another pull request?\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks. A docs patch for RDD would also be appreciated.\n2. Patrick McFadin: Imported from Github issue spark-63, originally reported by ogrisel", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "915cb8b51f9aa8134d82a86b07bb47a2", "issue_key": "SPARK-469", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Functionality to save RDDs to Hadoop files", "description": "Overview of changes 1> Introduced class HadoopFileWriter that uses Hadoop's libraries (OutputFormat, OutputCommitter, etc) to save the records in a RDD, each split of the RDD being a file in the destination directory. While instantiating HadoopFileWriter, one can specify which classes to use for key, value, output-format and output-committer. The way it is written, all subclasses of outputformat and outputcommitter should be applicable. 2> Introduced following functions in RDD (a) RDD[T].saveAsTextFile(<path>): toString will be used to convert records (type T) to String and saved as a text file. (b) RDD[T].saveAsObjectFile(<path>): Each record, if serializable, will be converted to bytes and saved as serialized objects. (b) RDD[(K,V)].saveAsHadoopFile(<path>): Key-value pair RDDs will be saved as a Hadoop file using whatever OutputFormat and OutputCommitter class that the user specifies. This has a few different overloaded syntaxes. (c) RDD[(Writable, Writable)].saveAsSequenceFile(<path>): Key-value pair RDDs with keys and value being subclasses of Writable (like IntWritable) or can be implicitly converted to Writable (Int -> IntWritable, etc) can be saved as a Sequential File. 3> Other changes (a) SparkContext.objectFile(<path>): opens files that have been saved by saveAsObjectFile (b) A number of implicit functions to convert basic data types to its Writable counterparts (c) Introduced class TaskContext, that provides a running Task with the contextual information like stage ID, task ID. (d) Scheduler and Task classes changed to generate and populate TaskContext information", "reporter": "Tathagata Das", "assignee": null, "created": "0011-06-27T11:00:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ijuma: Did you test this when using s3n Paths?", "created": "2011-08-03T10:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tdas: No, I havent tested it using s3n paths. I have tested it only with local and hdfs paths. It would be great if some one is able to test s3n paths.", "created": "2011-08-03T12:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I tried it and ran into some issues during the set-up of OutputCommitter. I'll file an issue with more details if it hasn't been verified to work then.", "created": "2011-08-03T12:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tdas: Great, I will look into it once we have the details. Thanks!", "created": "2011-08-03T12:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I filed https://github.com/mesos/spark/issues/81", "created": "2011-08-19T03:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-64, originally reported by tdas", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-469\nSummary: Functionality to save RDDs to Hadoop files\nDescription: Overview of changes 1> Introduced class HadoopFileWriter that uses Hadoop's libraries (OutputFormat, OutputCommitter, etc) to save the records in a RDD, each split of the RDD being a file in the destination directory. While instantiating HadoopFileWriter, one can specify which classes to use for key, value, output-format and output-committer. The way it is written, all subclasses of outputformat and outputcommitter should be applicable. 2> Introduced following functions in RDD (a) RDD[T].saveAsTextFile(<path>): toString will be used to convert records (type T) to String and saved as a text file. (b) RDD[T].saveAsObjectFile(<path>): Each record, if serializable, will be converted to bytes and saved as serialized objects. (b) RDD[(K,V)].saveAsHadoopFile(<path>): Key-value pair RDDs will be saved as a Hadoop file using whatever OutputFormat and OutputCommitter class that the user specifies. This has a few different overloaded syntaxes. (c) RDD[(Writable, Writable)].saveAsSequenceFile(<path>): Key-value pair RDDs with keys and value being subclasses of Writable (like IntWritable) or can be implicitly converted to Writable (Int -> IntWritable, etc) can be saved as a Sequential File. 3> Other changes (a) SparkContext.objectFile(<path>): opens files that have been saved by saveAsObjectFile (b) A number of implicit functions to convert basic data types to its Writable counterparts (c) Introduced class TaskContext, that provides a running Task with the contextual information like stage ID, task ID. (d) Scheduler and Task classes changed to generate and populate TaskContext information\n\nComments (6):\n1. Patrick McFadin: Github comment from ijuma: Did you test this when using s3n Paths?\n2. Patrick McFadin: Github comment from tdas: No, I havent tested it using s3n paths. I have tested it only with local and hdfs paths. It would be great if some one is able to test s3n paths.\n3. Patrick McFadin: Github comment from ijuma: I tried it and ran into some issues during the set-up of OutputCommitter. I'll file an issue with more details if it hasn't been verified to work then.\n4. Patrick McFadin: Github comment from tdas: Great, I will look into it once we have the details. Thanks!\n5. Patrick McFadin: Github comment from ijuma: I filed https://github.com/mesos/spark/issues/81\n6. Patrick McFadin: Imported from Github issue spark-64, originally reported by tdas", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.939801"}}
{"id": "e3b42cf7c01c91362c246e2787bfe569", "issue_key": "SPARK-468", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Change @serializable to extends Serializable in 2.9 branch", "description": "Looks like @serializable is deprecated now. It may make sense to do this in the 2.8 branch too (importing java.util.Serializable) but we can also wait until we no longer have a 2.8 branch.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-06-27T21:28:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ijuma: This can be closed now (I don't have the privileges to do it myself).", "created": "2011-08-03T00:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I added you as a committer to the project now; hopefully that means you can close issues too.", "created": "2011-11-07T20:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Thanks!", "created": "2011-11-08T00:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-65, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-468\nSummary: Change @serializable to extends Serializable in 2.9 branch\nDescription: Looks like @serializable is deprecated now. It may make sense to do this in the 2.8 branch too (importing java.util.Serializable) but we can also wait until we no longer have a 2.8 branch.\n\nComments (4):\n1. Patrick McFadin: Github comment from ijuma: This can be closed now (I don't have the privileges to do it myself).\n2. Patrick McFadin: Github comment from mateiz: I added you as a committer to the project now; hopefully that means you can close issues too.\n3. Patrick McFadin: Github comment from ijuma: Thanks!\n4. Patrick McFadin: Imported from Github issue spark-65, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.941560"}}
{"id": "590feaa6004397d4df5724e4f6c64083", "issue_key": "SPARK-561", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Implement unit tests for RDD save functionality", "description": "The RDD save functionality has been introduced with a number of assumptions about how the Hadoop library class OutputFormat and OutputCommitter is used to create Hadoop files. There is a chance that some assumptions are flawed and/or have been overlooked. To make sure things dont break, unit tests should be present. Also, the functionality is expected to work with other custom OutputFormat and OutputCommiiter classes, along with custom JobConf parameters, but it has not been tested with any OutputFormat other than SequenceFileOutputFormat and FileOutputCommitter.", "reporter": "Tathagata Das", "assignee": null, "created": "0011-06-27T22:09:00.000+0000", "updated": "2012-10-22T15:10:58.000+0000", "resolved": "2012-10-22T15:10:58.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I've added some unit tests to check that files are created and can be read back, though nothing for failures. That's going to be harder to test without some way to instrument the LocalScheduler and make it fail tasks.", "created": "2011-07-13T20:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-66, originally reported by tdas", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-561\nSummary: Implement unit tests for RDD save functionality\nDescription: The RDD save functionality has been introduced with a number of assumptions about how the Hadoop library class OutputFormat and OutputCommitter is used to create Hadoop files. There is a chance that some assumptions are flawed and/or have been overlooked. To make sure things dont break, unit tests should be present. Also, the functionality is expected to work with other custom OutputFormat and OutputCommiiter classes, along with custom JobConf parameters, but it has not been tested with any OutputFormat other than SequenceFileOutputFormat and FileOutputCommitter.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: I've added some unit tests to check that files are created and can be read back, though nothing for failures. That's going to be harder to test without some way to instrument the LocalScheduler and make it fail tasks.\n2. Patrick McFadin: Imported from Github issue spark-66, originally reported by tdas", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.941560"}}
{"id": "8ad012368f21bfe5b3aa4654e59f0c14", "issue_key": "SPARK-560", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "Specialize RDDs / iterators", "description": "When you're working on in-memory data, the overhead of boxing / unboxing starts to matter, and it looks like specializing would give a 2-4x speedup. We can't just throw in @specialized though because Scala's Iterator is not specialized. We probably need to make our own and also ensure that the right methods get called remotely when you have a chain of RDDs (i.e. it doesn't \"lose\" its specialization).", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-07-04T19:14:00.000+0000", "updated": "2015-02-06T19:33:37.000+0000", "resolved": "2015-02-06T19:33:37.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ijuma: Note that there's a plan to add @specialized to many of Scala collections going forward.", "created": "2011-07-05T00:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-67, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Nicholas Chammas", "body": "[~matei], [~pwendell], [~rxin]: Is this issue still valid?", "created": "2015-02-06T19:30:01.857+0000"}, {"author": "Reynold Xin", "body": "We should close this one. It is much easier to do with DataFrame, and we will just make DataFrame optimized for this.", "created": "2015-02-06T19:33:27.045+0000"}], "num_comments": 4, "text": "Issue: SPARK-560\nSummary: Specialize RDDs / iterators\nDescription: When you're working on in-memory data, the overhead of boxing / unboxing starts to matter, and it looks like specializing would give a 2-4x speedup. We can't just throw in @specialized though because Scala's Iterator is not specialized. We probably need to make our own and also ensure that the right methods get called remotely when you have a chain of RDDs (i.e. it doesn't \"lose\" its specialization).\n\nComments (4):\n1. Patrick McFadin: Github comment from ijuma: Note that there's a plan to add @specialized to many of Scala collections going forward.\n2. Patrick McFadin: Imported from Github issue spark-67, originally reported by mateiz\n3. Nicholas Chammas: [~matei], [~pwendell], [~rxin]: Is this issue still valid?\n4. Reynold Xin: We should close this one. It is much easier to do with DataFrame, and we will just make DataFrame optimized for this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.941560"}}
{"id": "a26527161826b26b89d7d04f3973cc45", "issue_key": "SPARK-467", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add API for controlling the number of splits on a Hadoop file", "description": "Mostly useful when someone wants to increase the number of splits per block, since you must have at least one split per block right now.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-07-06T16:42:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is now fixed.", "created": "2011-07-14T10:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-68, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-467\nSummary: Add API for controlling the number of splits on a Hadoop file\nDescription: Mostly useful when someone wants to increase the number of splits per block, since you must have at least one split per block right now.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is now fixed.\n2. Patrick McFadin: Imported from Github issue spark-68, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.941560"}}
{"id": "78f4744b834d567596d56e7f984090f9", "issue_key": "SPARK-466", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Enable -optimize in the build", "description": "The last time I tried this, I ran into some issues. I'll try again soon and investigate the issues (if they still exist).", "reporter": "Ismael Juma", "assignee": null, "created": "0011-07-14T17:18:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Sounds good. I'm curious, have you seen it make a significant difference in performance?", "created": "2011-07-14T17:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: It can make a difference in some cases like http://dcsobral.blogspot.com/2011/05/scala-29-optimizes-for-comprehensions.html . Not as good as it could be though. In Scala trunk, Paul Phillips fixed a bug where -optimize would not inline private methods and that seems to help some too.", "created": "2011-07-14T17:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Seems like this is OK now as all tests pass for me. See: https://github.com/mesos/spark/pull/77", "created": "2011-08-02T01:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks, I've merged it in.", "created": "2011-08-02T14:49:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-69, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-466\nSummary: Enable -optimize in the build\nDescription: The last time I tried this, I ran into some issues. I'll try again soon and investigate the issues (if they still exist).\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Sounds good. I'm curious, have you seen it make a significant difference in performance?\n2. Patrick McFadin: Github comment from ijuma: It can make a difference in some cases like http://dcsobral.blogspot.com/2011/05/scala-29-optimizes-for-comprehensions.html . Not as good as it could be though. In Scala trunk, Paul Phillips fixed a bug where -optimize would not inline private methods and that seems to help some too.\n3. Patrick McFadin: Github comment from ijuma: Seems like this is OK now as all tests pass for me. See: https://github.com/mesos/spark/pull/77\n4. Patrick McFadin: Github comment from mateiz: Thanks, I've merged it in.\n5. Patrick McFadin: Imported from Github issue spark-69, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.941560"}}
{"id": "0effee933a2d015ef5f770d58c9a3b01", "issue_key": "SPARK-465", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Port SBT build to SBT 0.10", "description": "I have started this and should have something for review in the next few days.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-07-14T17:19:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ijuma: By the way, what is XmlTestReport used for? I use junit_xml_listener for Hudson/Jenkins and it's a possible replacement that doesn't require forking. [1] http://henkelmann.eu/2011/02/02/junit_xml_listener_now_with_support_for_skipped_tests", "created": "2011-07-14T17:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I've pushed a commit in a work-in-progress branch: https://github.com/ijuma/spark/commit/f686e3dacb02d42dd2bb9695a96cecd85786d7b5 Almost everything works. The two outstanding issues are sbt-assembly and XmlTestReport. I sent a message to the person who ported the former to SBT 0.10 asking for a release for SBT 0.10.1. Regarding XmlTestReport, waiting for an answer to comment #1", "created": "2011-07-14T18:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: dep-jar now works. Would it be possible to get an answer to the XmlTestReport question? That's the only thing left from a fully functional sbt 0.10.1 build.", "created": "2011-07-18T01:36:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I don't think the XmlTestReport is critical. It was there just to output data to Hudson, but if junit_xml_listener can do this too, it's fine to replace it with that.", "created": "2011-07-19T13:36:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Great, it should be easy then.", "created": "2011-07-20T01:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I pushed the latest changes. The branch is ready for testing and review: https://github.com/ijuma/spark/tree/sbt-0.10", "created": "2011-07-20T16:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I tried building this but got the following error when SBT tried to update itself. Is this a temporary thing due to a server being down? Getting org.scala-tools.sbt sbt_2.8.1 0.10.1 ... :: problems summary :: :::: WARNINGS [NOT FOUND ] commons-codec#commons-codec;1.2!commons-codec.jar (2ms) ==== Maven2 Local: tried file:///Users/matei/.m2/repository/commons-codec/commons-codec/1.2/commons-codec-1.2.jar download failed: commons-codec#commons-codec;1.2!commons-codec.jar Error during sbt execution: Error retrieving required libraries (see /Users/matei/workspace/spark/project/boot/update.log for complete log) Error: Could not retrieve sbt 0.10.1", "created": "2011-07-20T22:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I managed to fix this by removing my Ivy cache (~/.ivy2), so I guess it was something local to my machine (maybe some download cancelled earlier on). I'm going to merge this momentarily.", "created": "2011-07-23T11:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Actually, now it builds but I see another problem: ScalaTest fails on every test with something like the following: Could not run test spark.ParallelCollectionSplitSuite: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable It's not clear why this would happen, except perhaps for some mismatch between the ScalaTest API in Scala 2.8 and 2.9 (http://groups.google.com/group/scalatest-users/tree/browse_frm/month/2011-02?_done=%2Fgroup%2Fscalatest-users%2Fbrowse_frm%2Fmonth%2F2011-02%3F&) due to collection API changes. Is SBT trying to run the tests through the ScalaTest 2.8 API?", "created": "2011-07-23T12:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: If I run \"test\" in sbt shell on my branch, all tests pass, but my branch is based on the scala-2.9 branch. The ScalaTest version specified requires Scala 2.9.0. Maybe you merged the branch to master? That won't work without some additional work to make sure the dependencies work with Scala 2.8.1. My intent was for this to be Scala 2.9.x only.", "created": "2011-07-24T13:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I finally fixed the problem by doing a clean checkout -- there must've been some old JARs left behind that were compiled with Scala 2.8 and that SBT didn't decide to update. I've merged this into the repository now.", "created": "2011-07-29T15:09:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Thanks!", "created": "2011-07-30T02:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-70, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 13, "text": "Issue: SPARK-465\nSummary: Port SBT build to SBT 0.10\nDescription: I have started this and should have something for review in the next few days.\n\nComments (13):\n1. Patrick McFadin: Github comment from ijuma: By the way, what is XmlTestReport used for? I use junit_xml_listener for Hudson/Jenkins and it's a possible replacement that doesn't require forking. [1] http://henkelmann.eu/2011/02/02/junit_xml_listener_now_with_support_for_skipped_tests\n2. Patrick McFadin: Github comment from ijuma: I've pushed a commit in a work-in-progress branch: https://github.com/ijuma/spark/commit/f686e3dacb02d42dd2bb9695a96cecd85786d7b5 Almost everything works. The two outstanding issues are sbt-assembly and XmlTestReport. I sent a message to the person who ported the former to SBT 0.10 asking for a release for SBT 0.10.1. Regarding XmlTestReport, waiting for an answer to comment #1\n3. Patrick McFadin: Github comment from ijuma: dep-jar now works. Would it be possible to get an answer to the XmlTestReport question? That's the only thing left from a fully functional sbt 0.10.1 build.\n4. Patrick McFadin: Github comment from mateiz: I don't think the XmlTestReport is critical. It was there just to output data to Hudson, but if junit_xml_listener can do this too, it's fine to replace it with that.\n5. Patrick McFadin: Github comment from ijuma: Great, it should be easy then.\n6. Patrick McFadin: Github comment from ijuma: I pushed the latest changes. The branch is ready for testing and review: https://github.com/ijuma/spark/tree/sbt-0.10\n7. Patrick McFadin: Github comment from mateiz: I tried building this but got the following error when SBT tried to update itself. Is this a temporary thing due to a server being down? Getting org.scala-tools.sbt sbt_2.8.1 0.10.1 ... :: problems summary :: :::: WARNINGS [NOT FOUND ] commons-codec#commons-codec;1.2!commons-codec.jar (2ms) ==== Maven2 Local: tried file:///Users/matei/.m2/repository/commons-codec/commons-codec/1.2/commons-codec-1.2.jar download failed: commons-codec#commons-codec;1.2!commons-codec.jar Error during sbt execution: Error retrieving required libraries (see /Users/matei/workspace/spark/project/boot/update.log for complete log) Error: Could not retrieve sbt 0.10.1\n8. Patrick McFadin: Github comment from mateiz: I managed to fix this by removing my Ivy cache (~/.ivy2), so I guess it was something local to my machine (maybe some download cancelled earlier on). I'm going to merge this momentarily.\n9. Patrick McFadin: Github comment from mateiz: Actually, now it builds but I see another problem: ScalaTest fails on every test with something like the following: Could not run test spark.ParallelCollectionSplitSuite: java.lang.ClassCastException: scala.collection.immutable.Set$EmptySet$ cannot be cast to scala.collection.generic.Addable It's not clear why this would happen, except perhaps for some mismatch between the ScalaTest API in Scala 2.8 and 2.9 (http://groups.google.com/group/scalatest-users/tree/browse_frm/month/2011-02?_done=%2Fgroup%2Fscalatest-users%2Fbrowse_frm%2Fmonth%2F2011-02%3F&) due to collection API changes. Is SBT trying to run the tests through the ScalaTest 2.8 API?\n10. Patrick McFadin: Github comment from ijuma: If I run \"test\" in sbt shell on my branch, all tests pass, but my branch is based on the scala-2.9 branch. The ScalaTest version specified requires Scala 2.9.0. Maybe you merged the branch to master? That won't work without some additional work to make sure the dependencies work with Scala 2.8.1. My intent was for this to be Scala 2.9.x only.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.941560"}}
{"id": "9c6f7a9f5f4fa9b28270c839ab09fc01", "issue_key": "SPARK-464", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "ClosureCleaner fails when instantiating with outer", "description": "Reproduction in the repl: scala> class T { | def a(x:Int) = x.toString | sc.parallelize(Array(1,2,3), 2).map(v => a(v)) | } defined class T scala> class W extends T defined class W scala> new W java.lang.IllegalArgumentException: Can not set final $iwC field T.$outer to $iwC at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:146) at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:150) at sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.set(UnsafeQualifiedObjectFieldAccessorImpl.java:65) at java.lang.reflect.Field.set(Field.java:657) at spark.ClosureCleaner$.spark$ClosureCleaner$$instantiateClass(ClosureCleaner.scala:116) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:79) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:78) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61) at scala.collection.immut... scala> new T res6: T = T@57c52e72 scala> trait A { | def a(x:Int) = x.toString | sc.parallelize(Array(1,2,3), 2).map(v => a(v)) | } defined trait A scala> class B extends A defined class B scala> new B java.lang.InstantiationError: A at sun.reflect.GeneratedSerializationConstructorAccessor55.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at spark.ClosureCleaner$.spark$ClosureCleaner$$instantiateClass(ClosureCleaner.scala:111) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:79) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:78) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61) at scala.collection.immutable.List.foreach(List.scala:45) at spark.ClosureCleaner$.clean(ClosureCleaner.scala:78) at spark.SparkContext.clean(SparkContext.scala:252) at spark.RDD.map(RDD.scala:85) at A$class.$init$(<console>:13) at B.<init>(<console>:14) at <init>(<console>:18) at...", "reporter": "chrisx", "assignee": null, "created": "0011-07-19T14:00:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: The closure cleaner has a number of issues. Meanwhile, you can define the function a in an object, that should work.", "created": "2011-07-19T17:16:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Maybe there should be a way to disable the closure cleaner then (if there isn't already)?", "created": "2011-10-10T23:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: I think the first task should be to clarify the semantics of what is accepted in the closure and what is not. In particular in this case, the cleaner is right to refuse to accept a method to the calling class (the calling class should not be serialzied since it is usually assumed to be the singleton containing the main function), but the error report is non intuitive at best. If we want to allow *any* scala construct in the closure, we may hit some hard corner cases that may be hard to get to work (this is already the case with some of the bug reports I opened). Eventually, I think the closure cleaner should be replaced by a plugin to the scala compiler, so that the compilation fails if the closure is not proper, but this is another iterm. The workaround in the particular case above is to put the a() method in a separate singleton object.", "created": "2011-10-16T16:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I've closed this issue now with some fixes to the ClosureCleaner that make the sample code work, but the closure cleaner is definitely not a finished project. The core problem is that if you have a class C in which you create a closure that calls methods/fields of C, we need to pass the entire instance of class C to remote nodes along with your closure. Otherwise, we have no way to tell which fields the methods you call will use. Unfortunately, this can lead to more fields being passed along than are really needed (and potentially things like NotSerializableException). Really the way users should be encouraged to work around it is by defining local variables that copy the fields they care about. However, with this fix, they can also use those variables/methods directly as in the example above, provided they also make the class C serializable. This should hopefully be enough warning that the whole of C will be passed along...", "created": "2011-11-08T00:39:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Forgot to add: Commit 9e4c79a4d39b890b3eb55ffe8cef7d21eb31f0e6 also adds unit tests for the closure cleaner.", "created": "2011-11-08T00:40:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: Ideally, if someone has some time, a compiler plugin should at least be able to warn you that you are going to build a potentially unsafe closure.", "created": "2011-11-08T07:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-71, originally reported by chrisx", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-464\nSummary: ClosureCleaner fails when instantiating with outer\nDescription: Reproduction in the repl: scala> class T { | def a(x:Int) = x.toString | sc.parallelize(Array(1,2,3), 2).map(v => a(v)) | } defined class T scala> class W extends T defined class W scala> new W java.lang.IllegalArgumentException: Can not set final $iwC field T.$outer to $iwC at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:146) at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:150) at sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.set(UnsafeQualifiedObjectFieldAccessorImpl.java:65) at java.lang.reflect.Field.set(Field.java:657) at spark.ClosureCleaner$.spark$ClosureCleaner$$instantiateClass(ClosureCleaner.scala:116) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:79) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:78) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61) at scala.collection.immut... scala> new T res6: T = T@57c52e72 scala> trait A { | def a(x:Int) = x.toString | sc.parallelize(Array(1,2,3), 2).map(v => a(v)) | } defined trait A scala> class B extends A defined class B scala> new B java.lang.InstantiationError: A at sun.reflect.GeneratedSerializationConstructorAccessor55.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at spark.ClosureCleaner$.spark$ClosureCleaner$$instantiateClass(ClosureCleaner.scala:111) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:79) at spark.ClosureCleaner$$anonfun$clean$3.apply(ClosureCleaner.scala:78) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61) at scala.collection.immutable.List.foreach(List.scala:45) at spark.ClosureCleaner$.clean(ClosureCleaner.scala:78) at spark.SparkContext.clean(SparkContext.scala:252) at spark.RDD.map(RDD.scala:85) at A$class.$init$(<console>:13) at B.<init>(<console>:14) at <init>(<console>:18) at...\n\nComments (7):\n1. Patrick McFadin: Github comment from tjhunter: The closure cleaner has a number of issues. Meanwhile, you can define the function a in an object, that should work.\n2. Patrick McFadin: Github comment from ijuma: Maybe there should be a way to disable the closure cleaner then (if there isn't already)?\n3. Patrick McFadin: Github comment from tjhunter: I think the first task should be to clarify the semantics of what is accepted in the closure and what is not. In particular in this case, the cleaner is right to refuse to accept a method to the calling class (the calling class should not be serialzied since it is usually assumed to be the singleton containing the main function), but the error report is non intuitive at best. If we want to allow *any* scala construct in the closure, we may hit some hard corner cases that may be hard to get to work (this is already the case with some of the bug reports I opened). Eventually, I think the closure cleaner should be replaced by a plugin to the scala compiler, so that the compilation fails if the closure is not proper, but this is another iterm. The workaround in the particular case above is to put the a() method in a separate singleton object.\n4. Patrick McFadin: Github comment from mateiz: I've closed this issue now with some fixes to the ClosureCleaner that make the sample code work, but the closure cleaner is definitely not a finished project. The core problem is that if you have a class C in which you create a closure that calls methods/fields of C, we need to pass the entire instance of class C to remote nodes along with your closure. Otherwise, we have no way to tell which fields the methods you call will use. Unfortunately, this can lead to more fields being passed along than are really needed (and potentially things like NotSerializableException). Really the way users should be encouraged to work around it is by defining local variables that copy the fields they care about. However, with this fix, they can also use those variables/methods directly as in the example above, provided they also make the class C serializable. This should hopefully be enough warning that the whole of C will be passed along...\n5. Patrick McFadin: Github comment from mateiz: Forgot to add: Commit 9e4c79a4d39b890b3eb55ffe8cef7d21eb31f0e6 also adds unit tests for the closure cleaner.\n6. Patrick McFadin: Github comment from tjhunter: Ideally, if someone has some time, a compiler plugin should at least be able to warn you that you are going to build a potentially unsafe closure.\n7. Patrick McFadin: Imported from Github issue spark-71, originally reported by chrisx", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.941560"}}
{"id": "2a2298c758f4449c996948bf065fd18b", "issue_key": "SPARK-463", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Kryo serializer sometimes fails to use no-argument constructor", "description": "For example, if you serialize a HashSet, our modified Kryo serializer will not call HashSet's no-arg constructor, leading to a NullPointerException.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-07-21T14:31:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-72, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-463\nSummary: Kryo serializer sometimes fails to use no-argument constructor\nDescription: For example, if you serialize a HashSet, our modified Kryo serializer will not call HashSet's no-arg constructor, leading to a NullPointerException.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-72, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.943570"}}
{"id": "913e935e161fb0f37a5e0424b86a7be0", "issue_key": "SPARK-559", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "Automatically register all classes used in fields of a class with Kryo", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-07-21T14:32:00.000+0000", "updated": "2014-09-22T01:41:58.000+0000", "resolved": "2014-09-21T15:36:54.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: We should close this ticket since we made kryo registration optional by default in this commit: https://github.com/mesos/spark/commit/968f75f6afc1383692f4f33d6a1a5f8ce2ac951d When we upgrade to Kryo 2.x, registration will also be automatic.", "created": "2012-05-02T12:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-73, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matthew Farrellee", "body": "the last comment on this, from 2 years ago, suggest this is resolved w/ an upgrade to kryo 2.x. i'm going to close this, but please re-open if you disagree.", "created": "2014-09-21T15:36:38.784+0000"}, {"author": "Andrew Ash", "body": "As of today in master we're using Twitter Chill version 0.3.6 which includes Kryo 2.21, so we are on the 2.x branch now", "created": "2014-09-22T01:41:58.082+0000"}], "num_comments": 4, "text": "Issue: SPARK-559\nSummary: Automatically register all classes used in fields of a class with Kryo\n\nComments (4):\n1. Patrick McFadin: Github comment from rxin: We should close this ticket since we made kryo registration optional by default in this commit: https://github.com/mesos/spark/commit/968f75f6afc1383692f4f33d6a1a5f8ce2ac951d When we upgrade to Kryo 2.x, registration will also be automatic.\n2. Patrick McFadin: Imported from Github issue spark-73, originally reported by mateiz\n3. Matthew Farrellee: the last comment on this, from 2 years ago, suggest this is resolved w/ an upgrade to kryo 2.x. i'm going to close this, but please re-open if you disagree.\n4. Andrew Ash: As of today in master we're using Twitter Chill version 0.3.6 which includes Kryo 2.21, so we are on the 2.x branch now", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.943570"}}
{"id": "65af331e474d23ccba36654dfb41cafa", "issue_key": "SPARK-462", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add APIs to Serializer for streams of objects of the same type", "description": "This could be a substantial optimization for faster serializers, because they wouldn't need to encode (and later decode) the class of each object in the stream repeatedly.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-07-22T23:47:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Done in dev branch.", "created": "2012-06-09T13:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-74, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-462\nSummary: Add APIs to Serializer for streams of objects of the same type\nDescription: This could be a substantial optimization for faster serializers, because they wouldn't need to encode (and later decode) the class of each object in the stream repeatedly.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Done in dev branch.\n2. Patrick McFadin: Imported from Github issue spark-74, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.943570"}}
{"id": "9e7e8481ca6a1243ba3e3f03a9846da6", "issue_key": "SPARK-461", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Workaround for scalac bug and publishTo configuration", "description": "Both commits are simple and the commit message for each explain what they're about.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-07-31T11:21:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: OK, merged. I've also merged the Scala 2.9 branch into master now.", "created": "2011-08-01T14:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Thanks!", "created": "2011-08-01T14:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-75, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-461\nSummary: Workaround for scalac bug and publishTo configuration\nDescription: Both commits are simple and the commit message for each explain what they're about.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: OK, merged. I've also merged the Scala 2.9 branch into master now.\n2. Patrick McFadin: Github comment from ijuma: Thanks!\n3. Patrick McFadin: Imported from Github issue spark-75, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.943570"}}
{"id": "b7c30fb2f0f3fcea8316d67cc4f5699a", "issue_key": "SPARK-460", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fix code not to use code deprecated in Scala 2.9", "description": "The commits are self-explanatory.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-08-02T01:26:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Awesome, thanks a lot for taking the time to do this!", "created": "2011-08-02T14:47:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-76, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-460\nSummary: Fix code not to use code deprecated in Scala 2.9\nDescription: The commits are self-explanatory.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Awesome, thanks a lot for taking the time to do this!\n2. Patrick McFadin: Imported from Github issue spark-76, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.943570"}}
{"id": "238fda6f36f63663d0d924cd5b034f7b", "issue_key": "SPARK-459", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Enable -optimize in the build", "description": "The commit is self-explanatory. This includes the deprecation fixes, so that pull request should be merged first.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-08-02T01:29:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-77, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-459\nSummary: Enable -optimize in the build\nDescription: The commit is self-explanatory. This includes the deprecation fixes, so that pull request should be merged first.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-77, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.945079"}}
{"id": "1242895bd40534c0b6ae84d1cc26d781", "issue_key": "SPARK-458", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Replace DepJar with sbt-assembly plugin", "description": "With recent changes to the sbt-assembly plugin we can just use it instead of having our own DepJar class. See: https://github.com/eed3si9n/sbt-assembly/issues/4#issuecomment-1697831", "reporter": "Ismael Juma", "assignee": null, "created": "0011-08-02T02:50:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Done.", "created": "2011-08-29T15:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-78, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-458\nSummary: Replace DepJar with sbt-assembly plugin\nDescription: With recent changes to the sbt-assembly plugin we can just use it instead of having our own DepJar class. See: https://github.com/eed3si9n/sbt-assembly/issues/4#issuecomment-1697831\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Done.\n2. Patrick McFadin: Imported from Github issue spark-78, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.945079"}}
{"id": "a724aaebc175f87331e07abe107fbaa9", "issue_key": "SPARK-558", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Simplify run script by relying on sbt to launch app", "description": "The run script replicates SBT's functionality in order to build the classpath. This could be avoided by creating a task in sbt that is responsible for calling the appropriate main method, configuring the environment variables from the script and then invoking sbt with the task name and arguments. Is there a reason why we should not do this?", "reporter": "Ismael Juma", "assignee": null, "created": "0011-08-02T05:15:00.000+0000", "updated": "2014-09-12T08:04:58.000+0000", "resolved": "2014-09-12T08:04:58.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This sounds OK to me; my only concern is whether SBT will add a substantial amount of memory usage or startup time, or somehow mess up the classloaders and the classpath. Do you know in more detail what it does when you run a class?", "created": "2011-08-02T14:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Fair points. SBT can either launch the app in the same process (like it does with tests) or it can fork. The latter is safer, but the original SBT process will continue running (and taking memory) until the forked process ends. Maybe we could configure the SBT JVM with a very small heap to minimise that. Still there will always be some overhead, so maybe the current approach is the best compromise even if a bit ugly.", "created": "2011-08-02T23:08:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: An alternative would be to use SBT to simply generate the classpath. A modified version of the following code by Mark Harrah: TaskKey[Unit](\"mkrun\") <<= (fullClasspath in Runtime, mainClass in (Compile,run), baseDirectory) map { (cp: Classpath, main: Option[String], base: File) => val mc = main.getOrElse(\"No main class specified\") val script = \"java -cp '\" + cp.files.absString + \"' \" + mc val out = base / \"run\" IO.write(out, script) out.setExecutable(true) } https://groups.google.com/d/msg/simple-build-tool/TEOjVzOwFOE/M5ggCPqjUFsJ", "created": "2011-09-04T09:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: That sounds good actually. Instead of writing the \"run\" script directly, we could also write a special file called \"classpath\", so that there can be other stuff in the script (e.g. the code that reads spark-env.sh.", "created": "2011-09-07T18:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I intend to have a look at this once SBT 0.11.0 is out (should be soon).", "created": "2011-09-23T07:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Ismael, I'm still interested in getting this to work, as the run script is pretty unwieldy. Is there a feature for it in SBT 0.11?", "created": "2011-11-02T09:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: There is no specific feature, but the approach we discussed should work. I am swamped, unfortunately, so won't be able to look at this before next month. By the way, SBT 0.11.1 was released today and it has vastly improved memory usage characteristics.", "created": "2011-11-07T10:08:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: No worries; I'll try to implement it myself.", "created": "2011-11-07T19:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-79, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Sean R. Owen", "body": "Is this stale too? given that SBT is less used, I don't imagine the run scripts will start relying on it for classpath generation.", "created": "2014-09-11T08:57:13.674+0000"}, {"author": "Ismael Juma", "body": "Probably stale, yes.", "created": "2014-09-11T08:59:10.310+0000"}], "num_comments": 11, "text": "Issue: SPARK-558\nSummary: Simplify run script by relying on sbt to launch app\nDescription: The run script replicates SBT's functionality in order to build the classpath. This could be avoided by creating a task in sbt that is responsible for calling the appropriate main method, configuring the environment variables from the script and then invoking sbt with the task name and arguments. Is there a reason why we should not do this?\n\nComments (11):\n1. Patrick McFadin: Github comment from mateiz: This sounds OK to me; my only concern is whether SBT will add a substantial amount of memory usage or startup time, or somehow mess up the classloaders and the classpath. Do you know in more detail what it does when you run a class?\n2. Patrick McFadin: Github comment from ijuma: Fair points. SBT can either launch the app in the same process (like it does with tests) or it can fork. The latter is safer, but the original SBT process will continue running (and taking memory) until the forked process ends. Maybe we could configure the SBT JVM with a very small heap to minimise that. Still there will always be some overhead, so maybe the current approach is the best compromise even if a bit ugly.\n3. Patrick McFadin: Github comment from ijuma: An alternative would be to use SBT to simply generate the classpath. A modified version of the following code by Mark Harrah: TaskKey[Unit](\"mkrun\") <<= (fullClasspath in Runtime, mainClass in (Compile,run), baseDirectory) map { (cp: Classpath, main: Option[String], base: File) => val mc = main.getOrElse(\"No main class specified\") val script = \"java -cp '\" + cp.files.absString + \"' \" + mc val out = base / \"run\" IO.write(out, script) out.setExecutable(true) } https://groups.google.com/d/msg/simple-build-tool/TEOjVzOwFOE/M5ggCPqjUFsJ\n4. Patrick McFadin: Github comment from mateiz: That sounds good actually. Instead of writing the \"run\" script directly, we could also write a special file called \"classpath\", so that there can be other stuff in the script (e.g. the code that reads spark-env.sh.\n5. Patrick McFadin: Github comment from ijuma: I intend to have a look at this once SBT 0.11.0 is out (should be soon).\n6. Patrick McFadin: Github comment from mateiz: Hey Ismael, I'm still interested in getting this to work, as the run script is pretty unwieldy. Is there a feature for it in SBT 0.11?\n7. Patrick McFadin: Github comment from ijuma: There is no specific feature, but the approach we discussed should work. I am swamped, unfortunately, so won't be able to look at this before next month. By the way, SBT 0.11.1 was released today and it has vastly improved memory usage characteristics.\n8. Patrick McFadin: Github comment from mateiz: No worries; I'll try to implement it myself.\n9. Patrick McFadin: Imported from Github issue spark-79, originally reported by ijuma\n10. Sean R. Owen: Is this stale too? given that SBT is less used, I don't imagine the run scripts will start relying on it for classpath generation.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.945079"}}
{"id": "76dd9f4b0b8ff20638762f014b72118b", "issue_key": "SPARK-457", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Delete scala-2.9 branch", "description": "The scala-2.9 branch has now been fully merged and can be deleted from the repository.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-08-03T00:31:00.000+0000", "updated": "2012-10-19T22:50:22.000+0000", "resolved": "2012-10-19T22:50:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Done.", "created": "2011-08-20T07:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-80, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-457\nSummary: Delete scala-2.9 branch\nDescription: The scala-2.9 branch has now been fully merged and can be deleted from the repository.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Done.\n2. Patrick McFadin: Imported from Github issue spark-80, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.945079"}}
{"id": "1098bd348b5b0b478cd47469506ed9f5", "issue_key": "SPARK-456", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Save RDDs should work for s3n paths", "description": "I tried a few variations and I could not get this to work. And since this was not tested (as mentioned here https://github.com/mesos/spark/pull/64#issuecomment-1721647), it's possible that it's a bug indeed. When I use a path like s3n://<access_key>:<secret_key>@<bucket_name>, I get a IllegalArgumentException caused by a java.net.URISyntaxException[1]. If I instead use a path like s3n://<access_key>:<secret_key>@<host>/<bucket_name>, I get a org.apache.hadoop.fs.s3.S3Exception caused by a org.jets3t.service.S3ServiceException[2]. You should be able to reproduce this by simply changing your test case to output to s3n instead of HDFS. Let me know if you have questions. [1] Exception in thread \"main\" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: s3n://access_key:secret_key@bucket_name_temporary at org.apache.hadoop.fs.Path.initialize(Path.java:148) at org.apache.hadoop.fs.Path.<init>(Path.java:71) at org.apache.hadoop.fs.Path.<init>(Path.java:50) at org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:50) at org.apache.hadoop.mapred.HadoopWriter.preSetup(HadoopWriter.scala:44) at spark.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:243) [...] Caused by: java.net.URISyntaxException: Relative path in absolute URI: s3n://access_key:secret_key@bucket_name_temporary at java.net.URI.checkPath(URI.java:1802) at java.net.URI.<init>(URI.java:752) at org.apache.hadoop.fs.Path.initialize(Path.java:145) ... 14 more [2] Exception in thread \"main\" org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: Request Error. HEAD '/bucket_name' on Host 's3-eu-west-1.amazonaws.com.s3.amazonaws.com' @ 'Fri, 19 Aug 2011 11:03:22 GMT' -- ResponseCode: 404, ResponseStatus: Not Found, RequestId: 000B648BEF2C9519, HostId: 6ROVsyivgLBAceNLaoj5sv8uOvx9uY33RdsXeKoOuNCUTbY0QaM8yFB+cSojps/k at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.handleServiceException(Jets3tNativeFileSystemStore.java:229) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:111) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59) at org.apache.hadoop.fs.s3native.$Proxy0.retrieveMetadata(Unknown Source) at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:392) at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdir(NativeS3FileSystem.java:505) at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdirs(NativeS3FileSystem.java:498) at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1226) at org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:52) at org.apache.hadoop.mapred.HadoopWriter.preSetup(HadoopWriter.scala:44) at spark.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:243) [...] Caused by: org.jets3t.service.S3ServiceException: Request Error. HEAD '/bucket_name' on Host 's3-eu-west-1.amazonaws.com.s3.amazonaws.com' @ 'Fri, 19 Aug 2011 11:03:22 GMT' -- ResponseCode: 404, ResponseStatus: Not Found, RequestId: 000B648BEF2C9519, HostId: 6ROVsyivgLBAceNLaoj5sv8uOvx9uY33RdsXeKoOuNCUTbY0QaM8yFB+cSojps/k at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:520) at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestHead(RestS3Service.java:868) at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:2016) at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectDetailsImpl(RestS3Service.java:1944) at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:3059) at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1940) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:103) ... 23 more Caused by: org.jets3t.service.impl.rest.HttpException at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:518) ... 29 more", "reporter": "Ismael Juma", "assignee": null, "created": "0011-08-19T03:09:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I haven't had a chance to test this yet, but here are a couple of pointers: 1) According to https://issues.apache.org/jira/browse/HADOOP-5805, you might need to use a subdirectory rather than a top-level bucket. In particular, Hadoop's S3 library won't create the bucket if it doesn't exist. 2) I'm not sure whether passing your credentials with @ is supposed to work in S3N URLs (it might well work), but if it doesn't, you can also try the versions of the save() method that take a JobConf and set your credentials in there through fs.s3.awsAccessKeyId and fs.s3.awsSecretAccessKey. Check out http://wiki.apache.org/hadoop/AmazonS3.", "created": "2011-08-20T07:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Hi Matei. The bucket does exist and the syntax I used works fine for InputFormat. I will try to use a subfolder, but from a brief investigation it seemed the usage of OutputCommitter was the issue.", "created": "2011-08-20T07:34:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: I had a quick go at using a subfolder and that didn't help.", "created": "2011-08-20T09:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: OK, I'll try to take a look at this, but I'm not sure I'll have time in the next couple of days due to some other stuff I need to finish.", "created": "2011-08-20T13:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Sure, there's no rush. I had written some code to upload to S3 before the OutputFormat code went in and I can continue to use that until you have a chance to look at this.", "created": "2011-08-20T13:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Any progress on this?", "created": "2011-09-23T07:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Sorry, haven't had a chance to look at it yet. I'll try to do that this weekend.", "created": "2011-09-23T14:07:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hi Ismael, Sorry for the delay on this, but I think I figured out the problem. Saving with s3n:// URIs works fine unless your access key ID or secret access key contain a \"/\" character, in which case you get the error you had above. This is because the S3 code in Hadoop tries to get a hostname from the URI and fails (presumably passwords encoded into URLs in the :@ format can't have slashes). I figured this out by creating new access keys until I got one without a slash in it. One way around this is to use the saveAsHadoopFile method and pass it a JobConf, but that's really ugly. For example: val data = sc.parallelize(1 to 10) val conf = new JobConf conf.set(\"fs.s3n.awsAccessKeyId\", \"XXX\") conf.set(\"fs.s3n.awsSecretAccessKey\", \"YYY\") val mapped = data.map(x => (NullWritable.get(), new Text(x.toString))) mapped.saveAsHadoopFile(\"s3n://matei-test/foo\", classOf[NullWritable], classOf[Text], classOf[TextOutputFormat[_, _]], conf) I'm also thinking of two other ways around it: * Give all the saveAsX() methods an optional JobConf parameter. * Give SparkContext's constructor an optional JobConf parameter, which will be used for all save operations in it. Any preferences?", "created": "2011-10-11T15:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: Thanks for looking into this and for the workaround. Good news that it's easy to solve. It's a bit weird because I also went through the same process of generating keys until I got one with no '/' in it. If it works for you though, then it looks like I probably had another issue in my code. About your question, I prefer the option of passing the optional JobConf to saveAsX methods.", "created": "2011-10-11T23:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Maybe you had some other weird character in the new keys that was causing a similar problem. I just know that mine worked eventually. In any case, I'll add those JobConf parameters and write somewhere that that is the preferred way to use S3.", "created": "2011-10-12T13:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-81, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 11, "text": "Issue: SPARK-456\nSummary: Save RDDs should work for s3n paths\nDescription: I tried a few variations and I could not get this to work. And since this was not tested (as mentioned here https://github.com/mesos/spark/pull/64#issuecomment-1721647), it's possible that it's a bug indeed. When I use a path like s3n://<access_key>:<secret_key>@<bucket_name>, I get a IllegalArgumentException caused by a java.net.URISyntaxException[1]. If I instead use a path like s3n://<access_key>:<secret_key>@<host>/<bucket_name>, I get a org.apache.hadoop.fs.s3.S3Exception caused by a org.jets3t.service.S3ServiceException[2]. You should be able to reproduce this by simply changing your test case to output to s3n instead of HDFS. Let me know if you have questions. [1] Exception in thread \"main\" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: s3n://access_key:secret_key@bucket_name_temporary at org.apache.hadoop.fs.Path.initialize(Path.java:148) at org.apache.hadoop.fs.Path.<init>(Path.java:71) at org.apache.hadoop.fs.Path.<init>(Path.java:50) at org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:50) at org.apache.hadoop.mapred.HadoopWriter.preSetup(HadoopWriter.scala:44) at spark.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:243) [...] Caused by: java.net.URISyntaxException: Relative path in absolute URI: s3n://access_key:secret_key@bucket_name_temporary at java.net.URI.checkPath(URI.java:1802) at java.net.URI.<init>(URI.java:752) at org.apache.hadoop.fs.Path.initialize(Path.java:145) ... 14 more [2] Exception in thread \"main\" org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: Request Error. HEAD '/bucket_name' on Host 's3-eu-west-1.amazonaws.com.s3.amazonaws.com' @ 'Fri, 19 Aug 2011 11:03:22 GMT' -- ResponseCode: 404, ResponseStatus: Not Found, RequestId: 000B648BEF2C9519, HostId: 6ROVsyivgLBAceNLaoj5sv8uOvx9uY33RdsXeKoOuNCUTbY0QaM8yFB+cSojps/k at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.handleServiceException(Jets3tNativeFileSystemStore.java:229) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:111) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59) at org.apache.hadoop.fs.s3native.$Proxy0.retrieveMetadata(Unknown Source) at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:392) at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdir(NativeS3FileSystem.java:505) at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdirs(NativeS3FileSystem.java:498) at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1226) at org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:52) at org.apache.hadoop.mapred.HadoopWriter.preSetup(HadoopWriter.scala:44) at spark.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:243) [...] Caused by: org.jets3t.service.S3ServiceException: Request Error. HEAD '/bucket_name' on Host 's3-eu-west-1.amazonaws.com.s3.amazonaws.com' @ 'Fri, 19 Aug 2011 11:03:22 GMT' -- ResponseCode: 404, ResponseStatus: Not Found, RequestId: 000B648BEF2C9519, HostId: 6ROVsyivgLBAceNLaoj5sv8uOvx9uY33RdsXeKoOuNCUTbY0QaM8yFB+cSojps/k at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:520) at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestHead(RestS3Service.java:868) at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:2016) at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectDetailsImpl(RestS3Service.java:1944) at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:3059) at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1940) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:103) ... 23 more Caused by: org.jets3t.service.impl.rest.HttpException at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:518) ... 29 more\n\nComments (11):\n1. Patrick McFadin: Github comment from mateiz: I haven't had a chance to test this yet, but here are a couple of pointers: 1) According to https://issues.apache.org/jira/browse/HADOOP-5805, you might need to use a subdirectory rather than a top-level bucket. In particular, Hadoop's S3 library won't create the bucket if it doesn't exist. 2) I'm not sure whether passing your credentials with @ is supposed to work in S3N URLs (it might well work), but if it doesn't, you can also try the versions of the save() method that take a JobConf and set your credentials in there through fs.s3.awsAccessKeyId and fs.s3.awsSecretAccessKey. Check out http://wiki.apache.org/hadoop/AmazonS3.\n2. Patrick McFadin: Github comment from ijuma: Hi Matei. The bucket does exist and the syntax I used works fine for InputFormat. I will try to use a subfolder, but from a brief investigation it seemed the usage of OutputCommitter was the issue.\n3. Patrick McFadin: Github comment from ijuma: I had a quick go at using a subfolder and that didn't help.\n4. Patrick McFadin: Github comment from mateiz: OK, I'll try to take a look at this, but I'm not sure I'll have time in the next couple of days due to some other stuff I need to finish.\n5. Patrick McFadin: Github comment from ijuma: Sure, there's no rush. I had written some code to upload to S3 before the OutputFormat code went in and I can continue to use that until you have a chance to look at this.\n6. Patrick McFadin: Github comment from ijuma: Any progress on this?\n7. Patrick McFadin: Github comment from mateiz: Sorry, haven't had a chance to look at it yet. I'll try to do that this weekend.\n8. Patrick McFadin: Github comment from mateiz: Hi Ismael, Sorry for the delay on this, but I think I figured out the problem. Saving with s3n:// URIs works fine unless your access key ID or secret access key contain a \"/\" character, in which case you get the error you had above. This is because the S3 code in Hadoop tries to get a hostname from the URI and fails (presumably passwords encoded into URLs in the :@ format can't have slashes). I figured this out by creating new access keys until I got one without a slash in it. One way around this is to use the saveAsHadoopFile method and pass it a JobConf, but that's really ugly. For example: val data = sc.parallelize(1 to 10) val conf = new JobConf conf.set(\"fs.s3n.awsAccessKeyId\", \"XXX\") conf.set(\"fs.s3n.awsSecretAccessKey\", \"YYY\") val mapped = data.map(x => (NullWritable.get(), new Text(x.toString))) mapped.saveAsHadoopFile(\"s3n://matei-test/foo\", classOf[NullWritable], classOf[Text], classOf[TextOutputFormat[_, _]], conf) I'm also thinking of two other ways around it: * Give all the saveAsX() methods an optional JobConf parameter. * Give SparkContext's constructor an optional JobConf parameter, which will be used for all save operations in it. Any preferences?\n9. Patrick McFadin: Github comment from ijuma: Thanks for looking into this and for the workaround. Good news that it's easy to solve. It's a bit weird because I also went through the same process of generating keys until I got one with no '/' in it. If it works for you though, then it looks like I probably had another issue in my code. About your question, I prefer the option of passing the optional JobConf to saveAsX methods.\n10. Patrick McFadin: Github comment from mateiz: Maybe you had some other weird character in the new keys that was causing a similar problem. I just know that mine worked eventually. In any case, I'll add those JobConf parameters and write somewhere that that is the preferred way to use S3.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.946169"}}
{"id": "807321de7515a00169fc4048059c389d", "issue_key": "SPARK-455", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Upgrade to Scala 2.9.1.", "description": "Scala 2.9.1 has been released and it includes a lot of improvements, including faster compilation: http://www.scala-lang.org/node/10780 A couple of small changes in the REPL support were needed in addition to the changes in the build file and run script. Please review them to see if they're correct. All tests pass and ./spark-shell launches successfully.", "reporter": "Ismael Juma", "assignee": null, "created": "0011-08-31T02:10:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2011-08-31T10:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-82, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-455\nSummary: Upgrade to Scala 2.9.1.\nDescription: Scala 2.9.1 has been released and it includes a lot of improvements, including faster compilation: http://www.scala-lang.org/node/10780 A couple of small changes in the REPL support were needed in addition to the changes in the build file and run script. Please review them to see if they're correct. All tests pass and ./spark-shell launches successfully.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n2. Patrick McFadin: Imported from Github issue spark-82, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.946169"}}
{"id": "4012faaac015349ffcb4a35265bfe8f0", "issue_key": "SPARK-454", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Upgrade to SBT 0.11.0", "description": "Upgrade to SBT 0.11.0 and update a couple of dependencies (Jetty and compress-lzf). SBT 0.11.0 was released recently and it includes a number of improvements: Release announcement: https://groups.google.com/d/topic/simple-build-tool/PiRxuWiuyic/discussion Changes: http://implicit.ly/simple-build-tool-0110", "reporter": "Ismael Juma", "assignee": null, "created": "0011-09-27T14:18:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for the patch!", "created": "2011-09-30T20:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-83, originally reported by ijuma", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-454\nSummary: Upgrade to SBT 0.11.0\nDescription: Upgrade to SBT 0.11.0 and update a couple of dependencies (Jetty and compress-lzf). SBT 0.11.0 was released recently and it includes a number of improvements: Release announcement: https://groups.google.com/d/topic/simple-build-tool/PiRxuWiuyic/discussion Changes: http://implicit.ly/simple-build-tool-0110\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks for the patch!\n2. Patrick McFadin: Imported from Github issue spark-83, originally reported by ijuma", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.946169"}}
{"id": "729bb976f40fba4219dbc43939d9ae57", "issue_key": "SPARK-453", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Report exceptions in tasks back to master to simplify debugging", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-10-14T09:50:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: +1", "created": "2011-10-16T16:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed now.", "created": "2012-06-09T13:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-84, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-453\nSummary: Report exceptions in tasks back to master to simplify debugging\n\nComments (3):\n1. Patrick McFadin: Github comment from tjhunter: +1\n2. Patrick McFadin: Github comment from mateiz: This is fixed now.\n3. Patrick McFadin: Imported from Github issue spark-84, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.946169"}}
{"id": "084a8577f015f93b672992e35cc9f288", "issue_key": "SPARK-452", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Wrong documentation for Mesos+Spark integration", "description": "It's not possible to build Mesos for Spark using instruction from https://github.com/mesos/spark/wiki/Running-spark-on-mesos Particularly, that ```git checkout -b pre-protobuf --track origin/pre-protobuf``` results in ```fatal: Not a git repository (or any of the parent directories): .git```", "reporter": "Yury Litvinov", "assignee": null, "created": "0011-10-17T04:03:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for reporting this. I think the issue is that you must do `cd mesos` to move into the directory you cloned into before calling `git checkout`. Try that out.", "created": "2011-10-17T11:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way, those instructions will only work for release 0.3 and earlier (the versions of Spark you can download from spark-project.org). If you want to work with the master branch of Spark, you will need to use the master branch of Mesos (so just don't do the `git checkout`, but do the `git clone`).", "created": "2011-10-17T11:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ylitvinov: Thank you. I've tried ```git checkout -b pre-protobuf --track origin/pre-protobuf``` and it resulted in ```Branch pre-protobuf set up to track remote branch pre-protobuf from origin. Switched to a new branch 'pre-protobuf'``` is it expected output?", "created": "2011-10-18T00:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Yes, that looks good. You should be able to build Mesos now.", "created": "2011-10-18T07:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I'm going to close this issue because I fixed the docs for this.", "created": "2011-11-08T00:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-85, originally reported by ylitvinov", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-452\nSummary: Wrong documentation for Mesos+Spark integration\nDescription: It's not possible to build Mesos for Spark using instruction from https://github.com/mesos/spark/wiki/Running-spark-on-mesos Particularly, that ```git checkout -b pre-protobuf --track origin/pre-protobuf``` results in ```fatal: Not a git repository (or any of the parent directories): .git```\n\nComments (6):\n1. Patrick McFadin: Github comment from mateiz: Thanks for reporting this. I think the issue is that you must do `cd mesos` to move into the directory you cloned into before calling `git checkout`. Try that out.\n2. Patrick McFadin: Github comment from mateiz: By the way, those instructions will only work for release 0.3 and earlier (the versions of Spark you can download from spark-project.org). If you want to work with the master branch of Spark, you will need to use the master branch of Mesos (so just don't do the `git checkout`, but do the `git clone`).\n3. Patrick McFadin: Github comment from ylitvinov: Thank you. I've tried ```git checkout -b pre-protobuf --track origin/pre-protobuf``` and it resulted in ```Branch pre-protobuf set up to track remote branch pre-protobuf from origin. Switched to a new branch 'pre-protobuf'``` is it expected output?\n4. Patrick McFadin: Github comment from mateiz: Yes, that looks good. You should be able to build Mesos now.\n5. Patrick McFadin: Github comment from mateiz: I'm going to close this issue because I fixed the docs for this.\n6. Patrick McFadin: Imported from Github issue spark-85, originally reported by ylitvinov", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.946169"}}
{"id": "65684ceb812503456916744261ab0c40", "issue_key": "SPARK-557", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Provide scripts for launching Spark through Amazon Elastic MapReduce", "description": "We talked with some Amazon folks a while back that said this should be possible through the \"bootstrap actions\" they provide. It would make it very easy to scale clusters up/down, use spot instances, etc.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-10-17T12:24:00.000+0000", "updated": "2013-04-05T20:27:17.000+0000", "resolved": "2013-04-05T20:27:17.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-86, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Amazon released a tutorial, \"Run Spark and Shark on Amazon Elastic MapReduce\", that does this: http://aws.amazon.com/articles/4926593393724923", "created": "2013-04-05T20:27:17.198+0000"}], "num_comments": 2, "text": "Issue: SPARK-557\nSummary: Provide scripts for launching Spark through Amazon Elastic MapReduce\nDescription: We talked with some Amazon folks a while back that said this should be possible through the \"bootstrap actions\" they provide. It would make it very easy to scale clusters up/down, use spot instances, etc.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-86, originally reported by mateiz\n2. Josh Rosen: Amazon released a tutorial, \"Run Spark and Shark on Amazon Elastic MapReduce\", that does this: http://aws.amazon.com/articles/4926593393724923", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.947174"}}
{"id": "f78341b552077c627d9fc6a7cdb86701", "issue_key": "SPARK-451", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Ranges of Longs don't get split properly by SparkContext.parallelize", "description": "We wanted Ranges to be split into smaller Range objects to avoid shipping the integers in them over the network, but Scala 2.9's Long ranges don't get matched as a Range by the ParallelCollection code and thus get converted to arrays, costing a lot of RAM.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-11-02T09:49:00.000+0000", "updated": "2013-10-10T18:00:04.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-87, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-451\nSummary: Ranges of Longs don't get split properly by SparkContext.parallelize\nDescription: We wanted Ranges to be split into smaller Range objects to avoid shipping the integers in them over the network, but Scala 2.9's Long ranges don't get matched as a Range by the ParallelCollection code and thus get converted to arrays, costing a lot of RAM.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-87, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.947174"}}
{"id": "6ecda1a199f779bf806b5601dc638f27", "issue_key": "SPARK-450", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Implement a sort() operation / RDD", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-11-02T09:50:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from shawny: Re, i like it.", "created": "2011-12-18T23:19:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from lgiorda: I would love this.", "created": "2012-02-08T14:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: We're working on it -- should be in soon!", "created": "2012-02-09T22:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-88, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-450\nSummary: Implement a sort() operation / RDD\n\nComments (4):\n1. Patrick McFadin: Github comment from shawny: Re, i like it.\n2. Patrick McFadin: Github comment from lgiorda: I would love this.\n3. Patrick McFadin: Github comment from mateiz: We're working on it -- should be in soon!\n4. Patrick McFadin: Imported from Github issue spark-88, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.947174"}}
{"id": "0848f28591644fd5b09c95a3adae1baf", "issue_key": "SPARK-449", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Tab completion doesn't work in REPL with Scala 2.9.1", "description": "This will either be an easy fix or a really hard one.. need to look at what changed in the REPL between 2.9.0.1 and 2.9.1.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-11-02T09:51:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-89, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-449\nSummary: Tab completion doesn't work in REPL with Scala 2.9.1\nDescription: This will either be an easy fix or a really hard one.. need to look at what changed in the REPL between 2.9.0.1 and 2.9.1.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-89, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.947174"}}
{"id": "164989f8a8cc0bcedebc2add0fd568b0", "issue_key": "SPARK-448", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Use Akka actors for communication", "description": "This might help avoid one annoying issue with Scala actors, which is that there's no way to just bind to a free port and figure out the port you bound to. (Instead, you must guess a free port number and call alive(port number), and there is no error if that port number in taken.) In addition, it could provide some performance benefits, though I'd have to test this.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-11-07T19:51:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Done in dev branch, although we will likely switch away from Akka.", "created": "2012-06-09T13:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-90, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-448\nSummary: Use Akka actors for communication\nDescription: This might help avoid one annoying issue with Scala actors, which is that there's no way to just bind to a free port and figure out the port you bound to. (Instead, you must guess a free port number and call alive(port number), and there is no error if that port number in taken.) In addition, it could provide some performance benefits, though I'd have to test this.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Done in dev branch, although we will likely switch away from Akka.\n2. Patrick McFadin: Imported from Github issue spark-90, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.947174"}}
{"id": "88176bdb5f0fd1c585143b907569793a", "issue_key": "SPARK-447", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Bagel unit tests broken after API change", "description": "It seems like the BagelSuite test doesn't compile, but this is hard to notice because SBT doesn't try to compile it when you aren't running a test. To reproduce it you can run sbt/sbt test (maybe typing project bagel first to limit its scope to that).", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-11-07T23:08:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ankurdave: Fixed by c5be7d2b2268e44e3eafb460d4bf0fb0badf9b22.", "created": "2011-11-08T12:01:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks!", "created": "2011-11-08T17:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-91, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-447\nSummary: Bagel unit tests broken after API change\nDescription: It seems like the BagelSuite test doesn't compile, but this is hard to notice because SBT doesn't try to compile it when you aren't running a test. To reproduce it you can run sbt/sbt test (maybe typing project bagel first to limit its scope to that).\n\nComments (3):\n1. Patrick McFadin: Github comment from ankurdave: Fixed by c5be7d2b2268e44e3eafb460d4bf0fb0badf9b22.\n2. Patrick McFadin: Github comment from mateiz: Great, thanks!\n3. Patrick McFadin: Imported from Github issue spark-91, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.947174"}}
{"id": "f2ce8f61f2521e0b5f89032dcfc61bea", "issue_key": "SPARK-446", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark doesn't return offers if it doesn't have any tasks for them", "description": "When Spark gets a resource offer for which it cannot launch a task, it never calls launchTask() for that offer. With the current (Apache trunk) implementation of Mesos, this appears to leave the offer pending rather than returning it so Spark can get more offers.", "reporter": "woggling", "assignee": null, "created": "0011-11-08T18:51:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Ah, this explains some weird stuff that Eemil (a visitor from Sweden) was seeing. I'll fix it momentarily.", "created": "2011-11-08T20:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Commit 07532021fee9e2d27ee954b21c30830687478d8b hopefully fixes this.", "created": "2011-11-08T23:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-92, originally reported by woggling", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-446\nSummary: Spark doesn't return offers if it doesn't have any tasks for them\nDescription: When Spark gets a resource offer for which it cannot launch a task, it never calls launchTask() for that offer. With the current (Apache trunk) implementation of Mesos, this appears to leave the offer pending rather than returning it so Spark can get more offers.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Ah, this explains some weird stuff that Eemil (a visitor from Sweden) was seeing. I'll fix it momentarily.\n2. Patrick McFadin: Github comment from mateiz: Commit 07532021fee9e2d27ee954b21c30830687478d8b hopefully fixes this.\n3. Patrick McFadin: Imported from Github issue spark-92, originally reported by woggling", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.947174"}}
{"id": "fb594afa328395e6d4147fe243ca3f24", "issue_key": "SPARK-445", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add a version of sample() that returns a fixed number of elements", "description": "Right now you can only sample an RDD by passing a fraction of elements to preserve. It would be nice to have a way to just pick exactly K elements out of it and return it as an array. This can be called collectSample().", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-11-17T13:57:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from holdenk: I've added a collectSample() function (see pull request #107).", "created": "2012-01-25T13:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Holden, Unfortunately we recently committed very similar functionality called takeSample in https://github.com/mesos/spark/pull/98. Does collectSample do anything extra beyond that? If so maybe we can merge the two.", "created": "2012-01-25T13:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from holdenk: I think my implementation is a bit more efficient, takeSample may re-sample the same array repeatedly until it gets a sample of at least size k ( https://github.com/edisontung/spark/commit/a3bc012af8a4f7c362a28bc1294942019d5a288d line 113), whereas collectSample does a single pass through. I can update my commit to replace takeSample rather than have a different name? On Wed, Jan 25, 2012 at 1:42 PM, Matei Zaharia < reply@reply.github.com > wrote: > Hey Holden, > > Unfortunately we recently committed very similar functionality called > takeSample in https://github.com/mesos/spark/pull/98. Does collectSample > do anything extra beyond that? If so maybe we can merge the two. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/93#issuecomment-3659689 > -- Cell : 425-233-8271", "created": "2012-01-25T13:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Sure, that would be great.", "created": "2012-01-25T14:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from holdenk: Cool, I'll update my commit tonight :) On Wed, Jan 25, 2012 at 2:12 PM, Matei Zaharia < reply@reply.github.com > wrote: > Sure, that would be great. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/93#issuecomment-3660242 > -- Cell : 425-233-8271", "created": "2012-01-25T14:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from holdenk: I've updated my pull request :)", "created": "2012-01-30T11:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-93, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-445\nSummary: Add a version of sample() that returns a fixed number of elements\nDescription: Right now you can only sample an RDD by passing a fraction of elements to preserve. It would be nice to have a way to just pick exactly K elements out of it and return it as an array. This can be called collectSample().\n\nComments (7):\n1. Patrick McFadin: Github comment from holdenk: I've added a collectSample() function (see pull request #107).\n2. Patrick McFadin: Github comment from mateiz: Hey Holden, Unfortunately we recently committed very similar functionality called takeSample in https://github.com/mesos/spark/pull/98. Does collectSample do anything extra beyond that? If so maybe we can merge the two.\n3. Patrick McFadin: Github comment from holdenk: I think my implementation is a bit more efficient, takeSample may re-sample the same array repeatedly until it gets a sample of at least size k ( https://github.com/edisontung/spark/commit/a3bc012af8a4f7c362a28bc1294942019d5a288d line 113), whereas collectSample does a single pass through. I can update my commit to replace takeSample rather than have a different name? On Wed, Jan 25, 2012 at 1:42 PM, Matei Zaharia < reply@reply.github.com > wrote: > Hey Holden, > > Unfortunately we recently committed very similar functionality called > takeSample in https://github.com/mesos/spark/pull/98. Does collectSample > do anything extra beyond that? If so maybe we can merge the two. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/93#issuecomment-3659689 > -- Cell : 425-233-8271\n4. Patrick McFadin: Github comment from mateiz: Sure, that would be great.\n5. Patrick McFadin: Github comment from holdenk: Cool, I'll update my commit tonight :) On Wed, Jan 25, 2012 at 2:12 PM, Matei Zaharia < reply@reply.github.com > wrote: > Sure, that would be great. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/93#issuecomment-3660242 > -- Cell : 425-233-8271\n6. Patrick McFadin: Github comment from holdenk: I've updated my pull request :)\n7. Patrick McFadin: Imported from Github issue spark-93, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.948183"}}
{"id": "5a2efeed1262897447b03c586c8de823", "issue_key": "SPARK-556", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Results from tasks should be returned through our own sockets rather than Mesos framework messages", "description": "With large results, the Mesos library might not like sending large framework messages. We can avoid this by sending the results to the master through a socket rather than through sendFrameworkMessage. Another thing that would help is warning the user in this case.", "reporter": "Matei Alexandru Zaharia", "assignee": "Kay Ousterhout", "created": "0011-11-19T14:56:00.000+0000", "updated": "2013-10-10T18:07:14.000+0000", "resolved": "2013-10-10T18:07:14.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-94, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-556\nSummary: Results from tasks should be returned through our own sockets rather than Mesos framework messages\nDescription: With large results, the Mesos library might not like sending large framework messages. We can avoid this by sending the results to the master through a socket rather than through sendFrameworkMessage. Another thing that would help is warning the user in this case.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-94, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.948183"}}
{"id": "a5d8fe781a8c52960f832eafa37da3f2", "issue_key": "SPARK-444", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add an accumulating fold operator", "description": "Often when reducing we want to add up objects of some type (e.g. vectors), in which case merging them with a sum function as in reduce(_ + _) is inefficient because it creates a new object each time. It would be nice to have a fold operator taking a \"zero value\" and a \"merge\" function that takes the result so far and adds an element onto it.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-11-23T18:05:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-95, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-444\nSummary: Add an accumulating fold operator\nDescription: Often when reducing we want to add up objects of some type (e.g. vectors), in which case merging them with a sum function as in reduce(_ + _) is inefficient because it creates a new object each time. It would be nice to have a fold operator taking a \"zero value\" and a \"merge\" function that takes the result so far and adds an element onto it.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-95, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.948183"}}
{"id": "4adc090cb2e291537a44b2a349400b7b", "issue_key": "SPARK-555", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Write a \"Spark internals\" wiki page", "description": "This would help new developers jump into the codebase.", "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "created": "0011-11-23T18:06:00.000+0000", "updated": "2013-12-07T12:56:44.000+0000", "resolved": "2013-12-07T12:56:44.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from shawny: good news", "created": "2011-12-16T08:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tsforce: +1", "created": "2012-07-11T19:23:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-96, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "The slides from the \"Introduction to Spark Internals\" talk (https://www.youtube.com/watch?v=49Hr5xZyTEA) probably serve as a good introduction to the internals.", "created": "2013-05-04T19:07:00.199+0000"}, {"author": "Josh Rosen", "body": "We now have a Spark Internals page on the Apache CWiki: https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals", "created": "2013-12-07T12:56:44.857+0000"}], "num_comments": 5, "text": "Issue: SPARK-555\nSummary: Write a \"Spark internals\" wiki page\nDescription: This would help new developers jump into the codebase.\n\nComments (5):\n1. Patrick McFadin: Github comment from shawny: good news\n2. Patrick McFadin: Github comment from tsforce: +1\n3. Patrick McFadin: Imported from Github issue spark-96, originally reported by mateiz\n4. Josh Rosen: The slides from the \"Introduction to Spark Internals\" talk (https://www.youtube.com/watch?v=49Hr5xZyTEA) probably serve as a good introduction to the internals.\n5. Josh Rosen: We now have a Spark Internals page on the Apache CWiki: https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.948183"}}
{"id": "bee779c4e76df07496ffcd6e25d30240", "issue_key": "SPARK-554", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add aggregateByKey", "description": "Similar to the new fold() and aggregate() methods in #95, we should have foldByKey and aggregateByKey for pair RDDs. The main thing that makes this slightly harder is that we'll have to change the combineByKey API and ShuffledRDD to allow taking in a \"zero value\".", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandy Ryza", "created": "0011-11-30T11:40:00.000+0000", "updated": "2014-06-12T15:15:34.000+0000", "resolved": "2014-06-12T15:15:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-97, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "foldByKey() was added in https://github.com/mesos/spark/pull/526.", "created": "2013-05-04T19:05:21.168+0000"}, {"author": "Sandy Ryza", "body": "I've run into a couple situations where aggregateByKey would have been really helpful - I'm going to have a go at adding it.", "created": "2014-05-08T20:14:33.562+0000"}, {"author": "Sandy Ryza", "body": "https://github.com/apache/spark/pull/705", "created": "2014-05-09T01:22:24.427+0000"}, {"author": "Patrick Wendell", "body": "Fixed in: https://github.com/apache/spark/pull/705", "created": "2014-06-12T15:15:34.431+0000"}], "num_comments": 5, "text": "Issue: SPARK-554\nSummary: Add aggregateByKey\nDescription: Similar to the new fold() and aggregate() methods in #95, we should have foldByKey and aggregateByKey for pair RDDs. The main thing that makes this slightly harder is that we'll have to change the combineByKey API and ShuffledRDD to allow taking in a \"zero value\".\n\nComments (5):\n1. Patrick McFadin: Imported from Github issue spark-97, originally reported by mateiz\n2. Josh Rosen: foldByKey() was added in https://github.com/mesos/spark/pull/526.\n3. Sandy Ryza: I've run into a couple situations where aggregateByKey would have been really helpful - I'm going to have a go at adding it.\n4. Sandy Ryza: https://github.com/apache/spark/pull/705\n5. Patrick Wendell: Fixed in: https://github.com/apache/spark/pull/705", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.948183"}}
{"id": "33ed7146674f5708e97f5a5fc11ff0bd", "issue_key": "SPARK-443", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added two examples. also added a method to RDD.scala", "description": "Added LocalKMeans and SparkLocalKMeans which are examples that compute K-Means clusters. Method added to RDD.scala is takeSamples, which returns a set number of samples in an array (the number is specified as an argument to the method.", "reporter": "edisontung", "assignee": null, "created": "0011-11-30T23:20:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-98, originally reported by edisontung", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-443\nSummary: Added two examples. also added a method to RDD.scala\nDescription: Added LocalKMeans and SparkLocalKMeans which are examples that compute K-Means clusters. Method added to RDD.scala is takeSamples, which returns a set number of samples in an array (the number is specified as an argument to the method.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-98, originally reported by edisontung", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.948183"}}
{"id": "d177bf68705e41ab8affb96dc961bc2b", "issue_key": "SPARK-553", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Write a tutorial for mining Wikipedia interactively on EC2", "description": "Would be a great way to get people started with Spark. It should basically let you do the following: http://www.youtube.com/user/iammatei#p/a/u/0/Jw9yNTJI8iM", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-12-02T21:34:00.000+0000", "updated": "2013-05-05T11:40:42.000+0000", "resolved": "2013-05-05T11:40:30.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: +1 (Cool demo by the way)", "created": "2011-12-07T10:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: Clustering on a WP dump is our current CS294 assignment :) I will write a tutorial and put the code online.", "created": "2012-04-03T20:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Have you gotten around to working on this yet? If not let me know and I'll write it up!", "created": "2012-07-13T08:36:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: I started a tutorial on some geo tagged data. The idea is exactly the same as in the wikipedia demo, though. You can check it out here: https://github.com/tjhunter/spark-geo-tutorial https://github.com/tjhunter/spark-geo-tutorial/blob/master/src/main/doc/tutorial.md It should work once I figure out a way to make the dataset on S3 publicicly available. Do you know how to do it?", "created": "2012-07-13T10:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: That's a cool tutorial. It should be linked from the Spark Wiki. Have you tried setting the permissions on the s3 files? In the S3 Console add \"Download/Open\" for everyone. EDIT: Also, make sure you give bucket listing permissions for everyone if you're using the wildcard in the input url.", "created": "2012-07-13T12:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: I changed the ACL on the bucket and updated the tutorial. Do you want to give it a try? The tutorial talks about caching, broadcast, map, reduce aggregate. Anything else that can easily be included? Feel free to send pull requests :-)", "created": "2012-07-13T13:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Just tried it out, I'm getting `org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 HEAD request failed for '/2009-10-9.txt' - ResponseCode=403, ResponseMessage=Forbidden` for the S3 files. The directory listing permissions seem to be correct, but the individual file permissions not?", "created": "2012-07-16T10:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: Fixed ACLs for the files.", "created": "2012-07-16T23:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Tim, is this something we should link to from the Spark wiki? (That is, it is it more or less finished?) If so you can actually edit the wiki to add it, or I can do it for you. We can maybe add a Tutorials section on the main page, and another link at the bottom of the programming guide.", "created": "2012-08-03T18:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-99, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "The AMP Camp training materials have examples of mining Wikipedia: http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html", "created": "2013-05-05T11:40:30.536+0000"}], "num_comments": 11, "text": "Issue: SPARK-553\nSummary: Write a tutorial for mining Wikipedia interactively on EC2\nDescription: Would be a great way to get people started with Spark. It should basically let you do the following: http://www.youtube.com/user/iammatei#p/a/u/0/Jw9yNTJI8iM\n\nComments (11):\n1. Patrick McFadin: Github comment from tjhunter: +1 (Cool demo by the way)\n2. Patrick McFadin: Github comment from tjhunter: Clustering on a WP dump is our current CS294 assignment :) I will write a tutorial and put the code online.\n3. Patrick McFadin: Github comment from dennybritz: Have you gotten around to working on this yet? If not let me know and I'll write it up!\n4. Patrick McFadin: Github comment from tjhunter: I started a tutorial on some geo tagged data. The idea is exactly the same as in the wikipedia demo, though. You can check it out here: https://github.com/tjhunter/spark-geo-tutorial https://github.com/tjhunter/spark-geo-tutorial/blob/master/src/main/doc/tutorial.md It should work once I figure out a way to make the dataset on S3 publicicly available. Do you know how to do it?\n5. Patrick McFadin: Github comment from dennybritz: That's a cool tutorial. It should be linked from the Spark Wiki. Have you tried setting the permissions on the s3 files? In the S3 Console add \"Download/Open\" for everyone. EDIT: Also, make sure you give bucket listing permissions for everyone if you're using the wildcard in the input url.\n6. Patrick McFadin: Github comment from tjhunter: I changed the ACL on the bucket and updated the tutorial. Do you want to give it a try? The tutorial talks about caching, broadcast, map, reduce aggregate. Anything else that can easily be included? Feel free to send pull requests :-)\n7. Patrick McFadin: Github comment from dennybritz: Just tried it out, I'm getting `org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 HEAD request failed for '/2009-10-9.txt' - ResponseCode=403, ResponseMessage=Forbidden` for the S3 files. The directory listing permissions seem to be correct, but the individual file permissions not?\n8. Patrick McFadin: Github comment from tjhunter: Fixed ACLs for the files.\n9. Patrick McFadin: Github comment from mateiz: Tim, is this something we should link to from the Spark wiki? (That is, it is it more or less finished?) If so you can actually edit the wiki to add it, or I can do it for you. We can maybe add a Tutorials section on the main page, and another link at the bottom of the programming guide.\n10. Patrick McFadin: Imported from Github issue spark-99, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.949090"}}
{"id": "91a20277e75807a811080f046ef4b6aa", "issue_key": "SPARK-442", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add logging/notification when RDDs are evicted from cache", "description": "As of now, there is no way for user and developer to know what RDDs are in the cache vs what have been evicted. It'd be useful to add some logging capabilities or even notification callbacks. This should be doable with reference queues.", "reporter": "Reynold Xin", "assignee": null, "created": "0011-12-07T01:47:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: Is there a strong use case for callbacks? I though the caching was supposed to be transparent to the user.", "created": "2011-12-07T10:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Logging is higher priority for debugging. As for callbacks, you could imagine higher level applications built on top of Spark might like to provide its own feedback to power users.", "created": "2011-12-07T11:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed in the code now (at least logging is there).", "created": "2012-06-06T15:16:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-100, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-442\nSummary: Add logging/notification when RDDs are evicted from cache\nDescription: As of now, there is no way for user and developer to know what RDDs are in the cache vs what have been evicted. It'd be useful to add some logging capabilities or even notification callbacks. This should be doable with reference queues.\n\nComments (4):\n1. Patrick McFadin: Github comment from tjhunter: Is there a strong use case for callbacks? I though the caching was supposed to be transparent to the user.\n2. Patrick McFadin: Github comment from rxin: Logging is higher priority for debugging. As for callbacks, you could imagine higher level applications built on top of Spark might like to provide its own feedback to power users.\n3. Patrick McFadin: Github comment from mateiz: This is fixed in the code now (at least logging is there).\n4. Patrick McFadin: Imported from Github issue spark-100, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.949090"}}
{"id": "26ab850dc809cc0f4b50f2c4897d0674", "issue_key": "SPARK-441", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Update documentation for building with Scala 2.8", "description": "https://github.com/mesos/spark/wiki currently says under \"Building\" subheading: \"Spark requires Scala 2.9 or 2.8.\" ... and then later: \"Once you have set up Scala, run sbt/sbt compile (in the scala-2.8 branch, sbt/sbt update compile) in the top-level Spark directory to build Spark. (Spark uses Simple Build Tool, which is bundled with it.)\" It would be very helpful if the documentation more explicitly laid out the slightly different instructions you should follow for building with Scala 2.8 vs. 2.9. Maybe that final paragraph under \"Build\" can be reworked to be: Once you have set up Scala, in the top-level Spark directory.... If you are using Scala 2.8.*: 1. Check out the scala-2.8 git branch, by running: `git checkout -b scala-2.8 --track origin/scala-2.8` 2. Update and compile, by running: `sbt/sbt update compile` 3. Consider upgrading to Scala 2.9 to access the newest features of Spark! :) If you are using Scala 2.9.*: 1. Compile, by running: sbt/sbt compile Note: as you see above, Spark uses Simple Build Tool, which is bundled with it.", "reporter": "Andy Konwinski", "assignee": null, "created": "0011-12-15T12:22:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from lagerspetz: Sounds like a good idea. Check the wiki now.", "created": "2011-12-15T12:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from andyk: Looks good to me!", "created": "2011-12-15T12:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-101, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-441\nSummary: Update documentation for building with Scala 2.8\nDescription: https://github.com/mesos/spark/wiki currently says under \"Building\" subheading: \"Spark requires Scala 2.9 or 2.8.\" ... and then later: \"Once you have set up Scala, run sbt/sbt compile (in the scala-2.8 branch, sbt/sbt update compile) in the top-level Spark directory to build Spark. (Spark uses Simple Build Tool, which is bundled with it.)\" It would be very helpful if the documentation more explicitly laid out the slightly different instructions you should follow for building with Scala 2.8 vs. 2.9. Maybe that final paragraph under \"Build\" can be reworked to be: Once you have set up Scala, in the top-level Spark directory.... If you are using Scala 2.8.*: 1. Check out the scala-2.8 git branch, by running: `git checkout -b scala-2.8 --track origin/scala-2.8` 2. Update and compile, by running: `sbt/sbt update compile` 3. Consider upgrading to Scala 2.9 to access the newest features of Spark! :) If you are using Scala 2.9.*: 1. Compile, by running: sbt/sbt compile Note: as you see above, Spark uses Simple Build Tool, which is bundled with it.\n\nComments (3):\n1. Patrick McFadin: Github comment from lagerspetz: Sounds like a good idea. Check the wiki now.\n2. Patrick McFadin: Github comment from andyk: Looks good to me!\n3. Patrick McFadin: Imported from Github issue spark-101, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.949090"}}
{"id": "20cc6c82938fb92e63beb49fa5de288d", "issue_key": "SPARK-440", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Why \"./run spark.examples.SparkPi 1@localhost:5050\" can't get result of Pi", "description": "I setup mesos on local machine: { 1000 29724 0.0 0.4 49556 9728 pts/0 Sl 21:44 0:02 ./bin mesos-master 1000 29774 0.0 0.4 48864 8756 pts/0 Sl 21:44 0:01 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050 } When i run \"./run spark.examples.SparkPi 1@localhost:5050\", it will stop at the following last line: 11/12/16 23:04:45 INFO CacheTrackerActor: Registered actor on port 50501 11/12/16 23:04:45 INFO MapOutputTrackerActor: Registered actor on port 50501 11/12/16 23:04:45 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-d10f5f39-4d21-4062-ac67-7e27ebd15794 11/12/16 23:04:46 INFO MesosScheduler: JAR server started at http://127.0.0.1:40518 11/12/16 23:04:46 INFO SparkContext: Starting job... 11/12/16 23:04:46 INFO CacheTracker: Registering RDD ID 0 with cache 11/12/16 23:04:46 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions 11/12/16 23:04:46 INFO CacheTrackerActor: Asked for current cache locations 11/12/16 23:04:46 INFO MesosScheduler: Final stage: Stage 0 11/12/16 23:04:46 INFO MesosScheduler: Parents of final stage: List() 11/12/16 23:04:46 INFO MesosScheduler: Missing parents: List() 11/12/16 23:04:46 INFO MesosScheduler: Submitting Stage 0, which has no missing parents 11/12/16 23:04:46 INFO MesosScheduler: Got a job with 2 tasks But when i run \"./run spark.examples.SparkPi mesos://master@localhost:5050\", i can get the result of Pi: 11/12/16 22:57:44 INFO MapOutputTrackerActor: Registered actor on port 50501 11/12/16 22:57:44 INFO CacheTrackerActor: Registered actor on port 50501 11/12/16 22:57:44 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-f173b70d-bf22-4dc7-a1c5-c241a67af3dc 11/12/16 22:57:45 INFO MesosScheduler: JAR server started at http://127.0.0.1:40327 11/12/16 22:57:45 INFO SparkContext: Starting job... 11/12/16 22:57:45 INFO CacheTracker: Registering RDD ID 0 with cache 11/12/16 22:57:45 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions 11/12/16 22:57:45 INFO CacheTrackerActor: Asked for current cache locations 11/12/16 22:57:45 INFO MesosScheduler: Final stage: Stage 0 11/12/16 22:57:45 INFO MesosScheduler: Parents of final stage: List() 11/12/16 22:57:45 INFO MesosScheduler: Missing parents: List() 11/12/16 22:57:45 INFO MesosScheduler: Submitting Stage 0, which has no missing parents 11/12/16 22:57:45 INFO MesosScheduler: Got a job with 2 tasks 11/12/16 22:57:45 INFO MesosScheduler: Registered as framework ID 201112162144-0-0002 11/12/16 22:57:45 INFO MesosScheduler: Adding job with ID 0 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:0 as TID 0 on slave 201112162144-0-0: localhost (preferred) 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:1 as TID 1 on slave 201112162144-0-0: localhost (preferred) 11/12/16 22:57:46 INFO SimpleJob: Finished TID 1 (progress: 1/2) 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 1) 11/12/16 22:57:47 INFO SimpleJob: Finished TID 0 (progress: 2/2) 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 0) 11/12/16 22:57:47 INFO SparkContext: Job finished in 1.827198029 s Pi is roughly 3.14944 ==> but SparkPi still as active frameworks from UI ( Active Frameworks ID User Name Running Tasks CPUs MEM Max Share Connected 201112162144-0-0002 xuanhouysh SparkPi 0 0 200.0 MB 0.22 2011-12-16 22:57:45 ) My Spark config file as follows: export MESOS_HOME=/home/xuanhouysh/software/mesos export SCALA_HOME=/home/xuanhouysh/software/scala-2.9.1.final export SPARK_MEM=200m", "reporter": "songhe", "assignee": null, "created": "0011-12-16T07:10:00.000+0000", "updated": "2012-10-19T22:50:21.000+0000", "resolved": "2012-10-19T22:50:21.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hi Shawn, You're supposed to use master@localhost:5050, not 1@localhost:5050. That's an older format. To prevent the job remaining on the Mesos web UI, you need to add the argument --failover_timeout=0 to mesos-master. We're going to make it default to 0 soon because this is confusing. Matei On Dec 16, 2011, at 4:10 PM, shawny wrote: > I setup mesos on local machine: > > { > 1000 29724 0.0 0.4 49556 9728 pts/0 Sl 21:44 0:02 ./bin mesos-master > 1000 29774 0.0 0.4 48864 8756 pts/0 Sl 21:44 0:01 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050 > } > > When i run \"./run spark.examples.SparkPi 1@localhost:5050\", it will stop at the following last line: > > 11/12/16 23:04:45 INFO CacheTrackerActor: Registered actor on port 50501 > 11/12/16 23:04:45 INFO MapOutputTrackerActor: Registered actor on port 50501 > 11/12/16 23:04:45 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-d10f5f39-4d21-4062-ac67-7e27ebd15794 > 11/12/16 23:04:46 INFO MesosScheduler: JAR server started at http://127.0.0.1:40518 > 11/12/16 23:04:46 INFO SparkContext: Starting job... > 11/12/16 23:04:46 INFO CacheTracker: Registering RDD ID 0 with cache > 11/12/16 23:04:46 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions > 11/12/16 23:04:46 INFO CacheTrackerActor: Asked for current cache locations > 11/12/16 23:04:46 INFO MesosScheduler: Final stage: Stage 0 > 11/12/16 23:04:46 INFO MesosScheduler: Parents of final stage: List() > 11/12/16 23:04:46 INFO MesosScheduler: Missing parents: List() > 11/12/16 23:04:46 INFO MesosScheduler: Submitting Stage 0, which has no missing parents > 11/12/16 23:04:46 INFO MesosScheduler: Got a job with 2 tasks > > > But when i run \"./run spark.examples.SparkPi mesos://master@localhost:5050\", i can get the result of Pi: > > 11/12/16 22:57:44 INFO MapOutputTrackerActor: Registered actor on port 50501 > 11/12/16 22:57:44 INFO CacheTrackerActor: Registered actor on port 50501 > 11/12/16 22:57:44 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-f173b70d-bf22-4dc7-a1c5-c241a67af3dc > 11/12/16 22:57:45 INFO MesosScheduler: JAR server started at http://127.0.0.1:40327 > 11/12/16 22:57:45 INFO SparkContext: Starting job... > 11/12/16 22:57:45 INFO CacheTracker: Registering RDD ID 0 with cache > 11/12/16 22:57:45 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions > 11/12/16 22:57:45 INFO CacheTrackerActor: Asked for current cache locations > 11/12/16 22:57:45 INFO MesosScheduler: Final stage: Stage 0 > 11/12/16 22:57:45 INFO MesosScheduler: Parents of final stage: List() > 11/12/16 22:57:45 INFO MesosScheduler: Missing parents: List() > 11/12/16 22:57:45 INFO MesosScheduler: Submitting Stage 0, which has no missing parents > 11/12/16 22:57:45 INFO MesosScheduler: Got a job with 2 tasks > 11/12/16 22:57:45 INFO MesosScheduler: Registered as framework ID 201112162144-0-0002 > 11/12/16 22:57:45 INFO MesosScheduler: Adding job with ID 0 > 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:0 as TID 0 on slave 201112162144-0-0: localhost (preferred) > 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:1 as TID 1 on slave 201112162144-0-0: localhost (preferred) > 11/12/16 22:57:46 INFO SimpleJob: Finished TID 1 (progress: 1/2) > 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 1) > 11/12/16 22:57:47 INFO SimpleJob: Finished TID 0 (progress: 2/2) > 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 0) > 11/12/16 22:57:47 INFO SparkContext: Job finished in 1.827198029 s > Pi is roughly 3.14944 > > ==> but SparkPi still as active frameworks from UI > ( > Active Frameworks > > ID User Name Running Tasks CPUs MEM Max Share Connected > 201112162144-0-0002 xuanhouysh SparkPi 0 0 200.0 MB 0.22 2011-12-16 22:57:45 > ) > > My Spark config file as follows: > > export MESOS_HOME=/home/xuanhouysh/software/mesos > export SCALA_HOME=/home/xuanhouysh/software/scala-2.9.1.final > export SPARK_MEM=200m > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/102", "created": "2011-12-16T07:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shawny: Thanks for your quickly reply :) You are right, i test it again using prefix master, success!", "created": "2011-12-16T07:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shawny: Sorry, about the \"failover_timeout\", it seems that assign it to 0 can't affect the active frameworks in UI. mesos master and slave process =============== 1000 4609 0.2 0.4 49332 9820 pts/0 Sl 00:02 0:00 ./bin mesos-master --failover_timeout=0 1000 4659 0.1 0.4 48264 8876 pts/0 Sl 00:02 0:00 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050 UI ====== Active Frameworks ID User Name Running Tasks CPUs MEM Max Share Connected 201112170008-0-0000 xuanhouysh SparkPi 0 0 200.0 MB 0.22 2011-12-17 00:08:29", "created": "2011-12-16T08:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-102, originally reported by shawny", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-440\nSummary: Why \"./run spark.examples.SparkPi 1@localhost:5050\" can't get result of Pi\nDescription: I setup mesos on local machine: { 1000 29724 0.0 0.4 49556 9728 pts/0 Sl 21:44 0:02 ./bin mesos-master 1000 29774 0.0 0.4 48864 8756 pts/0 Sl 21:44 0:01 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050 } When i run \"./run spark.examples.SparkPi 1@localhost:5050\", it will stop at the following last line: 11/12/16 23:04:45 INFO CacheTrackerActor: Registered actor on port 50501 11/12/16 23:04:45 INFO MapOutputTrackerActor: Registered actor on port 50501 11/12/16 23:04:45 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-d10f5f39-4d21-4062-ac67-7e27ebd15794 11/12/16 23:04:46 INFO MesosScheduler: JAR server started at http://127.0.0.1:40518 11/12/16 23:04:46 INFO SparkContext: Starting job... 11/12/16 23:04:46 INFO CacheTracker: Registering RDD ID 0 with cache 11/12/16 23:04:46 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions 11/12/16 23:04:46 INFO CacheTrackerActor: Asked for current cache locations 11/12/16 23:04:46 INFO MesosScheduler: Final stage: Stage 0 11/12/16 23:04:46 INFO MesosScheduler: Parents of final stage: List() 11/12/16 23:04:46 INFO MesosScheduler: Missing parents: List() 11/12/16 23:04:46 INFO MesosScheduler: Submitting Stage 0, which has no missing parents 11/12/16 23:04:46 INFO MesosScheduler: Got a job with 2 tasks But when i run \"./run spark.examples.SparkPi mesos://master@localhost:5050\", i can get the result of Pi: 11/12/16 22:57:44 INFO MapOutputTrackerActor: Registered actor on port 50501 11/12/16 22:57:44 INFO CacheTrackerActor: Registered actor on port 50501 11/12/16 22:57:44 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-f173b70d-bf22-4dc7-a1c5-c241a67af3dc 11/12/16 22:57:45 INFO MesosScheduler: JAR server started at http://127.0.0.1:40327 11/12/16 22:57:45 INFO SparkContext: Starting job... 11/12/16 22:57:45 INFO CacheTracker: Registering RDD ID 0 with cache 11/12/16 22:57:45 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions 11/12/16 22:57:45 INFO CacheTrackerActor: Asked for current cache locations 11/12/16 22:57:45 INFO MesosScheduler: Final stage: Stage 0 11/12/16 22:57:45 INFO MesosScheduler: Parents of final stage: List() 11/12/16 22:57:45 INFO MesosScheduler: Missing parents: List() 11/12/16 22:57:45 INFO MesosScheduler: Submitting Stage 0, which has no missing parents 11/12/16 22:57:45 INFO MesosScheduler: Got a job with 2 tasks 11/12/16 22:57:45 INFO MesosScheduler: Registered as framework ID 201112162144-0-0002 11/12/16 22:57:45 INFO MesosScheduler: Adding job with ID 0 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:0 as TID 0 on slave 201112162144-0-0: localhost (preferred) 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:1 as TID 1 on slave 201112162144-0-0: localhost (preferred) 11/12/16 22:57:46 INFO SimpleJob: Finished TID 1 (progress: 1/2) 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 1) 11/12/16 22:57:47 INFO SimpleJob: Finished TID 0 (progress: 2/2) 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 0) 11/12/16 22:57:47 INFO SparkContext: Job finished in 1.827198029 s Pi is roughly 3.14944 ==> but SparkPi still as active frameworks from UI ( Active Frameworks ID User Name Running Tasks CPUs MEM Max Share Connected 201112162144-0-0002 xuanhouysh SparkPi 0 0 200.0 MB 0.22 2011-12-16 22:57:45 ) My Spark config file as follows: export MESOS_HOME=/home/xuanhouysh/software/mesos export SCALA_HOME=/home/xuanhouysh/software/scala-2.9.1.final export SPARK_MEM=200m\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Hi Shawn, You're supposed to use master@localhost:5050, not 1@localhost:5050. That's an older format. To prevent the job remaining on the Mesos web UI, you need to add the argument --failover_timeout=0 to mesos-master. We're going to make it default to 0 soon because this is confusing. Matei On Dec 16, 2011, at 4:10 PM, shawny wrote: > I setup mesos on local machine: > > { > 1000 29724 0.0 0.4 49556 9728 pts/0 Sl 21:44 0:02 ./bin mesos-master > 1000 29774 0.0 0.4 48864 8756 pts/0 Sl 21:44 0:01 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050 > } > > When i run \"./run spark.examples.SparkPi 1@localhost:5050\", it will stop at the following last line: > > 11/12/16 23:04:45 INFO CacheTrackerActor: Registered actor on port 50501 > 11/12/16 23:04:45 INFO MapOutputTrackerActor: Registered actor on port 50501 > 11/12/16 23:04:45 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-d10f5f39-4d21-4062-ac67-7e27ebd15794 > 11/12/16 23:04:46 INFO MesosScheduler: JAR server started at http://127.0.0.1:40518 > 11/12/16 23:04:46 INFO SparkContext: Starting job... > 11/12/16 23:04:46 INFO CacheTracker: Registering RDD ID 0 with cache > 11/12/16 23:04:46 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions > 11/12/16 23:04:46 INFO CacheTrackerActor: Asked for current cache locations > 11/12/16 23:04:46 INFO MesosScheduler: Final stage: Stage 0 > 11/12/16 23:04:46 INFO MesosScheduler: Parents of final stage: List() > 11/12/16 23:04:46 INFO MesosScheduler: Missing parents: List() > 11/12/16 23:04:46 INFO MesosScheduler: Submitting Stage 0, which has no missing parents > 11/12/16 23:04:46 INFO MesosScheduler: Got a job with 2 tasks > > > But when i run \"./run spark.examples.SparkPi mesos://master@localhost:5050\", i can get the result of Pi: > > 11/12/16 22:57:44 INFO MapOutputTrackerActor: Registered actor on port 50501 > 11/12/16 22:57:44 INFO CacheTrackerActor: Registered actor on port 50501 > 11/12/16 22:57:44 INFO MesosScheduler: Temp directory for JARs: /tmp/spark-f173b70d-bf22-4dc7-a1c5-c241a67af3dc > 11/12/16 22:57:45 INFO MesosScheduler: JAR server started at http://127.0.0.1:40327 > 11/12/16 22:57:45 INFO SparkContext: Starting job... > 11/12/16 22:57:45 INFO CacheTracker: Registering RDD ID 0 with cache > 11/12/16 22:57:45 INFO CacheTrackerActor: Registering RDD 0 with 2 partitions > 11/12/16 22:57:45 INFO CacheTrackerActor: Asked for current cache locations > 11/12/16 22:57:45 INFO MesosScheduler: Final stage: Stage 0 > 11/12/16 22:57:45 INFO MesosScheduler: Parents of final stage: List() > 11/12/16 22:57:45 INFO MesosScheduler: Missing parents: List() > 11/12/16 22:57:45 INFO MesosScheduler: Submitting Stage 0, which has no missing parents > 11/12/16 22:57:45 INFO MesosScheduler: Got a job with 2 tasks > 11/12/16 22:57:45 INFO MesosScheduler: Registered as framework ID 201112162144-0-0002 > 11/12/16 22:57:45 INFO MesosScheduler: Adding job with ID 0 > 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:0 as TID 0 on slave 201112162144-0-0: localhost (preferred) > 11/12/16 22:57:45 INFO SimpleJob: Starting task 0:1 as TID 1 on slave 201112162144-0-0: localhost (preferred) > 11/12/16 22:57:46 INFO SimpleJob: Finished TID 1 (progress: 1/2) > 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 1) > 11/12/16 22:57:47 INFO SimpleJob: Finished TID 0 (progress: 2/2) > 11/12/16 22:57:47 INFO MesosScheduler: Completed ResultTask(0, 0) > 11/12/16 22:57:47 INFO SparkContext: Job finished in 1.827198029 s > Pi is roughly 3.14944 > > ==> but SparkPi still as active frameworks from UI > ( > Active Frameworks > > ID User Name Running Tasks CPUs MEM Max Share Connected > 201112162144-0-0002 xuanhouysh SparkPi 0 0 200.0 MB 0.22 2011-12-16 22:57:45 > ) > > My Spark config file as follows: > > export MESOS_HOME=/home/xuanhouysh/software/mesos > export SCALA_HOME=/home/xuanhouysh/software/scala-2.9.1.final > export SPARK_MEM=200m > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/102\n2. Patrick McFadin: Github comment from shawny: Thanks for your quickly reply :) You are right, i test it again using prefix master, success!\n3. Patrick McFadin: Github comment from shawny: Sorry, about the \"failover_timeout\", it seems that assign it to 0 can't affect the active frameworks in UI. mesos master and slave process =============== 1000 4609 0.2 0.4 49332 9820 pts/0 Sl 00:02 0:00 ./bin mesos-master --failover_timeout=0 1000 4659 0.1 0.4 48264 8876 pts/0 Sl 00:02 0:00 ./bin mesos-slave --resources=cpus:4;mem:916 -m mesos://master@localhost:5050 UI ====== Active Frameworks ID User Name Running Tasks CPUs MEM Max Share Connected 201112170008-0-0000 xuanhouysh SparkPi 0 0 200.0 MB 0.22 2011-12-17 00:08:29\n4. Patrick McFadin: Imported from Github issue spark-102, originally reported by shawny", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.950089"}}
{"id": "ac689125578ff63a830abec059cecca3", "issue_key": "SPARK-439", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Made improvements to takeSample. Also changed SparkLocalKMeans to SparkKMeans", "description": "Instead of calling count on an RDD multiple times, stored the count so it can be used. Also, called the collect method for the array earlier also to reduce the number of times count() is called.", "reporter": "edisontung", "assignee": null, "created": "0011-12-16T12:04:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Edison, This looks good but I made a few comments on the diff, including finding a couple of bugs (another extra call to count() and a potential infinite loop). The other thing I noticed is that your new kmeans_data.txt is huge -- 750K lines. It would be better to replace it with the simple one we had before (or just generate another small one), so that people can look at it more easily.", "created": "2011-12-30T08:34:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good. Thanks!", "created": "2012-01-13T19:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-103, originally reported by edisontung", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-439\nSummary: Made improvements to takeSample. Also changed SparkLocalKMeans to SparkKMeans\nDescription: Instead of calling count on an RDD multiple times, stored the count so it can be used. Also, called the collect method for the array earlier also to reduce the number of times count() is called.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Hey Edison, This looks good but I made a few comments on the diff, including finding a couple of bugs (another extra call to count() and a potential infinite loop). The other thing I noticed is that your new kmeans_data.txt is huge -- 750K lines. It would be better to replace it with the simple one we had before (or just generate another small one), so that people can look at it more easily.\n2. Patrick McFadin: Github comment from mateiz: Looks good. Thanks!\n3. Patrick McFadin: Imported from Github issue spark-103, originally reported by edisontung", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.950089"}}
{"id": "10ba46b5526963a219fbecc863cdb821", "issue_key": "SPARK-552", "issue_type": "New Feature", "status": "Closed", "priority": null, "resolution": null, "summary": "spark api doc", "description": "If spark can provide the document of api, it would be much better for beginners.", "reporter": "songhe", "assignee": null, "created": "0011-12-19T20:26:00.000+0000", "updated": "2012-10-20T16:57:18.000+0000", "resolved": "2012-10-20T16:57:18.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from yiihsia: I think so", "created": "2011-12-23T00:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: +1", "created": "2012-04-03T20:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: +1", "created": "2012-04-14T11:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tsforce: +1", "created": "2012-07-11T19:23:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-104, originally reported by shawny", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "The Spark 0.6.0 release introduced Scaladoc API documentation for Spark: http://www.spark-project.org/docs/0.6.0/api/core/index.html", "created": "2012-10-20T16:57:18.946+0000"}], "num_comments": 6, "text": "Issue: SPARK-552\nSummary: spark api doc\nDescription: If spark can provide the document of api, it would be much better for beginners.\n\nComments (6):\n1. Patrick McFadin: Github comment from yiihsia: I think so\n2. Patrick McFadin: Github comment from tjhunter: +1\n3. Patrick McFadin: Github comment from rxin: +1\n4. Patrick McFadin: Github comment from tsforce: +1\n5. Patrick McFadin: Imported from Github issue spark-104, originally reported by shawny\n6. Josh Rosen: The Spark 0.6.0 release introduced Scaladoc API documentation for Spark: http://www.spark-project.org/docs/0.6.0/api/core/index.html", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.950089"}}
{"id": "4bf07098df918196ec03c0750a0ccd45", "issue_key": "SPARK-438", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Using 0 partition RDD in a shuffle causes crashes", "description": "If one uses a zero partition RDD (e.g. the result of creating a Hadoop RDD from empty files with certain input formats) as part of a shuffle dependency, spark will get confused: DAGScheduler realizes that it doesn't need to realize the RDD, but it won't tell MapOutputTracker about it, so MapOutputTracker will return null when the reduce tasks ask for the shuffle locations.", "reporter": "woggling", "assignee": null, "created": "0011-12-25T12:20:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for spotting this, Charles! A number of people had run into these errors. So I guess the solution is to (a) register those RDDs with the output tracker and (b) handle 0-partition RDDs on the reduce side?", "created": "2011-12-25T15:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I've fixed this and added a test where I just create an RDD from an empty directory, but let me know if there is a better way to test it (any other case in which this happens).", "created": "2012-01-05T13:00:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-105, originally reported by woggling", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-438\nSummary: Using 0 partition RDD in a shuffle causes crashes\nDescription: If one uses a zero partition RDD (e.g. the result of creating a Hadoop RDD from empty files with certain input formats) as part of a shuffle dependency, spark will get confused: DAGScheduler realizes that it doesn't need to realize the RDD, but it won't tell MapOutputTracker about it, so MapOutputTracker will return null when the reduce tasks ask for the shuffle locations.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Thanks for spotting this, Charles! A number of people had run into these errors. So I guess the solution is to (a) register those RDDs with the output tracker and (b) handle 0-partition RDDs on the reduce side?\n2. Patrick McFadin: Github comment from mateiz: I've fixed this and added a test where I just create an RDD from an empty directory, but let me know if there is a better way to test it (any other case in which this happens).\n3. Patrick McFadin: Imported from Github issue spark-105, originally reported by woggling", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.950089"}}
{"id": "f9465806271ff542fde559692c428eed", "issue_key": "SPARK-551", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "slave disconnected", "description": "I am a new mesos user. I run mesos on single machine. If I start the master and slave via the script deploy/start-mesos and run the example, it ends successfully but the slaves would be disconnected. However, if I start the master and slave as foreground process, it does work. What's wrong?", "reporter": "dukeecnu", "assignee": null, "created": "0011-12-26T00:37:00.000+0000", "updated": "2013-12-07T14:12:13.000+0000", "resolved": "2013-12-07T14:12:13.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: If you're not the one who posted about this on the mailing list, let me know what output you see from the slave. Look in the logs directory of Mesos for something called mesos-slave.xxx.INFO.", "created": "2011-12-30T08:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dukeecnu: I1229 11:17:20.043180 2678 process_based_isolation_module.cpp:114] Forked executor at = 2701 I1229 11:17:20.106433 2678 slave.cpp:725] Got registration for executor 'default' of framework 201112291117-0-0000 I1229 11:17:20.106547 2678 slave.cpp:779] Flushing queued tasks for framework 201112291117-0-0000 I1229 11:17:20.231096 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:20.231884 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.234006 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:21.234973 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.236644 2678 slave.cpp:398] Got assigned task 1 for framework 201112291117-0-0000 I1229 11:17:21.239364 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:21.240097 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.239802 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:22.240749 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.242307 2678 slave.cpp:398] Got assigned task 2 for framework 201112291117-0-0000 I1229 11:17:22.244925 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:22.245684 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.245420 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:23.246287 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.248219 2678 slave.cpp:398] Got assigned task 3 for framework 201112291117-0-0000 I1229 11:17:23.251009 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:23.251930 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.251518 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:24.252471 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.254112 2678 slave.cpp:398] Got assigned task 4 for framework 201112291117-0-0000 I1229 11:17:24.256762 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:24.257613 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.257258 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:25.259258 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.259996 2678 slave.cpp:568] Asked to shut down framework 201112291117-0-0000 I1229 11:17:25.260043 2678 slave.cpp:572] Shutting down framework 201112291117-0-0000 I1229 11:17:25.260066 2678 slave.cpp:1306] Shutting down executor 'default' of framework 201112291117-0-0000 I1229 11:17:26.216948 2678 process_based_isolation_module.cpp:217] Telling slave of lost executor default of framework 201112291117-0-0000 F1229 11:17:26.217048 2678 utils.hpp:130] Expecting 'MESOS_HOME' in environment variables", "created": "2012-01-05T22:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dukeecnu: when I use one machine as master and another as slave, I run them as foreground process. Unfortunately, I fail too. At the end of the console terminal, there are *** Check failure stack trace: *** @ 0x59240d google::LogMessage::Fail() @ 0x5980c7 google::LogMessage::SendToLog() @ 0x593cc4 google::LogMessage::Flush() @ 0x593f26 google::LogMessageFatal::~LogMessageFatal() @ 0x473605 mesos::internal::utils::process::killtree() @ 0x46dc0c mesos::internal::slave::ProcessBasedIsolationModule::killExecutor() @ 0x46c5d4 mesos::internal::slave::ProcessBasedIsolationModule::processExited() @ 0x473fc9 std::tr1::_Function_handler<>::_M_invoke() @ 0x47453e std::tr1::_Function_handler<>::_M_invoke() @ 0x5adbc5 process::ProcessBase::serve() @ 0x44c541 process::ProcessBase::operator()() @ 0x5b45f2 process::ProcessManager::run() @ 0x5b4760 process::trampoline() @ 0x3cbca41820 (unknown)", "created": "2012-01-05T22:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-106, originally reported by dukeecnu", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-551\nSummary: slave disconnected\nDescription: I am a new mesos user. I run mesos on single machine. If I start the master and slave via the script deploy/start-mesos and run the example, it ends successfully but the slaves would be disconnected. However, if I start the master and slave as foreground process, it does work. What's wrong?\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: If you're not the one who posted about this on the mailing list, let me know what output you see from the slave. Look in the logs directory of Mesos for something called mesos-slave.xxx.INFO.\n2. Patrick McFadin: Github comment from dukeecnu: I1229 11:17:20.043180 2678 process_based_isolation_module.cpp:114] Forked executor at = 2701 I1229 11:17:20.106433 2678 slave.cpp:725] Got registration for executor 'default' of framework 201112291117-0-0000 I1229 11:17:20.106547 2678 slave.cpp:779] Flushing queued tasks for framework 201112291117-0-0000 I1229 11:17:20.231096 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:20.231884 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.234006 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:21.234973 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.236644 2678 slave.cpp:398] Got assigned task 1 for framework 201112291117-0-0000 I1229 11:17:21.239364 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:21.240097 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.239802 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:22.240749 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.242307 2678 slave.cpp:398] Got assigned task 2 for framework 201112291117-0-0000 I1229 11:17:22.244925 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:22.245684 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.245420 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:23.246287 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.248219 2678 slave.cpp:398] Got assigned task 3 for framework 201112291117-0-0000 I1229 11:17:23.251009 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:23.251930 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.251518 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:24.252471 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.254112 2678 slave.cpp:398] Got assigned task 4 for framework 201112291117-0-0000 I1229 11:17:24.256762 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:24.257613 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.257258 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:25.259258 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.259996 2678 slave.cpp:568] Asked to shut down framework 201112291117-0-0000 I1229 11:17:25.260043 2678 slave.cpp:572] Shutting down framework 201112291117-0-0000 I1229 11:17:25.260066 2678 slave.cpp:1306] Shutting down executor 'default' of framework 201112291117-0-0000 I1229 11:17:26.216948 2678 process_based_isolation_module.cpp:217] Telling slave of lost executor default of framework 201112291117-0-0000 F1229 11:17:26.217048 2678 utils.hpp:130] Expecting 'MESOS_HOME' in environment variables\n3. Patrick McFadin: Github comment from dukeecnu: when I use one machine as master and another as slave, I run them as foreground process. Unfortunately, I fail too. At the end of the console terminal, there are *** Check failure stack trace: *** @ 0x59240d google::LogMessage::Fail() @ 0x5980c7 google::LogMessage::SendToLog() @ 0x593cc4 google::LogMessage::Flush() @ 0x593f26 google::LogMessageFatal::~LogMessageFatal() @ 0x473605 mesos::internal::utils::process::killtree() @ 0x46dc0c mesos::internal::slave::ProcessBasedIsolationModule::killExecutor() @ 0x46c5d4 mesos::internal::slave::ProcessBasedIsolationModule::processExited() @ 0x473fc9 std::tr1::_Function_handler<>::_M_invoke() @ 0x47453e std::tr1::_Function_handler<>::_M_invoke() @ 0x5adbc5 process::ProcessBase::serve() @ 0x44c541 process::ProcessBase::operator()() @ 0x5b45f2 process::ProcessManager::run() @ 0x5b4760 process::trampoline() @ 0x3cbca41820 (unknown)\n4. Patrick McFadin: Imported from Github issue spark-106, originally reported by dukeecnu", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.951086"}}
{"id": "f4c2ca13e54724cb238885637ea410e0", "issue_key": "SPARK-437", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add support for fixed sized samples", "description": "Fix for issue #93, adding support for fixed sized samples.", "reporter": "Holden Karau", "assignee": null, "created": "0012-01-23T23:38:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey, sorry for being a bit late on this, I didn't see you posted a new pull request. Would you like me to grab the SortedRDD too or just the rest? As far as I understand, SortedRDD is not complete. We actually had our own effort to implement one so it might be better to use that instead.", "created": "2012-02-05T20:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from holdenk: Oh crap I forgot about sortedrdd being in my branch, just the other stuff would be cool :) On Feb 5, 2012 8:55 PM, \"Matei Zaharia\" < reply@reply.github.com> wrote: > Hey, sorry for being a bit late on this, I didn't see you posted a new > pull request. Would you like me to grab the SortedRDD too or just the rest? > As far as I understand, SortedRDD is not complete. We actually had our own > effort to implement one so it might be better to use that instead. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/107#issuecomment-3823642 >", "created": "2012-02-05T22:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Okay, so I looked at this more carefully, but unfortunately I think there are some problems: 1) There is an array index out of bounds exception on CollectSampledRDD.scala line 41 (in sampleData.update(i,oldData(i))), because the for loop there goes over indices from 1 to sampleSize instead of 0 to sampleSize-1. 2) It looks like the code is pulling out sampleSize items inside each split (partition) of the dataset, rather than sampleSize total items. This is not going to work as the user expects for datasets with more than 1 partition, unless we explicitly document this as the use of the function, which I don't like because I don't want the user to worry about the number of partitions. More generally, partitions might also have different numbers of items, so some might have fewer than sampleSize while some have more. I think it might be better to stick with the approach in the current collectSample for now, because it avoids these issues. In particular, it would be really hard to know in advance how many items are in each partition of the dataset, and pass the right number of items to the tasks that work on that object. The current code just tries to take a sample that is slightly bigger than the required number of items using independent coin flips on each element, and then pulls out the right number of items.", "created": "2012-02-05T23:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I closed the pull request because the older collectSample seems to work okay, but thanks for trying this out!", "created": "2012-03-06T13:39:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-107, originally reported by holdenk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-437\nSummary: Add support for fixed sized samples\nDescription: Fix for issue #93, adding support for fixed sized samples.\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Hey, sorry for being a bit late on this, I didn't see you posted a new pull request. Would you like me to grab the SortedRDD too or just the rest? As far as I understand, SortedRDD is not complete. We actually had our own effort to implement one so it might be better to use that instead.\n2. Patrick McFadin: Github comment from holdenk: Oh crap I forgot about sortedrdd being in my branch, just the other stuff would be cool :) On Feb 5, 2012 8:55 PM, \"Matei Zaharia\" < reply@reply.github.com> wrote: > Hey, sorry for being a bit late on this, I didn't see you posted a new > pull request. Would you like me to grab the SortedRDD too or just the rest? > As far as I understand, SortedRDD is not complete. We actually had our own > effort to implement one so it might be better to use that instead. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/107#issuecomment-3823642 >\n3. Patrick McFadin: Github comment from mateiz: Okay, so I looked at this more carefully, but unfortunately I think there are some problems: 1) There is an array index out of bounds exception on CollectSampledRDD.scala line 41 (in sampleData.update(i,oldData(i))), because the for loop there goes over indices from 1 to sampleSize instead of 0 to sampleSize-1. 2) It looks like the code is pulling out sampleSize items inside each split (partition) of the dataset, rather than sampleSize total items. This is not going to work as the user expects for datasets with more than 1 partition, unless we explicitly document this as the use of the function, which I don't like because I don't want the user to worry about the number of partitions. More generally, partitions might also have different numbers of items, so some might have fewer than sampleSize while some have more. I think it might be better to stick with the approach in the current collectSample for now, because it avoids these issues. In particular, it would be really hard to know in advance how many items are in each partition of the dataset, and pass the right number of items to the tasks that work on that object. The current code just tries to take a sample that is slightly bigger than the required number of items using independent coin flips on each element, and then pulls out the right number of items.\n4. Patrick McFadin: Github comment from mateiz: I closed the pull request because the older collectSample seems to work okay, but thanks for trying this out!\n5. Patrick McFadin: Imported from Github issue spark-107, originally reported by holdenk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.951086"}}
{"id": "e3e72ffd0cb9c2fef8bc9990a7dcc5c2", "issue_key": "SPARK-436", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added immutable map registration in kryo serializer", "description": "Reused existing map serializer.", "reporter": "patelh", "assignee": null, "created": "0012-01-26T15:33:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-01-30T16:31:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-108, originally reported by patelh", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-436\nSummary: Added immutable map registration in kryo serializer\nDescription: Reused existing map serializer.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n2. Patrick McFadin: Imported from Github issue spark-108, originally reported by patelh", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.951086"}}
{"id": "864f0feb4cc329537b70f88db54e08a0", "issue_key": "SPARK-435", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "LocalFileShuffle should not be an object", "description": "Right now this creates problems in programs that use multiple SparkContexts running locally (not on Mesos), most notably our tests. We should instead have a separate ShuffleManager per local scheduler.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-02-05T22:55:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Fixed in commit 0e93891d3d7df849cff6", "created": "2012-02-09T22:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-109, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-435\nSummary: LocalFileShuffle should not be an object\nDescription: Right now this creates problems in programs that use multiple SparkContexts running locally (not on Mesos), most notably our tests. We should instead have a separate ShuffleManager per local scheduler.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Fixed in commit 0e93891d3d7df849cff6\n2. Patrick McFadin: Imported from Github issue spark-109, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.951086"}}
{"id": "0347c87bf8adb6980c14e486de2e5ec6", "issue_key": "SPARK-434", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Bad behavior when saving to S3", "description": "Right now the output committer part of save() operations runs on the master, but unfortunately, when accessing S3 storage through Hadoop, this operation takes quite a long time. We should either run it in parallel (as a set of tasks) or not use the Hadoop IO libraries for S3.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-02-09T22:36:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-110, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-434\nSummary: Bad behavior when saving to S3\nDescription: Right now the output committer part of save() operations runs on the master, but unfortunately, when accessing S3 storage through Hadoop, this operation takes quite a long time. We should either run it in parallel (as a set of tasks) or not use the Hadoop IO libraries for S3.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-110, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.951086"}}
{"id": "68c69396efbd3bf0bb08767bbde83f42", "issue_key": "SPARK-433", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "Adding sorting to RDDs", "description": "Fix for issue #88, adding sort to RDDs. This adds a sortByKey(ascending) method to RDDs of key/value pairs where keys have an Ordered trait. The 'ascending' argument is a boolean to specify order. (Note: still getting unchecked / eliminated by erasure warnings for getPartition in RangePartitioner class)", "reporter": "Antonio Lupher", "assignee": "Reynold Xin", "created": "0012-02-11T01:10:00.000+0000", "updated": "2014-06-18T07:58:27.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Antonio, I made a few comments on the code (in the Diff tab). Also, do you mind adding a unit test for this? Add a file in core/src/test/spark. We use ScalaTest for testing (http://www.scalatest.org/). You can probably just copy and edit one of the existing tests to get the scaffolding right.", "created": "2012-02-12T10:14:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from alupher: Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well.", "created": "2012-02-13T00:09:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from alupher: By the way, that does take care of the type erasure warnings :)", "created": "2012-02-13T00:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great. By the way, the fixes look good, but I think you forgot to git add the test file. Matei On Feb 13, 2012, at 12:09 AM, Antonio Lupher wrote: > Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-3935835", "created": "2012-02-13T11:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Antonio, have you had a chance to work on a unit test for sorting yet? Matei On Feb 13, 2012, at 12:09 AM, Antonio Lupher wrote: > Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-3935835", "created": "2012-02-19T22:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from alupher: Hi Matei, just added some unit tests, let me know what you think. -Antonio", "created": "2012-02-21T19:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good. Thanks Antonio!", "created": "2012-02-24T15:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Antonio, Just FYI, I just made a commit to fix a few issues with sorting: - The range partitioner would crash with a division by zero if the RDD was smaller than the number of partitions requested. - Some stuff was inefficient -- in particular, all the local vals you declared to build up the rangeBounds remained as fields of the RangePartitioner object, which made it huge. - The unit test with high parallelism was taking way too long; we can speed it up further by replacing the getPartition code with a binary search but for now I've just left it out. Here's the commit: https://github.com/mesos/spark/commit/c7af538ac160727147eea6a1008dc16a2efd802e. Might be helpful for Shark too. Matei On Feb 21, 2012, at 7:57 PM, Antonio Lupher wrote: > Hi Matei, just added some unit tests, let me know what you think. > > -Antonio > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-4097751", "created": "2012-03-17T12:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from alupher: Hey Matei, Thanks for your notes on the commit, that was very helpful. We'll make sure to incorporate them in the version of Spark we're using. Sorry for missing some of those issues the first time around. We also recently updated our Spark to use defaultParallelism as the default number of partitions for the sortedRDD (and defined a sortByKey that lets users choose this value). Here's the relevant commit: https://github.com/alupher/spark/commit/13d5ffaf087e06e900ac9168d060a927274ff33e Do you think this would be good merge into the spark core? If so, I can send a pull request. Antonio On Sat, Mar 17, 2012 at 1:54 PM, Matei Zaharia <reply@reply.github.com> wrote: > Hey Antonio, > > Just FYI, I just made a commit to fix a few issues with sorting: > - The range partitioner would crash with a division by zero if the RDD was smaller than the number of partitions requested. > - Some stuff was inefficient -- in particular, all the local vals you declared to build up the rangeBounds remained as fields of the RangePartitioner object, which made it huge. > - The unit test with high parallelism was taking way too long; we can speed it up further by replacing the getPartition code with a binary search but for now I've just left it out. > > Here's the commit: https://github.com/mesos/spark/commit/c7af538ac160727147eea6a1008dc16a2efd802e. Might be helpful for Shark too. > > Matei > > On Feb 21, 2012, at 7:57 PM, Antonio Lupher wrote: > >> Hi Matei, just added some unit tests, let me know what you think. >> >> -Antonio >> >> --- >> Reply to this email directly or view it on GitHub: >> https://github.com/mesos/spark/pull/111#issuecomment-4097751 > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-4556990", "created": "2012-03-18T23:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-111, originally reported by alupher", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 10, "text": "Issue: SPARK-433\nSummary: Adding sorting to RDDs\nDescription: Fix for issue #88, adding sort to RDDs. This adds a sortByKey(ascending) method to RDDs of key/value pairs where keys have an Ordered trait. The 'ascending' argument is a boolean to specify order. (Note: still getting unchecked / eliminated by erasure warnings for getPartition in RangePartitioner class)\n\nComments (10):\n1. Patrick McFadin: Github comment from mateiz: Hey Antonio, I made a few comments on the code (in the Diff tab). Also, do you mind adding a unit test for this? Add a file in core/src/test/spark. We use ScalaTest for testing (http://www.scalatest.org/). You can probably just copy and edit one of the existing tests to get the scaffolding right.\n2. Patrick McFadin: Github comment from alupher: Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well.\n3. Patrick McFadin: Github comment from alupher: By the way, that does take care of the type erasure warnings :)\n4. Patrick McFadin: Github comment from mateiz: Great. By the way, the fixes look good, but I think you forgot to git add the test file. Matei On Feb 13, 2012, at 12:09 AM, Antonio Lupher wrote: > Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-3935835\n5. Patrick McFadin: Github comment from mateiz: Hey Antonio, have you had a chance to work on a unit test for sorting yet? Matei On Feb 13, 2012, at 12:09 AM, Antonio Lupher wrote: > Great, thanks for your comments, Matei. I've made the changes in my fork and will add a unit test as well. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-3935835\n6. Patrick McFadin: Github comment from alupher: Hi Matei, just added some unit tests, let me know what you think. -Antonio\n7. Patrick McFadin: Github comment from mateiz: Looks good. Thanks Antonio!\n8. Patrick McFadin: Github comment from mateiz: Hey Antonio, Just FYI, I just made a commit to fix a few issues with sorting: - The range partitioner would crash with a division by zero if the RDD was smaller than the number of partitions requested. - Some stuff was inefficient -- in particular, all the local vals you declared to build up the rangeBounds remained as fields of the RangePartitioner object, which made it huge. - The unit test with high parallelism was taking way too long; we can speed it up further by replacing the getPartition code with a binary search but for now I've just left it out. Here's the commit: https://github.com/mesos/spark/commit/c7af538ac160727147eea6a1008dc16a2efd802e. Might be helpful for Shark too. Matei On Feb 21, 2012, at 7:57 PM, Antonio Lupher wrote: > Hi Matei, just added some unit tests, let me know what you think. > > -Antonio > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-4097751\n9. Patrick McFadin: Github comment from alupher: Hey Matei, Thanks for your notes on the commit, that was very helpful. We'll make sure to incorporate them in the version of Spark we're using. Sorry for missing some of those issues the first time around. We also recently updated our Spark to use defaultParallelism as the default number of partitions for the sortedRDD (and defined a sortByKey that lets users choose this value). Here's the relevant commit: https://github.com/alupher/spark/commit/13d5ffaf087e06e900ac9168d060a927274ff33e Do you think this would be good merge into the spark core? If so, I can send a pull request. Antonio On Sat, Mar 17, 2012 at 1:54 PM, Matei Zaharia <reply@reply.github.com> wrote: > Hey Antonio, > > Just FYI, I just made a commit to fix a few issues with sorting: > - The range partitioner would crash with a division by zero if the RDD was smaller than the number of partitions requested. > - Some stuff was inefficient -- in particular, all the local vals you declared to build up the rangeBounds remained as fields of the RangePartitioner object, which made it huge. > - The unit test with high parallelism was taking way too long; we can speed it up further by replacing the getPartition code with a binary search but for now I've just left it out. > > Here's the commit: https://github.com/mesos/spark/commit/c7af538ac160727147eea6a1008dc16a2efd802e. Might be helpful for Shark too. > > Matei > > On Feb 21, 2012, at 7:57 PM, Antonio Lupher wrote: > >> Hi Matei, just added some unit tests, let me know what you think. >> >> -Antonio >> >> --- >> Reply to this email directly or view it on GitHub: >> https://github.com/mesos/spark/pull/111#issuecomment-4097751 > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/111#issuecomment-4556990\n10. Patrick McFadin: Imported from Github issue spark-111, originally reported by alupher", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.951086"}}
{"id": "94e6c5ae295dfa3e3dd264eccd862201", "issue_key": "SPARK-432", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Changed HadoopRDD to get key and value containers from the RecordReader instead of through reflection", "description": "This allows users to specify the key or value class of a HadoopRDD as Writable, whereas they needed to use a subclass of Writable before. I ran the FileSuite and everything passed.", "reporter": "Cliff Engle", "assignee": null, "created": "0012-02-29T16:44:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks great, thanks!", "created": "2012-03-06T13:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-112, originally reported by cengle", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-432\nSummary: Changed HadoopRDD to get key and value containers from the RecordReader instead of through reflection\nDescription: This allows users to specify the key or value class of a HadoopRDD as Writable, whereas they needed to use a subclass of Writable before. I ran the FileSuite and everything passed.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks great, thanks!\n2. Patrick McFadin: Imported from Github issue spark-112, originally reported by cengle", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.952088"}}
{"id": "9527892a83c007a482d9d65fac79cd85", "issue_key": "SPARK-431", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Update to work with Mesos 0.9 / 1.0 API", "description": "Mesos is going to make a first stable release as an Apache project soon, and as a result, there have been some API changes in it for long term evolvability. We need to update Spark In the meantime, revision 1205738 of Mesos is what I recommend for people using the current Spark code. You can check it out from SVN with svn checkout -r 1205738 http://svn.apache.org/repos/asf/incubator/mesos/trunk mesos", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-03-06T13:42:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Branch mesos-0.9 (https://github.com/mesos/spark/tree/mesos-0.9) now supports the newest Mesos. It also uses Hadoop 0.20.205 by default instead of 0.20 as before.", "created": "2012-03-17T12:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Just updated the branch with further fixes that work with Mesos 0.9 RC3.", "created": "2012-03-30T09:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: Right now, mesos is in flux and some people are reporting some issues either with running spark or with building mesos. Can we make sure that the wiki states somewhere that casual users should use revision 1205738 of mesos?", "created": "2012-04-23T14:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: FYI, the last working setup I have is: mesos r1205738 + spark branch 0.3-scala-2.9", "created": "2012-04-23T14:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: The master branch of Spark works fine with Mesos revision 1205738, and it's documented here: https://github.com/mesos/spark/wiki/Running-spark-on-mesos. I actually recommend using the master branch over 0.3 because it contains many fixes. We'll make a new release as soon as there is an official Apache release of Mesos (which we're just waiting for some votes on from the Apache incubator PMC).", "created": "2012-04-23T14:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This is released now.", "created": "2012-06-15T22:08:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-113, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-431\nSummary: Update to work with Mesos 0.9 / 1.0 API\nDescription: Mesos is going to make a first stable release as an Apache project soon, and as a result, there have been some API changes in it for long term evolvability. We need to update Spark In the meantime, revision 1205738 of Mesos is what I recommend for people using the current Spark code. You can check it out from SVN with svn checkout -r 1205738 http://svn.apache.org/repos/asf/incubator/mesos/trunk mesos\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: Branch mesos-0.9 (https://github.com/mesos/spark/tree/mesos-0.9) now supports the newest Mesos. It also uses Hadoop 0.20.205 by default instead of 0.20 as before.\n2. Patrick McFadin: Github comment from mateiz: Just updated the branch with further fixes that work with Mesos 0.9 RC3.\n3. Patrick McFadin: Github comment from tjhunter: Right now, mesos is in flux and some people are reporting some issues either with running spark or with building mesos. Can we make sure that the wiki states somewhere that casual users should use revision 1205738 of mesos?\n4. Patrick McFadin: Github comment from tjhunter: FYI, the last working setup I have is: mesos r1205738 + spark branch 0.3-scala-2.9\n5. Patrick McFadin: Github comment from mateiz: The master branch of Spark works fine with Mesos revision 1205738, and it's documented here: https://github.com/mesos/spark/wiki/Running-spark-on-mesos. I actually recommend using the master branch over 0.3 because it contains many fixes. We'll make a new release as soon as there is an official Apache release of Mesos (which we're just waiting for some votes on from the Apache incubator PMC).\n6. Patrick McFadin: Github comment from mateiz: This is released now.\n7. Patrick McFadin: Imported from Github issue spark-113, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.952088"}}
{"id": "348902157dacb6473596dbd83e3070c2", "issue_key": "SPARK-430", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Arthur: Replay debugger for Spark", "description": "Arthur provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets `spark.arthur.logPath`. The user can then load the log and replay the program using `EventLogReader`.", "reporter": "Ankur Dave", "assignee": null, "created": "0012-03-21T13:07:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-114, originally reported by ankurdave", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-430\nSummary: Arthur: Replay debugger for Spark\nDescription: Arthur provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets `spark.arthur.logPath`. The user can then load the log and replay the program using `EventLogReader`.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-114, originally reported by ankurdave", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.952088"}}
{"id": "cf3ac262918936f1285b0e4ecbd0fc93", "issue_key": "SPARK-550", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "Hiding the default spark context in the spark shell creates serialization issues", "description": "I copy-pasted a piece of code along these lines in the spark shell: ... val sc = new SparkContext(\"local[%d]\" format num_splits,\"myframework\") val my_rdd = sc.textFile(...) my_rdd.count() This leads to the shell crashing with a java.io.NotSerializableException: spark.SparkContext It took me a while to realize it was due to the new spark context created. Maybe a warning/error should be triggered if the user tries to change the definition of sc?", "reporter": "tjhunter", "assignee": null, "created": "0012-04-03T20:41:00.000+0000", "updated": "2014-09-21T15:35:26.000+0000", "resolved": "2014-09-21T15:35:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-115, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matthew Farrellee", "body": "a lot of code has changed in this space over the past 2 years. i'm going to close this, but feel free to re-open if you feel it's still an issue.", "created": "2014-09-21T15:35:13.823+0000"}], "num_comments": 2, "text": "Issue: SPARK-550\nSummary: Hiding the default spark context in the spark shell creates serialization issues\nDescription: I copy-pasted a piece of code along these lines in the spark shell: ... val sc = new SparkContext(\"local[%d]\" format num_splits,\"myframework\") val my_rdd = sc.textFile(...) my_rdd.count() This leads to the shell crashing with a java.io.NotSerializableException: spark.SparkContext It took me a while to realize it was due to the new spark context created. Maybe a warning/error should be triggered if the user tries to change the definition of sc?\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-115, originally reported by tjhunter\n2. Matthew Farrellee: a lot of code has changed in this space over the past 2 years. i'm going to close this, but feel free to re-open if you feel it's still an issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.952088"}}
{"id": "229646adcfbcda945087ad96eb15ac76", "issue_key": "SPARK-549", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Move mesos.jar to a maven repository", "description": "The mesos jar file is embedded in the code and is not deployed when spark is published locally: using publish-local and then adding an sbt dependency to a project that depends on spark does not work correctly. I am not sure if it is a spark issue or a mesos issue at this point. Feel free to close the bug and to report it upstream to mesos.", "reporter": "tjhunter", "assignee": null, "created": "0012-04-04T13:28:00.000+0000", "updated": "2012-10-22T15:10:31.000+0000", "resolved": "2012-10-22T15:10:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Good idea; we'll try to do this when we make the first Apache release of Mesos, which should be soon.", "created": "2012-04-05T13:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-116, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Looks like this was done in the latest releases.", "created": "2012-10-20T17:10:49.122+0000"}], "num_comments": 3, "text": "Issue: SPARK-549\nSummary: Move mesos.jar to a maven repository\nDescription: The mesos jar file is embedded in the code and is not deployed when spark is published locally: using publish-local and then adding an sbt dependency to a project that depends on spark does not work correctly. I am not sure if it is a spark issue or a mesos issue at this point. Feel free to close the bug and to report it upstream to mesos.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Good idea; we'll try to do this when we make the first Apache release of Mesos, which should be soon.\n2. Patrick McFadin: Imported from Github issue spark-116, originally reported by tjhunter\n3. Josh Rosen: Looks like this was done in the latest releases.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.952088"}}
{"id": "cf457bf3ef5ba1e357a553742b51577d", "issue_key": "SPARK-429", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "spark.master.host should be set to local IP address instead of hostname", "description": "In some clusters, the machine's short hostname, returned by InetAddress.getLocalHost().getHostName(), isn't its fully qualified name, which makes it not resolve from other machines. We should use the local IP instead.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-04-05T13:47:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-117, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-429\nSummary: spark.master.host should be set to local IP address instead of hostname\nDescription: In some clusters, the machine's short hostname, returned by InetAddress.getLocalHost().getHostName(), isn't its fully qualified name, which makes it not resolve from other machines. We should use the local IP instead.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-117, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.953088"}}
{"id": "fd44ec7f04e14404b7693b13f6d5c27d", "issue_key": "SPARK-428", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Update the examples to show how to ship the job's code to a cluster", "description": "They need to use the extended SparkContext constructor with a sparkHome and a list of JARs.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-04-06T09:08:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-118, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-428\nSummary: Update the examples to show how to ship the job's code to a cluster\nDescription: They need to use the extended SparkContext constructor with a sparkHome and a list of JARs.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-118, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.953088"}}
{"id": "1af77d5c336fa81f58c5a8d58282018f", "issue_key": "SPARK-427", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Report entry dropping in BoundedMemoryCache", "description": "When BoundedMemoryCache dropped a key, it previously only logged the event to the local log on the slave. Now it reports that fact to the master as well. This makes it possible to write tools that process the driver output and visualize cache usage across the cluster. It also will make it easier for Arthur to track this information.", "reporter": "Ankur Dave", "assignee": null, "created": "0012-04-06T14:56:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: This is awesome :)", "created": "2012-04-06T14:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks!", "created": "2012-04-06T14:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-119, originally reported by ankurdave", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-427\nSummary: Report entry dropping in BoundedMemoryCache\nDescription: When BoundedMemoryCache dropped a key, it previously only logged the event to the local log on the slave. Now it reports that fact to the master as well. This makes it possible to write tools that process the driver output and visualize cache usage across the cluster. It also will make it easier for Arthur to track this information.\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: This is awesome :)\n2. Patrick McFadin: Github comment from mateiz: Thanks!\n3. Patrick McFadin: Imported from Github issue spark-119, originally reported by ankurdave", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.953088"}}
{"id": "61d0189eecfca600f80ebf6a361522c3", "issue_key": "SPARK-548", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Update Kryo to use 2.x", "description": "Kryo just released new 2.x versions. It has a number of useful changes. We should upgrade it soon. The only downside is that there is no maven pom for the new versions yet. A few useful things: 1. Custom Serializer interface is now different. It no longer uses a ByteBuffer. 2. Better performance for serializing byte arrays.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "0012-04-09T15:33:00.000+0000", "updated": "2013-01-20T12:29:48.000+0000", "resolved": "2013-01-20T12:29:48.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Cool! Let's wait until it's on Maven for now though because it seems like they want to do that soon. If you want to play with it before that, make your own separate branch.", "created": "2012-04-09T15:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: I pushed a first implementation to kryo-2.x branch. Should still wait for this project to be updated: https://github.com/magro/kryo-serializers", "created": "2012-04-17T11:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from velvia: By the way, I believe the code to serialize options (ie Some) does not work, because Kryo 1.x requires a no-arg constructor, and the Some() constructor requires a value.", "created": "2012-05-16T10:09:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: de.javakaffee.kryoserializers.KryoReflectionFactorySupport should add the support for no-arg constructor. It uses sun.reflect.ReflectionFactory to find the constructors ..", "created": "2012-05-16T10:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from velvia: Ah, that won't work for openjdk will it? -Evan Carry your candle, run to the darkness Seek out the helpless, deceived and poor Hold out your candle for all to see it Take your candle, and go light your world On May 16, 2012, at 11:30 AM, Reynold Xin<reply@reply.github.com> wrote: > de.javakaffee.kryoserializers.KryoReflectionFactorySupport should add the support for no-arg constructor. > > It uses sun.reflect.ReflectionFactory to find the constructors .. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/120#issuecomment-5748216", "created": "2012-05-16T12:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Fortunately openjdk implements this package too :) http://hg.openjdk.java.net/jdk6/jdk6/jdk/file/tip/src/share/classes/sun/reflect/ReflectionFactory.java", "created": "2012-05-16T12:49:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: But yes - it is not a standard feature and it is plausible that not all JDKs have it.", "created": "2012-05-16T12:49:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-120, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Looks like this was done in https://github.com/mesos/spark/pull/341, so I'm resolving this issue.", "created": "2013-01-20T12:29:48.248+0000"}], "num_comments": 9, "text": "Issue: SPARK-548\nSummary: Update Kryo to use 2.x\nDescription: Kryo just released new 2.x versions. It has a number of useful changes. We should upgrade it soon. The only downside is that there is no maven pom for the new versions yet. A few useful things: 1. Custom Serializer interface is now different. It no longer uses a ByteBuffer. 2. Better performance for serializing byte arrays.\n\nComments (9):\n1. Patrick McFadin: Github comment from mateiz: Cool! Let's wait until it's on Maven for now though because it seems like they want to do that soon. If you want to play with it before that, make your own separate branch.\n2. Patrick McFadin: Github comment from rxin: I pushed a first implementation to kryo-2.x branch. Should still wait for this project to be updated: https://github.com/magro/kryo-serializers\n3. Patrick McFadin: Github comment from velvia: By the way, I believe the code to serialize options (ie Some) does not work, because Kryo 1.x requires a no-arg constructor, and the Some() constructor requires a value.\n4. Patrick McFadin: Github comment from rxin: de.javakaffee.kryoserializers.KryoReflectionFactorySupport should add the support for no-arg constructor. It uses sun.reflect.ReflectionFactory to find the constructors ..\n5. Patrick McFadin: Github comment from velvia: Ah, that won't work for openjdk will it? -Evan Carry your candle, run to the darkness Seek out the helpless, deceived and poor Hold out your candle for all to see it Take your candle, and go light your world On May 16, 2012, at 11:30 AM, Reynold Xin<reply@reply.github.com> wrote: > de.javakaffee.kryoserializers.KryoReflectionFactorySupport should add the support for no-arg constructor. > > It uses sun.reflect.ReflectionFactory to find the constructors .. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/120#issuecomment-5748216\n6. Patrick McFadin: Github comment from rxin: Fortunately openjdk implements this package too :) http://hg.openjdk.java.net/jdk6/jdk6/jdk/file/tip/src/share/classes/sun/reflect/ReflectionFactory.java\n7. Patrick McFadin: Github comment from rxin: But yes - it is not a standard feature and it is plausible that not all JDKs have it.\n8. Patrick McFadin: Imported from Github issue spark-120, originally reported by rxin\n9. Josh Rosen: Looks like this was done in https://github.com/mesos/spark/pull/341, so I'm resolving this issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.953088"}}
{"id": "4e7ddce18aed7390c1610b774e2f5a09", "issue_key": "SPARK-426", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added an option (spark.closure.serializer) to specify the serializer for closures.", "description": "This enables using Kryo as the closure serializer.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-04-09T21:02:00.000+0000", "updated": "2014-11-10T17:44:48.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Cool! Does this mean that closures can be serialized properly this way? Since the parameter is called spark.closure.serializer, I would not change Utils.serialize and Utils.deserialize to use it, because the names for those would be misleading. Instead, create an instance of the closure serializer in SparkEnv and change the places that pass a closure or a TaskResult to use it.", "created": "2012-04-09T22:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: I will make the change accordingly. I tested it and it worked. Previously the problem was that Class[_] was not serialized properly. This was solved by using a ClassSerializer in Kryo. (In 2.0, this becomes automatic.)", "created": "2012-04-09T22:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Updated. PTAL.", "created": "2012-04-10T12:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good. Thanks!", "created": "2012-04-10T13:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-121, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-426\nSummary: Added an option (spark.closure.serializer) to specify the serializer for closures.\nDescription: This enables using Kryo as the closure serializer.\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Cool! Does this mean that closures can be serialized properly this way? Since the parameter is called spark.closure.serializer, I would not change Utils.serialize and Utils.deserialize to use it, because the names for those would be misleading. Instead, create an instance of the closure serializer in SparkEnv and change the places that pass a closure or a TaskResult to use it.\n2. Patrick McFadin: Github comment from rxin: I will make the change accordingly. I tested it and it worked. Previously the problem was that Class[_] was not serialized properly. This was solved by using a ClassSerializer in Kryo. (In 2.0, this becomes automatic.)\n3. Patrick McFadin: Github comment from rxin: Updated. PTAL.\n4. Patrick McFadin: Github comment from mateiz: Looks good. Thanks!\n5. Patrick McFadin: Imported from Github issue spark-121, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.953088"}}
{"id": "e30f38af6fc08a3ab439dd561b0bbaac", "issue_key": "SPARK-547", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Provide a means to package Spark's executor into a tgz", "description": "Often the most convenient way to run Spark on Mesos would be to have a tar.gz that gets fetched by the nodes through HTTP or HDFS. We should have a sbt target that produces a streamlined tgz (probably just our JARs and the executor script).", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-04-13T21:39:00.000+0000", "updated": "2014-10-21T07:38:26.000+0000", "resolved": "2014-10-21T07:38:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-122, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Patrick Wendell", "body": "This was fixed a long time ago.", "created": "2014-10-21T07:38:26.896+0000"}], "num_comments": 2, "text": "Issue: SPARK-547\nSummary: Provide a means to package Spark's executor into a tgz\nDescription: Often the most convenient way to run Spark on Mesos would be to have a tar.gz that gets fetched by the nodes through HTTP or HDFS. We should have a sbt target that produces a streamlined tgz (probably just our JARs and the executor script).\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-122, originally reported by mateiz\n2. Patrick Wendell: This was fixed a long time ago.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.953088"}}
{"id": "84eadf6799271c5d27c6e8639f675a96", "issue_key": "SPARK-546", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support full outer join and multiple join in a single shuffle", "description": "RDD[(K,V)] now supports left/right outer join but not full outer join. Also it'd be nice to provide a way for users to join multiple RDDs on the same key in a single shuffle.", "reporter": "Reynold Xin", "assignee": "Aaron Staple", "created": "0012-04-14T11:21:00.000+0000", "updated": "2014-12-04T17:51:56.000+0000", "resolved": "2014-09-25T03:39:44.000+0000", "labels": [], "components": ["DStreams", "Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-123, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "sam", "body": "We use a pimp-my-library pattern to add this functionality. Basically here's our code http://pastebin.com/EQ0Jm4Rj Hope it helps :) (disclaimer - code in testing)", "created": "2014-07-09T15:39:49.000+0000"}, {"author": "Aaron Staple", "body": "I created a PR for a full outer join implementation here: https://github.com/apache/spark/pull/1395 If there is interest I can also implement multiJoin.", "created": "2014-07-14T15:33:23.507+0000"}, {"author": "Patrick Wendell", "body": "Fixed by: https://github.com/apache/spark/pull/1395", "created": "2014-09-25T03:39:44.148+0000"}, {"author": "Aaron Staple", "body": "Hi, I think there are two features requested in this ticket: 1) full outer join 2) an RDD function to join >2 rdds in a single shuffle (e.g. multiJoin function) I’ve implemented #1 in my recent PR, but not #2. I’m happy to implement #2 as well though. Would it make sense to reopen this ticket? File a new ticket?", "created": "2014-09-25T18:03:01.641+0000"}, {"author": "Thiago Souza", "body": "What about #2? Did you file a new ticket? I'm quite interested on this!", "created": "2014-12-04T17:27:56.434+0000"}, {"author": "Reynold Xin", "body": "Actually my experience implementing full join in a single shuffle is that it is fairly complicated and very hard to maintain. Since it is doable entirely in user code and given SparkSQL's SchemaRDD already supports it, I suggest not pulling this in Spark core.", "created": "2014-12-04T17:51:56.442+0000"}], "num_comments": 7, "text": "Issue: SPARK-546\nSummary: Support full outer join and multiple join in a single shuffle\nDescription: RDD[(K,V)] now supports left/right outer join but not full outer join. Also it'd be nice to provide a way for users to join multiple RDDs on the same key in a single shuffle.\n\nComments (7):\n1. Patrick McFadin: Imported from Github issue spark-123, originally reported by rxin\n2. sam: We use a pimp-my-library pattern to add this functionality. Basically here's our code http://pastebin.com/EQ0Jm4Rj Hope it helps :) (disclaimer - code in testing)\n3. Aaron Staple: I created a PR for a full outer join implementation here: https://github.com/apache/spark/pull/1395 If there is interest I can also implement multiJoin.\n4. Patrick Wendell: Fixed by: https://github.com/apache/spark/pull/1395\n5. Aaron Staple: Hi, I think there are two features requested in this ticket: 1) full outer join 2) an RDD function to join >2 rdds in a single shuffle (e.g. multiJoin function) I’ve implemented #1 in my recent PR, but not #2. I’m happy to implement #2 as well though. Would it make sense to reopen this ticket? File a new ticket?\n6. Thiago Souza: What about #2? Did you file a new ticket? I'm quite interested on this!\n7. Reynold Xin: Actually my experience implementing full join in a single shuffle is that it is fairly complicated and very hard to maintain. Since it is doable entirely in user code and given SparkSQL's SchemaRDD already supports it, I suggest not pulling this in Spark core.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.953088"}}
{"id": "e0c684cfe59cdcc013753e9df095bb11", "issue_key": "SPARK-425", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added the ability to set environmental variables in piped rdd.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-04-17T15:41:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks great, thanks!", "created": "2012-04-20T11:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-124, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-425\nSummary: Added the ability to set environmental variables in piped rdd.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks great, thanks!\n2. Patrick McFadin: Imported from Github issue spark-124, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.954178"}}
{"id": "1c5d690a06234762e4b7487020622040", "issue_key": "SPARK-424", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fix issues with JAR server in mesos-0.9 branch", "description": "Found an issue testing this code at Twitter: in the mesos-0.9 branch, the ExecutorInfo in MesosScheduler is initialized before we have the right list of JARs to send, leading to none of the users' JARs being sent.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-04-20T12:01:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed now.", "created": "2012-06-06T13:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-125, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-424\nSummary: Fix issues with JAR server in mesos-0.9 branch\nDescription: Found an issue testing this code at Twitter: in the mesos-0.9 branch, the ExecutorInfo in MesosScheduler is initialized before we have the right list of JARs to send, leading to none of the users' JARs being sent.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is fixed now.\n2. Patrick McFadin: Imported from Github issue spark-125, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.954178"}}
{"id": "905912ff83a991a979f8e0dbc5fb9427", "issue_key": "SPARK-423", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark executor should use Mesos-provided hostname for itself in cache tracker updates, etc", "description": "One user reported having machines where Mesos gets the FQDN and Spark only gets the unqualified host name. This was causing the scheduler to not realize that it can launch data-local tasks on those machines. Maybe we can also call getCanonicalHostname() instead, but this approach seems safer.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-04-20T23:31:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is done in the dev branch.", "created": "2012-06-29T15:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-126, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-423\nSummary: Spark executor should use Mesos-provided hostname for itself in cache tracker updates, etc\nDescription: One user reported having machines where Mesos gets the FQDN and Spark only gets the unqualified host name. This was causing the scheduler to not realize that it can launch data-local tasks on those machines. Maybe we can also call getCanonicalHostname() instead, but this approach seems safer.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is done in the dev branch.\n2. Patrick McFadin: Imported from Github issue spark-126, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.954178"}}
{"id": "cb943914e63d8018f3a4a92ff95eae19", "issue_key": "SPARK-422", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "End task instead of just exiting in LocalScheduler for tasks that throw exceptions", "description": "(our Shark testing suite driver was hanging on tests for which an exception was thrown on a worker)", "reporter": "Antonio Lupher", "assignee": null, "created": "0012-04-22T23:39:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Two comments: - Don't leave commented-out code in the repo (the //System.exit(1)); we can always see the old code using version control. - Do Spark's own unit tests (sbt/sbt test) still pass? Some of them do test exceptions thrown in tasks (but use the LocalScheduler setting that allows some failures).", "created": "2012-04-23T09:51:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from alupher: Yep, the tests pass. I've removed the commented-out line.", "created": "2012-04-23T10:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-04-24T15:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-127, originally reported by alupher", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-422\nSummary: End task instead of just exiting in LocalScheduler for tasks that throw exceptions\nDescription: (our Shark testing suite driver was hanging on tests for which an exception was thrown on a worker)\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Two comments: - Don't leave commented-out code in the repo (the //System.exit(1)); we can always see the old code using version control. - Do Spark's own unit tests (sbt/sbt test) still pass? Some of them do test exceptions thrown in tasks (but use the LocalScheduler setting that allows some failures).\n2. Patrick McFadin: Github comment from alupher: Yep, the tests pass. I've removed the commented-out line.\n3. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n4. Patrick McFadin: Imported from Github issue spark-127, originally reported by alupher", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.954178"}}
{"id": "8c7680edf418ce28d2c5826cfd1e790b", "issue_key": "SPARK-421", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Update spark-yarn project to support newest version of YARN", "description": "This is not a huge change but requires some work due to modifications to the YARN API.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-04-23T09:49:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Now fixed in the yarn branch of Spark.", "created": "2012-08-27T11:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-128, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-421\nSummary: Update spark-yarn project to support newest version of YARN\nDescription: This is not a huge change but requires some work due to modifications to the YARN API.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Now fixed in the yarn branch of Spark.\n2. Patrick McFadin: Imported from Github issue spark-128, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.954178"}}
{"id": "c733787010436ce79e61fe2bde10fb49", "issue_key": "SPARK-420", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Force serialize/deserialize task results in local execution mode.", "description": "In Shark, we were caught by surprise when we tested map join implementation on a cluster that it'd failed because of serialization. This commit forces a serialization/deserialization of task result data even for local modes to avoid such surprises.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-04-24T13:58:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: I added a new test for this too. Note that the test passing is contingent upon Antonio's patch (otherwise System.exit wouldn't give scala test a chance to intercept the exception).", "created": "2012-04-24T14:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-04-24T15:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-129, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-420\nSummary: Force serialize/deserialize task results in local execution mode.\nDescription: In Shark, we were caught by surprise when we tested map join implementation on a cluster that it'd failed because of serialization. This commit forces a serialization/deserialization of task result data even for local modes to avoid such surprises.\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: I added a new test for this too. Note that the test passing is contingent upon Antonio's patch (otherwise System.exit wouldn't give scala test a chance to intercept the exception).\n2. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n3. Patrick McFadin: Imported from Github issue spark-129, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.955185"}}
{"id": "44142e4707a68e75dd3556fd62510c0c", "issue_key": "SPARK-545", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "support external sort", "description": "Currently, sort operation can put too much pressure on memory and it is hard to guess what the right number of reduce partitions to use for sort. External sort can mitigate this problem.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-05-02T13:17:00.000+0000", "updated": "2013-12-07T12:59:15.000+0000", "resolved": "2013-12-07T12:59:15.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-130, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-545\nSummary: support external sort\nDescription: Currently, sort operation can put too much pressure on memory and it is hard to guess what the right number of reduce partitions to use for sort. External sort can mitigate this problem.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-130, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.955185"}}
{"id": "843f3d54dc5db1a2c30fe02c48f0a8f4", "issue_key": "SPARK-419", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Return size estimation, cache usage, and cache capacity from slave nodes to CacheTracker", "description": "Also updated the log messages to give information on cache capacity when entries are added/dropped.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-05-15T11:07:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: I added another commit into this. It is the one that reduces the amount of data to serialize in closure serialization by making dependencies field transient.", "created": "2012-05-16T13:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks!", "created": "2012-05-18T14:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-131, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-419\nSummary: Return size estimation, cache usage, and cache capacity from slave nodes to CacheTracker\nDescription: Also updated the log messages to give information on cache capacity when entries are added/dropped.\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: I added another commit into this. It is the one that reduces the amount of data to serialize in closure serialization by making dependencies field transient.\n2. Patrick McFadin: Github comment from mateiz: Great, thanks!\n3. Patrick McFadin: Imported from Github issue spark-131, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.955185"}}
{"id": "deb6d28dd53c928b9e54d81e1f5db440", "issue_key": "SPARK-418", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Little refactoring and unit tests for CacheTrackerActor", "description": "", "reporter": "Benky", "assignee": null, "created": "0012-05-20T00:12:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks a lot for these fixes! I had some comments on Utils.scala though -- in particular, leave the fast splitString method in because we've used it in some benchmarks, and revert the simplification of copyStream because it will be confusing to people without a lot of knowledge of how those FP operations work. If you make those changes, I'll merge this in.", "created": "2012-05-21T13:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for the updates. I'll add in a program to use the splitString thing soon (we basically used it to build a fast WordCount program, but it's mostly been in demos we typed at the command line).", "created": "2012-05-26T12:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-132, originally reported by Benky", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-418\nSummary: Little refactoring and unit tests for CacheTrackerActor\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Thanks a lot for these fixes! I had some comments on Utils.scala though -- in particular, leave the fast splitString method in because we've used it in some benchmarks, and revert the simplification of copyStream because it will be confusing to people without a lot of knowledge of how those FP operations work. If you make those changes, I'll merge this in.\n2. Patrick McFadin: Github comment from mateiz: Thanks for the updates. I'll add in a program to use the splitString thing soon (we basically used it to build a fast WordCount program, but it's mostly been in demos we typed at the command line).\n3. Patrick McFadin: Imported from Github issue spark-132, originally reported by Benky", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.955185"}}
{"id": "a6a85e5471f99160f8c2ce7b57460d84", "issue_key": "SPARK-417", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "BoundedMemoryCache.put should fail when estimated size of 'value' is larger than cache capacity", "description": "Current implementation allows you to store object larger than BoundedMemoryCache.getCapacity into cache, which seems to be wrong.", "reporter": "Benky", "assignee": null, "created": "0012-05-20T13:30:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-05-26T11:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-133, originally reported by Benky", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-417\nSummary: BoundedMemoryCache.put should fail when estimated size of 'value' is larger than cache capacity\nDescription: Current implementation allows you to store object larger than BoundedMemoryCache.getCapacity into cache, which seems to be wrong.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n2. Patrick McFadin: Imported from Github issue spark-133, originally reported by Benky", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.955185"}}
{"id": "b5c231a387e56aedbced1304e3443682", "issue_key": "SPARK-416", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "ShuffleManager & RDD refactored. Some unit tests added.", "description": "", "reporter": "Benky", "assignee": null, "created": "0012-05-27T14:42:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for posting this. Unfortunately this is a rather busy week for me, but I hope to look at it in the next few days.", "created": "2012-05-31T10:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Finally got a chance to look at this. It looks great except for some formatting and a bug and possible performance regression in count() (you should check how Iterator.size is implemented -- I think it uses foreach() and ends up being slower than what I wrote).", "created": "2012-06-06T14:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Closing this pull request because the code has diverged very far now. Let me know if you'd still like some of these to be added.", "created": "2012-08-17T02:31:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-134, originally reported by Benky", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-416\nSummary: ShuffleManager & RDD refactored. Some unit tests added.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Thanks for posting this. Unfortunately this is a rather busy week for me, but I hope to look at it in the next few days.\n2. Patrick McFadin: Github comment from mateiz: Finally got a chance to look at this. It looks great except for some formatting and a bug and possible performance regression in count() (you should check how Iterator.size is implemented -- I think it uses foreach() and ends up being slower than what I wrote).\n3. Patrick McFadin: Github comment from mateiz: Closing this pull request because the code has diverged very far now. Let me know if you'd still like some of these to be added.\n4. Patrick McFadin: Imported from Github issue spark-134, originally reported by Benky", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.955185"}}
{"id": "7aa0d5d7a097d231784fa194269e9168", "issue_key": "SPARK-415", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make spark.repl.Main.interp_ publicly accessible", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-05-30T17:45:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks.", "created": "2012-05-30T17:49:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-135, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-415\nSummary: Make spark.repl.Main.interp_ publicly accessible\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks.\n2. Patrick McFadin: Imported from Github issue spark-135, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.956183"}}
{"id": "d18b216cba46070f4e8707f2fb3992ab", "issue_key": "SPARK-413", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Standalone deploy mode", "description": "Spark should provide a pure Java way of deploying on a cluster that people can try out if they only want to run just Spark, without Mesos or Hadoop alongside.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-06-02T12:22:00.000+0000", "updated": "2012-10-22T14:55:31.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is pretty much done in the dev branch now.", "created": "2012-08-04T16:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-137, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-413\nSummary: Standalone deploy mode\nDescription: Spark should provide a pure Java way of deploying on a cluster that people can try out if they only want to run just Spark, without Mesos or Hadoop alongside.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is pretty much done in the dev branch now.\n2. Patrick McFadin: Imported from Github issue spark-137, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.956183"}}
{"id": "13239e481e6fe7799b06923e5b755441", "issue_key": "SPARK-414", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "SizeEstimator's sampling should reuse SearchState", "description": "If you have an array where many elements share a pointer to the same object, the SizeEstimator currently double-counts that object, leading to overly large estimates.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-06-02T12:22:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-136, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-414\nSummary: SizeEstimator's sampling should reuse SearchState\nDescription: If you have an array where many elements share a pointer to the same object, the SizeEstimator currently double-counts that object, leading to overly large estimates.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-136, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.956183"}}
{"id": "9bcb2383e684bc825cd02ec7da2156a8", "issue_key": "SPARK-412", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "run spark.examples.SparkPi hangs with no results", "description": "Hi there, I've been trying to make spark work on Mesos and have been stuck. I've tried everything I could have think of. Unfortuantely I can't seem to figure out what's not setup right. When i run ./run spark.examples.SparkPi master@10.244.1.147:5050 Spark gets stuck at: 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 here's the complete output. I'm also adding the output from my mesos-master. Any help would be truly appreciated. Spark Output: ./run spark.examples.SparkPi master@10.244.1.147:5050 12/06/03 17:42:26 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 12/06/03 17:42:26 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal 12/06/03 17:42:27 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/06/03 17:42:27 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-71f91c61-c132-4abb-9437-ac736ca0ed8f/shuffle 12/06/03 17:42:27 INFO server.Server: jetty-7.5.3.v20111011 12/06/03 17:42:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33864 STARTING 12/06/03 17:42:27 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:33864 12/06/03 17:42:27 INFO spark.SparkContext: Starting job... 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Asked for current cache locations 12/06/03 17:42:27 INFO spark.MesosScheduler: Final stage: Stage 0 12/06/03 17:42:27 INFO spark.MesosScheduler: Parents of final stage: List() 12/06/03 17:42:27 INFO spark.MesosScheduler: Missing parents: List() 12/06/03 17:42:27 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/03 17:42:27 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/03 17:42:27 INFO spark.MesosScheduler: Registered as framework ID 201206031742-0-0000 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 Master: I0603 17:42:18.808145 31518 logging.cpp:70] Logging to /tmp/install/mesos/logs I0603 17:42:18.809634 31518 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop I0603 17:42:18.809680 31518 main.cpp:96] Starting Mesos master I0603 17:42:18.810984 31519 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 I0603 17:42:18.811133 31519 master.cpp:279] Master ID: 201206031742-0 I0603 17:42:18.811455 31519 master.cpp:462] Elected as master! I0603 17:42:22.068495 31519 master.cpp:814] Attempting to register slave 201206031742-0-0 at slave@10.244.1.196:52813 I0603 17:42:22.069000 31519 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:52813 as active I0603 17:42:22.069066 31519 master.cpp:1588] Adding slave 201206031742-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=1; mem=1024 I0603 17:42:22.069295 31519 simple_allocator.cpp:71] Added slave 201206031742-0-0 with cpus=1; mem=1024 I0603 17:42:27.985856 31519 master.cpp:492] Registering framework 201206031742-0-0000 at 2@10.244.1.147:55998 I0603 17:42:27.986047 31519 simple_allocator.cpp:48] Added framework 201206031742-0-0000 I0603 17:42:27.986146 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 I0603 17:42:27.995363 31519 master.cpp:718] Reviving offers for framework 201206031742-0-0000 I0603 17:42:27.995427 31519 simple_allocator.cpp:145] Filters removed for framework 201206031742-0-0000 I0603 17:42:28.003842 31519 master.cpp:679] Received reply for offer 201206031742-0-0 I0603 17:42:28.003937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds I0603 17:42:29.862329 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1 I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds I0603 17:42:30.866348 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 I0603 17:42:30.867856 31519 master.cpp:679] Received reply for offer 201206031742-0-2 I0603 17:42:30.867918 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds I0603 17:42:31.796823 31519 master.cpp:1118] Framework 201206031742-0-0000 disconnected", "reporter": "Parviz Deyhim", "assignee": null, "created": "0012-06-03T09:47:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: From this part: I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1 I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds It looks like Spark is not accepting to run on your slave node. Is it possible perhaps that your Mesos slave is reporting too little memory? What did you set SPARK_MEM to? The Mesos slave's memory should actually be slightly higher than SPARK_MEM, by a few hundred MB, or it won't allocate it. Matei On Jun 3, 2012, at 10:47 AM, pdeyhim wrote: > Hi there, > > I've been trying to make spark work on Mesos and have been stuck. I've tried everything I could have think of. Unfortuantely I can't seem to figure out what's not setup right. > > When i run ./run spark.examples.SparkPi master@10.244.1.147:5050 Spark gets stuck at: > > 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 > > here's the complete output. I'm also adding the output from my mesos-master. Any help would be truly appreciated. > > > > Spark Output: ./run spark.examples.SparkPi master@10.244.1.147:5050 > > 12/06/03 17:42:26 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 > 12/06/03 17:42:26 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal > 12/06/03 17:42:27 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/06/03 17:42:27 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-71f91c61-c132-4abb-9437-ac736ca0ed8f/shuffle > 12/06/03 17:42:27 INFO server.Server: jetty-7.5.3.v20111011 > 12/06/03 17:42:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33864 STARTING > 12/06/03 17:42:27 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:33864 > 12/06/03 17:42:27 INFO spark.SparkContext: Starting job... > 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 1 with cache > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions > 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 0 with cache > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Asked for current cache locations > 12/06/03 17:42:27 INFO spark.MesosScheduler: Final stage: Stage 0 > 12/06/03 17:42:27 INFO spark.MesosScheduler: Parents of final stage: List() > 12/06/03 17:42:27 INFO spark.MesosScheduler: Missing parents: List() > 12/06/03 17:42:27 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents > 12/06/03 17:42:27 INFO spark.MesosScheduler: Got a job with 2 tasks > 12/06/03 17:42:27 INFO spark.MesosScheduler: Registered as framework ID 201206031742-0-0000 > 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 > > Master: > > I0603 17:42:18.808145 31518 logging.cpp:70] Logging to /tmp/install/mesos/logs > I0603 17:42:18.809634 31518 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop > I0603 17:42:18.809680 31518 main.cpp:96] Starting Mesos master > I0603 17:42:18.810984 31519 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 > I0603 17:42:18.811133 31519 master.cpp:279] Master ID: 201206031742-0 > I0603 17:42:18.811455 31519 master.cpp:462] Elected as master! > I0603 17:42:22.068495 31519 master.cpp:814] Attempting to register slave 201206031742-0-0 at slave@10.244.1.196:52813 > I0603 17:42:22.069000 31519 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:52813 as active > I0603 17:42:22.069066 31519 master.cpp:1588] Adding slave 201206031742-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=1; mem=1024 > I0603 17:42:22.069295 31519 simple_allocator.cpp:71] Added slave 201206031742-0-0 with cpus=1; mem=1024 > I0603 17:42:27.985856 31519 master.cpp:492] Registering framework 201206031742-0-0000 at 2@10.244.1.147:55998 > I0603 17:42:27.986047 31519 simple_allocator.cpp:48] Added framework 201206031742-0-0000 > I0603 17:42:27.986146 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 > I0603 17:42:27.995363 31519 master.cpp:718] Reviving offers for framework 201206031742-0-0000 > I0603 17:42:27.995427 31519 simple_allocator.cpp:145] Filters removed for framework 201206031742-0-0000 > I0603 17:42:28.003842 31519 master.cpp:679] Received reply for offer 201206031742-0-0 > I0603 17:42:28.003937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds > I0603 17:42:29.862329 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 > I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1 > I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds > I0603 17:42:30.866348 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 > I0603 17:42:30.867856 31519 master.cpp:679] Received reply for offer 201206031742-0-2 > I0603 17:42:30.867918 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds > I0603 17:42:31.796823 31519 master.cpp:1118] Framework 201206031742-0-0000 disconnected > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/138", "created": "2012-06-03T09:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: Thanks for the quick reply Matei. SPARK_MEM is set to 2g. I checked the master and noticed that slave was getting resgistered with 1024m of memory. I started running slave with the following resources: --resources=cpus:2;mem:3449. I no longer get the \"filtered slave\" from master but I still get stuck here: from Master: I0604 01:35:00.843688 29562 logging.cpp:70] Logging to /tmp/install/mesos/logs I0604 01:35:00.845057 29562 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop I0604 01:35:00.845092 29562 main.cpp:96] Starting Mesos master I0604 01:35:00.846283 29563 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 I0604 01:35:00.846410 29563 master.cpp:279] Master ID: 201206040135-0 I0604 01:35:00.846696 29563 master.cpp:462] Elected as master! I0604 01:35:01.642606 29563 master.cpp:814] Attempting to register slave 201206040135-0-0 at slave@10.244.1.196:43955 I0604 01:35:01.643165 29563 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:43955 as active I0604 01:35:01.643241 29563 master.cpp:1588] Adding slave 201206040135-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=2 I0604 01:35:01.643457 29563 simple_allocator.cpp:71] Added slave 201206040135-0-0 with cpus=2 I0604 01:35:13.313666 29563 master.cpp:492] Registering framework 201206040135-0-0000 at 2@10.244.1.147:45655 I0604 01:35:13.313947 29563 simple_allocator.cpp:48] Added framework 201206040135-0-0000 I0604 01:35:13.324192 29563 master.cpp:718] Reviving offers for framework 201206040135-0-0000 I0604 01:35:13.324252 29563 simple_allocator.cpp:145] Filters removed for framework 201206040135-0-0000 from Spark: 12/06/04 01:35:12 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal 12/06/04 01:35:12 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/06/04 01:35:12 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-9b045c91-c9d5-403b-90a2-2a6bce617d63/shuffle 12/06/04 01:35:12 INFO server.Server: jetty-7.5.3.v20111011 12/06/04 01:35:12 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39527 STARTING 12/06/04 01:35:12 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:39527 12/06/04 01:35:13 INFO spark.SparkContext: Starting job... 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Asked for current cache locations 12/06/04 01:35:13 INFO spark.MesosScheduler: Final stage: Stage 0 12/06/04 01:35:13 INFO spark.MesosScheduler: Parents of final stage: List() 12/06/04 01:35:13 INFO spark.MesosScheduler: Missing parents: List() 12/06/04 01:35:13 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/04 01:35:13 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/04 01:35:13 INFO spark.MesosScheduler: Registered as framework ID 201206040135-0-0000 12/06/04 01:35:13 INFO spark.MesosScheduler: Adding job with ID 0", "created": "2012-06-03T17:36:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Oh, actually you need to put the \"--resources=cpus:2;mem:3449\" in quotes on your command line. Otherwise, bash thinks that the semicolon \";\" is the end of a statement, and treats the mem:3449 as a separate command. Matei On Jun 3, 2012, at 6:36 PM, pdeyhim wrote: > Thanks for the quick reply Matei. I'm SPARK_MEM is set to 2g. I checked the master and noticed that slave was getting resgistered with 1024m of memory. I started running slave with the following resources: --resources=cpus:2;mem:3449. I no longer get the \"filtered slave\" from master but I still get stuck here: > > from Master: > > I0604 01:35:00.843688 29562 logging.cpp:70] Logging to /tmp/install/mesos/logs > I0604 01:35:00.845057 29562 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop > I0604 01:35:00.845092 29562 main.cpp:96] Starting Mesos master > I0604 01:35:00.846283 29563 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 > I0604 01:35:00.846410 29563 master.cpp:279] Master ID: 201206040135-0 > I0604 01:35:00.846696 29563 master.cpp:462] Elected as master! > I0604 01:35:01.642606 29563 master.cpp:814] Attempting to register slave 201206040135-0-0 at slave@10.244.1.196:43955 > I0604 01:35:01.643165 29563 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:43955 as active > I0604 01:35:01.643241 29563 master.cpp:1588] Adding slave 201206040135-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=2 > I0604 01:35:01.643457 29563 simple_allocator.cpp:71] Added slave 201206040135-0-0 with cpus=2 > I0604 01:35:13.313666 29563 master.cpp:492] Registering framework 201206040135-0-0000 at 2@10.244.1.147:45655 > I0604 01:35:13.313947 29563 simple_allocator.cpp:48] Added framework 201206040135-0-0000 > I0604 01:35:13.324192 29563 master.cpp:718] Reviving offers for framework 201206040135-0-0000 > I0604 01:35:13.324252 29563 simple_allocator.cpp:145] Filters removed for framework 201206040135-0-0000 > > > from Spark: > > 12/06/04 01:35:12 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 > 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal > 12/06/04 01:35:12 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/06/04 01:35:12 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-9b045c91-c9d5-403b-90a2-2a6bce617d63/shuffle > 12/06/04 01:35:12 INFO server.Server: jetty-7.5.3.v20111011 > 12/06/04 01:35:12 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39527 STARTING > 12/06/04 01:35:12 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:39527 > 12/06/04 01:35:13 INFO spark.SparkContext: Starting job... > 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 1 with cache > 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions > 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 0 with cache > 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions > 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Asked for current cache locations > 12/06/04 01:35:13 INFO spark.MesosScheduler: Final stage: Stage 0 > 12/06/04 01:35:13 INFO spark.MesosScheduler: Parents of final stage: List() > 12/06/04 01:35:13 INFO spark.MesosScheduler: Missing parents: List() > 12/06/04 01:35:13 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents > 12/06/04 01:35:13 INFO spark.MesosScheduler: Got a job with 2 tasks > 12/06/04 01:35:13 INFO spark.MesosScheduler: Registered as framework ID 201206040135-0-0000 > 12/06/04 01:35:13 INFO spark.MesosScheduler: Adding job with ID 0 > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/138#issuecomment-6090958", "created": "2012-06-03T17:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: nice! getting very close :) Seeing a Jetty login error instead. Have no clue if its related to my issue though. Is Jetty the issue? Spark: 12/06/04 04:18:32 INFO spark.MesosScheduler: Missing parents: List() 12/06/04 04:18:32 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/04 04:18:32 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/04 04:18:32 INFO spark.MesosScheduler: Registered as framework ID 201206040418-0-0000 12/06/04 04:18:32 INFO spark.MesosScheduler: Adding job with ID 0 12/06/04 04:18:46 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201206040418-0-0: ip-10-244-1-147.us-west-2.compute.internal (preferred) 12/06/04 04:18:46 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 39 ms to serialize by spark.JavaSerializerInstance 12/06/04 04:18:46 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201206040418-0-0: ip-10-244-1-147.us-west-2.compute.internal (preferred) 12/06/04 04:18:46 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/06/04 04:18:47 INFO spark.CacheTrackerActor: Started slave cache (size 2.5GB) on ip-10-244-1-147.us-west-2.compute.internal On Slave: I0604 04:18:46.169631 17416 slave.cpp:340] Registered with master; given slave ID 201206040418-0-0 I0604 04:18:46.226757 17416 slave.cpp:398] Got assigned task 0 for framework 201206040418-0-0000 I0604 04:18:46.226852 17416 slave.cpp:1388] Generating a unique work directory for executor 'default' of framework 201206040418-0-0000 I0604 04:18:46.227252 17416 slave.cpp:465] Using '/root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0' as work directory for executor 'default' of framework 201206040418-0-0000 I0604 04:18:46.228713 17416 slave.cpp:398] Got assigned task 1 for framework 201206040418-0-0000 I0604 04:18:46.228749 17416 slave.cpp:436] Queuing task '1' for executor default of framework '201206040418-0-0000 I0604 04:18:46.228801 17416 process_based_isolation_module.cpp:91] Launching default (/root/spark/spark-executor) in /root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0 with resources mem=4096' for framework 201206040418-0-0000 I0604 04:18:46.229663 17416 process_based_isolation_module.cpp:114] Forked executor at = 17419 I0604 04:18:47.003084 17416 slave.cpp:725] Got registration for executor 'default' of framework 201206040418-0-0000 I0604 04:18:47.003365 17416 slave.cpp:779] Flushing queued tasks for framework 201206040418-0-0000 in /root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0/stderr i see this: 12/06/04 04:18:47 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 2716595650 12/06/04 04:18:47 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-54004563-055d-49fd-be07-537cebbe58aa/shuffle 12/06/04 04:18:47 INFO server.Server: jetty-7.5.3.v20111011 12/06/04 04:18:47 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:55566 STARTING 12/06/04 04:18:47 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:55566 java.io.IOException: failure to login at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:433) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:395) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1435) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1336) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:244)", "created": "2012-06-03T20:23:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: looks like the issue was with HDFS. If I remove DFS from java_opt, everything seems to work a little better. I get \"Pi is roughly 3.13654\" back but the framework never disconnects from master. So looks like im in a better shape but two issues: 1) HDFS issue 2) Spark never disconnects from master master: I0604 21:23:26.347866 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 1 of framework 201206042121-0-0001 is now in state TASK_RUNNING I0604 21:23:26.349199 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 0 of framework 201206042121-0-0001 is now in state TASK_RUNNING I0604 21:23:26.520745 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 0 of framework 201206042121-0-0001 is now in state TASK_FINISHED I0604 21:23:26.522171 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 1 of framework 201206042121-0-0001 is now in state TASK_FINISHED I0604 21:23:27.134407 7125 master.cpp:1184] Sending 1 offers to framework 201206042121-0-0001 I0604 21:23:27.135937 7125 master.cpp:679] Received reply for offer 201206042121-0-13 I0604 21:23:27.136024 7125 master.cpp:1403] Filtered slave 201206042121-0-0 for framework 201206042121-0-0001 for 1 seconds I0604 21:23:28.138468 7125 master.cpp:1184] Sending 1 offers to framework 201206042121-0-0001 I0604 21:23:28.219060 7125 master.cpp:1118] Framework 201206042121-0-0001 disconnected Spark: 12/06/04 21:27:04 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/06/04 21:27:04 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/06/04 21:27:04 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/06/04 21:27:04 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/06/04 21:27:04 INFO spark.CacheTrackerActor: Asked for current cache locations 12/06/04 21:27:04 INFO spark.MesosScheduler: Final stage: Stage 0 12/06/04 21:27:04 INFO spark.MesosScheduler: Parents of final stage: List() 12/06/04 21:27:04 INFO spark.MesosScheduler: Missing parents: List() 12/06/04 21:27:04 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/04 21:27:04 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/04 21:27:04 INFO spark.MesosScheduler: Registered as framework ID 201206042121-0-0002 12/06/04 21:27:04 INFO spark.MesosScheduler: Adding job with ID 0 12/06/04 21:27:13 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201206042121-0-1: ip-10-244-1-196.us-west-2.compute.internal (preferred) 12/06/04 21:27:13 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 43 ms to serialize by spark.JavaSerializerInstance 12/06/04 21:27:13 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201206042121-0-1: ip-10-244-1-196.us-west-2.compute.internal (preferred) 12/06/04 21:27:13 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/06/04 21:27:14 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-196.us-west-2.compute.internal 12/06/04 21:27:15 INFO spark.SimpleJob: Finished TID 0 (progress: 1/2) 12/06/04 21:27:15 INFO spark.SimpleJob: Finished TID 1 (progress: 2/2) 12/06/04 21:27:15 INFO spark.MesosScheduler: Completed ResultTask(0, 0) 12/06/04 21:27:15 INFO spark.MesosScheduler: Completed ResultTask(0, 1) 12/06/04 21:27:15 INFO spark.SparkContext: Job finished in 10.738588868 s Pi is roughly 3.1472", "created": "2012-06-04T13:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Ah, could it be that your version of HDFS is different from the one Spark is compiled with? You can change the Hadoop version that Spark builds against in project/SparkBuild.scala. By default it's Hadoop 0.20.2. Note that 0.20.205 introduced security to HDFS, which is a major change in protocol. Maybe that's why this isn't working. If you look at the version of SparkBuild in the mesos-0.9 branch (https://github.com/mesos/spark/blob/mesos-0.9/project/SparkBuild.scala), it lists a few options for setting the Hadoop version. As for the second issue, with the framework not disconnecting, you need to pass the parameter --failover_timeout=1 to the mesos-master command, or put it in Mesos's conf/mesos.conf file. This is documented at https://github.com/mesos/spark/wiki/Running-spark-on-mesos but it's easy to miss.", "created": "2012-06-04T13:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: I've tried the failover_timeout=1 with no luck. I see that on master my scala process lingers around even after I disconnect the Spark run command root 28315 28298 0 15:52 ? 00:01:10 java -Djava.library.path=:/root/spark/lib:/root/spark/src/main/native:/tmp/install/mesos/lib/java -Xms4g -Xmx4g -Dspark.dfs=hdfs://10.244.1.147:9000 -Xbootclasspath/a:/tmp/install/scala-2.9.2/lib/jline.jar:/tmp/install/scala-2.9.2/lib/scala-compiler.jar:/tmp/install/scala-2.9.2/lib/scala-dbc.jar:/tmp/install/scala-2.9.2/lib/scala-library.jar:/tmp/install/scala-2.9.2/lib/scala-partest.jar:/tmp/install/scala-2.9.2/lib/scala-swing.jar:/tmp/install/scala-2.9.2/lib/scalacheck.jar:/tmp/install/scala-2.9.2/lib/scalap.jar -Dscala.usejavacp=true -Dscala.home=/tmp/install/scala-2.9.2 -Denv.emacs= scala.tools.nsc.MainGenericRunner -cp :/root/spark/core/target/scala-2.9.1/classes:/tmp/install/mesos/lib/java/mesos.jar:/root/spark/conf:/root/spark/repl/target/scala-2.9.1/classes:/root/spark/examples/target/scala-2.9.1/classes:/root/spark/core/lib/mesos.jar:/root/spark/lib_managed/jars/commons-net/commons is that normal?", "created": "2012-06-04T14:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Oh, do you call System.exit(0) at the end of the program? Some of our example jobs unfortunately don't have that, because you used to not have to do it. Matei On Jun 4, 2012, at 3:03 PM, pdeyhim wrote: > I've tried the failover_timeout=1 with no luck. I see that on master my scala process lingers around even after I disconnect the Spark run command > > root 28315 28298 0 15:52 ? 00:01:10 java -Djava.library.path=:/root/spark/lib:/root/spark/src/main/native:/tmp/install/mesos/lib/java -Xms4g -Xmx4g -Dspark.dfs=hdfs://10.244.1.147:9000 -Xbootclasspath/a:/tmp/install/scala-2.9.2/lib/jline.jar:/tmp/install/scala-2.9.2/lib/scala-compiler.jar:/tmp/install/scala-2.9.2/lib/scala-dbc.jar:/tmp/install/scala-2.9.2/lib/scala-library.jar:/tmp/install/scala-2.9.2/lib/scala-partest.jar:/tmp/install/scala-2.9.2/lib/scala-swing.jar:/tmp/install/scala-2.9.2/lib/scalacheck.jar:/tmp/install/scala-2.9.2/lib/scalap.jar -Dscala.usejavacp=true -Dscala.home=/tmp/install/scala-2.9.2 -Denv.emacs= scala.tools.nsc.MainGenericRunner -cp :/root/spark/core/target/scala-2.9.1/classes:/tmp/install/mesos/lib/java/mesos.jar:/root/spark/conf:/root/spark/repl/target/scala-2.9.1/classes:/root/spark/examples/target/scala-2.9.1/classes:/root/spark/core/lib/mesos.jar:/root/spark/lib_managed/jars/commons-net/commons > > is that normal? > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/138#issuecomment-6112775", "created": "2012-06-04T16:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: great! So looks like I've everything setup and working. Thanks again for your help so far!! The only thing that I need to finish up is fixing the HDFS issue. What's the best way to build the mesos-0.9 branch? What Mesos branch do I have to use? prebuff? I pretty much tried every mesos build I could have think of but I keep getting the following error when I run Spark's example app: /run spark.examples.SparkPi master@10.244.1.147:5050 12/06/05 21:46:24 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 339585269 12/06/05 21:46:24 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/06/05 21:46:24 INFO spark.CacheTrackerActor: Started slave cache (size 323.9MB) on ip-10-244-1-147.us-west-2.compute.internal 12/06/05 21:46:24 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/06/05 21:46:24 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-0131017e-70f3-46fc-9700-0e328062662c/shuffle 12/06/05 21:46:24 INFO server.Server: jetty-7.5.3.v20111011 12/06/05 21:46:24 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:35309 STARTING 12/06/05 21:46:24 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:35309 Exception in thread \"Spark scheduler\" java.lang.UnsatisfiedLinkError: org.apache.mesos.MesosSchedulerDriver.initialize()V at org.apache.mesos.MesosSchedulerDriver.initialize(Native Method) at org.apache.mesos.MesosSchedulerDriver.<init>(MesosSchedulerDriver.java:89) at spark.MesosScheduler$$anon$2.run(MesosScheduler.scala:105)", "created": "2012-06-05T13:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: It will work with either with the trunk version of Mesos or the 0.9 release from http://www.mesosproject.org/download.html, but I think the error is because the configuration has changed a little. Take a look at https://github.com/mesos/spark/blob/mesos-0.9/conf/spark-env.sh.template -- you need to set a new variable called MESOS_NATIVE_LIBRARY to point to your libmesos.so, instead of the old MESOS_HOME variable. The Readme in this branch also describes this in more detail at the bottom: https://github.com/mesos/spark/blob/mesos-0.9/README.md. Sorry this isn't immediately apparent from the site -- I will make a new Spark release using 0.9 as the default soon and I'll update the docs then.", "created": "2012-06-05T13:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way, you don't need the Mesos 0.9 branch to use a different version of HDFS! Just change the SparkBuild.scala in the old branch (search for hadoop in there). I was only showing the 0.9 stuff as an example because it lists the Maven dependency IDs for various Hadoop versions.", "created": "2012-06-05T13:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: Thanks! Done a few builds and still seeing the HDFS login issue which is very strange. Some googling got me here: https://issues.apache.org/jira/browse/HADOOP-7982 At the end of that thread, someone mentions this: \"Here's a test case which shows the issue. We tracked it down to a JNI issue – if when using libhdfs, the thread that started the JVM isn't the same thread that first uses libhdfs, it will fail with the error described in the JIRA. The fix as committed solves the problem.\" I'm not sure if that's whats going on here but wanted to post it here just in case if it rings any bell :) 12/06/05 22:11:07 INFO spark.ShuffleManager: Local URI: http://10.244.1.196:57591 java.io.IOException: failure to login at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:433) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:395) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1435) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1336) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:244) at spark.broadcast.DfsBroadcast$.initialize(DfsBroadcast.scala:84) at spark.broadcast.DfsBroadcastFactory.initialize(DfsBroadcast.scala:58) at spark.broadcast.Broadcast$.initialize(Broadcast.scala:50) at spark.Executor.init(Executor.scala:41) Caused by: javax.security.auth.login.LoginException: unable to find LoginModule class: org/apache/hadoop/security/UserGroupInformation$HadoopLoginModule at javax.security.auth.login.LoginContext.invoke(LoginContext.java:808) at javax.security.auth.login.LoginContext.access$000(LoginContext.java:186) at javax.security.auth.login.LoginContext$4.run(LoginContext.java:683) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) at javax.security.auth.login.LoginContext.login(LoginContext.java:579) at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:414) ... 8 more java.lang.NullPointerException at spark.Executor.launchTask(Executor.scala:53) java.lang.NullPointerException at spark.Executor.launchTask(Executor.scala:53)", "created": "2012-06-05T15:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Oh, I think I've seen something like this before. It's because we have two threads that use HDFS -- one would be the main thread, and one is this \"broadcast\" feature that can write files to HDFS in order to broadcast them to worker nodes. I would suggest removing the HDFS parameter from java-opts; that should make the second one not run. You can still access files in HDFS by passing the full hdfs://... URL. Or are you seeing the error despite having already removed this?", "created": "2012-06-05T15:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: interesting. Removing java_opt fixes the issue but I wasn't sure if removing namdenode's address from java_opt would still allows to me access HDFS. Without the java_opt, how would it know where namenode is?", "created": "2012-06-05T15:31:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: You can just specify the namenode as part of the hdfs:// URL. For example, val file = sparkContext.textFile(\"hdfs://my-machine:9000/user/foo/file.txt\")", "created": "2012-06-05T15:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pdeyhim: great! what's the best way to contact you? I'm working on getting spark deployed on potentially a large scale deployment and wanted to make sure I have everything covered. It'd be great to run some stuff with you.", "created": "2012-06-05T15:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: You can email me at matei@eecs.berkeley.edu. If you want we can arrange a phone call or something at some point.", "created": "2012-06-05T15:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way, I'm going to close this issue because I've fixed the SparkPi example to call System.exit now and we figured out the other things. We can continue discussing over email.", "created": "2012-06-06T13:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-138, originally reported by pdeyhim", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 19, "text": "Issue: SPARK-412\nSummary: run spark.examples.SparkPi hangs with no results\nDescription: Hi there, I've been trying to make spark work on Mesos and have been stuck. I've tried everything I could have think of. Unfortuantely I can't seem to figure out what's not setup right. When i run ./run spark.examples.SparkPi master@10.244.1.147:5050 Spark gets stuck at: 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 here's the complete output. I'm also adding the output from my mesos-master. Any help would be truly appreciated. Spark Output: ./run spark.examples.SparkPi master@10.244.1.147:5050 12/06/03 17:42:26 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 12/06/03 17:42:26 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal 12/06/03 17:42:27 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/06/03 17:42:27 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-71f91c61-c132-4abb-9437-ac736ca0ed8f/shuffle 12/06/03 17:42:27 INFO server.Server: jetty-7.5.3.v20111011 12/06/03 17:42:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33864 STARTING 12/06/03 17:42:27 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:33864 12/06/03 17:42:27 INFO spark.SparkContext: Starting job... 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Asked for current cache locations 12/06/03 17:42:27 INFO spark.MesosScheduler: Final stage: Stage 0 12/06/03 17:42:27 INFO spark.MesosScheduler: Parents of final stage: List() 12/06/03 17:42:27 INFO spark.MesosScheduler: Missing parents: List() 12/06/03 17:42:27 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/03 17:42:27 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/03 17:42:27 INFO spark.MesosScheduler: Registered as framework ID 201206031742-0-0000 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 Master: I0603 17:42:18.808145 31518 logging.cpp:70] Logging to /tmp/install/mesos/logs I0603 17:42:18.809634 31518 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop I0603 17:42:18.809680 31518 main.cpp:96] Starting Mesos master I0603 17:42:18.810984 31519 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 I0603 17:42:18.811133 31519 master.cpp:279] Master ID: 201206031742-0 I0603 17:42:18.811455 31519 master.cpp:462] Elected as master! I0603 17:42:22.068495 31519 master.cpp:814] Attempting to register slave 201206031742-0-0 at slave@10.244.1.196:52813 I0603 17:42:22.069000 31519 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:52813 as active I0603 17:42:22.069066 31519 master.cpp:1588] Adding slave 201206031742-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=1; mem=1024 I0603 17:42:22.069295 31519 simple_allocator.cpp:71] Added slave 201206031742-0-0 with cpus=1; mem=1024 I0603 17:42:27.985856 31519 master.cpp:492] Registering framework 201206031742-0-0000 at 2@10.244.1.147:55998 I0603 17:42:27.986047 31519 simple_allocator.cpp:48] Added framework 201206031742-0-0000 I0603 17:42:27.986146 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 I0603 17:42:27.995363 31519 master.cpp:718] Reviving offers for framework 201206031742-0-0000 I0603 17:42:27.995427 31519 simple_allocator.cpp:145] Filters removed for framework 201206031742-0-0000 I0603 17:42:28.003842 31519 master.cpp:679] Received reply for offer 201206031742-0-0 I0603 17:42:28.003937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds I0603 17:42:29.862329 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1 I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds I0603 17:42:30.866348 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 I0603 17:42:30.867856 31519 master.cpp:679] Received reply for offer 201206031742-0-2 I0603 17:42:30.867918 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds I0603 17:42:31.796823 31519 master.cpp:1118] Framework 201206031742-0-0000 disconnected\n\nComments (19):\n1. Patrick McFadin: Github comment from mateiz: From this part: I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1 I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds It looks like Spark is not accepting to run on your slave node. Is it possible perhaps that your Mesos slave is reporting too little memory? What did you set SPARK_MEM to? The Mesos slave's memory should actually be slightly higher than SPARK_MEM, by a few hundred MB, or it won't allocate it. Matei On Jun 3, 2012, at 10:47 AM, pdeyhim wrote: > Hi there, > > I've been trying to make spark work on Mesos and have been stuck. I've tried everything I could have think of. Unfortuantely I can't seem to figure out what's not setup right. > > When i run ./run spark.examples.SparkPi master@10.244.1.147:5050 Spark gets stuck at: > > 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 > > here's the complete output. I'm also adding the output from my mesos-master. Any help would be truly appreciated. > > > > Spark Output: ./run spark.examples.SparkPi master@10.244.1.147:5050 > > 12/06/03 17:42:26 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 > 12/06/03 17:42:26 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal > 12/06/03 17:42:27 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/06/03 17:42:27 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-71f91c61-c132-4abb-9437-ac736ca0ed8f/shuffle > 12/06/03 17:42:27 INFO server.Server: jetty-7.5.3.v20111011 > 12/06/03 17:42:27 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33864 STARTING > 12/06/03 17:42:27 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:33864 > 12/06/03 17:42:27 INFO spark.SparkContext: Starting job... > 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 1 with cache > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions > 12/06/03 17:42:27 INFO spark.CacheTracker: Registering RDD ID 0 with cache > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions > 12/06/03 17:42:27 INFO spark.CacheTrackerActor: Asked for current cache locations > 12/06/03 17:42:27 INFO spark.MesosScheduler: Final stage: Stage 0 > 12/06/03 17:42:27 INFO spark.MesosScheduler: Parents of final stage: List() > 12/06/03 17:42:27 INFO spark.MesosScheduler: Missing parents: List() > 12/06/03 17:42:27 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents > 12/06/03 17:42:27 INFO spark.MesosScheduler: Got a job with 2 tasks > 12/06/03 17:42:27 INFO spark.MesosScheduler: Registered as framework ID 201206031742-0-0000 > 12/06/03 17:42:27 INFO spark.MesosScheduler: Adding job with ID 0 > > Master: > > I0603 17:42:18.808145 31518 logging.cpp:70] Logging to /tmp/install/mesos/logs > I0603 17:42:18.809634 31518 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop > I0603 17:42:18.809680 31518 main.cpp:96] Starting Mesos master > I0603 17:42:18.810984 31519 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 > I0603 17:42:18.811133 31519 master.cpp:279] Master ID: 201206031742-0 > I0603 17:42:18.811455 31519 master.cpp:462] Elected as master! > I0603 17:42:22.068495 31519 master.cpp:814] Attempting to register slave 201206031742-0-0 at slave@10.244.1.196:52813 > I0603 17:42:22.069000 31519 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:52813 as active > I0603 17:42:22.069066 31519 master.cpp:1588] Adding slave 201206031742-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=1; mem=1024 > I0603 17:42:22.069295 31519 simple_allocator.cpp:71] Added slave 201206031742-0-0 with cpus=1; mem=1024 > I0603 17:42:27.985856 31519 master.cpp:492] Registering framework 201206031742-0-0000 at 2@10.244.1.147:55998 > I0603 17:42:27.986047 31519 simple_allocator.cpp:48] Added framework 201206031742-0-0000 > I0603 17:42:27.986146 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 > I0603 17:42:27.995363 31519 master.cpp:718] Reviving offers for framework 201206031742-0-0000 > I0603 17:42:27.995427 31519 simple_allocator.cpp:145] Filters removed for framework 201206031742-0-0000 > I0603 17:42:28.003842 31519 master.cpp:679] Received reply for offer 201206031742-0-0 > I0603 17:42:28.003937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds > I0603 17:42:29.862329 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 > I0603 17:42:29.863867 31519 master.cpp:679] Received reply for offer 201206031742-0-1 > I0603 17:42:29.863937 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds > I0603 17:42:30.866348 31519 master.cpp:1184] Sending 1 offers to framework 201206031742-0-0000 > I0603 17:42:30.867856 31519 master.cpp:679] Received reply for offer 201206031742-0-2 > I0603 17:42:30.867918 31519 master.cpp:1403] Filtered slave 201206031742-0-0 for framework 201206031742-0-0000 for 1 seconds > I0603 17:42:31.796823 31519 master.cpp:1118] Framework 201206031742-0-0000 disconnected > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/138\n2. Patrick McFadin: Github comment from pdeyhim: Thanks for the quick reply Matei. SPARK_MEM is set to 2g. I checked the master and noticed that slave was getting resgistered with 1024m of memory. I started running slave with the following resources: --resources=cpus:2;mem:3449. I no longer get the \"filtered slave\" from master but I still get stuck here: from Master: I0604 01:35:00.843688 29562 logging.cpp:70] Logging to /tmp/install/mesos/logs I0604 01:35:00.845057 29562 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop I0604 01:35:00.845092 29562 main.cpp:96] Starting Mesos master I0604 01:35:00.846283 29563 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 I0604 01:35:00.846410 29563 master.cpp:279] Master ID: 201206040135-0 I0604 01:35:00.846696 29563 master.cpp:462] Elected as master! I0604 01:35:01.642606 29563 master.cpp:814] Attempting to register slave 201206040135-0-0 at slave@10.244.1.196:43955 I0604 01:35:01.643165 29563 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:43955 as active I0604 01:35:01.643241 29563 master.cpp:1588] Adding slave 201206040135-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=2 I0604 01:35:01.643457 29563 simple_allocator.cpp:71] Added slave 201206040135-0-0 with cpus=2 I0604 01:35:13.313666 29563 master.cpp:492] Registering framework 201206040135-0-0000 at 2@10.244.1.147:45655 I0604 01:35:13.313947 29563 simple_allocator.cpp:48] Added framework 201206040135-0-0000 I0604 01:35:13.324192 29563 master.cpp:718] Reviving offers for framework 201206040135-0-0000 I0604 01:35:13.324252 29563 simple_allocator.cpp:145] Filters removed for framework 201206040135-0-0000 from Spark: 12/06/04 01:35:12 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal 12/06/04 01:35:12 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/06/04 01:35:12 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-9b045c91-c9d5-403b-90a2-2a6bce617d63/shuffle 12/06/04 01:35:12 INFO server.Server: jetty-7.5.3.v20111011 12/06/04 01:35:12 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39527 STARTING 12/06/04 01:35:12 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:39527 12/06/04 01:35:13 INFO spark.SparkContext: Starting job... 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Asked for current cache locations 12/06/04 01:35:13 INFO spark.MesosScheduler: Final stage: Stage 0 12/06/04 01:35:13 INFO spark.MesosScheduler: Parents of final stage: List() 12/06/04 01:35:13 INFO spark.MesosScheduler: Missing parents: List() 12/06/04 01:35:13 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/04 01:35:13 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/04 01:35:13 INFO spark.MesosScheduler: Registered as framework ID 201206040135-0-0000 12/06/04 01:35:13 INFO spark.MesosScheduler: Adding job with ID 0\n3. Patrick McFadin: Github comment from mateiz: Oh, actually you need to put the \"--resources=cpus:2;mem:3449\" in quotes on your command line. Otherwise, bash thinks that the semicolon \";\" is the end of a statement, and treats the mem:3449 as a separate command. Matei On Jun 3, 2012, at 6:36 PM, pdeyhim wrote: > Thanks for the quick reply Matei. I'm SPARK_MEM is set to 2g. I checked the master and noticed that slave was getting resgistered with 1024m of memory. I started running slave with the following resources: --resources=cpus:2;mem:3449. I no longer get the \"filtered slave\" from master but I still get stuck here: > > from Master: > > I0604 01:35:00.843688 29562 logging.cpp:70] Logging to /tmp/install/mesos/logs > I0604 01:35:00.845057 29562 main.cpp:95] Build: 2012-06-03 15:59:45 by hadoop > I0604 01:35:00.845092 29562 main.cpp:96] Starting Mesos master > I0604 01:35:00.846283 29563 master.cpp:264] Master started at mesos://master@10.244.1.147:5050 > I0604 01:35:00.846410 29563 master.cpp:279] Master ID: 201206040135-0 > I0604 01:35:00.846696 29563 master.cpp:462] Elected as master! > I0604 01:35:01.642606 29563 master.cpp:814] Attempting to register slave 201206040135-0-0 at slave@10.244.1.196:43955 > I0604 01:35:01.643165 29563 master.cpp:1057] Master now considering a slave at ip-10-244-1-196.us-west-2.compute.internal:43955 as active > I0604 01:35:01.643241 29563 master.cpp:1588] Adding slave 201206040135-0-0 at ip-10-244-1-196.us-west-2.compute.internal with cpus=2 > I0604 01:35:01.643457 29563 simple_allocator.cpp:71] Added slave 201206040135-0-0 with cpus=2 > I0604 01:35:13.313666 29563 master.cpp:492] Registering framework 201206040135-0-0000 at 2@10.244.1.147:45655 > I0604 01:35:13.313947 29563 simple_allocator.cpp:48] Added framework 201206040135-0-0000 > I0604 01:35:13.324192 29563 master.cpp:718] Reviving offers for framework 201206040135-0-0000 > I0604 01:35:13.324252 29563 simple_allocator.cpp:145] Filters removed for framework 201206040135-0-0000 > > > from Spark: > > 12/06/04 01:35:12 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1358297825 > 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/06/04 01:35:12 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-147.us-west-2.compute.internal > 12/06/04 01:35:12 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/06/04 01:35:12 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-9b045c91-c9d5-403b-90a2-2a6bce617d63/shuffle > 12/06/04 01:35:12 INFO server.Server: jetty-7.5.3.v20111011 > 12/06/04 01:35:12 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39527 STARTING > 12/06/04 01:35:12 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:39527 > 12/06/04 01:35:13 INFO spark.SparkContext: Starting job... > 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 1 with cache > 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions > 12/06/04 01:35:13 INFO spark.CacheTracker: Registering RDD ID 0 with cache > 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions > 12/06/04 01:35:13 INFO spark.CacheTrackerActor: Asked for current cache locations > 12/06/04 01:35:13 INFO spark.MesosScheduler: Final stage: Stage 0 > 12/06/04 01:35:13 INFO spark.MesosScheduler: Parents of final stage: List() > 12/06/04 01:35:13 INFO spark.MesosScheduler: Missing parents: List() > 12/06/04 01:35:13 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents > 12/06/04 01:35:13 INFO spark.MesosScheduler: Got a job with 2 tasks > 12/06/04 01:35:13 INFO spark.MesosScheduler: Registered as framework ID 201206040135-0-0000 > 12/06/04 01:35:13 INFO spark.MesosScheduler: Adding job with ID 0 > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/138#issuecomment-6090958\n4. Patrick McFadin: Github comment from pdeyhim: nice! getting very close :) Seeing a Jetty login error instead. Have no clue if its related to my issue though. Is Jetty the issue? Spark: 12/06/04 04:18:32 INFO spark.MesosScheduler: Missing parents: List() 12/06/04 04:18:32 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/04 04:18:32 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/04 04:18:32 INFO spark.MesosScheduler: Registered as framework ID 201206040418-0-0000 12/06/04 04:18:32 INFO spark.MesosScheduler: Adding job with ID 0 12/06/04 04:18:46 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201206040418-0-0: ip-10-244-1-147.us-west-2.compute.internal (preferred) 12/06/04 04:18:46 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 39 ms to serialize by spark.JavaSerializerInstance 12/06/04 04:18:46 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201206040418-0-0: ip-10-244-1-147.us-west-2.compute.internal (preferred) 12/06/04 04:18:46 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/06/04 04:18:47 INFO spark.CacheTrackerActor: Started slave cache (size 2.5GB) on ip-10-244-1-147.us-west-2.compute.internal On Slave: I0604 04:18:46.169631 17416 slave.cpp:340] Registered with master; given slave ID 201206040418-0-0 I0604 04:18:46.226757 17416 slave.cpp:398] Got assigned task 0 for framework 201206040418-0-0000 I0604 04:18:46.226852 17416 slave.cpp:1388] Generating a unique work directory for executor 'default' of framework 201206040418-0-0000 I0604 04:18:46.227252 17416 slave.cpp:465] Using '/root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0' as work directory for executor 'default' of framework 201206040418-0-0000 I0604 04:18:46.228713 17416 slave.cpp:398] Got assigned task 1 for framework 201206040418-0-0000 I0604 04:18:46.228749 17416 slave.cpp:436] Queuing task '1' for executor default of framework '201206040418-0-0000 I0604 04:18:46.228801 17416 process_based_isolation_module.cpp:91] Launching default (/root/spark/spark-executor) in /root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0 with resources mem=4096' for framework 201206040418-0-0000 I0604 04:18:46.229663 17416 process_based_isolation_module.cpp:114] Forked executor at = 17419 I0604 04:18:47.003084 17416 slave.cpp:725] Got registration for executor 'default' of framework 201206040418-0-0000 I0604 04:18:47.003365 17416 slave.cpp:779] Flushing queued tasks for framework 201206040418-0-0000 in /root/mesos/work/slaves/201206040418-0-0/frameworks/201206040418-0-0000/executors/default/runs/0/stderr i see this: 12/06/04 04:18:47 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 2716595650 12/06/04 04:18:47 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-54004563-055d-49fd-be07-537cebbe58aa/shuffle 12/06/04 04:18:47 INFO server.Server: jetty-7.5.3.v20111011 12/06/04 04:18:47 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:55566 STARTING 12/06/04 04:18:47 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:55566 java.io.IOException: failure to login at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:433) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:395) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1435) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1336) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:244)\n5. Patrick McFadin: Github comment from pdeyhim: looks like the issue was with HDFS. If I remove DFS from java_opt, everything seems to work a little better. I get \"Pi is roughly 3.13654\" back but the framework never disconnects from master. So looks like im in a better shape but two issues: 1) HDFS issue 2) Spark never disconnects from master master: I0604 21:23:26.347866 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 1 of framework 201206042121-0-0001 is now in state TASK_RUNNING I0604 21:23:26.349199 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 0 of framework 201206042121-0-0001 is now in state TASK_RUNNING I0604 21:23:26.520745 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 0 of framework 201206042121-0-0001 is now in state TASK_FINISHED I0604 21:23:26.522171 7125 master.cpp:902] Status update from slave@10.244.1.196:45099: task 1 of framework 201206042121-0-0001 is now in state TASK_FINISHED I0604 21:23:27.134407 7125 master.cpp:1184] Sending 1 offers to framework 201206042121-0-0001 I0604 21:23:27.135937 7125 master.cpp:679] Received reply for offer 201206042121-0-13 I0604 21:23:27.136024 7125 master.cpp:1403] Filtered slave 201206042121-0-0 for framework 201206042121-0-0001 for 1 seconds I0604 21:23:28.138468 7125 master.cpp:1184] Sending 1 offers to framework 201206042121-0-0001 I0604 21:23:28.219060 7125 master.cpp:1118] Framework 201206042121-0-0001 disconnected Spark: 12/06/04 21:27:04 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/06/04 21:27:04 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/06/04 21:27:04 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/06/04 21:27:04 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/06/04 21:27:04 INFO spark.CacheTrackerActor: Asked for current cache locations 12/06/04 21:27:04 INFO spark.MesosScheduler: Final stage: Stage 0 12/06/04 21:27:04 INFO spark.MesosScheduler: Parents of final stage: List() 12/06/04 21:27:04 INFO spark.MesosScheduler: Missing parents: List() 12/06/04 21:27:04 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/06/04 21:27:04 INFO spark.MesosScheduler: Got a job with 2 tasks 12/06/04 21:27:04 INFO spark.MesosScheduler: Registered as framework ID 201206042121-0-0002 12/06/04 21:27:04 INFO spark.MesosScheduler: Adding job with ID 0 12/06/04 21:27:13 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201206042121-0-1: ip-10-244-1-196.us-west-2.compute.internal (preferred) 12/06/04 21:27:13 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 43 ms to serialize by spark.JavaSerializerInstance 12/06/04 21:27:13 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201206042121-0-1: ip-10-244-1-196.us-west-2.compute.internal (preferred) 12/06/04 21:27:13 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/06/04 21:27:14 INFO spark.CacheTrackerActor: Started slave cache (size 1295.4MB) on ip-10-244-1-196.us-west-2.compute.internal 12/06/04 21:27:15 INFO spark.SimpleJob: Finished TID 0 (progress: 1/2) 12/06/04 21:27:15 INFO spark.SimpleJob: Finished TID 1 (progress: 2/2) 12/06/04 21:27:15 INFO spark.MesosScheduler: Completed ResultTask(0, 0) 12/06/04 21:27:15 INFO spark.MesosScheduler: Completed ResultTask(0, 1) 12/06/04 21:27:15 INFO spark.SparkContext: Job finished in 10.738588868 s Pi is roughly 3.1472\n6. Patrick McFadin: Github comment from mateiz: Ah, could it be that your version of HDFS is different from the one Spark is compiled with? You can change the Hadoop version that Spark builds against in project/SparkBuild.scala. By default it's Hadoop 0.20.2. Note that 0.20.205 introduced security to HDFS, which is a major change in protocol. Maybe that's why this isn't working. If you look at the version of SparkBuild in the mesos-0.9 branch (https://github.com/mesos/spark/blob/mesos-0.9/project/SparkBuild.scala), it lists a few options for setting the Hadoop version. As for the second issue, with the framework not disconnecting, you need to pass the parameter --failover_timeout=1 to the mesos-master command, or put it in Mesos's conf/mesos.conf file. This is documented at https://github.com/mesos/spark/wiki/Running-spark-on-mesos but it's easy to miss.\n7. Patrick McFadin: Github comment from pdeyhim: I've tried the failover_timeout=1 with no luck. I see that on master my scala process lingers around even after I disconnect the Spark run command root 28315 28298 0 15:52 ? 00:01:10 java -Djava.library.path=:/root/spark/lib:/root/spark/src/main/native:/tmp/install/mesos/lib/java -Xms4g -Xmx4g -Dspark.dfs=hdfs://10.244.1.147:9000 -Xbootclasspath/a:/tmp/install/scala-2.9.2/lib/jline.jar:/tmp/install/scala-2.9.2/lib/scala-compiler.jar:/tmp/install/scala-2.9.2/lib/scala-dbc.jar:/tmp/install/scala-2.9.2/lib/scala-library.jar:/tmp/install/scala-2.9.2/lib/scala-partest.jar:/tmp/install/scala-2.9.2/lib/scala-swing.jar:/tmp/install/scala-2.9.2/lib/scalacheck.jar:/tmp/install/scala-2.9.2/lib/scalap.jar -Dscala.usejavacp=true -Dscala.home=/tmp/install/scala-2.9.2 -Denv.emacs= scala.tools.nsc.MainGenericRunner -cp :/root/spark/core/target/scala-2.9.1/classes:/tmp/install/mesos/lib/java/mesos.jar:/root/spark/conf:/root/spark/repl/target/scala-2.9.1/classes:/root/spark/examples/target/scala-2.9.1/classes:/root/spark/core/lib/mesos.jar:/root/spark/lib_managed/jars/commons-net/commons is that normal?\n8. Patrick McFadin: Github comment from mateiz: Oh, do you call System.exit(0) at the end of the program? Some of our example jobs unfortunately don't have that, because you used to not have to do it. Matei On Jun 4, 2012, at 3:03 PM, pdeyhim wrote: > I've tried the failover_timeout=1 with no luck. I see that on master my scala process lingers around even after I disconnect the Spark run command > > root 28315 28298 0 15:52 ? 00:01:10 java -Djava.library.path=:/root/spark/lib:/root/spark/src/main/native:/tmp/install/mesos/lib/java -Xms4g -Xmx4g -Dspark.dfs=hdfs://10.244.1.147:9000 -Xbootclasspath/a:/tmp/install/scala-2.9.2/lib/jline.jar:/tmp/install/scala-2.9.2/lib/scala-compiler.jar:/tmp/install/scala-2.9.2/lib/scala-dbc.jar:/tmp/install/scala-2.9.2/lib/scala-library.jar:/tmp/install/scala-2.9.2/lib/scala-partest.jar:/tmp/install/scala-2.9.2/lib/scala-swing.jar:/tmp/install/scala-2.9.2/lib/scalacheck.jar:/tmp/install/scala-2.9.2/lib/scalap.jar -Dscala.usejavacp=true -Dscala.home=/tmp/install/scala-2.9.2 -Denv.emacs= scala.tools.nsc.MainGenericRunner -cp :/root/spark/core/target/scala-2.9.1/classes:/tmp/install/mesos/lib/java/mesos.jar:/root/spark/conf:/root/spark/repl/target/scala-2.9.1/classes:/root/spark/examples/target/scala-2.9.1/classes:/root/spark/core/lib/mesos.jar:/root/spark/lib_managed/jars/commons-net/commons > > is that normal? > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/138#issuecomment-6112775\n9. Patrick McFadin: Github comment from pdeyhim: great! So looks like I've everything setup and working. Thanks again for your help so far!! The only thing that I need to finish up is fixing the HDFS issue. What's the best way to build the mesos-0.9 branch? What Mesos branch do I have to use? prebuff? I pretty much tried every mesos build I could have think of but I keep getting the following error when I run Spark's example app: /run spark.examples.SparkPi master@10.244.1.147:5050 12/06/05 21:46:24 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 339585269 12/06/05 21:46:24 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/06/05 21:46:24 INFO spark.CacheTrackerActor: Started slave cache (size 323.9MB) on ip-10-244-1-147.us-west-2.compute.internal 12/06/05 21:46:24 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/06/05 21:46:24 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-0131017e-70f3-46fc-9700-0e328062662c/shuffle 12/06/05 21:46:24 INFO server.Server: jetty-7.5.3.v20111011 12/06/05 21:46:24 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:35309 STARTING 12/06/05 21:46:24 INFO spark.ShuffleManager: Local URI: http://10.244.1.147:35309 Exception in thread \"Spark scheduler\" java.lang.UnsatisfiedLinkError: org.apache.mesos.MesosSchedulerDriver.initialize()V at org.apache.mesos.MesosSchedulerDriver.initialize(Native Method) at org.apache.mesos.MesosSchedulerDriver.<init>(MesosSchedulerDriver.java:89) at spark.MesosScheduler$$anon$2.run(MesosScheduler.scala:105)\n10. Patrick McFadin: Github comment from mateiz: It will work with either with the trunk version of Mesos or the 0.9 release from http://www.mesosproject.org/download.html, but I think the error is because the configuration has changed a little. Take a look at https://github.com/mesos/spark/blob/mesos-0.9/conf/spark-env.sh.template -- you need to set a new variable called MESOS_NATIVE_LIBRARY to point to your libmesos.so, instead of the old MESOS_HOME variable. The Readme in this branch also describes this in more detail at the bottom: https://github.com/mesos/spark/blob/mesos-0.9/README.md. Sorry this isn't immediately apparent from the site -- I will make a new Spark release using 0.9 as the default soon and I'll update the docs then.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.957185"}}
{"id": "dbc44f9088805026260bf03aed7784cc", "issue_key": "SPARK-411", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "The default broadcast implementation should not use HDFS", "description": "There seems to be a bug in some versions of HDFS that disallow it being initialized in two threads, and more generally, some users might not have HDFS. The default implementation should just launch an in-process HTTP server.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-06-06T16:35:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-139, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-411\nSummary: The default broadcast implementation should not use HDFS\nDescription: There seems to be a bug in some versions of HDFS that disallow it being initialized in two threads, and more generally, some users might not have HDFS. The default implementation should just launch an in-process HTTP server.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-139, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.959085"}}
{"id": "3e6ccb150c2e0f7af41bf73c81c3c10b", "issue_key": "SPARK-544", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Provide a Configuration class in addition to system properties", "description": "This is a much better option for people who want to connect to multiple Spark clusters in the same program, and for unit tests.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "0012-06-15T22:29:00.000+0000", "updated": "2014-04-30T00:43:06.000+0000", "resolved": "2014-04-30T00:43:05.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: In order to configure my applications, I have used the following system from twitter: https://github.com/twitter/util/blob/master/util- core/src/main/scala/com/twitter/util/Config.scala Here is a rationale: http://robey.lag.net/2012/03/26/why-config.html Tim On Friday, June 15, 2012 11:29:53 PM you wrote: > This is a much better option for people who want to connect to multiple > Spark clusters in the same program, and for unit tests. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/140 -- Timothy Hunter Ph.D Student Computer Science University of California - Berkeley www.eecs.berkeley.edu/~tjhunter/ T. 404 421 3075", "created": "2012-06-17T19:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-140, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Denny Britz", "body": "I like the approach in http://robey.lag.net/2012/03/26/why-config.html. The config classes in https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Config.scala is not much extra code either. Matei, are you okay with using this?", "created": "2012-10-25T15:06:21.028+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I find this kind of confusing and too specific to Scala. I'd prefer something where you just say string key-value pairs. Then it will be easy to explain to existing users, and it will be easy to support it in Java, Python and Shark as well.", "created": "2012-10-25T16:14:44.640+0000"}, {"author": "Denny Britz", "body": "Yeah, it is pretty Scala specific, that's a fair point.", "created": "2012-10-25T16:25:43.586+0000"}, {"author": "Ankur Bansal", "body": "Why not just use regular commons-configuration or JSON based configuration. They are fairly universal and very readable. I would be more than happy to take this up if no one else is working on this already. I went ahead an did a simple grep of the `System.properies` and listed out all the properties as a json document and as a properties file. IMO JSON looks *nicer* but properties files are easier to handle. Any opinions? https://gist.github.com/ankurcha/5655646", "created": "2013-05-26T23:24:13.211+0000"}, {"author": "Shane Huang", "body": "We started a discussion about the design of this on dev mailing list and collected a few opinions, which I summarized below. The link to the original discussion is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html. 1) Define a Configuration class which contains all the options available for Spark application. A Configuration instance can be de-/serialized from/to a formatted file. Most of us tend to agree that Typesafe Config library is a good choice for the Configuration class. 2) Each application (SparkContext) has one Configuration instance and it is initialized by the application which creates it (either coded in app (apps could explicitly read from io stream or command line arguments), or system properties, or env vars). 3) For an application the overriding rule should be code > system properties > env vars. Over time we will deprecate the env vars and maybe even system properties. 4) When launching an Executor on a slave node, the Configuration is firstly initialized using the node-local configuration file as default (instead of the env vars at present), and then the Configuration passed from application driver context will override specific options specified in default. Certain options in app's Configuration will always override those in node-local, because these options need to be the consistent across all the slave nodes, e.g. spark.serializer. In this case if any such options is not set in app's Config, a value will be provided by the system. On the other hand, some options in app's Config will never override those in node-local. as they're not meat to be set in app, e.g. spark.local.dir.", "created": "2013-09-21T20:57:11.816+0000"}, {"author": "rektide de la fey", "body": "\"Inventing a class with its own interface to hold a piece of information is like inventing a new language to write every short story.\" http://www.codequarterly.com/2011/rich-hickey/ Is there tech that can be leveraged here? It's- alas- a library on top of a rather sizable framework, but Chronos, for example, uses Dropwizard Configurable AssetsBundle for configuration (https://github.com/bazaarvoice/dropwizard-configurable-assets-bundle). I get the idea that the use case here, the need, is to serialize some state around, but Configuration is a massive ops concern and it'd be good to party up, I feel.", "created": "2013-09-25T15:25:29.656+0000"}, {"author": "rektide de la fey", "body": "Aurora- a Mesos scheduler from Twitter, the other Mesos scheduler- has code commited as of two days ago. They saw fit to create IDL definitions for tasks and their associate configuration. Not recommending any particular solution here nor there, but perhaps and if perhaps not maybe still interesting & useful reference- https://github.com/twitter/aurora/blob/master/thrift/src/main/thrift/com/twitter/aurora/gen/api.thrift#L130-L160 or specifically now, https://github.com/twitter/aurora/commit/c248931344e46a0e99bfbad6fdf3e08d7473008b#L130-160", "created": "2013-09-25T17:30:15.014+0000"}, {"author": "Evan Chan", "body": "re: @rektide Typesafe Config is used in Akka, Spray, Play, and other Scala frameworks. We use it for all of our Scala apps. If you look at how Akka uses it, they don't need to build a Configuration class on top. Instead, you just pass in a com.typesafe.Config object into your class, say SparkContext. For non-Java/Scala apps, they can write JSON config files, which Typesafe Config can parse easily. The Config object can be initialized and created in multiple ways, but typically from a file, or from a Map, and you can merge it with a default file loaded from resources / jar, or even with system properties. For example, let's say you have class SparkContext(config: Config) { val port = config.getInt(\"spark.port\") That's pretty succinct for getting the value out. The advantage of a config class is that you have a type-safe access to the properties, but the disadvantage is that you have to maintain the API. I honestly feel like it's been OK to use the config without a formal class. -Evan", "created": "2013-09-25T23:58:02.139+0000"}, {"author": "Evan Chan", "body": "By the way, I've started work on this, using Typesafe Config-based configuration objects (which can parse from JSON as well). The first, primary goal is to move completely away from using System properties as global variables (there are multiple places that get, then set, for example, \"spark.driver.port\"). This will be a big step towards being able to safely have multiple contexts within the same process. This will also allow much richer config options than simple strings.", "created": "2013-09-28T08:13:28.396+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Cool, looking forward to it!", "created": "2013-09-28T09:44:50.653+0000"}, {"author": "Evan Chan", "body": "Pull Request for the first part has been submitted, in case anybody wants to have a look: https://github.com/apache/incubator-spark/pull/55", "created": "2013-10-14T13:34:39.475+0000"}, {"author": "Evan Chan", "body": "By the way, new progress is taking place in a new pull request: https://github.com/apache/incubator-spark/pull/230", "created": "2013-12-12T10:21:43.595+0000"}], "num_comments": 14, "text": "Issue: SPARK-544\nSummary: Provide a Configuration class in addition to system properties\nDescription: This is a much better option for people who want to connect to multiple Spark clusters in the same program, and for unit tests.\n\nComments (14):\n1. Patrick McFadin: Github comment from tjhunter: In order to configure my applications, I have used the following system from twitter: https://github.com/twitter/util/blob/master/util- core/src/main/scala/com/twitter/util/Config.scala Here is a rationale: http://robey.lag.net/2012/03/26/why-config.html Tim On Friday, June 15, 2012 11:29:53 PM you wrote: > This is a much better option for people who want to connect to multiple > Spark clusters in the same program, and for unit tests. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/140 -- Timothy Hunter Ph.D Student Computer Science University of California - Berkeley www.eecs.berkeley.edu/~tjhunter/ T. 404 421 3075\n2. Patrick McFadin: Imported from Github issue spark-140, originally reported by mateiz\n3. Denny Britz: I like the approach in http://robey.lag.net/2012/03/26/why-config.html. The config classes in https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Config.scala is not much extra code either. Matei, are you okay with using this?\n4. Matei Alexandru Zaharia: I find this kind of confusing and too specific to Scala. I'd prefer something where you just say string key-value pairs. Then it will be easy to explain to existing users, and it will be easy to support it in Java, Python and Shark as well.\n5. Denny Britz: Yeah, it is pretty Scala specific, that's a fair point.\n6. Ankur Bansal: Why not just use regular commons-configuration or JSON based configuration. They are fairly universal and very readable. I would be more than happy to take this up if no one else is working on this already. I went ahead an did a simple grep of the `System.properies` and listed out all the properties as a json document and as a properties file. IMO JSON looks *nicer* but properties files are easier to handle. Any opinions? https://gist.github.com/ankurcha/5655646\n7. Shane Huang: We started a discussion about the design of this on dev mailing list and collected a few opinions, which I summarized below. The link to the original discussion is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html. 1) Define a Configuration class which contains all the options available for Spark application. A Configuration instance can be de-/serialized from/to a formatted file. Most of us tend to agree that Typesafe Config library is a good choice for the Configuration class. 2) Each application (SparkContext) has one Configuration instance and it is initialized by the application which creates it (either coded in app (apps could explicitly read from io stream or command line arguments), or system properties, or env vars). 3) For an application the overriding rule should be code > system properties > env vars. Over time we will deprecate the env vars and maybe even system properties. 4) When launching an Executor on a slave node, the Configuration is firstly initialized using the node-local configuration file as default (instead of the env vars at present), and then the Configuration passed from application driver context will override specific options specified in default. Certain options in app's Configuration will always override those in node-local, because these options need to be the consistent across all the slave nodes, e.g. spark.serializer. In this case if any such options is not set in app's Config, a value will be provided by the system. On the other hand, some options in app's Config will never override those in node-local. as they're not meat to be set in app, e.g. spark.local.dir.\n8. rektide de la fey: \"Inventing a class with its own interface to hold a piece of information is like inventing a new language to write every short story.\" http://www.codequarterly.com/2011/rich-hickey/ Is there tech that can be leveraged here? It's- alas- a library on top of a rather sizable framework, but Chronos, for example, uses Dropwizard Configurable AssetsBundle for configuration (https://github.com/bazaarvoice/dropwizard-configurable-assets-bundle). I get the idea that the use case here, the need, is to serialize some state around, but Configuration is a massive ops concern and it'd be good to party up, I feel.\n9. rektide de la fey: Aurora- a Mesos scheduler from Twitter, the other Mesos scheduler- has code commited as of two days ago. They saw fit to create IDL definitions for tasks and their associate configuration. Not recommending any particular solution here nor there, but perhaps and if perhaps not maybe still interesting & useful reference- https://github.com/twitter/aurora/blob/master/thrift/src/main/thrift/com/twitter/aurora/gen/api.thrift#L130-L160 or specifically now, https://github.com/twitter/aurora/commit/c248931344e46a0e99bfbad6fdf3e08d7473008b#L130-160\n10. Evan Chan: re: @rektide Typesafe Config is used in Akka, Spray, Play, and other Scala frameworks. We use it for all of our Scala apps. If you look at how Akka uses it, they don't need to build a Configuration class on top. Instead, you just pass in a com.typesafe.Config object into your class, say SparkContext. For non-Java/Scala apps, they can write JSON config files, which Typesafe Config can parse easily. The Config object can be initialized and created in multiple ways, but typically from a file, or from a Map, and you can merge it with a default file loaded from resources / jar, or even with system properties. For example, let's say you have class SparkContext(config: Config) { val port = config.getInt(\"spark.port\") That's pretty succinct for getting the value out. The advantage of a config class is that you have a type-safe access to the properties, but the disadvantage is that you have to maintain the API. I honestly feel like it's been OK to use the config without a formal class. -Evan", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.961085"}}
{"id": "593fab759ecf5f5afeb874b0e2688e7c", "issue_key": "SPARK-543", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark for Python", "description": "I want to use Spark from Python, not Scala.", "reporter": "Russell Jurney", "assignee": "Josh Rosen", "created": "0012-06-26T18:21:00.000+0000", "updated": "2013-01-20T12:34:51.000+0000", "resolved": "2013-01-20T12:34:51.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from alexy: Patches welcome!", "created": "2012-06-26T20:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Yeah, agree with Alexy :). If you're interested in this I'd be glad to provide some advice. We are also working on a Java API now.", "created": "2012-06-28T10:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tjhunter: Note that you can already use jython. You loose the benefit of using C libraries like numpy, though, so I found the benefit over scala to be quite marginal. If you try to integrate cpython with some java code, there are non-trivial issues to solve. Maybe using piped rdds would work?", "created": "2012-06-30T19:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-141, originally reported by rjurney", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Resolving this since PySpark has been merged into the master branch.", "created": "2013-01-20T12:34:51.526+0000"}], "num_comments": 5, "text": "Issue: SPARK-543\nSummary: Spark for Python\nDescription: I want to use Spark from Python, not Scala.\n\nComments (5):\n1. Patrick McFadin: Github comment from alexy: Patches welcome!\n2. Patrick McFadin: Github comment from mateiz: Yeah, agree with Alexy :). If you're interested in this I'd be glad to provide some advice. We are also working on a Java API now.\n3. Patrick McFadin: Github comment from tjhunter: Note that you can already use jython. You loose the benefit of using C libraries like numpy, though, so I found the benefit over scala to be quite marginal. If you try to integrate cpython with some java code, there are non-trivial issues to solve. Maybe using piped rdds would work?\n4. Patrick McFadin: Imported from Github issue spark-141, originally reported by rjurney\n5. Josh Rosen: Resolving this since PySpark has been merged into the master branch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.963087"}}
{"id": "8f2736c535fcf64c6bf814eb52b5d1fb", "issue_key": "SPARK-410", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Scalacheck groupId has changed https://github.com/rickynils/scalacheck/i...", "description": "...ssues/24. Necessary to build with scalaVersion 2.9.2. Works with 2.9.1 too.", "reporter": "rrmckinley", "assignee": null, "created": "0012-06-29T11:00:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for the fix!", "created": "2012-06-29T15:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-142, originally reported by rrmckinley", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-410\nSummary: Scalacheck groupId has changed https://github.com/rickynils/scalacheck/i...\nDescription: ...ssues/24. Necessary to build with scalaVersion 2.9.2. Works with 2.9.1 too.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks for the fix!\n2. Patrick McFadin: Imported from Github issue spark-142, originally reported by rrmckinley", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.963087"}}
{"id": "373dc87dfbdb2914349a9ec21db1dfec", "issue_key": "SPARK-409", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Include Shark on default Spark AMI", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-06-29T15:45:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is now done!", "created": "2012-07-06T15:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-143, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-409\nSummary: Include Shark on default Spark AMI\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is now done!\n2. Patrick McFadin: Imported from Github issue spark-143, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.963087"}}
{"id": "0bd47da546a7f239baef3a08f182093c", "issue_key": "SPARK-408", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "User's JARs are not on the classpath when instantiating custom serializer", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-07-06T15:12:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This has been fixed now by Denny's code.", "created": "2012-08-03T18:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-144, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-408\nSummary: User's JARs are not on the classpath when instantiating custom serializer\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This has been fixed now by Denny's code.\n2. Patrick McFadin: Imported from Github issue spark-144, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.965197"}}
{"id": "b136b7750a459b4a7de9bccd5d407dac", "issue_key": "SPARK-407", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "add Accumulatable, add corresponding docs & tests for accumulators", "description": "Preliminary version of some updates to accumulators -- it works, but probably requires some discussion around the exact API. I didn't want to break all existing accumulators, so to add an `addInPlace(T,Y)` method meant creating a whole new type, which I've called `Accumulatable`. Also, I had to distinguish the old `+=` method from the new one. I chose `+:=`, but that was rather arbitrary, I'm no expert on scala idioms. (It seems that the old operator is really closer to `++=`, and the new one should be `+=`, but I guess we can't change that.) I've also added tests for accumulators. These include an example where the current value of an accumulator is read during a task. The example application is stochastic gradient descent, where seeing the current value is really critical. Should `accumulator.value` throw an exception (so the user doesn't think they're getting the global value), but maybe instead we expose `accumulator.localValue`?", "reporter": "squito", "assignee": null, "created": "0012-07-12T09:05:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey, so one major comment on this. Instead of making Accumulable a subclass of Accumulator, I think it would be better to do things the other way around. That is, Accumulable[T, Y] should be the main class that Spark cares about, and Accumulator[T] should extend Accumulable[T, T]. This will require changing a bit more stuff in the rest of the Spark code that depends on accumulators, but it's worth it. Also, if you do this, then the +:= method can just be called +=. The += in Accumulator will be a special case for when T and Y are the same. Does this sound good?", "created": "2012-07-12T09:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from squito: this is a great suggestion. I'll make the change and update the pull request On Thu, Jul 12, 2012 at 10:46 AM, Matei Zaharia <reply@reply.github.com> wrote: > Hey, so one major comment on this. Instead of making Accumulable a subclass of Accumulator, I think it would be better to do things the other way around. That is, Accumulable[T, Y] should be the main class that Spark cares about, and Accumulator[T] should extend Accumulable[T, T]. This will require changing a bit more stuff in the rest of the Spark code that depends on accumulators, but it's worth it. Also, if you do this, then the +:= method can just be called +=. The += in Accumulator will be a special case for when T and Y are the same. Does this sound good? > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/145#issuecomment-6942302", "created": "2012-07-12T11:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks good! I made a few changes on the diff but once you have those I think it's good to merge in. The most important is keeping the name of the old addInPlace method on accumulator so that people don't have to modify their code.", "created": "2012-07-13T16:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey, not sure if you saw, but I posted a line comment about the closure failing in the test. I think it's best to remove the whole test because the value being readable on slave nodes is actually not the intended behavior right now, and only happens in local mode, not on a cluster. See the explanation there. (Just posting here in case you don't get notified on line comments).", "created": "2012-07-23T19:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from squito: Hi, sorry I hadn't gotten to this yet, just got sidetracked (I did get notified of the inline comments too, btw). I can definitely make this change and resubmit. I'd like to also make reading from accumulator.value throw an exception on slave nodes (the same way that assigning a new value does), so that this is more clear. I know this will break some existing code (including some of our own), but really this is just silently waiting to explode when someone goes from local mode to a cluster. I think we'd prefer to prevent those as soon as possible. I still don't really understand why the value shouldn't be readable, though. Its not any more efficient to make the value a local variable and send that workers, is it? You'd just rather the user have to do it explicitly, so they realize how much data they are sending around? I'll try to write the SGD using mapPartitions example at some point in a separate set of commits ... thanks On Mon, Jul 23, 2012 at 8:26 PM, Matei Zaharia <reply@reply.github.com> wrote: > Hey, not sure if you saw, but I posted a line comment about the closure failing in the test. I think it's best to remove the whole test because the value being readable on slave nodes is actually not the intended behavior right now, and only happens in local mode, not on a cluster. See the explanation there. (Just posting here in case you don't get notified on line comments). > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/145#issuecomment-7197850", "created": "2012-07-25T19:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks a lot! The reason that making value readable is tricky is because we actually try to initialize it to \"zero\" in each thread in readObject(), in order to add back all the changes at the end. This just didn't happen in local mode with one thread because we already have that accumulator in our thread. So we'd need to have two fields -- the \"real\" value and the \"accumulating\" value on this worker thread -- and two separate ways to read them. Anyway, I'll think more about including this. Having the exception in there will help a lot until then.", "created": "2012-07-26T16:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-145, originally reported by squito", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-407\nSummary: add Accumulatable, add corresponding docs & tests for accumulators\nDescription: Preliminary version of some updates to accumulators -- it works, but probably requires some discussion around the exact API. I didn't want to break all existing accumulators, so to add an `addInPlace(T,Y)` method meant creating a whole new type, which I've called `Accumulatable`. Also, I had to distinguish the old `+=` method from the new one. I chose `+:=`, but that was rather arbitrary, I'm no expert on scala idioms. (It seems that the old operator is really closer to `++=`, and the new one should be `+=`, but I guess we can't change that.) I've also added tests for accumulators. These include an example where the current value of an accumulator is read during a task. The example application is stochastic gradient descent, where seeing the current value is really critical. Should `accumulator.value` throw an exception (so the user doesn't think they're getting the global value), but maybe instead we expose `accumulator.localValue`?\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: Hey, so one major comment on this. Instead of making Accumulable a subclass of Accumulator, I think it would be better to do things the other way around. That is, Accumulable[T, Y] should be the main class that Spark cares about, and Accumulator[T] should extend Accumulable[T, T]. This will require changing a bit more stuff in the rest of the Spark code that depends on accumulators, but it's worth it. Also, if you do this, then the +:= method can just be called +=. The += in Accumulator will be a special case for when T and Y are the same. Does this sound good?\n2. Patrick McFadin: Github comment from squito: this is a great suggestion. I'll make the change and update the pull request On Thu, Jul 12, 2012 at 10:46 AM, Matei Zaharia <reply@reply.github.com> wrote: > Hey, so one major comment on this. Instead of making Accumulable a subclass of Accumulator, I think it would be better to do things the other way around. That is, Accumulable[T, Y] should be the main class that Spark cares about, and Accumulator[T] should extend Accumulable[T, T]. This will require changing a bit more stuff in the rest of the Spark code that depends on accumulators, but it's worth it. Also, if you do this, then the +:= method can just be called +=. The += in Accumulator will be a special case for when T and Y are the same. Does this sound good? > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/145#issuecomment-6942302\n3. Patrick McFadin: Github comment from mateiz: This looks good! I made a few changes on the diff but once you have those I think it's good to merge in. The most important is keeping the name of the old addInPlace method on accumulator so that people don't have to modify their code.\n4. Patrick McFadin: Github comment from mateiz: Hey, not sure if you saw, but I posted a line comment about the closure failing in the test. I think it's best to remove the whole test because the value being readable on slave nodes is actually not the intended behavior right now, and only happens in local mode, not on a cluster. See the explanation there. (Just posting here in case you don't get notified on line comments).\n5. Patrick McFadin: Github comment from squito: Hi, sorry I hadn't gotten to this yet, just got sidetracked (I did get notified of the inline comments too, btw). I can definitely make this change and resubmit. I'd like to also make reading from accumulator.value throw an exception on slave nodes (the same way that assigning a new value does), so that this is more clear. I know this will break some existing code (including some of our own), but really this is just silently waiting to explode when someone goes from local mode to a cluster. I think we'd prefer to prevent those as soon as possible. I still don't really understand why the value shouldn't be readable, though. Its not any more efficient to make the value a local variable and send that workers, is it? You'd just rather the user have to do it explicitly, so they realize how much data they are sending around? I'll try to write the SGD using mapPartitions example at some point in a separate set of commits ... thanks On Mon, Jul 23, 2012 at 8:26 PM, Matei Zaharia <reply@reply.github.com> wrote: > Hey, not sure if you saw, but I posted a line comment about the closure failing in the test. I think it's best to remove the whole test because the value being readable on slave nodes is actually not the intended behavior right now, and only happens in local mode, not on a cluster. See the explanation there. (Just posting here in case you don't get notified on line comments). > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/145#issuecomment-7197850\n6. Patrick McFadin: Github comment from mateiz: Looks good, thanks a lot! The reason that making value readable is tricky is because we actually try to initialize it to \"zero\" in each thread in readObject(), in order to add back all the changes at the end. This just didn't happen in local mode with one thread because we already have that accumulator in our thread. So we'd need to have two fields -- the \"real\" value and the \"accumulating\" value on this worker thread -- and two separate ways to read them. Anyway, I'll think more about including this. Having the exception in there will help a lot until then.\n7. Patrick McFadin: Imported from Github issue spark-145, originally reported by squito", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.965197"}}
{"id": "6737b95c964d3a4b3c4f6f3fa978be21", "issue_key": "SPARK-542", "issue_type": "Bug", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Cache Miss when machine have multiple hostname", "description": "HI, I encountered a weird runtime of pagerank in last few day. After debugging the job, I found it was caused by the DNS name. The machines of my cluster have multiple hostname, for example, slave 1 have name (c001 and c001.cm.cluster) when spark adding cache in cacheTracker, it get \"c001\" and add cache use it. But when schedule task in SimpleJob, the msos offer give spark \"c001.cm.cluster\". so It will never get preferred location! I thinks spark should handle the multiple hostname case(by using ip instead of hostname, or some other methods). Thanks!", "reporter": "frankvictor", "assignee": null, "created": "0012-07-13T00:51:00.000+0000", "updated": "2014-11-06T06:57:08.000+0000", "resolved": "2014-11-06T06:57:08.000+0000", "labels": [], "components": ["Mesos"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-146, originally reported by frankvictor", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Shixiong Zhu", "body": "using ip has similar problem. A machine can have multiple ips.", "created": "2014-04-09T08:23:37.864+0000"}, {"author": "Mridul Muralidharan", "body": "Spark uses only hostnames - not ip's. Even for hostnames, it should ideally pick only the canonical hostname - not the others. This was done by design in 0.8 ... try to find if multiple host names/ip's are all referring to the same physical host/container is fraught with too many issues.", "created": "2014-04-11T22:35:54.821+0000"}, {"author": "Matei Alexandru Zaharia", "body": "New versions of Spark have ways to specify the hostname and IP address to bind to that should address this issue.", "created": "2014-11-06T06:57:08.537+0000"}], "num_comments": 4, "text": "Issue: SPARK-542\nSummary: Cache Miss when machine have multiple hostname\nDescription: HI, I encountered a weird runtime of pagerank in last few day. After debugging the job, I found it was caused by the DNS name. The machines of my cluster have multiple hostname, for example, slave 1 have name (c001 and c001.cm.cluster) when spark adding cache in cacheTracker, it get \"c001\" and add cache use it. But when schedule task in SimpleJob, the msos offer give spark \"c001.cm.cluster\". so It will never get preferred location! I thinks spark should handle the multiple hostname case(by using ip instead of hostname, or some other methods). Thanks!\n\nComments (4):\n1. Patrick McFadin: Imported from Github issue spark-146, originally reported by frankvictor\n2. Shixiong Zhu: using ip has similar problem. A machine can have multiple ips.\n3. Mridul Muralidharan: Spark uses only hostnames - not ip's. Even for hostnames, it should ideally pick only the canonical hostname - not the others. This was done by design in 0.8 ... try to find if multiple host names/ip's are all referring to the same physical host/container is fraught with too many issues.\n4. Matei Alexandru Zaharia: New versions of Spark have ways to specify the hostname and IP address to bind to that should address this issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.967206"}}
{"id": "e26e076ccba233e231339e9a1334b653", "issue_key": "SPARK-406", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Broadcast refactoring/cleaning up", "description": "- Removed DfsBroadcast completely. - Removed ChainedBroadcast (TreeBroadcast is a superset of this) - Refactored code for TreeBroadcast and BitTorrentBroadcast (still there seems to be a lot of code overlapped, but they have differences in the details). The common object is called MultiTracker that basically acts as the torrent indexing service for ongoing broadcasts. - Added stop() for the broadcast subsystem. Haven't added it to SparkContext though - Merged several configuration options to one for simplicity. The ones that remain are setup to work without any changes. Just changing the broadcastFactory should do. - Changed \"isLocal\" in SparkContext to not consider localhost as local. To use local you have to use \"local\" and not \"localhost\". The reasoning is that using the localhost IP still acts like that, but localhost doesn't; plus, it helps in checking broadcast in the same machine without having to use the actual IP. TODO - Add a HttpBroadcast backup for TreeBroadcast and BitTorrentBroadcast methods just in case they fail. TreeBroadcast shouldn't. BitTorrentBroadcast has a weird corner case that will take some time in figuring out and hasn't been addressed in this patch.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0012-07-15T10:17:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Mosharaf, did you ever get this to work with the block manager? Apart from that, the other thing we should do is make the broadcast objects be classes instead of singleton objects. You can then initialize and stop them in SparkContext. I've recently done that for the BlockManagerMaster too.", "created": "2012-08-03T18:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mosharaf: BroadcastManager is a class now with init() and stop() methods that are called from SparkEnv. Individual broadcast mechanisms still have their own objects though.", "created": "2012-08-05T00:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks. Are you planning to remove the individual objects too or is that too much work for right now?", "created": "2012-08-08T08:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: (Just wondering whether this is ready to merge)", "created": "2012-08-08T08:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I've merged this to make it easier to develop new stuff. Thanks! But we should still remove the standalone objects for MultiTracker & co, for people who want to have multiple SparkContexts in the same app.", "created": "2012-08-23T18:39:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-147, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-406\nSummary: Broadcast refactoring/cleaning up\nDescription: - Removed DfsBroadcast completely. - Removed ChainedBroadcast (TreeBroadcast is a superset of this) - Refactored code for TreeBroadcast and BitTorrentBroadcast (still there seems to be a lot of code overlapped, but they have differences in the details). The common object is called MultiTracker that basically acts as the torrent indexing service for ongoing broadcasts. - Added stop() for the broadcast subsystem. Haven't added it to SparkContext though - Merged several configuration options to one for simplicity. The ones that remain are setup to work without any changes. Just changing the broadcastFactory should do. - Changed \"isLocal\" in SparkContext to not consider localhost as local. To use local you have to use \"local\" and not \"localhost\". The reasoning is that using the localhost IP still acts like that, but localhost doesn't; plus, it helps in checking broadcast in the same machine without having to use the actual IP. TODO - Add a HttpBroadcast backup for TreeBroadcast and BitTorrentBroadcast methods just in case they fail. TreeBroadcast shouldn't. BitTorrentBroadcast has a weird corner case that will take some time in figuring out and hasn't been addressed in this patch.\n\nComments (6):\n1. Patrick McFadin: Github comment from mateiz: Hey Mosharaf, did you ever get this to work with the block manager? Apart from that, the other thing we should do is make the broadcast objects be classes instead of singleton objects. You can then initialize and stop them in SparkContext. I've recently done that for the BlockManagerMaster too.\n2. Patrick McFadin: Github comment from mosharaf: BroadcastManager is a class now with init() and stop() methods that are called from SparkEnv. Individual broadcast mechanisms still have their own objects though.\n3. Patrick McFadin: Github comment from mateiz: Thanks. Are you planning to remove the individual objects too or is that too much work for right now?\n4. Patrick McFadin: Github comment from mateiz: (Just wondering whether this is ready to merge)\n5. Patrick McFadin: Github comment from mateiz: I've merged this to make it easier to develop new stuff. Thanks! But we should still remove the standalone objects for MultiTracker & co, for people who want to have multiple SparkContexts in the same app.\n6. Patrick McFadin: Imported from Github issue spark-147, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.967937"}}
{"id": "2c07117da71de1c6e4df586aaec79d31", "issue_key": "SPARK-405", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Use test fixtures or setup/teardown methods in unit tests", "description": "I've noticed that individual test failures can cause cascades of \"Failed to bind\" / \"Address already in use\" errors, because the `SparkContext.stop()` cleanup method is not called after the failure. One solution would be to move the creation and destruction of `SparkContext` objects to `before()` and `after()`methods, using ScalaTests's [BeforeAndAfter](http://www.scalatest.org/scaladoc/1.8/#org.scalatest.BeforeAndAfter) trait. Another option would be to override the [withFixture](http://www.scalatest.org/scaladoc/1.8/#org.scalatest.fixture.FunSuite) method to supply the `SparkContext` objects to the tests.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-07-16T17:26:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-148, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-405\nSummary: Use test fixtures or setup/teardown methods in unit tests\nDescription: I've noticed that individual test failures can cause cascades of \"Failed to bind\" / \"Address already in use\" errors, because the `SparkContext.stop()` cleanup method is not called after the failure. One solution would be to move the creation and destruction of `SparkContext` objects to `before()` and `after()`methods, using ScalaTests's [BeforeAndAfter](http://www.scalatest.org/scaladoc/1.8/#org.scalatest.BeforeAndAfter) trait. Another option would be to override the [withFixture](http://www.scalatest.org/scaladoc/1.8/#org.scalatest.fixture.FunSuite) method to supply the `SparkContext` objects to the tests.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-148, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.967937"}}
{"id": "b71bea6a5778b2f7acafa8b2f2b38907", "issue_key": "SPARK-404", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Instantiating custom serializer using user's classpath", "description": "- Fixes #144", "reporter": "Denny Britz", "assignee": null, "created": "0012-07-17T13:14:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-07-21T20:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-149, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-404\nSummary: Instantiating custom serializer using user's classpath\nDescription: - Fixes #144\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n2. Patrick McFadin: Imported from Github issue spark-149, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.968953"}}
{"id": "5021ece35071790fcceca35a9f3e6673", "issue_key": "SPARK-403", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Failing Test: FileSuite - Read SequenceFile using new Hadoop API", "description": "", "reporter": "Denny Britz", "assignee": null, "created": "0012-07-18T12:13:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from JoshRosen: A few days ago, I found the commit that introduced this bug and commented on it: https://github.com/mesos/spark/commit/6980b67557105490b6354dbb5331adace52d685c#core-src-main-scala-spark-sparkcontext-scala-P8", "created": "2012-07-18T12:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Fixed this right now in commit 6f44c0db74cc065c676d4d8341da76d86d74365e.", "created": "2012-07-21T20:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-150, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-403\nSummary: Failing Test: FileSuite - Read SequenceFile using new Hadoop API\n\nComments (3):\n1. Patrick McFadin: Github comment from JoshRosen: A few days ago, I found the commit that introduced this bug and commented on it: https://github.com/mesos/spark/commit/6980b67557105490b6354dbb5331adace52d685c#core-src-main-scala-spark-sparkcontext-scala-P8\n2. Patrick McFadin: Github comment from mateiz: Fixed this right now in commit 6f44c0db74cc065c676d4d8341da76d86d74365e.\n3. Patrick McFadin: Imported from Github issue spark-150, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.968953"}}
{"id": "09d4e56adc2e3a474601a12ec3fc42bb", "issue_key": "SPARK-402", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Examples ship to to cluster", "description": "- Fixes #118 - I wasn't sure how to best get the path to the example JAR, I didn't want to do it manually in every examples. I added another variable in the ./run script that's being used by the examples. Let me know if that's not the best way. - Should update the example documentation to tell people to run \"sbt/sbt package\" before running the examples.", "reporter": "Denny Britz", "assignee": null, "created": "0012-07-18T12:18:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks good, but can you also update the README to say that you should run sbt package instead of sbt compile? Otherwise the examples JAR won't get built. Also, rename the EXAMPLES_JAR variable to SPARK_EXAMPLES_JAR so that it doesn't conflict with other environment variables the user might make.", "created": "2012-07-23T15:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Denny, not sure if you saw my comment here, but can you rename the environment variable as mentioned above, and update the README to say sbt package? Otherwise it looks good.", "created": "2012-08-02T10:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I made the changes you suggested.", "created": "2012-08-04T15:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks!", "created": "2012-08-04T16:39:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-151, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-402\nSummary: Examples ship to to cluster\nDescription: - Fixes #118 - I wasn't sure how to best get the path to the example JAR, I didn't want to do it manually in every examples. I added another variable in the ./run script that's being used by the examples. Let me know if that's not the best way. - Should update the example documentation to tell people to run \"sbt/sbt package\" before running the examples.\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: This looks good, but can you also update the README to say that you should run sbt package instead of sbt compile? Otherwise the examples JAR won't get built. Also, rename the EXAMPLES_JAR variable to SPARK_EXAMPLES_JAR so that it doesn't conflict with other environment variables the user might make.\n2. Patrick McFadin: Github comment from mateiz: Hey Denny, not sure if you saw my comment here, but can you rename the environment variable as mentioned above, and update the README to say sbt package? Otherwise it looks good.\n3. Patrick McFadin: Github comment from dennybritz: I made the changes you suggested.\n4. Patrick McFadin: Github comment from mateiz: Great, thanks!\n5. Patrick McFadin: Imported from Github issue spark-151, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.968953"}}
{"id": "dd723cfd633d291a9d2d7651d2055092", "issue_key": "SPARK-401", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Always destroy SparkContext in after block for the unit tests.", "description": "- Fixes #148 - Tested. Tests still passing, except for one that wasn't passing before, see #150 - I intentionally didn't instantiate a `SparkContext` in a before{} block, because every test may instantiate one with different parameters, or not use one at all.", "reporter": "Denny Britz", "assignee": null, "created": "0012-07-18T12:25:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks! Merged this.", "created": "2012-07-23T15:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-152, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-401\nSummary: Always destroy SparkContext in after block for the unit tests.\nDescription: - Fixes #148 - Tested. Tests still passing, except for one that wasn't passing before, see #150 - I intentionally didn't instantiate a `SparkContext` in a before{} block, because every test may instantiate one with different parameters, or not use one at all.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks! Merged this.\n2. Patrick McFadin: Imported from Github issue spark-152, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.969946"}}
{"id": "fa58bd906411cebba939224197d15a28", "issue_key": "SPARK-400", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Java API", "description": "An initial version of a Java API for Spark. In addition to the Java API, this commit adds a `distinct()` method to RDDs and fixes some issues related to `DoubleRDDFunctions`.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-07-18T19:07:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks really good, but a couple of comments: * Can you rename the package to just spark.java? Seems a bit simpler. * It would be better to replace the JavaLR with a JavaHdfsLR that reads from a file. The reason is that this will be more common for people wanting to use Spark, so we might as well include it as an example. * In the WordCount example, I would inline the functions as anonymous inner classes just to show that syntax. * In your Scala function definitions (def), always add a return type instead of relying on type inference. This will ensure that we notice when we break binary compatibility by changing a type. We've tried to do this in the rest of the code.", "created": "2012-07-21T21:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: > Can you rename the package to just `spark.java`? Seems a bit simpler. Scala package references are relative, so import statements like ```scala package spark import java.util.List ``` would be resolved against the `spark.java` package, causing compile errors. A workaround is to use the top-level [`_root_`](http://stackoverflow.com/questions/687071/what-is-the-root-package-in-scala) package to import from the correct top-level `java` package: ```scala package spark import _root_.java.util.List ``` It's easy to add the `_root_` prefixes to the existing code, but the change would affect a lot of files. Do you still want to rename the package?", "created": "2012-07-22T12:08:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Oh, got it. Let's not rename it then. Matei On Jul 22, 2012, at 1:08 PM, Josh Rosen <reply@reply.github.com> wrote: >> Can you rename the package to just `spark.java`? Seems a bit simpler. > > Scala package references are relative, so import statements like > > ```scala > package spark > import java.util.List > ``` > > would be resolved against the `spark.java` package, causing compile errors. A workaround is to use the top-level [`_root_`](http://stackoverflow.com/questions/687071/what-is-the-root-package-in-scala) package to import from the correct top-level `java` package: > > ```scala > package spark > import _root_.java.util.List > ``` > > It's easy to add the `_root_` prefixes to the existing code, but the change would affect a lot of files. Do you still want to rename the package? > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/153#issuecomment-7165012", "created": "2012-07-22T12:51:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: I pushed some changes that address your comments; let me know what you think.", "created": "2012-07-22T17:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: Thanks for the feedback; I pushed another commit addressing these comments.", "created": "2012-07-24T09:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: I removed the dependency on `StringOps` and added in a few RDD methods that were missing from the Java API (`persist`, `splits`, `glom`, and `mapPartitions`).", "created": "2012-07-26T11:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good. I'm going to merge it now, and we can make further changes as smaller commits. I'm also trying to get feedback on the API from a few other people. But this is an awesome start!", "created": "2012-07-26T16:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-153, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 8, "text": "Issue: SPARK-400\nSummary: Java API\nDescription: An initial version of a Java API for Spark. In addition to the Java API, this commit adds a `distinct()` method to RDDs and fixes some issues related to `DoubleRDDFunctions`.\n\nComments (8):\n1. Patrick McFadin: Github comment from mateiz: This looks really good, but a couple of comments: * Can you rename the package to just spark.java? Seems a bit simpler. * It would be better to replace the JavaLR with a JavaHdfsLR that reads from a file. The reason is that this will be more common for people wanting to use Spark, so we might as well include it as an example. * In the WordCount example, I would inline the functions as anonymous inner classes just to show that syntax. * In your Scala function definitions (def), always add a return type instead of relying on type inference. This will ensure that we notice when we break binary compatibility by changing a type. We've tried to do this in the rest of the code.\n2. Patrick McFadin: Github comment from JoshRosen: > Can you rename the package to just `spark.java`? Seems a bit simpler. Scala package references are relative, so import statements like ```scala package spark import java.util.List ``` would be resolved against the `spark.java` package, causing compile errors. A workaround is to use the top-level [`_root_`](http://stackoverflow.com/questions/687071/what-is-the-root-package-in-scala) package to import from the correct top-level `java` package: ```scala package spark import _root_.java.util.List ``` It's easy to add the `_root_` prefixes to the existing code, but the change would affect a lot of files. Do you still want to rename the package?\n3. Patrick McFadin: Github comment from mateiz: Oh, got it. Let's not rename it then. Matei On Jul 22, 2012, at 1:08 PM, Josh Rosen <reply@reply.github.com> wrote: >> Can you rename the package to just `spark.java`? Seems a bit simpler. > > Scala package references are relative, so import statements like > > ```scala > package spark > import java.util.List > ``` > > would be resolved against the `spark.java` package, causing compile errors. A workaround is to use the top-level [`_root_`](http://stackoverflow.com/questions/687071/what-is-the-root-package-in-scala) package to import from the correct top-level `java` package: > > ```scala > package spark > import _root_.java.util.List > ``` > > It's easy to add the `_root_` prefixes to the existing code, but the change would affect a lot of files. Do you still want to rename the package? > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/pull/153#issuecomment-7165012\n4. Patrick McFadin: Github comment from JoshRosen: I pushed some changes that address your comments; let me know what you think.\n5. Patrick McFadin: Github comment from JoshRosen: Thanks for the feedback; I pushed another commit addressing these comments.\n6. Patrick McFadin: Github comment from JoshRosen: I removed the dependency on `StringOps` and added in a few RDD methods that were missing from the Java API (`persist`, `splits`, `glom`, and `mapPartitions`).\n7. Patrick McFadin: Github comment from mateiz: Looks good. I'm going to merge it now, and we can make further changes as smaller commits. I'm also trying to get feedback on the API from a few other people. But this is an awesome start!\n8. Patrick McFadin: Imported from Github issue spark-153, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.969946"}}
{"id": "6c5d1faf4f133d9d79f7bdc84fd38b58", "issue_key": "SPARK-399", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Broadcast UUID cannot be casted to Integer", "description": "When using a lot of broadcast variables, I sometimes get the following exception. java.lang.ClassCastException: java.util.UUID cannot be cast to java.lang.Integer at scala.runtime.BoxesRunTime.unboxToInt(Unknown Source) at scala.Tuple2._2$mcI$sp(Tuple2.scala:22) at spark.BoundedMemoryCache.reportEntryDropped(BoundedMemoryCache.scala:94) at spark.BoundedMemoryCache.ensureFreeSpace(BoundedMemoryCache.scala:84) at spark.BoundedMemoryCache.put(BoundedMemoryCache.scala:40) at spark.KeySpace.put(Cache.scala:60) at spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:38) at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:616) Here is my guess. The UUID assigned to a broadcasted variable using UUID.randomUUID is a 128 bit value. When a broadcast variable needs to be dropped, reportEntryDropped attempts to cast the UUID to Int for reporting to the cacheTracker. Thats where the casting fails.", "reporter": "Tathagata Das", "assignee": null, "created": "0012-07-23T00:35:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from henryemilner: I encountered what looks like a related issue: 2012-07-27 17:41:48,660 [main] INFO spark.BoundedMemoryCache - Dropping key ((1,4003b6c9-737f-4504-8144-ea39d1d0b529), 0) of size 255414 to make space scala.MatchError: (1,4003b6c9-737f-4504-8144-ea39d1d0b529) (of class scala.Tuple2) at spark.CacheTracker.dropEntry(CacheTracker.scala:228) at spark.BoundedMemoryCache.reportEntryDropped(BoundedMemoryCache.scala:93) at spark.BoundedMemoryCache.ensureFreeSpace(BoundedMemoryCache.scala:84) at spark.BoundedMemoryCache.put(BoundedMemoryCache.scala:40) at spark.KeySpace.put(Cache.scala:60) [...] It looks like dropEntry expects @datasetId to be an (Int, Int), but when evicting a broadcast variable it is an (Int, UUID). It seems like this was just overlooked when broadcast variables were added - the rest of the code calls this thing an rddId and expects it to be an Int. In fact, it appears to me that the master is never notified via an AddedToCache message when a slave fetches a broadcast variable - TreeBroadcast, for example, calls KeySpace.put() directly. This means it would not make sense to send a DroppedFromCache message to the master anyway. So I assume it is safe to work around this exception by adding a catchall case to CacheTracker.dropEntry(). But this means that the master doesn't include slaves' broadcast variables in its estimates of their memory usages. That seems like a bigger bug to me, if it is one. Can someone clarify what's intended here?", "created": "2012-07-27T18:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tdas: Here are my thoughts. The CacheTracker is designed to keep track of the locations of RDD partitions. So cacheTracker.dropEntry() is meant to notify the master when a RDD's partition falls off the cache. Now the same BoundedMemoryCache is also being using by the broadcast stuff. Since broadcast variables are not something that the CacheTracker's master cares about, it does not need to be notified if a broadcast variable falls off cache. In that case, a simple check that calls cacheTracker.dropEntry only when the datasetId is of type (Int, Int) should suffice. I fixed this today and it works. However, its in my own private repo that will eventually be pushed to mesos/spark/dev branch. For now, you can fix it immediately by this. Hope this helps! ```scala protected def reportEntryDropped(datasetId: Any, partition: Int, entry: Entry) { logInfo(\"Dropping key (%s, %d) of size %d to make space\".format(datasetId, partition, entry.size)) // TODO: remove BoundedMemoryCache val (keySpaceId, innerDatasetId) = datasetId.asInstanceOf[(Any, Any)] innerDatasetId match { case rddId: Int => SparkEnv.get.cacheTracker.dropEntry(rddId, partition) case broadcastUUID: java.util.UUID => // TODO: Maybe something should be done if the broadcasted variable falls out of cache case _ => } } ``` I agree that this would lead to miscalculation in the master's estimate of the cache usage of the slave. So yes there is a bigger problem that has been cause by Broadcast using BoundedMemoryCache. This will be correctly fixed in the dev branch (and therefore next version of Spark) when the BoundedMemoryCache will be replaced by the new BlockManager.", "created": "2012-07-27T22:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from henryemilner: Great, thanks for the clarification.", "created": "2012-07-30T10:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I've implemented a fix similar to TD's in master now too.", "created": "2012-08-03T11:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-154, originally reported by tdas", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-399\nSummary: Broadcast UUID cannot be casted to Integer\nDescription: When using a lot of broadcast variables, I sometimes get the following exception. java.lang.ClassCastException: java.util.UUID cannot be cast to java.lang.Integer at scala.runtime.BoxesRunTime.unboxToInt(Unknown Source) at scala.Tuple2._2$mcI$sp(Tuple2.scala:22) at spark.BoundedMemoryCache.reportEntryDropped(BoundedMemoryCache.scala:94) at spark.BoundedMemoryCache.ensureFreeSpace(BoundedMemoryCache.scala:84) at spark.BoundedMemoryCache.put(BoundedMemoryCache.scala:40) at spark.KeySpace.put(Cache.scala:60) at spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:38) at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:616) Here is my guess. The UUID assigned to a broadcasted variable using UUID.randomUUID is a 128 bit value. When a broadcast variable needs to be dropped, reportEntryDropped attempts to cast the UUID to Int for reporting to the cacheTracker. Thats where the casting fails.\n\nComments (5):\n1. Patrick McFadin: Github comment from henryemilner: I encountered what looks like a related issue: 2012-07-27 17:41:48,660 [main] INFO spark.BoundedMemoryCache - Dropping key ((1,4003b6c9-737f-4504-8144-ea39d1d0b529), 0) of size 255414 to make space scala.MatchError: (1,4003b6c9-737f-4504-8144-ea39d1d0b529) (of class scala.Tuple2) at spark.CacheTracker.dropEntry(CacheTracker.scala:228) at spark.BoundedMemoryCache.reportEntryDropped(BoundedMemoryCache.scala:93) at spark.BoundedMemoryCache.ensureFreeSpace(BoundedMemoryCache.scala:84) at spark.BoundedMemoryCache.put(BoundedMemoryCache.scala:40) at spark.KeySpace.put(Cache.scala:60) [...] It looks like dropEntry expects @datasetId to be an (Int, Int), but when evicting a broadcast variable it is an (Int, UUID). It seems like this was just overlooked when broadcast variables were added - the rest of the code calls this thing an rddId and expects it to be an Int. In fact, it appears to me that the master is never notified via an AddedToCache message when a slave fetches a broadcast variable - TreeBroadcast, for example, calls KeySpace.put() directly. This means it would not make sense to send a DroppedFromCache message to the master anyway. So I assume it is safe to work around this exception by adding a catchall case to CacheTracker.dropEntry(). But this means that the master doesn't include slaves' broadcast variables in its estimates of their memory usages. That seems like a bigger bug to me, if it is one. Can someone clarify what's intended here?\n2. Patrick McFadin: Github comment from tdas: Here are my thoughts. The CacheTracker is designed to keep track of the locations of RDD partitions. So cacheTracker.dropEntry() is meant to notify the master when a RDD's partition falls off the cache. Now the same BoundedMemoryCache is also being using by the broadcast stuff. Since broadcast variables are not something that the CacheTracker's master cares about, it does not need to be notified if a broadcast variable falls off cache. In that case, a simple check that calls cacheTracker.dropEntry only when the datasetId is of type (Int, Int) should suffice. I fixed this today and it works. However, its in my own private repo that will eventually be pushed to mesos/spark/dev branch. For now, you can fix it immediately by this. Hope this helps! ```scala protected def reportEntryDropped(datasetId: Any, partition: Int, entry: Entry) { logInfo(\"Dropping key (%s, %d) of size %d to make space\".format(datasetId, partition, entry.size)) // TODO: remove BoundedMemoryCache val (keySpaceId, innerDatasetId) = datasetId.asInstanceOf[(Any, Any)] innerDatasetId match { case rddId: Int => SparkEnv.get.cacheTracker.dropEntry(rddId, partition) case broadcastUUID: java.util.UUID => // TODO: Maybe something should be done if the broadcasted variable falls out of cache case _ => } } ``` I agree that this would lead to miscalculation in the master's estimate of the cache usage of the slave. So yes there is a bigger problem that has been cause by Broadcast using BoundedMemoryCache. This will be correctly fixed in the dev branch (and therefore next version of Spark) when the BoundedMemoryCache will be replaced by the new BlockManager.\n3. Patrick McFadin: Github comment from henryemilner: Great, thanks for the clarification.\n4. Patrick McFadin: Github comment from mateiz: I've implemented a fix similar to TD's in master now too.\n5. Patrick McFadin: Imported from Github issue spark-154, originally reported by tdas", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.970947"}}
{"id": "2cf64ca32e096f3a9dc5fb6943a933fc", "issue_key": "SPARK-398", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support for external hashing and sorting", "description": "Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.", "reporter": "Harvey Feng", "assignee": null, "created": "0012-07-23T15:43:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from harveyfeng: Sorry, meant this for the 'dev' branch...", "created": "2012-07-23T15:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-155, originally reported by harveyfeng", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-398\nSummary: Support for external hashing and sorting\nDescription: Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.\n\nComments (2):\n1. Patrick McFadin: Github comment from harveyfeng: Sorry, meant this for the 'dev' branch...\n2. Patrick McFadin: Imported from Github issue spark-155, originally reported by harveyfeng", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.970947"}}
{"id": "b76689d2854045728157a410f81c960d", "issue_key": "SPARK-397", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support for external sorting and hashing", "description": "Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.", "reporter": "Harvey Feng", "assignee": null, "created": "0012-07-23T15:50:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: For a change of this size, it would be nice to have a high level writeup for it (1 - 5 pages) to explain the general structure / components. It'd also be very useful for reviewers and others to read when they want to modify this in the future.", "created": "2012-07-30T11:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-156, originally reported by harveyfeng", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-397\nSummary: Support for external sorting and hashing\nDescription: Also includes a small fix in RangePartitioner - ascending/descending is accounted for in getPartition(), so sorting rddSample doesn't need the ascending check.\n\nComments (2):\n1. Patrick McFadin: Github comment from rxin: For a change of this size, it would be nice to have a high level writeup for it (1 - 5 pages) to explain the general structure / components. It'd also be very useful for reviewers and others to read when they want to modify this in the future.\n2. Patrick McFadin: Imported from Github issue spark-156, originally reported by harveyfeng", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.970947"}}
{"id": "a94dc9ef9612bc90e7941684fe405c47", "issue_key": "SPARK-396", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Logging Throwables in Info and Debug", "description": "Logging Throwables in logInfo and logDebug instead of swallowing them.", "reporter": "Paul Cavallaro", "assignee": null, "created": "0012-07-30T09:44:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for catching this! Looks like a useful fix.", "created": "2012-08-02T05:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-157, originally reported by paulcavallaro", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-396\nSummary: Logging Throwables in Info and Debug\nDescription: Logging Throwables in logInfo and logDebug instead of swallowing them.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks for catching this! Looks like a useful fix.\n2. Patrick McFadin: Imported from Github issue spark-157, originally reported by paulcavallaro", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "89ea841391991650d7462058a15559f7", "issue_key": "SPARK-395", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Merge Akka reference.conf files in sbt assembly task", "description": "This fixes an issue when running Spark from the jar generated by the sbt `assembly` task. To reproduce the issue: ```java import spark.SparkContext; public class Test { public static void main(String[] args) { new SparkContext(\"local\", \"test\"); System.exit(0); } } ``` Place this code in a file (but not in the root of the `SPARK_HOME` directory), then compile and run it against the assembly jar: ``` $ javac -cp ../core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar Test.java $ java -cp ../core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar:. Test Exception in thread \"main\" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka.version' at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:115) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:135) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:140) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:108) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:146) at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:188) at akka.actor.ActorSystem$Settings.<init>(ActorSystem.scala:116) at akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:429) at akka.actor.ActorSystem$.apply(ActorSystem.scala:103) at spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:36) at spark.SparkEnv$.createFromSystemProperties(SparkEnv.scala:58) at spark.SparkContext.<init>(SparkContext.scala:71) at spark.SparkContext.<init>(SparkContext.scala:55) at Test.main(Test.java:5) ``` This issue is caused by the way that Akka `reference.conf` files are handled in the `assembly` task; the commit copies the solution provided by the Akka Team Blog: http://letitcrash.com/post/21025950392/howto-sbt-assembly-vs-reference-conf.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-07-30T21:28:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Woah, that's pretty ugly, but I guess we have to do it. I guess this reference.conf stuff was not a problem before because we weren't using Akka.", "created": "2012-07-30T21:36:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ijuma: No need for all this ugly code. Use a newer sbt-assembly. See http://letitcrash.com/post/21706121997/follow-up-sbt-assembly-now-likes-reference-conf", "created": "2012-07-31T00:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: It turns out that we're already using sbt-assembly 0.8.3. The default `mergeStrategy` used by sbt-assembly should do the right thing for `reference.conf`, but it was not applied because commit ede615d7 added a custom `mergeStrategy` that did not extend the default. The default `mergeStrategy` fails when trying to deduplicate `javax/servlet/SingleThreadModel.class`, which is probably why the custom `mergeStrategy` was added: ``` [info] Merging 'javax/servlet/SingleThreadModel.class' with strategy 'deduplicate' [error] {file:/Users/josh/Documents/programming/spark/spark/}repl/*:assembly: deduplicate: different file contents found in the following: [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/javax.servlet/servlet-api/servlet-api-2.5.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api-2.5/servlet-api-2.5-6.1.14.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api/servlet-api-2.5-20081211.jar:javax/servlet/SingleThreadModel.class [error] {file:/Users/josh/Documents/programming/spark/spark/}core/*:assembly: deduplicate: different file contents found in the following: [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/javax.servlet/servlet-api/servlet-api-2.5.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api-2.5/servlet-api-2.5-6.1.14.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api/servlet-api-2.5-20081211.jar:javax/servlet/SingleThreadModel.class [error] Total time: 93 s, completed Aug 1, 2012 11:04:10 PM ``` Adding `case \"reference.conf\" => MergeStrategy.concat` to our `mergeStrategy` solves the problem.", "created": "2012-08-01T22:07:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Ah, makes sense. Do you mind sending another pull request with that then?", "created": "2012-08-02T05:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-158, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-395\nSummary: Merge Akka reference.conf files in sbt assembly task\nDescription: This fixes an issue when running Spark from the jar generated by the sbt `assembly` task. To reproduce the issue: ```java import spark.SparkContext; public class Test { public static void main(String[] args) { new SparkContext(\"local\", \"test\"); System.exit(0); } } ``` Place this code in a file (but not in the root of the `SPARK_HOME` directory), then compile and run it against the assembly jar: ``` $ javac -cp ../core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar Test.java $ java -cp ../core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar:. Test Exception in thread \"main\" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka.version' at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:115) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:135) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:140) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:108) at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:146) at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:188) at akka.actor.ActorSystem$Settings.<init>(ActorSystem.scala:116) at akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:429) at akka.actor.ActorSystem$.apply(ActorSystem.scala:103) at spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:36) at spark.SparkEnv$.createFromSystemProperties(SparkEnv.scala:58) at spark.SparkContext.<init>(SparkContext.scala:71) at spark.SparkContext.<init>(SparkContext.scala:55) at Test.main(Test.java:5) ``` This issue is caused by the way that Akka `reference.conf` files are handled in the `assembly` task; the commit copies the solution provided by the Akka Team Blog: http://letitcrash.com/post/21025950392/howto-sbt-assembly-vs-reference-conf.\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Woah, that's pretty ugly, but I guess we have to do it. I guess this reference.conf stuff was not a problem before because we weren't using Akka.\n2. Patrick McFadin: Github comment from ijuma: No need for all this ugly code. Use a newer sbt-assembly. See http://letitcrash.com/post/21706121997/follow-up-sbt-assembly-now-likes-reference-conf\n3. Patrick McFadin: Github comment from JoshRosen: It turns out that we're already using sbt-assembly 0.8.3. The default `mergeStrategy` used by sbt-assembly should do the right thing for `reference.conf`, but it was not applied because commit ede615d7 added a custom `mergeStrategy` that did not extend the default. The default `mergeStrategy` fails when trying to deduplicate `javax/servlet/SingleThreadModel.class`, which is probably why the custom `mergeStrategy` was added: ``` [info] Merging 'javax/servlet/SingleThreadModel.class' with strategy 'deduplicate' [error] {file:/Users/josh/Documents/programming/spark/spark/}repl/*:assembly: deduplicate: different file contents found in the following: [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/javax.servlet/servlet-api/servlet-api-2.5.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api-2.5/servlet-api-2.5-6.1.14.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api/servlet-api-2.5-20081211.jar:javax/servlet/SingleThreadModel.class [error] {file:/Users/josh/Documents/programming/spark/spark/}core/*:assembly: deduplicate: different file contents found in the following: [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/javax.servlet/servlet-api/servlet-api-2.5.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api-2.5/servlet-api-2.5-6.1.14.jar:javax/servlet/SingleThreadModel.class [error] /Users/josh/Documents/programming/spark/spark/lib_managed/jars/org.mortbay.jetty/servlet-api/servlet-api-2.5-20081211.jar:javax/servlet/SingleThreadModel.class [error] Total time: 93 s, completed Aug 1, 2012 11:04:10 PM ``` Adding `case \"reference.conf\" => MergeStrategy.concat` to our `mergeStrategy` solves the problem.\n4. Patrick McFadin: Github comment from mateiz: Ah, makes sense. Do you mind sending another pull request with that then?\n5. Patrick McFadin: Imported from Github issue spark-158, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "e362e0b9abe5e3d8dd20e18c1249a4f2", "issue_key": "SPARK-394", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark WebUI", "description": "- Uses spray.twirl (https://github.com/spray/twirl) - CSS and JS files are included twice, once for the worker and once for the master resources. We may want to have just one shared resources folder since most people won't separate the code. - One problem that (I think, untested) remains is that the code uses the machine's local IP address (same as the spark server) in the URL links, even though the WebUI server is bound to 0.0.0.0/0. That means if you're running on a cluster with both a private and public ip address the private one will be used and you won't be able to follow the links from outside the cluster. - Jobs are always marked as completed. We need to add conditions under which a job counts as FAILED.", "reporter": "Denny Britz", "assignee": null, "created": "0012-08-01T18:49:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks! Just to respond to the comments you raised: * It would be nice to add a shared \"static\" folder for the CSS and JS. I believe you can change the paths in Spray so that anything starting with /static is served out of that directory. * The IP address thing is tricky. For the Mesos scripts, we let the users set a \"public hostname\" parameter for each machine that was used in the web UI. Maybe we can do the same here. It will definitely matter when we run on EC2. * Figuring out what's FAILED is tricky as well due to disconnections. I think that right now, the client just disconnects. Ideally you would add an \"unregister\" message for a successful job, and add a shutdown hook on the client (Runtime.addShutdownHook) to send it. You'd also need a version of \"unregister\" that reports failure, because the Spark master can decide it's failed if a task fails too many times. And finally, for other disconnects, you would count them as failures.", "created": "2012-08-01T18:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: - I'll look into the static folder. - I guess we don't need to worry about the public ip address too much. Let's instead modify the scripts to give the --ip argument to the master and workers. The user is then responsible for giving a public ip.", "created": "2012-08-01T20:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Files are now in a static folder.", "created": "2012-08-02T08:49:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks good to merge except for a few formatting things (overly long lines of text).", "created": "2012-08-02T12:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Thanks, I made the changes you suggested. I couldn't get the non-blocking calls to work with Spray (I had actually tried that before), because of its built-in implicit Marshallers. However, I found that it actually supports implicit conversions of Futures.", "created": "2012-08-02T13:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks! Just merged it.", "created": "2012-08-02T13:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-159, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-394\nSummary: Spark WebUI\nDescription: - Uses spray.twirl (https://github.com/spray/twirl) - CSS and JS files are included twice, once for the worker and once for the master resources. We may want to have just one shared resources folder since most people won't separate the code. - One problem that (I think, untested) remains is that the code uses the machine's local IP address (same as the spark server) in the URL links, even though the WebUI server is bound to 0.0.0.0/0. That means if you're running on a cluster with both a private and public ip address the private one will be used and you won't be able to follow the links from outside the cluster. - Jobs are always marked as completed. We need to add conditions under which a job counts as FAILED.\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: Thanks! Just to respond to the comments you raised: * It would be nice to add a shared \"static\" folder for the CSS and JS. I believe you can change the paths in Spray so that anything starting with /static is served out of that directory. * The IP address thing is tricky. For the Mesos scripts, we let the users set a \"public hostname\" parameter for each machine that was used in the web UI. Maybe we can do the same here. It will definitely matter when we run on EC2. * Figuring out what's FAILED is tricky as well due to disconnections. I think that right now, the client just disconnects. Ideally you would add an \"unregister\" message for a successful job, and add a shutdown hook on the client (Runtime.addShutdownHook) to send it. You'd also need a version of \"unregister\" that reports failure, because the Spark master can decide it's failed if a task fails too many times. And finally, for other disconnects, you would count them as failures.\n2. Patrick McFadin: Github comment from dennybritz: - I'll look into the static folder. - I guess we don't need to worry about the public ip address too much. Let's instead modify the scripts to give the --ip argument to the master and workers. The user is then responsible for giving a public ip.\n3. Patrick McFadin: Github comment from dennybritz: Files are now in a static folder.\n4. Patrick McFadin: Github comment from mateiz: This looks good to merge except for a few formatting things (overly long lines of text).\n5. Patrick McFadin: Github comment from dennybritz: Thanks, I made the changes you suggested. I couldn't get the non-blocking calls to work with Spray (I had actually tried that before), because of its built-in implicit Marshallers. However, I found that it actually supports implicit conversions of Futures.\n6. Patrick McFadin: Github comment from mateiz: Thanks! Just merged it.\n7. Patrick McFadin: Imported from Github issue spark-159, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "ea5abc9472a71e4f64c0db16b8a3e0e9", "issue_key": "SPARK-393", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Standalone cluster scripts", "description": "- Heavily influenced by Hadoop scripts. Basically works the same way, but with different argument names :) A list of slaves is expected to be in conf/slaves. - Added a EC2 script, ec2-standalone, based on the mesos script, that starts a spark cluster in standalone mode. It works the same way as the mesos one (and takes the same arguments). The biggest differences are that there are no zoo keeper nodes, different security groups, and that it executes bin/start-all.sh to setup the cluster. - Created an AMI for the standalone mode, ami-1ff25976 (https://s3.amazonaws.com/spark-standalone-amis/latest-spark). All it has is the latest spark and scala. - Tested the scripts locally and on EC2. - We probably want to write up a tutorial on how to use the scripts, though it should be rather straightforward for people familiar with the hadoop ones. - One TODO is the configuration. How do we want users to specify things like master port, webUiPort, worker memory, worke cores, and so on? The simplest way would be to put these as env variables into the spark-env.sh, but that can get messy rather quickly. - Removed explicit netty dependency. This caused trouble because 2 netty jars were included. Akka already has a netty dependency.", "reporter": "Denny Britz", "assignee": null, "created": "0012-08-01T19:44:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey, just a couple of comments: * Regarding the configuration, I think environment variables are the best way to go for now, to be consistent with out other config stuff. Just call them things like SPARK_MASTER_PORT. * Would it be possible to reuse the spark-ec2 script from before, but just with a different AMI? Since a lot of the setup is handled by the script on the AMI, I believe it should be possible to reuse the client-side code. If necessary, you can add options to it. That's better than having two nearly identical files.", "created": "2012-08-02T10:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Yes, I could reuse the existing EC2 script. I'll just have to add another switch to it and make the security groups and ssh commands conditional.", "created": "2012-08-02T12:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I merged the two scripts into one. Still working on the configuration, but it's working fine with default settings right now.", "created": "2012-08-02T14:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Added configuration variables.", "created": "2012-08-02T15:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks. I looked at it some more and I have just two other suggestions: * If nontrivial parts of the code, such as the \"rotate logs\" bit, are copied from Hadoop, add Hadoop's Apache license notice to the the top of that file. If it's not copied but just based on reading the code, then there's probably no need. * Why didn't you make the master be configured in a separate \"masters\" file as well? I thought about this a bit and I think it's good to do it the same way as Hadoop because people will be familiar with that. It's also useful in case someone tries to stop or start the cluster from a machine other than the master (because the scripts will be everywhere).", "created": "2012-08-03T18:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: * Added the Apache license to the two scripts that are a bit more complex and obviously taken from Hadoop. * The masters file in Hadoop is actually not what most people expect it to be. In Hadoop the NameNode and DataNode are only started on the machine where the script is executed. The masters file only specifies the locations where a SecondaryNameNode should be started. So, the naming of the *masters* file in Hadoop is a bit off. Since Spark doesn't have the notion of a SecondaryNameNode (which isn't a backup for the NameNode anyway) I thought a masters file was unnecessary.", "created": "2012-08-04T16:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Oh, I see. That makes sense then. I'm going to merge this, and I guess the last thing that will be left is to document it. For the documentation, I was thinking of switching away from the wiki to a set of HTML docs generated from markup files in our source tree, using a system such as Jekyll (https://github.com/mojombo/jekyll). I'll send an email later about that, but it can wait a bit.", "created": "2012-08-04T16:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-160, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 8, "text": "Issue: SPARK-393\nSummary: Standalone cluster scripts\nDescription: - Heavily influenced by Hadoop scripts. Basically works the same way, but with different argument names :) A list of slaves is expected to be in conf/slaves. - Added a EC2 script, ec2-standalone, based on the mesos script, that starts a spark cluster in standalone mode. It works the same way as the mesos one (and takes the same arguments). The biggest differences are that there are no zoo keeper nodes, different security groups, and that it executes bin/start-all.sh to setup the cluster. - Created an AMI for the standalone mode, ami-1ff25976 (https://s3.amazonaws.com/spark-standalone-amis/latest-spark). All it has is the latest spark and scala. - Tested the scripts locally and on EC2. - We probably want to write up a tutorial on how to use the scripts, though it should be rather straightforward for people familiar with the hadoop ones. - One TODO is the configuration. How do we want users to specify things like master port, webUiPort, worker memory, worke cores, and so on? The simplest way would be to put these as env variables into the spark-env.sh, but that can get messy rather quickly. - Removed explicit netty dependency. This caused trouble because 2 netty jars were included. Akka already has a netty dependency.\n\nComments (8):\n1. Patrick McFadin: Github comment from mateiz: Hey, just a couple of comments: * Regarding the configuration, I think environment variables are the best way to go for now, to be consistent with out other config stuff. Just call them things like SPARK_MASTER_PORT. * Would it be possible to reuse the spark-ec2 script from before, but just with a different AMI? Since a lot of the setup is handled by the script on the AMI, I believe it should be possible to reuse the client-side code. If necessary, you can add options to it. That's better than having two nearly identical files.\n2. Patrick McFadin: Github comment from dennybritz: Yes, I could reuse the existing EC2 script. I'll just have to add another switch to it and make the security groups and ssh commands conditional.\n3. Patrick McFadin: Github comment from dennybritz: I merged the two scripts into one. Still working on the configuration, but it's working fine with default settings right now.\n4. Patrick McFadin: Github comment from dennybritz: Added configuration variables.\n5. Patrick McFadin: Github comment from mateiz: Thanks. I looked at it some more and I have just two other suggestions: * If nontrivial parts of the code, such as the \"rotate logs\" bit, are copied from Hadoop, add Hadoop's Apache license notice to the the top of that file. If it's not copied but just based on reading the code, then there's probably no need. * Why didn't you make the master be configured in a separate \"masters\" file as well? I thought about this a bit and I think it's good to do it the same way as Hadoop because people will be familiar with that. It's also useful in case someone tries to stop or start the cluster from a machine other than the master (because the scripts will be everywhere).\n6. Patrick McFadin: Github comment from dennybritz: * Added the Apache license to the two scripts that are a bit more complex and obviously taken from Hadoop. * The masters file in Hadoop is actually not what most people expect it to be. In Hadoop the NameNode and DataNode are only started on the machine where the script is executed. The masters file only specifies the locations where a SecondaryNameNode should be started. So, the naming of the *masters* file in Hadoop is a bit off. Since Spark doesn't have the notion of a SecondaryNameNode (which isn't a backup for the NameNode anyway) I thought a masters file was unnecessary.\n7. Patrick McFadin: Github comment from mateiz: Oh, I see. That makes sense then. I'm going to merge this, and I guess the last thing that will be left is to document it. For the documentation, I was thinking of switching away from the wiki to a set of HTML docs generated from markup files in our source tree, using a system such as Jekyll (https://github.com/mojombo/jekyll). I'll send an email later about that, but it can wait a bit.\n8. Patrick McFadin: Imported from Github issue spark-160, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "b3376e5d4f8568b2cfc95772059d9431", "issue_key": "SPARK-392", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Use sbt mergeStrategy for reference.conf files.", "description": "Cleans up the code as discussed in #158.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-08-02T09:33:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-08-02T09:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-161, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-392\nSummary: Use sbt mergeStrategy for reference.conf files.\nDescription: Cleans up the code as discussed in #158.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n2. Patrick McFadin: Imported from Github issue spark-161, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "1d6437d16fd209cda051fd1c54f1e8ef", "issue_key": "SPARK-391", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Use maxMemory to better estimate memory available for BlockManager cache", "description": "", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-08-02T11:09:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from shivaram: Github seems to add any extra commits to my branch to the same pull request. Let me know if I should close this one and separate them out into two different pull requests.", "created": "2012-08-02T11:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Yes, you can only have one pull request from each of your branches at a time. Further updates to the branch count as updates to that request. But anyway, it looks good.", "created": "2012-08-02T11:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-162, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-391\nSummary: Use maxMemory to better estimate memory available for BlockManager cache\n\nComments (3):\n1. Patrick McFadin: Github comment from shivaram: Github seems to add any extra commits to my branch to the same pull request. Let me know if I should close this one and separate them out into two different pull requests.\n2. Patrick McFadin: Github comment from mateiz: Yes, you can only have one pull request from each of your branches at a time. Further updates to the branch count as updates to that request. But anyway, it looks good.\n3. Patrick McFadin: Imported from Github issue spark-162, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "6df22d8c30e9b6eb3caf03efa0a2187a", "issue_key": "SPARK-541", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Passing bad master address to SparkContext results in unhelpful Mesos error message", "description": "Passing an invalid master address to `SparkContext` results in an unhelpful error message from Mesos. On the current (0.5.0) Spark EC2 release: ``` [root@ip-10-29-192-248 spark]# ./run spark.examples.SparkPi 127.0.0.1 12/08/02 22:41:04 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1370106101 12/08/02 22:41:05 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/08/02 22:41:05 INFO spark.CacheTrackerActor: Started slave cache (size 1306.6MB) on ip-10-29-192-248.ec2.internal 12/08/02 22:41:05 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/08/02 22:41:05 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-d7e47033-5684-41db-a5b8-83190eb3d983/shuffle 12/08/02 22:41:05 INFO server.Server: jetty-7.5.3.v20111011 12/08/02 22:41:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33844 STARTING 12/08/02 22:41:05 INFO spark.ShuffleManager: Local URI: http://10.29.192.248:33844 12/08/02 22:41:05 INFO server.Server: jetty-7.5.3.v20111011 12/08/02 22:41:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:48492 STARTING 12/08/02 22:41:05 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.29.192.248:48492 I0802 22:41:06.010596 2834 logging.cpp:86] Logging to STDERR java: /root/mesos/build/../src/common/try.hpp:77: T Try<T>::get() const [with T = mesos::internal::MasterDetector*]: Assertion `state == SOME' failed. /root/scala-2.9.1.final/bin/scala: line 161: 2801 Aborted \"${JAVACMD:=java}\" $JAVA_OPTS \"${java_args[@]}\" ${CPSELECT}${TOOL_CLASSPATH} -Dscala.usejavacp=true -Dscala.home=\"$SCALA_HOME\" -Denv.emacs=\"$EMACS\" $CYGWIN_JLINE_TERMINAL scala.tools.nsc.MainGenericRunner \"$@\" ``` I see the same error with 71a958b on the dev branch.", "reporter": "Josh Rosen", "assignee": "Patrick McFadin", "created": "0012-08-02T15:05:00.000+0000", "updated": "2013-01-24T16:00:07.000+0000", "resolved": "2013-01-24T16:00:07.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Yeah, it's annoying, but it's hard to fix without modifying Mesos. I guess we could use a regex to check the format of the address in Spark.", "created": "2012-08-02T16:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-163, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-541\nSummary: Passing bad master address to SparkContext results in unhelpful Mesos error message\nDescription: Passing an invalid master address to `SparkContext` results in an unhelpful error message from Mesos. On the current (0.5.0) Spark EC2 release: ``` [root@ip-10-29-192-248 spark]# ./run spark.examples.SparkPi 127.0.0.1 12/08/02 22:41:04 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1370106101 12/08/02 22:41:05 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/08/02 22:41:05 INFO spark.CacheTrackerActor: Started slave cache (size 1306.6MB) on ip-10-29-192-248.ec2.internal 12/08/02 22:41:05 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/08/02 22:41:05 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-d7e47033-5684-41db-a5b8-83190eb3d983/shuffle 12/08/02 22:41:05 INFO server.Server: jetty-7.5.3.v20111011 12/08/02 22:41:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:33844 STARTING 12/08/02 22:41:05 INFO spark.ShuffleManager: Local URI: http://10.29.192.248:33844 12/08/02 22:41:05 INFO server.Server: jetty-7.5.3.v20111011 12/08/02 22:41:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:48492 STARTING 12/08/02 22:41:05 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.29.192.248:48492 I0802 22:41:06.010596 2834 logging.cpp:86] Logging to STDERR java: /root/mesos/build/../src/common/try.hpp:77: T Try<T>::get() const [with T = mesos::internal::MasterDetector*]: Assertion `state == SOME' failed. /root/scala-2.9.1.final/bin/scala: line 161: 2801 Aborted \"${JAVACMD:=java}\" $JAVA_OPTS \"${java_args[@]}\" ${CPSELECT}${TOOL_CLASSPATH} -Dscala.usejavacp=true -Dscala.home=\"$SCALA_HOME\" -Denv.emacs=\"$EMACS\" $CYGWIN_JLINE_TERMINAL scala.tools.nsc.MainGenericRunner \"$@\" ``` I see the same error with 71a958b on the dev branch.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Yeah, it's annoying, but it's hard to fix without modifying Mesos. I guess we could use a regex to check the format of the address in Spark.\n2. Patrick McFadin: Imported from Github issue spark-163, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "61461c49273cb56e38865493f718ae68", "issue_key": "SPARK-390", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Bug fix in RangePartitioner for partitioning when sorting in descending order.", "description": "", "reporter": "Harvey Feng", "assignee": null, "created": "0012-08-03T11:26:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good. Thanks!", "created": "2012-08-03T11:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-164, originally reported by harveyfeng", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-390\nSummary: Bug fix in RangePartitioner for partitioning when sorting in descending order.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good. Thanks!\n2. Patrick McFadin: Imported from Github issue spark-164, originally reported by harveyfeng", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "af4f72c4e68a278e4dde518afd24c537", "issue_key": "SPARK-389", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fix test checkpoint to reuse spark context defined in the class", "description": "I sometimes see a crash in the after clause calling sc.stop and this seems to fix it", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-08-03T18:02:00.000+0000", "updated": "2012-10-19T22:50:19.000+0000", "resolved": "2012-10-19T22:50:19.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, but out of curiosity, what kind of crash is it? Is it because sc got reused from the previous test? Maybe we should set sc=null after to prevent this in the future.", "created": "2012-08-03T18:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: The stack trace I saw was: ``` [info] Exception encountered when attempting to run a suite with class name: spark.RDDSuite *** ABORTED *** [info] java.lang.NullPointerException: [info] at spark.SparkContext.stop(SparkContext.scala:303) [info] at spark.RDDSuite$$anonfun$5.apply(RDDSuite.scala:14) [info] at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:180) [info] at spark.RDDSuite.runTest(RDDSuite.scala:8) [info] at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304) [info] at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304) [info] at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:226) [info] at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:215) [info] at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info] at scala.collection.immutable.List.foreach(List.scala:45) ``` The stack trace indicates that sc.stop was called but the dagScheduler was null (line 303 was dagScheduler.stop in my tree). My guess is that somehow the spark context (sc) used by the after method was not null as sc had been created by the test \"checkpointing\". So sc.stop was executed twice and that led to the NullPointerException. I am not sure why there could have been any confusion between the function local sc and the class variable sc though.", "created": "2012-08-03T21:00:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I think the problem is just that it call the after() part twice, calling stop() twice on an old SparkContext that had been left over by a previous test. Each test initializes the class variable sc at the start.", "created": "2012-08-03T21:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: Do you think its a good idea to use the trait OneInstancePerTest to prevent tests from influencing each other ?", "created": "2012-08-03T21:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: It probably wouldn't hurt for most of our tests, but we should do it consistently, which will take a bit of work. It's also easy to forget to add on new tests, so from that point of view it may not be great.", "created": "2012-08-03T21:08:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: You were right - I added some debug information and the SparkContext from the previous test was being destructed twice. Adding sc = null in the after clause does handle those cases as well - so it might be safe to have it.", "created": "2012-08-03T21:39:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-165, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-389\nSummary: Fix test checkpoint to reuse spark context defined in the class\nDescription: I sometimes see a crash in the after clause calling sc.stop and this seems to fix it\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: Looks good, but out of curiosity, what kind of crash is it? Is it because sc got reused from the previous test? Maybe we should set sc=null after to prevent this in the future.\n2. Patrick McFadin: Github comment from shivaram: The stack trace I saw was: ``` [info] Exception encountered when attempting to run a suite with class name: spark.RDDSuite *** ABORTED *** [info] java.lang.NullPointerException: [info] at spark.SparkContext.stop(SparkContext.scala:303) [info] at spark.RDDSuite$$anonfun$5.apply(RDDSuite.scala:14) [info] at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:180) [info] at spark.RDDSuite.runTest(RDDSuite.scala:8) [info] at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304) [info] at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304) [info] at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:226) [info] at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:215) [info] at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) [info] at scala.collection.immutable.List.foreach(List.scala:45) ``` The stack trace indicates that sc.stop was called but the dagScheduler was null (line 303 was dagScheduler.stop in my tree). My guess is that somehow the spark context (sc) used by the after method was not null as sc had been created by the test \"checkpointing\". So sc.stop was executed twice and that led to the NullPointerException. I am not sure why there could have been any confusion between the function local sc and the class variable sc though.\n3. Patrick McFadin: Github comment from mateiz: I think the problem is just that it call the after() part twice, calling stop() twice on an old SparkContext that had been left over by a previous test. Each test initializes the class variable sc at the start.\n4. Patrick McFadin: Github comment from shivaram: Do you think its a good idea to use the trait OneInstancePerTest to prevent tests from influencing each other ?\n5. Patrick McFadin: Github comment from mateiz: It probably wouldn't hurt for most of our tests, but we should do it consistently, which will take a bit of work. It's also easy to forget to add on new tests, so from that point of view it may not be great.\n6. Patrick McFadin: Github comment from shivaram: You were right - I added some debug information and the SparkContext from the previous test was being destructed twice. Adding sc = null in the after clause does handle those cases as well - so it might be safe to have it.\n7. Patrick McFadin: Imported from Github issue spark-165, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "fca6039ef18236b3237300de16a9f174", "issue_key": "SPARK-388", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Avoid a copy in ShuffleMapTask", "description": "Avoid a copy in ShuffleMapTask by creating an iterator that will be used by the block manager.", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-08-07T23:50:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-08-08T08:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: Thanks Josh for pointing this ! I don't think there is a performance concern as it just wraps the java map https://github.com/scala/scala/blob/2.9.x/src/library/scala/collection/JavaConversions.scala#L485 Making a change to just use the JavaConversions call now...", "created": "2012-08-08T13:07:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: New pull request at https://github.com/mesos/spark/pull/168", "created": "2012-08-08T13:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-166, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-388\nSummary: Avoid a copy in ShuffleMapTask\nDescription: Avoid a copy in ShuffleMapTask by creating an iterator that will be used by the block manager.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n2. Patrick McFadin: Github comment from shivaram: Thanks Josh for pointing this ! I don't think there is a performance concern as it just wraps the java map https://github.com/scala/scala/blob/2.9.x/src/library/scala/collection/JavaConversions.scala#L485 Making a change to just use the JavaConversions call now...\n3. Patrick McFadin: Github comment from shivaram: New pull request at https://github.com/mesos/spark/pull/168\n4. Patrick McFadin: Imported from Github issue spark-166, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "3187683ef138d7be88a9da5bb59af9b5", "issue_key": "SPARK-387", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Detect non-zero exit status from PipedRDD process", "description": "Changes PipedRDD to detect a non-zero exit status from its child process, rather than silently failing.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-08-07T23:59:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from JoshRosen: I added the exit status to the exception.", "created": "2012-08-09T23:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks.", "created": "2012-08-09T23:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-167, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-387\nSummary: Detect non-zero exit status from PipedRDD process\nDescription: Changes PipedRDD to detect a non-zero exit status from its child process, rather than silently failing.\n\nComments (3):\n1. Patrick McFadin: Github comment from JoshRosen: I added the exit status to the exception.\n2. Patrick McFadin: Github comment from mateiz: Great, thanks.\n3. Patrick McFadin: Imported from Github issue spark-167, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "3839b11c72d0adab6a20c9298a08aeec", "issue_key": "SPARK-386", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Use JavaConversion to get a scala iterator", "description": "", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-08-08T13:13:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Isn't there an implicit conversion from Java iterators to Scala iterators or Seq? That would be cleaner.", "created": "2012-08-08T13:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks.", "created": "2012-08-09T23:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-168, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-386\nSummary: Use JavaConversion to get a scala iterator\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Isn't there an implicit conversion from Java iterators to Scala iterators or Seq? That would be cleaner.\n2. Patrick McFadin: Github comment from mateiz: Great, thanks.\n3. Patrick McFadin: Imported from Github issue spark-168, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "b080b4f50c29bf0c7fc127643328d08a", "issue_key": "SPARK-385", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Changes to SizeEstimator more accurate", "description": "Motivation: This patch is motivated by an observation that the amount of heap space used by the BoundedMemoryCache is often much larger than what we account for. For example running with 10 files from the RITA dataset[1] and 4GB for the cache, we see the currBytes variable to be 3917.67. However analyzing the memory heap with Eclipse MAT[2] shows that the BoundedMemoryCache in fact occupies 4360.55 MB. Changes made: This patch tries to address the discrepancy by making some changes to the SizeEstimator. The object size and reference size are changed based on the architecture in use and if or not CompressedOops are in use by the JVM. This results in the object size changing from 8 to 12 or 16 and references being either 4 or 8 bytes long. We also account for the fact that arrays have an object header + an int for the length. Lastly, this patch also account for the fact that fields and objects are aligned to 8-byte boundaries by the JVM. Changes are based on information from [3,4,5] Tests: Changes are verified by comparing the results from spark.SizeEstimator.estimate to those from MAT. An example can be found in https://github.com/shivaram/spark/tree/size-estimate-test/res where the first 100 lines from the 1990 dataset was used. The file spark-err-log.txt shows that the size estimate was 25000 bytes which matches the size of the hashmap entry in BoundedMemoryCache entry found in spark-bounded-memory.txt. Also, a simple script that can be used to run such a test with any text file can be found in the `size-estimate-test` tree as `run_size_estimate_test` With this patch and the original dataset from the motivation, we get an estimate of 3878.81MB while MAT reports memory usage of 3879.61MB. The difference is explained below. Caveats: Arrays are still sampled during estimation and this could lead to small variations as seen with the example above. Also, the patch uses HotSpot diagnostic MXBean to figure out if compressed oops are being used by the JVM and this may fail on non-hotspot JVMs. Finally, the patch has been tested only on 64-bit HotSpot JVMs (1.6.0_24 and 1.7.0_147-icedtea). I don't have access to a 32-bit machine to test it, but we can do it on EC2. [1] http://stat-computing.org/dataexpo/2009/the-data.html [2] http://eclipse.org/mat [3] https://wikis.oracle.com/display/HotSpotInternals/CompressedOops [4] http://kohlerm.blogspot.com/2008/12/how-much-memory-is-used-by-my-java.html [5] http://lingpipe-blog.com/2010/06/22/the-unbearable-heaviness-jav-strings/", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-08-11T17:07:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks great! One thing on the tests though: can you add a couple of tests with other settings of 64-bit and compressed oops? It does't need to be a lot, but just to cover those code paths.", "created": "2012-08-12T01:16:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: Added tests for the 32-bit and no-compressed oops cases. I had to move some of the variables to be initialized inside a method to get the tests to work correctly. Also, this change should be applicable to the dev branch as well. Do you usually port patches from master to dev or should I use another pull request for that ?", "created": "2012-08-12T16:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks. If you have a pull request for dev that will save me some trouble, otherwise I'll cherry-pick these commits into it. (Unfortunately right now you can't merge master into dev.)", "created": "2012-08-12T21:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: I will open another pull request as I am in the process of applying these changes to dev.", "created": "2012-08-12T22:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-169, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-385\nSummary: Changes to SizeEstimator more accurate\nDescription: Motivation: This patch is motivated by an observation that the amount of heap space used by the BoundedMemoryCache is often much larger than what we account for. For example running with 10 files from the RITA dataset[1] and 4GB for the cache, we see the currBytes variable to be 3917.67. However analyzing the memory heap with Eclipse MAT[2] shows that the BoundedMemoryCache in fact occupies 4360.55 MB. Changes made: This patch tries to address the discrepancy by making some changes to the SizeEstimator. The object size and reference size are changed based on the architecture in use and if or not CompressedOops are in use by the JVM. This results in the object size changing from 8 to 12 or 16 and references being either 4 or 8 bytes long. We also account for the fact that arrays have an object header + an int for the length. Lastly, this patch also account for the fact that fields and objects are aligned to 8-byte boundaries by the JVM. Changes are based on information from [3,4,5] Tests: Changes are verified by comparing the results from spark.SizeEstimator.estimate to those from MAT. An example can be found in https://github.com/shivaram/spark/tree/size-estimate-test/res where the first 100 lines from the 1990 dataset was used. The file spark-err-log.txt shows that the size estimate was 25000 bytes which matches the size of the hashmap entry in BoundedMemoryCache entry found in spark-bounded-memory.txt. Also, a simple script that can be used to run such a test with any text file can be found in the `size-estimate-test` tree as `run_size_estimate_test` With this patch and the original dataset from the motivation, we get an estimate of 3878.81MB while MAT reports memory usage of 3879.61MB. The difference is explained below. Caveats: Arrays are still sampled during estimation and this could lead to small variations as seen with the example above. Also, the patch uses HotSpot diagnostic MXBean to figure out if compressed oops are being used by the JVM and this may fail on non-hotspot JVMs. Finally, the patch has been tested only on 64-bit HotSpot JVMs (1.6.0_24 and 1.7.0_147-icedtea). I don't have access to a 32-bit machine to test it, but we can do it on EC2. [1] http://stat-computing.org/dataexpo/2009/the-data.html [2] http://eclipse.org/mat [3] https://wikis.oracle.com/display/HotSpotInternals/CompressedOops [4] http://kohlerm.blogspot.com/2008/12/how-much-memory-is-used-by-my-java.html [5] http://lingpipe-blog.com/2010/06/22/the-unbearable-heaviness-jav-strings/\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Looks great! One thing on the tests though: can you add a couple of tests with other settings of 64-bit and compressed oops? It does't need to be a lot, but just to cover those code paths.\n2. Patrick McFadin: Github comment from shivaram: Added tests for the 32-bit and no-compressed oops cases. I had to move some of the variables to be initialized inside a method to get the tests to work correctly. Also, this change should be applicable to the dev branch as well. Do you usually port patches from master to dev or should I use another pull request for that ?\n3. Patrick McFadin: Github comment from mateiz: Thanks. If you have a pull request for dev that will save me some trouble, otherwise I'll cherry-pick these commits into it. (Unfortunately right now you can't merge master into dev.)\n4. Patrick McFadin: Github comment from shivaram: I will open another pull request as I am in the process of applying these changes to dev.\n5. Patrick McFadin: Imported from Github issue spark-169, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.971945"}}
{"id": "3e1a0c7feaa49782ea1ba5075f3ed5a9", "issue_key": "SPARK-384", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Launching Spark over YARN", "description": "**Usage Example:** SPARK_JAR=./core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar ./run spark.deploy.yarn.Client **--jar** examples/target/scala-2.9.1/spark-examples_2.9.1-0.6.0-SNAPSHOT.jar **--class** spark.examples.SparkPi **--args** standalone **--num-workers** 3 **--worker-memory** 512M **Notes:** - Need to run sbt-assembly to generate the core-assembly jar used above. - YARN does not support requesting containers/resources by the number of cores. Using `Runtime.getRuntime.availableProcessors()` for each slave for now. - The user must use a the String \"standalone\" as the master's URL. That starts the scheduler without trying to connect to a cluster. - There are several TODOs. In particular 1. Checking for and handling container/app failure, 2. Letting the user set the context priority, and some code refactoring. - Tested locally, but not on a real cluster.", "reporter": "Denny Britz", "assignee": null, "created": "0012-08-13T12:28:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Denny, Let me know when you've made the changes above and I'll merge this. It would be nice to have this in the main repo.", "created": "2012-08-27T09:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I made the changes. One thing that I couldn't quite figure out is why the application is marked as \"FAILED\" by YARN even though the ApplicationMaster exits with exit code 0. I am even setting the status to FinalApplicationStatus.SUCCEEDED explicitly. The error message isn't very enlightening: `Application application_1346087295431_0014 failed 1 times due to AM Container for appattempt_1346087295431_0014_000001 exited with exitCode: 0 due to: .Failing this attempt.. Failing the application.` Any ideas?", "created": "2012-08-27T10:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks. Let's look into that later.", "created": "2012-08-27T11:19:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-170, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-384\nSummary: Launching Spark over YARN\nDescription: **Usage Example:** SPARK_JAR=./core/target/spark-core-assembly-0.6.0-SNAPSHOT.jar ./run spark.deploy.yarn.Client **--jar** examples/target/scala-2.9.1/spark-examples_2.9.1-0.6.0-SNAPSHOT.jar **--class** spark.examples.SparkPi **--args** standalone **--num-workers** 3 **--worker-memory** 512M **Notes:** - Need to run sbt-assembly to generate the core-assembly jar used above. - YARN does not support requesting containers/resources by the number of cores. Using `Runtime.getRuntime.availableProcessors()` for each slave for now. - The user must use a the String \"standalone\" as the master's URL. That starts the scheduler without trying to connect to a cluster. - There are several TODOs. In particular 1. Checking for and handling container/app failure, 2. Letting the user set the context priority, and some code refactoring. - Tested locally, but not on a real cluster.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Hey Denny, Let me know when you've made the changes above and I'll merge this. It would be nice to have this in the main repo.\n2. Patrick McFadin: Github comment from dennybritz: I made the changes. One thing that I couldn't quite figure out is why the application is marked as \"FAILED\" by YARN even though the ApplicationMaster exits with exit code 0. I am even setting the status to FinalApplicationStatus.SUCCEEDED explicitly. The error message isn't very enlightening: `Application application_1346087295431_0014 failed 1 times due to AM Container for appattempt_1346087295431_0014_000001 exited with exitCode: 0 due to: .Failing this attempt.. Failing the application.` Any ideas?\n3. Patrick McFadin: Github comment from mateiz: Thanks. Let's look into that later.\n4. Patrick McFadin: Imported from Github issue spark-170, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "bc0fba99ce6c79ff8575cd3d34a83044", "issue_key": "SPARK-383", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Size estimator changes for dev", "description": "Based on pull request #169 - Additionally change BlockManagerSuite to make sure tests pass.", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-08-13T12:42:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Shivaram!", "created": "2012-08-13T14:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: BTW I think we should open source just this size estimator - others will find it useful in other projects.", "created": "2012-08-13T21:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Yeah, it's a good idea. Matei On Aug 14, 2012, at 8:43 AM, Reynold Xin wrote: > BTW I think we should open source just this size estimator - others will find it useful in other projects. > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-08-13T22:01:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-171, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-383\nSummary: Size estimator changes for dev\nDescription: Based on pull request #169 - Additionally change BlockManagerSuite to make sure tests pass.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Thanks Shivaram!\n2. Patrick McFadin: Github comment from rxin: BTW I think we should open source just this size estimator - others will find it useful in other projects.\n3. Patrick McFadin: Github comment from mateiz: Yeah, it's a good idea. Matei On Aug 14, 2012, at 8:43 AM, Reynold Xin wrote: > BTW I think we should open source just this size estimator - others will find it useful in other projects. > > — > Reply to this email directly or view it on GitHub. > >\n4. Patrick McFadin: Imported from Github issue spark-171, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "2671dcc7593c8f00243ec5301efdbbd3", "issue_key": "SPARK-382", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Rsync root directory in EC2 script", "description": "- Will modify the Standalone AMI to allow for root login as well.", "reporter": "Denny Britz", "assignee": null, "created": "0012-08-14T08:30:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from dennybritz: Modified Standalone AMI to use root instead of ec2-user.", "created": "2012-08-14T09:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks.", "created": "2012-08-14T12:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-172, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-382\nSummary: Rsync root directory in EC2 script\nDescription: - Will modify the Standalone AMI to allow for root login as well.\n\nComments (3):\n1. Patrick McFadin: Github comment from dennybritz: Modified Standalone AMI to use root instead of ec2-user.\n2. Patrick McFadin: Github comment from mateiz: Great, thanks.\n3. Patrick McFadin: Imported from Github issue spark-172, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "0c5a157ab6218b16a9b49fed90d7ecd2", "issue_key": "SPARK-381", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "make accumulator.localValue public, add tests", "description": "", "reporter": "squito", "assignee": null, "created": "0012-08-14T13:15:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: This seems to break the abstraction as the user can abuse it and make changes that are not associative. For this usage, I think you can just implement an AccumulableParam for sets class SetAccumulableParam[T] extends AccumulableParam[Set, T] { def addAccumulator(t1: R, t2: T) : R = { t1 += t2 } def addInPlace(t1: R, t2: R): R = { t1 ++= t2 } def zero(initialValue: R): R = new Set }", "created": "2012-08-15T10:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I think Imran had other usages, right? The main one is to read the value? I agree that the test shouldn't be using += on localValue though. That just encourages potentially error-prone use of accumulators.", "created": "2012-08-17T02:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from squito: Yeah, the use of Set is not really a compelling reason for this feature, it was just the easiest to write a test case to verify the behavior. As Matei said, one reason is to be able to read the current value of the accumulator, eg. something like SGD. I agree that the user certainly *could* abuse localValue, but I feel like the name & comment are a fair warning. I don't really understand the argument against calling += directly on localValue, though. In fact, pretty much the only thing that calling += on most (all?) accumulators does is delegate to += on localValue. Another reason to allow access to localValue is that you may get access to a richer api through localValue. Eg., adding to a Set can tell you whether the element already existed or not, and adding to a Map can give you the previously stored value for the key. That functionality isn't available via the accumulator interface. The final reason is that if you want to call some method on localValue which takes multiple arguments, its just kind of pain (and requires needless object creation) to have to (a) define a helper \"update\" type and (b) create instances of the update type. Eg., imagine you had: class PartsOfSpeechCounts { def addCounts(nouns:Int, verbs: Int) {...} ... } to use this in an accumulator, you now have to create another object class PartsOfSpeechUpdate(val nouns: Int, val verbs: Int) and then change all your updates from acc.localValue.addCounts(nouns,verbs) to acc += new PartsOfSpeechUpdate(nouns, verbs) It isn't the end of the world, but seems like a headache with no benefit.", "created": "2012-08-17T12:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-08-21T23:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-173, originally reported by squito", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-381\nSummary: make accumulator.localValue public, add tests\n\nComments (5):\n1. Patrick McFadin: Github comment from rxin: This seems to break the abstraction as the user can abuse it and make changes that are not associative. For this usage, I think you can just implement an AccumulableParam for sets class SetAccumulableParam[T] extends AccumulableParam[Set, T] { def addAccumulator(t1: R, t2: T) : R = { t1 += t2 } def addInPlace(t1: R, t2: R): R = { t1 ++= t2 } def zero(initialValue: R): R = new Set }\n2. Patrick McFadin: Github comment from mateiz: I think Imran had other usages, right? The main one is to read the value? I agree that the test shouldn't be using += on localValue though. That just encourages potentially error-prone use of accumulators.\n3. Patrick McFadin: Github comment from squito: Yeah, the use of Set is not really a compelling reason for this feature, it was just the easiest to write a test case to verify the behavior. As Matei said, one reason is to be able to read the current value of the accumulator, eg. something like SGD. I agree that the user certainly *could* abuse localValue, but I feel like the name & comment are a fair warning. I don't really understand the argument against calling += directly on localValue, though. In fact, pretty much the only thing that calling += on most (all?) accumulators does is delegate to += on localValue. Another reason to allow access to localValue is that you may get access to a richer api through localValue. Eg., adding to a Set can tell you whether the element already existed or not, and adding to a Map can give you the previously stored value for the key. That functionality isn't available via the accumulator interface. The final reason is that if you want to call some method on localValue which takes multiple arguments, its just kind of pain (and requires needless object creation) to have to (a) define a helper \"update\" type and (b) create instances of the update type. Eg., imagine you had: class PartsOfSpeechCounts { def addCounts(nouns:Int, verbs: Int) {...} ... } to use this in an accumulator, you now have to create another object class PartsOfSpeechUpdate(val nouns: Int, val verbs: Int) and then change all your updates from acc.localValue.addCounts(nouns,verbs) to acc += new PartsOfSpeechUpdate(nouns, verbs) It isn't the end of the world, but seems like a headache with no benefit.\n4. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n5. Patrick McFadin: Imported from Github issue spark-173, originally reported by squito", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "b2327a95bf79f296d9e1395142a519b7", "issue_key": "SPARK-380", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make spark-ec2 detect and handle VMs that fail to start correctly", "description": "See https://groups.google.com/forum/?fromgroups#!topic/spark-users/SpAHATspJTA The script can check that the Mesos webui comes up and that the correct number of slaves come up and register with the Mesos master.", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-08-16T16:25:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from shivaram: I have seen the same issue and quoting from the email, the problem is from the following lines: `[/sbin/fsck.ext3 (1) -- /mnt] fsck.ext3 -a /dev/xvdb [/sbin/fsck.ext3 (2) -- /mnt2] fsck.ext3 -a /dev/xvdc fsck.ext3: Device or resource busy while trying to open /dev/xvdc Filesystem mounted or opened exclusively by another program? /dev/xvdb: Adding dirhash hint to filesystem. ` I think the problem is that sometimes /dev/xvdc is not available in the machine and fsck fails. A workaround is to disable fsck checking especially for the ephemeral drives as we don't have any data on them. We will need to regenerate the AMI when we make this change though. Finally the AMI assumes that we have all four ephemeral drives while setting up HDFS. This is not always true (http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/InstanceStorage.html) and we should probably use something like /proc/partitions to find this automatically.", "created": "2012-08-16T22:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: I was wrong about the last one - looking again I think spark_ec2 already sets the HDFS data dirs correctly.", "created": "2012-08-16T22:14:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I think this is a problem with the latest AMI in particular. Maybe it got created at a time when it was very close to needing to fsck. Either way though, if we can disable that, that would be great. Matei On Aug 17, 2012, at 9:14 AM, shivaram wrote: > I was wrong about the last one - looking again I think spark_ec2 already sets the HDFS data dirs correctly. > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-08-16T23:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: I made some changes to avoid fsck as a part of creating the AMP Camp AMI. If things look good I'll make the same changes for the Spark AMI as well.", "created": "2012-08-17T12:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I've updated the default Spark AMI with Shivaram's fix. Thanks Shivaram!", "created": "2012-08-29T22:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-174, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-380\nSummary: Make spark-ec2 detect and handle VMs that fail to start correctly\nDescription: See https://groups.google.com/forum/?fromgroups#!topic/spark-users/SpAHATspJTA The script can check that the Mesos webui comes up and that the correct number of slaves come up and register with the Mesos master.\n\nComments (6):\n1. Patrick McFadin: Github comment from shivaram: I have seen the same issue and quoting from the email, the problem is from the following lines: `[/sbin/fsck.ext3 (1) -- /mnt] fsck.ext3 -a /dev/xvdb [/sbin/fsck.ext3 (2) -- /mnt2] fsck.ext3 -a /dev/xvdc fsck.ext3: Device or resource busy while trying to open /dev/xvdc Filesystem mounted or opened exclusively by another program? /dev/xvdb: Adding dirhash hint to filesystem. ` I think the problem is that sometimes /dev/xvdc is not available in the machine and fsck fails. A workaround is to disable fsck checking especially for the ephemeral drives as we don't have any data on them. We will need to regenerate the AMI when we make this change though. Finally the AMI assumes that we have all four ephemeral drives while setting up HDFS. This is not always true (http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/InstanceStorage.html) and we should probably use something like /proc/partitions to find this automatically.\n2. Patrick McFadin: Github comment from shivaram: I was wrong about the last one - looking again I think spark_ec2 already sets the HDFS data dirs correctly.\n3. Patrick McFadin: Github comment from mateiz: I think this is a problem with the latest AMI in particular. Maybe it got created at a time when it was very close to needing to fsck. Either way though, if we can disable that, that would be great. Matei On Aug 17, 2012, at 9:14 AM, shivaram wrote: > I was wrong about the last one - looking again I think spark_ec2 already sets the HDFS data dirs correctly. > > — > Reply to this email directly or view it on GitHub. > >\n4. Patrick McFadin: Github comment from shivaram: I made some changes to avoid fsck as a part of creating the AMP Camp AMI. If things look good I'll make the same changes for the Spark AMI as well.\n5. Patrick McFadin: Github comment from mateiz: I've updated the default Spark AMI with Shivaram's fix. Thanks Shivaram!\n6. Patrick McFadin: Imported from Github issue spark-174, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "4151618d69f437cdb41263ac811579c7", "issue_key": "SPARK-379", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "add accumulators for mutable collections, with correct typing!", "description": "add accumulator params for all \"mutable collection\" types (anything that is growable and traversable). Rather than create a bunch of implicit objects, there is a helper method which just creates the AccumulableParam needed. This way, you also get correct typing of the accumulator. This does require a different method, though -- I called it accumulableCollection, not really tied to that name. (this is completely independent from the other pull request on accumulable.localValue)", "reporter": "squito", "assignee": null, "created": "0012-08-17T15:02:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from squito: Hi Matei, any update on this? I have some real use cases for this, so it would be nice to get it merged in. thanks!", "created": "2012-09-07T13:01:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Sorry, I haven't had a ton of time to look at it, but I also haven't found a better way to do it, so we should probably just go with your approach (accumulableCollection). Can you update the pull request to be based on the latest dev branch though? Just git pull in your own repo. Right now it's not automatically mergeable.", "created": "2012-09-07T13:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Oh actually I see your pull request is against master. Update it for the latest master code then, and I'll copy the commits into the dev branch too.", "created": "2012-09-07T13:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for the update -- I've committed it. Just to let you know though, there was a bug that actually caused the test to fail sometimes: GrowableAccumulatorParam.zero() returned the initial collection, so if we had added some values into it before sending out the last few tasks, we ended up with multiple copies of those values. I've fixed it in commit 2498f95199bc642119ab52981d1c6508f71ff2ff.", "created": "2012-09-11T14:19:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-175, originally reported by squito", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-379\nSummary: add accumulators for mutable collections, with correct typing!\nDescription: add accumulator params for all \"mutable collection\" types (anything that is growable and traversable). Rather than create a bunch of implicit objects, there is a helper method which just creates the AccumulableParam needed. This way, you also get correct typing of the accumulator. This does require a different method, though -- I called it accumulableCollection, not really tied to that name. (this is completely independent from the other pull request on accumulable.localValue)\n\nComments (5):\n1. Patrick McFadin: Github comment from squito: Hi Matei, any update on this? I have some real use cases for this, so it would be nice to get it merged in. thanks!\n2. Patrick McFadin: Github comment from mateiz: Sorry, I haven't had a ton of time to look at it, but I also haven't found a better way to do it, so we should probably just go with your approach (accumulableCollection). Can you update the pull request to be based on the latest dev branch though? Just git pull in your own repo. Right now it's not automatically mergeable.\n3. Patrick McFadin: Github comment from mateiz: Oh actually I see your pull request is against master. Update it for the latest master code then, and I'll copy the commits into the dev branch too.\n4. Patrick McFadin: Github comment from mateiz: Thanks for the update -- I've committed it. Just to let you know though, there was a bug that actually caused the test to fail sometimes: GrowableAccumulatorParam.zero() returned the initial collection, so if we had added some values into it before sending out the last few tasks, we ended up with multiple copies of those values. I've fixed it in commit 2498f95199bc642119ab52981d1c6508f71ff2ff.\n5. Patrick McFadin: Imported from Github issue spark-175, originally reported by squito", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "47a38d86479c1762bec566feaf184350", "issue_key": "SPARK-540", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add API to customize in-memory representation of RDDs", "description": "Right now the choice between serialized caching and just Java objects in dev is fine, but it might be cool to also support structures such as column-oriented storage through arrays of primitives without forcing it through the serialization interface.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-08-18T15:32:00.000+0000", "updated": "2016-01-18T10:21:10.000+0000", "resolved": "2016-01-18T10:21:10.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-176, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Nicholas Chammas", "body": "[~matei], [~rxin]: Is this issue still valid? (I'm going through old issues that don't have an assigned component.)", "created": "2015-02-06T19:38:16.712+0000"}, {"author": "Reynold Xin", "body": "Yes it is.", "created": "2015-02-06T21:52:09.923+0000"}, {"author": "Sean R. Owen", "body": "I assume this is subsumed by things like dataframes and the dataset API.", "created": "2016-01-18T10:21:10.253+0000"}], "num_comments": 4, "text": "Issue: SPARK-540\nSummary: Add API to customize in-memory representation of RDDs\nDescription: Right now the choice between serialized caching and just Java objects in dev is fine, but it might be cool to also support structures such as column-oriented storage through arrays of primitives without forcing it through the serialization interface.\n\nComments (4):\n1. Patrick McFadin: Imported from Github issue spark-176, originally reported by mateiz\n2. Nicholas Chammas: [~matei], [~rxin]: Is this issue still valid? (I'm going through old issues that don't have an assigned component.)\n3. Reynold Xin: Yes it is.\n4. Sean R. Owen: I assume this is subsumed by things like dataframes and the dataset API.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "1d0641c9355bfdffb7a6fd1baff72666", "issue_key": "SPARK-378", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Unresolved dependencies when running sbt to install spark", "description": "When following the instructions on https://github.com/mesos/spark/wiki, I get this error: ``` [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: UNRESOLVED DEPENDENCIES :: [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: com.typesafe.sbteclipse#sbteclipse-plugin;2.0.0-M2: not found [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] [warn] Note: Some unresolved dependencies have extra attributes. Check that these dependencies exist with the requested attributes. [warn] com.typesafe.sbteclipse:sbteclipse-plugin:2.0.0-M2 (sbtVersion=0.11.1, scalaVersion=2.9.1) [warn] [error] {file:/Users/tomdz/.sbt/plugins/}default-3ae66e/*:update: sbt.ResolveException: unresolved dependency: com.typesafe.sbteclipse#sbteclipse-plugin;2.0.0-M2: not found Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? ``` Ignore doesn't work, the session will not be initialized and hence compile won't be available. The spark version is commit 680df96c433bc72713377e0f3ebb0a7cca7c11d8. The scala version is `Scala code runner version 2.9.1.final -- Copyright 2002-2011, LAMP/EPFL`", "reporter": "Thomas Dudziak", "assignee": null, "created": "0012-08-21T09:03:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tomdz: Nvm, for some reason there was an entry for the eclipse plugin in `~/.sbt/plugins/plugins.sbt`.", "created": "2012-08-21T09:07:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-177, originally reported by tomdz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-378\nSummary: Unresolved dependencies when running sbt to install spark\nDescription: When following the instructions on https://github.com/mesos/spark/wiki, I get this error: ``` [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: UNRESOLVED DEPENDENCIES :: [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: com.typesafe.sbteclipse#sbteclipse-plugin;2.0.0-M2: not found [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] [warn] Note: Some unresolved dependencies have extra attributes. Check that these dependencies exist with the requested attributes. [warn] com.typesafe.sbteclipse:sbteclipse-plugin:2.0.0-M2 (sbtVersion=0.11.1, scalaVersion=2.9.1) [warn] [error] {file:/Users/tomdz/.sbt/plugins/}default-3ae66e/*:update: sbt.ResolveException: unresolved dependency: com.typesafe.sbteclipse#sbteclipse-plugin;2.0.0-M2: not found Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? ``` Ignore doesn't work, the session will not be initialized and hence compile won't be available. The spark version is commit 680df96c433bc72713377e0f3ebb0a7cca7c11d8. The scala version is `Scala code runner version 2.9.1.final -- Copyright 2002-2011, LAMP/EPFL`\n\nComments (2):\n1. Patrick McFadin: Github comment from tomdz: Nvm, for some reason there was an entry for the eclipse plugin in `~/.sbt/plugins/plugins.sbt`.\n2. Patrick McFadin: Imported from Github issue spark-177, originally reported by tomdz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "1001b5c5ac015171bcbab6bd302214e5", "issue_key": "SPARK-377", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Replay debugger for Spark", "description": "This pull request provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets spark.debugger.enable to \"true\". The user can then load the log and replay the program using spark.debugger.EventLogReader.", "reporter": "Ankur Dave", "assignee": null, "created": "0012-08-23T17:55:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-178, originally reported by ankurdave", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-377\nSummary: Replay debugger for Spark\nDescription: This pull request provides replay debugging for deterministic errors in Spark programs. Execution recording is turned off by default unless the user sets spark.debugger.enable to \"true\". The user can then load the log and replay the program using spark.debugger.EventLogReader.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-178, originally reported by ankurdave", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "1a26f17ff1d0c666bd0ba8a20c4bdad4", "issue_key": "SPARK-376", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Cache points in SparkLR example", "description": "For consistency with the SparkHDFSLR example, the SparkLR example should cache the data points used to train its model.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-08-26T14:28:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks.", "created": "2012-08-26T14:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-179, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-376\nSummary: Cache points in SparkLR example\nDescription: For consistency with the SparkHDFSLR example, the SparkLR example should cache the data points used to train its model.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks.\n2. Patrick McFadin: Imported from Github issue spark-179, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "e26d521bee3ff638de7188e72bf92d1b", "issue_key": "SPARK-539", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Permission denied(publickey)", "description": "419 command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" + 420 \"'%s/' 'root@%s:/'\") % (opts.identity_file, tmp_dir, active_master)) The script always produces an error when executing this command. and the error produced is: Permission denied (publickey)...(This error does not have to do with the permissions on the .pem file) Any ideas?", "reporter": "thikonom", "assignee": "Matei Alexandru Zaharia", "created": "0012-08-27T06:57:00.000+0000", "updated": "2012-12-11T10:06:30.000+0000", "resolved": "2012-12-10T15:18:53.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Weird, what OS is this on? And does the path to the key contain spaces by any chance? Matei On Aug 27, 2012, at 7:57 AM, Theodoros Ikonomou wrote: > 419 command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" + > 420 \"'%s/' 'root@%s:/'\") % (opts.identity_file, tmp_dir, active_master)) > > The script always produces an error when executing this command. > and the error produced is: > Permission denied (publickey)...(This error does not have to do with the permissions on the .pem file) > > Any ideas? > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-08-27T07:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from thikonom: I'm on a Mac with Snow Leopard and the path does not contain spaces.", "created": "2012-08-27T07:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: The only thing I can think of is that you might not have specified the -k parameter when launching the VM to tell Amazon to use the private key for that key pair. Can you SSH into the machine by hand using your .pem file? Matei On Aug 27, 2012, at 8:35 AM, Theodoros Ikonomou wrote: > I'm on a Mac with Snow Leopard and the path does not contain spaces. > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-08-27T07:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from thikonom: I'm always using this command which includes the -k option: ./spark-ec2 -i /Users/thikonom/Desktop/Big/key.pem -k key_name launch cluster_name I get the same error every time i'm trying to ssh from the command line: ssh -i /Users/thikonom/Desktop/Big/key.pem root@...compute-1.amazonaws.com Permission denied (publickey).", "created": "2012-08-27T08:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-180, originally reported by thikonom", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Alexander Polev", "body": "What fixed it for me is adding a \"chown 600\" line in spark_ec2.py: if deploy_ssh_key: print \"Copying SSH key %s to master...\" % opts.identity_file ssh(master, opts, 'mkdir -p ~/.ssh') scp(master, opts, opts.identity_file, '~/.ssh/id_rsa') ssh(master, opts, 'chown 600 ~/.ssh/id_rsa') print \"Running setup on master...\"", "created": "2012-12-04T05:23:21.415+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Thanks Alex -- I guess it's because the local file that we copied over did not have 600 permissions. Was this on Windows? On Linux/Mac I don't think you could even log into the node with a private key file that doesn't have 600 permissions set.", "created": "2012-12-10T14:59:59.274+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I've fixed this in https://github.com/mesos/spark/commit/597520ae201513a51901c8e50f3815aff737d3d5, with Alexander's fix.", "created": "2012-12-10T15:18:53.365+0000"}, {"author": "Alexander Polev", "body": "Actually this was launched from another Linux EC2 host, but using different key file which had wrong permissions. Since for new cluster it allows to point to any private key as \"opts.identity_file\" (not the one that was already used for SSH before) we cant assume it will have correct permissions.", "created": "2012-12-11T00:34:24.275+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Ah, makes sense. Thanks for the fix.", "created": "2012-12-11T10:06:30.329+0000"}], "num_comments": 10, "text": "Issue: SPARK-539\nSummary: Permission denied(publickey)\nDescription: 419 command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" + 420 \"'%s/' 'root@%s:/'\") % (opts.identity_file, tmp_dir, active_master)) The script always produces an error when executing this command. and the error produced is: Permission denied (publickey)...(This error does not have to do with the permissions on the .pem file) Any ideas?\n\nComments (10):\n1. Patrick McFadin: Github comment from mateiz: Weird, what OS is this on? And does the path to the key contain spaces by any chance? Matei On Aug 27, 2012, at 7:57 AM, Theodoros Ikonomou wrote: > 419 command = ((\"rsync -rv -e 'ssh -o StrictHostKeyChecking=no -i %s' \" + > 420 \"'%s/' 'root@%s:/'\") % (opts.identity_file, tmp_dir, active_master)) > > The script always produces an error when executing this command. > and the error produced is: > Permission denied (publickey)...(This error does not have to do with the permissions on the .pem file) > > Any ideas? > > — > Reply to this email directly or view it on GitHub. > >\n2. Patrick McFadin: Github comment from thikonom: I'm on a Mac with Snow Leopard and the path does not contain spaces.\n3. Patrick McFadin: Github comment from mateiz: The only thing I can think of is that you might not have specified the -k parameter when launching the VM to tell Amazon to use the private key for that key pair. Can you SSH into the machine by hand using your .pem file? Matei On Aug 27, 2012, at 8:35 AM, Theodoros Ikonomou wrote: > I'm on a Mac with Snow Leopard and the path does not contain spaces. > > — > Reply to this email directly or view it on GitHub. > >\n4. Patrick McFadin: Github comment from thikonom: I'm always using this command which includes the -k option: ./spark-ec2 -i /Users/thikonom/Desktop/Big/key.pem -k key_name launch cluster_name I get the same error every time i'm trying to ssh from the command line: ssh -i /Users/thikonom/Desktop/Big/key.pem root@...compute-1.amazonaws.com Permission denied (publickey).\n5. Patrick McFadin: Imported from Github issue spark-180, originally reported by thikonom\n6. Alexander Polev: What fixed it for me is adding a \"chown 600\" line in spark_ec2.py: if deploy_ssh_key: print \"Copying SSH key %s to master...\" % opts.identity_file ssh(master, opts, 'mkdir -p ~/.ssh') scp(master, opts, opts.identity_file, '~/.ssh/id_rsa') ssh(master, opts, 'chown 600 ~/.ssh/id_rsa') print \"Running setup on master...\"\n7. Matei Alexandru Zaharia: Thanks Alex -- I guess it's because the local file that we copied over did not have 600 permissions. Was this on Windows? On Linux/Mac I don't think you could even log into the node with a private key file that doesn't have 600 permissions set.\n8. Matei Alexandru Zaharia: I've fixed this in https://github.com/mesos/spark/commit/597520ae201513a51901c8e50f3815aff737d3d5, with Alexander's fix.\n9. Alexander Polev: Actually this was launched from another Linux EC2 host, but using different key file which had wrong permissions. Since for new cluster it allows to point to any private key as \"opts.identity_file\" (not the one that was already used for SSH before) we cant assume it will have correct permissions.\n10. Matei Alexandru Zaharia: Ah, makes sense. Thanks for the fix.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "2fdd37f09b0ece8831540dcbf76c16cf", "issue_key": "SPARK-375", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Removed the deserialization cache for ShuffleMapTask", "description": "Removed the deserialization cache for ShuffleMapTask becuase it was causing concurrency problems (some variables in Shark get set to null). The cost of task deserialization on slaves is trivial compared with the execution time of the task anyway.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-08-27T21:35:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks, looks good.", "created": "2012-08-27T21:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-181, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-375\nSummary: Removed the deserialization cache for ShuffleMapTask\nDescription: Removed the deserialization cache for ShuffleMapTask becuase it was causing concurrency problems (some variables in Shark get set to null). The cost of task deserialization on slaves is trivial compared with the execution time of the task anyway.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks, looks good.\n2. Patrick McFadin: Imported from Github issue spark-181, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "f045dc99a1b44cc400d54d95cfaf1710", "issue_key": "SPARK-374", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add a limit on the number of parallel fetches in the reduce stage", "description": "Made the numer of parallel fetches per reduce task System.getProperty(\"spark.default.parallelism\", \"8\") by default, but it can be set by setting the 'spark.blockManager.parallelFetches' property.", "reporter": "Harvey Feng", "assignee": null, "created": "0012-08-29T15:49:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Harvey, Just to understand, is this going to request the first 8 blocks, then wait until they've *all* been fetched, then request the next 8, etc? It would be nicer to start fetching the next block as soon as the previous one was obtained, so that you don't have times of idleness when waiting for the last one. The way you'd do this is by somehow creating one big queue of the block requests, and sending the next request whenever a Future completes and you've got the current one.", "created": "2012-08-29T22:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from harveyfeng: Hi Matei, Yeah, the parallel fetching is doing 8 requests at a time right now - I'll change it. Also, to confirm what you mentioned earlier, the serializing bug is because the callbacks are being executed on the same thread, so two deserializeStreams are using the same ThreadLocal objectBuffer...I'll include a fix to this too. Thanks! On Wed, Aug 29, 2012 at 11:33 PM, Matei Zaharia <notifications@github.com>wrote: > Hey Harvey, > > Just to understand, is this going to request the first 8 blocks, then wait > until they've *all* been fetched, then request the next 8, etc? It would > be nicer to start fetching the next block as soon as the previous one was > obtained, so that you don't have times of idleness when waiting for the > last one. The way you'd do this is by somehow creating one big queue of the > block requests, and sending the next request whenever a Future completes > and you've got the current one. > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/182#issuecomment-8150506>. > >", "created": "2012-08-29T23:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Harvey, Just FYI, I pushed a fix to the deserializer issue, by doing all the deserialization in the caller's thread for getMultiple. It shouldn't affect your work in a major way but make sure you merge that in.", "created": "2012-08-30T19:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Harvey! This looks good.", "created": "2012-09-03T15:14:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-182, originally reported by harveyfeng", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-374\nSummary: Add a limit on the number of parallel fetches in the reduce stage\nDescription: Made the numer of parallel fetches per reduce task System.getProperty(\"spark.default.parallelism\", \"8\") by default, but it can be set by setting the 'spark.blockManager.parallelFetches' property.\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Hey Harvey, Just to understand, is this going to request the first 8 blocks, then wait until they've *all* been fetched, then request the next 8, etc? It would be nicer to start fetching the next block as soon as the previous one was obtained, so that you don't have times of idleness when waiting for the last one. The way you'd do this is by somehow creating one big queue of the block requests, and sending the next request whenever a Future completes and you've got the current one.\n2. Patrick McFadin: Github comment from harveyfeng: Hi Matei, Yeah, the parallel fetching is doing 8 requests at a time right now - I'll change it. Also, to confirm what you mentioned earlier, the serializing bug is because the callbacks are being executed on the same thread, so two deserializeStreams are using the same ThreadLocal objectBuffer...I'll include a fix to this too. Thanks! On Wed, Aug 29, 2012 at 11:33 PM, Matei Zaharia <notifications@github.com>wrote: > Hey Harvey, > > Just to understand, is this going to request the first 8 blocks, then wait > until they've *all* been fetched, then request the next 8, etc? It would > be nicer to start fetching the next block as soon as the previous one was > obtained, so that you don't have times of idleness when waiting for the > last one. The way you'd do this is by somehow creating one big queue of the > block requests, and sending the next request whenever a Future completes > and you've got the current one. > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/182#issuecomment-8150506>. > >\n3. Patrick McFadin: Github comment from mateiz: Hey Harvey, Just FYI, I pushed a fix to the deserializer issue, by doing all the deserialization in the caller's thread for getMultiple. It shouldn't affect your work in a major way but make sure you merge that in.\n4. Patrick McFadin: Github comment from mateiz: Thanks Harvey! This looks good.\n5. Patrick McFadin: Imported from Github issue spark-182, originally reported by harveyfeng", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "97d3d37cdf045e55730272b2140f460a", "issue_key": "SPARK-373", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Disable running combiners on map tasks when mergeCombiners function is not specified by the user.", "description": "I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback. Basically if the mergeCombiners field is not set (null), combiners are not applied on map side. It doesn't break any existing API.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-08-29T22:06:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-183, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-373\nSummary: Disable running combiners on map tasks when mergeCombiners function is not specified by the user.\nDescription: I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback. Basically if the mergeCombiners field is not set (null), combiners are not applied on map side. It doesn't break any existing API.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-183, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "61c263d26a9cfa52990606720f271432", "issue_key": "SPARK-372", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Disable running combiners on map tasks when mergeCombiners function is not specified by the user.", "description": "I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback. Basically if the mergeCombiners field is not set (null), combiners are not applied on map side. It doesn't break any existing API.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-08-29T22:33:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: Ok I added a test. Ready for review/commit.", "created": "2012-08-30T11:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks.", "created": "2012-08-30T13:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-184, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-372\nSummary: Disable running combiners on map tasks when mergeCombiners function is not specified by the user.\nDescription: I haven't done much testing yet beyond running sbt/sbt test. Want to get some early feedback. Basically if the mergeCombiners field is not set (null), combiners are not applied on map side. It doesn't break any existing API.\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: Ok I added a test. Ready for review/commit.\n2. Patrick McFadin: Github comment from mateiz: Looks good, thanks.\n3. Patrick McFadin: Imported from Github issue spark-184, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.976002"}}
{"id": "1ca9780f6ca486a01a3ba609b9af892c", "issue_key": "SPARK-371", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "run spark.examples.SparkPi 127.0.1.1:5050 spark gets error", "description": "Hi there, I've been trying to make spark work on Mesos and have got an error. Any help would be truly appreciated. When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this: ------------------------- 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8 java.lang.NullPointerException at java.io.File.<init>(File.java:222) at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334) at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333) at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125) at spark.MesosScheduler.<init>(MesosScheduler.scala:80) at spark.SparkContext.<init>(SparkContext.scala:78) at spark.examples.SparkPi$.main(SparkPi.scala:15) at spark.examples.SparkPi.main(SparkPi.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78) at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88) at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33) at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40) at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56) at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80) at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89) at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) master: -------------------------- I0830 18:32:33.000991 11586 logging.cpp:72] Logging to <stderr> I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050 I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586 I0830 18:32:33.005802 11588 master.cpp:483] Elected as master! I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py' Bottle server starting up (using WSGIRefServer())... Listening on http://0.0.0.0:8080/ Use Ctrl-C to quit. I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321 I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4 I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4", "reporter": "vince67", "assignee": null, "created": "0012-08-30T03:34:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Is this with the dev branch? You might need to do sbt/sbt package before running, in order to generate a JAR file with the example code. Matei On Aug 30, 2012, at 4:34 AM, vince67 wrote: > Hi there, > I've been trying to make spark work on Mesos and have got an error. > Any help would be truly appreciated. > > When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this: > > 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX > 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING > 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950 > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING > 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700 > 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8 > java.lang.NullPointerException > at java.io.File.(File.java:222) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333) > at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) > at scala.collection.immutable.List.foreach(List.scala:76) > at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333) > at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125) > at spark.MesosScheduler.(MesosScheduler.scala:80) > at spark.SparkContext.(SparkContext.scala:78) > at spark.examples.SparkPi$.main(SparkPi.scala:15) > at spark.examples.SparkPi.main(SparkPi.scala) > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) > at java.lang.reflect.Method.invoke(Method.java:597) > at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88) > at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) > at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33) > at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40) > at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56) > at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80) > at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89) > > at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) > > master: > > I0830 18:32:33.000991 11586 logging.cpp:72] Logging to > I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root > I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master > I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050 > I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586 > I0830 18:32:33.005802 11588 master.cpp:483] Elected as master! > I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py' > Bottle server starting up (using WSGIRefServer())... > Listening on http://0.0.0.0:8080/ > Use Ctrl-C to quit. > > I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321 > I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active > I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4 > > I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4 > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-08-30T07:34:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Actually you need to do sbt/sbt package even on the master branch. I'll update the docs to say that. On Aug 30, 2012, at 4:34 AM, vince67 wrote: > Hi there, > I've been trying to make spark work on Mesos and have got an error. > Any help would be truly appreciated. > > When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this: > > 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX > 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING > 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950 > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING > 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700 > 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8 > java.lang.NullPointerException > at java.io.File.(File.java:222) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333) > at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) > at scala.collection.immutable.List.foreach(List.scala:76) > at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333) > at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125) > at spark.MesosScheduler.(MesosScheduler.scala:80) > at spark.SparkContext.(SparkContext.scala:78) > at spark.examples.SparkPi$.main(SparkPi.scala:15) > at spark.examples.SparkPi.main(SparkPi.scala) > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) > at java.lang.reflect.Method.invoke(Method.java:597) > at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88) > at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) > at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33) > at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40) > at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56) > at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80) > at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89) > > at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) > > master: > > I0830 18:32:33.000991 11586 logging.cpp:72] Logging to > I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root > I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master > I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050 > I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586 > I0830 18:32:33.005802 11588 master.cpp:483] Elected as master! > I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py' > Bottle server starting up (using WSGIRefServer())... > Listening on http://0.0.0.0:8080/ > Use Ctrl-C to quit. > > I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321 > I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active > I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4 > > I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4 > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-08-30T07:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from vince67: In fact I've done sbt/sbt package,but it may not work perfectly. I re-download the spark comprssed package and do sbt/sbt again,Fortunaately,when I run the command ,the error has disappeared.", "created": "2012-08-30T08:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-185, originally reported by vince67", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-371\nSummary: run spark.examples.SparkPi 127.0.1.1:5050 spark gets error\nDescription: Hi there, I've been trying to make spark work on Mesos and have got an error. Any help would be truly appreciated. When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this: ------------------------- 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8 java.lang.NullPointerException at java.io.File.<init>(File.java:222) at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334) at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333) at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125) at spark.MesosScheduler.<init>(MesosScheduler.scala:80) at spark.SparkContext.<init>(SparkContext.scala:78) at spark.examples.SparkPi$.main(SparkPi.scala:15) at spark.examples.SparkPi.main(SparkPi.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78) at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88) at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78) at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33) at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40) at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56) at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80) at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89) at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) master: -------------------------- I0830 18:32:33.000991 11586 logging.cpp:72] Logging to <stderr> I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050 I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586 I0830 18:32:33.005802 11588 master.cpp:483] Elected as master! I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py' Bottle server starting up (using WSGIRefServer())... Listening on http://0.0.0.0:8080/ Use Ctrl-C to quit. I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321 I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4 I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Is this with the dev branch? You might need to do sbt/sbt package before running, in order to generate a JAR file with the example code. Matei On Aug 30, 2012, at 4:34 AM, vince67 wrote: > Hi there, > I've been trying to make spark work on Mesos and have got an error. > Any help would be truly appreciated. > > When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this: > > 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX > 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING > 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950 > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING > 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700 > 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8 > java.lang.NullPointerException > at java.io.File.(File.java:222) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333) > at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) > at scala.collection.immutable.List.foreach(List.scala:76) > at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333) > at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125) > at spark.MesosScheduler.(MesosScheduler.scala:80) > at spark.SparkContext.(SparkContext.scala:78) > at spark.examples.SparkPi$.main(SparkPi.scala:15) > at spark.examples.SparkPi.main(SparkPi.scala) > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) > at java.lang.reflect.Method.invoke(Method.java:597) > at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88) > at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) > at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33) > at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40) > at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56) > at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80) > at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89) > > at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) > > master: > > I0830 18:32:33.000991 11586 logging.cpp:72] Logging to > I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root > I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master > I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050 > I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586 > I0830 18:32:33.005802 11588 master.cpp:483] Elected as master! > I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py' > Bottle server starting up (using WSGIRefServer())... > Listening on http://0.0.0.0:8080/ > Use Ctrl-C to quit. > > I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321 > I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active > I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4 > > I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4 > > — > Reply to this email directly or view it on GitHub. > >\n2. Patrick McFadin: Github comment from mateiz: Actually you need to do sbt/sbt package even on the master branch. I'll update the docs to say that. On Aug 30, 2012, at 4:34 AM, vince67 wrote: > Hi there, > I've been trying to make spark work on Mesos and have got an error. > Any help would be truly appreciated. > > When I run spark.examples.SparkPi 127.0.1.1:5050 ,sparks gives description like this: > > 12/08/30 18:33:37 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 1326463057 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.CacheTrackerActor: Started slave cache (size 1265.0MB) on vince67-ThinkCentre-XXXX > 12/08/30 18:33:37 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 > 12/08/30 18:33:37 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-a3f754a1-cce3-4516-bf79-b2b423a8e6f0/shuffle > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:53950 STARTING > 12/08/30 18:33:38 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:53950 > 12/08/30 18:33:38 INFO server.Server: jetty-7.5.3.v20111011 > 12/08/30 18:33:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:39700 STARTING > 12/08/30 18:33:38 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:39700 > 12/08/30 18:33:38 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-1f3583ae-e06e-4e4e-97f3-7812abdf4db8 > java.lang.NullPointerException > at java.io.File.(File.java:222) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:334) > at spark.MesosScheduler$$anonfun$createJarServer$3.apply(MesosScheduler.scala:333) > at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) > at scala.collection.immutable.List.foreach(List.scala:76) > at spark.MesosScheduler.createJarServer(MesosScheduler.scala:333) > at spark.MesosScheduler.createExecutorInfo(MesosScheduler.scala:125) > at spark.MesosScheduler.(MesosScheduler.scala:80) > at spark.SparkContext.(SparkContext.scala:78) > at spark.examples.SparkPi$.main(SparkPi.scala:15) > at spark.examples.SparkPi.main(SparkPi.scala) > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) > at java.lang.reflect.Method.invoke(Method.java:597) > at scala.tools.nsc.util.ScalaClassLoader$$anonfun$run$1.apply(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:24) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.asContext(ScalaClassLoader.scala:88) > at scala.tools.nsc.util.ScalaClassLoader$class.run(ScalaClassLoader.scala:78) > at scala.tools.nsc.util.ScalaClassLoader$URLClassLoader.run(ScalaClassLoader.scala:101) > at scala.tools.nsc.ObjectRunner$.run(ObjectRunner.scala:33) > at scala.tools.nsc.ObjectRunner$.runAndCatch(ObjectRunner.scala:40) > at scala.tools.nsc.MainGenericRunner.runTarget$1(MainGenericRunner.scala:56) > at scala.tools.nsc.MainGenericRunner.process(MainGenericRunner.scala:80) > at scala.tools.nsc.MainGenericRunner$.main(MainGenericRunner.scala:89) > > at scala.tools.nsc.MainGenericRunner.main(MainGenericRunner.scala) > > master: > > I0830 18:32:33.000991 11586 logging.cpp:72] Logging to > I0830 18:32:33.002667 11586 main.cpp:105] Build: 2012-08-30 18:28:46 by root > I0830 18:32:33.002779 11586 main.cpp:106] Starting Mesos master > I0830 18:32:33.003238 11588 master.cpp:268] Master started on 127.0.1.1:5050 > I0830 18:32:33.005064 11588 master.cpp:283] Master ID: 20120830183216842879-5050-11586 > I0830 18:32:33.005802 11588 master.cpp:483] Elected as master! > I0830 18:32:33.046126 11590 webui_utils.cpp:45] Loading webui script at '/usr/local/mesos/share/mesos/webui/master/webui.py' > Bottle server starting up (using WSGIRefServer())... > Listening on http://0.0.0.0:8080/ > Use Ctrl-C to quit. > > I0830 18:32:59.907269 11587 master.cpp:844] Attempting to register slave 20120830183216842879-5050-11586-0 at slave@127.0.1.1:33321 > I0830 18:32:59.907335 11587 master.cpp:1097] Master now considering a slave at vince67-ThinkCentre-XXXX:33321 as active > I0830 18:32:59.907357 11587 master.cpp:1633] Adding slave 20120830183216842879-5050-11586-0 at vince67-ThinkCentre-XXXX with mem=2000; cpus=4 > > I0830 18:32:59.908043 11587 simple_allocator.cpp:69] Added slave 20120830183216842879-5050-11586-0 with mem=2000; cpus=4 > > — > Reply to this email directly or view it on GitHub. > >\n3. Patrick McFadin: Github comment from vince67: In fact I've done sbt/sbt package,but it may not work perfectly. I re-download the spark comprssed package and do sbt/sbt again,Fortunaately,when I run the command ,the error has disappeared.\n4. Patrick McFadin: Imported from Github issue spark-185, originally reported by vince67", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.979174"}}
{"id": "97fd1d38dcd2e63ae5fd563b24db4f7f", "issue_key": "SPARK-370", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark HTTP FileServer", "description": "A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver. - Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing. - Test with standalone mode locally and local mode, but not on a mesos cluster.", "reporter": "Denny Britz", "assignee": null, "created": "0012-08-30T10:09:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I pointed out a few small formatting issues but I think there is also a problem with ShuffleMapTask's special serialization -- check that out and add a test for it. Otherwise it looks pretty good.", "created": "2012-08-30T16:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I made the changes you suggested and added the functionality of dynamically adding JAR files. Like you said, I had to modify the ShuffleMapTask serialization to make it work.", "created": "2012-09-04T18:01:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks okay except for one big thing: creating a new ClassLoader for each task in the Executor is going to be very expensive, and is going to result in many copies of each task being loaded. Can you instead reuse the same ClassLoader, but give it more paths to search? One easy way would be to have a custom subclass of ClassLoader that holds an array of URLClassLoaders, and queries them to find the first one that contains a file. As a more minor suggestion, in ShuffleMapTask, can you cache the serialized versions of the JAR and file HashMaps the same way we cache the RDDs? That serialization code becomes a bottleneck when you have a lot of tasks, and these maps will not change for subsequent runs of the task (or at least we shouldn't let a user use a JAR *before* they add it to the system). You may need to change the Executor to ignore older JARs as well in case it gets an old task. Also, in the serialization part, you may be better off doing a toArray on these and serializing that instead of serializing a HashMap; then, just do a toMap on the Executor.", "created": "2012-09-04T21:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Yep, that make sense. I'll make the changes.", "created": "2012-09-04T21:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I am closing this pull request here because I want to submit the request from another, clean, branch.", "created": "2012-09-10T14:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-186, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-370\nSummary: Spark HTTP FileServer\nDescription: A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver. - Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing. - Test with standalone mode locally and local mode, but not on a mesos cluster.\n\nComments (6):\n1. Patrick McFadin: Github comment from mateiz: I pointed out a few small formatting issues but I think there is also a problem with ShuffleMapTask's special serialization -- check that out and add a test for it. Otherwise it looks pretty good.\n2. Patrick McFadin: Github comment from dennybritz: I made the changes you suggested and added the functionality of dynamically adding JAR files. Like you said, I had to modify the ShuffleMapTask serialization to make it work.\n3. Patrick McFadin: Github comment from mateiz: This looks okay except for one big thing: creating a new ClassLoader for each task in the Executor is going to be very expensive, and is going to result in many copies of each task being loaded. Can you instead reuse the same ClassLoader, but give it more paths to search? One easy way would be to have a custom subclass of ClassLoader that holds an array of URLClassLoaders, and queries them to find the first one that contains a file. As a more minor suggestion, in ShuffleMapTask, can you cache the serialized versions of the JAR and file HashMaps the same way we cache the RDDs? That serialization code becomes a bottleneck when you have a lot of tasks, and these maps will not change for subsequent runs of the task (or at least we shouldn't let a user use a JAR *before* they add it to the system). You may need to change the Executor to ignore older JARs as well in case it gets an old task. Also, in the serialization part, you may be better off doing a toArray on these and serializing that instead of serializing a HashMap; then, just do a toMap on the Executor.\n4. Patrick McFadin: Github comment from dennybritz: Yep, that make sense. I'll make the changes.\n5. Patrick McFadin: Github comment from dennybritz: I am closing this pull request here because I want to submit the request from another, clean, branch.\n6. Patrick McFadin: Imported from Github issue spark-186, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.979738"}}
{"id": "68c0f28b9b955ac8e741749a66cdf020", "issue_key": "SPARK-369", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "sbin/mesos-start-cluster.sh ulimit: error setting limit (Operation not permitted)", "description": "Thanks for your attention. You know, I have done everything following the steps on the web (Runnin-Spark-on-Mesos) https://github.com/mesos/spark/wiki/Running-Spark-on-Mesos. But,I have got this in the seventh step: usrname@ThinkCentre-XXXX:/usr/local/mesos$ sbin/mesos-start-cluster.sh ----------------------------------------------------------------------------------------- Starting mesos-master on Host@Ip.ip.ip.ip ssh -o StrictHostKeyChecking=no -o ConnectTimeout=2 surname@ip.ip.ip.ip /usr/local/mesos/sbin/mesos-daemon.sh mesos-master </dev/null >/dev/null masters@ip.ip.ip.ip's password: /usr/local/mesos/sbin/mesos-daemon.sh: 7: ulimit: error setting limit (Operation not permitted) -------------------------------------------------------------------------------------------------------------------------------- Starting mesos-slave on spark@ip.ip.ip.ip ssh -o StrictHostKeyChecking=no -o ConnectTimeout=2 spark@ip.ip.ip.ip /usr/local/mesos/sbin/mesos-daemon.sh mesos-slave </dev/null >/dev/null spark@ip.ip.ip.ip's password: /usr/local/mesos/sbin/mesos-daemon.sh: 7: ulimit: error setting limit (Operation not permitted) -------------------------------------------------------------------------------------------------------------------------------- Everything's started! Any words would be truly appreciated.", "reporter": "vince67", "assignee": null, "created": "0012-08-31T04:28:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-187, originally reported by vince67", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-369\nSummary: sbin/mesos-start-cluster.sh ulimit: error setting limit (Operation not permitted)\nDescription: Thanks for your attention. You know, I have done everything following the steps on the web (Runnin-Spark-on-Mesos) https://github.com/mesos/spark/wiki/Running-Spark-on-Mesos. But,I have got this in the seventh step: usrname@ThinkCentre-XXXX:/usr/local/mesos$ sbin/mesos-start-cluster.sh ----------------------------------------------------------------------------------------- Starting mesos-master on Host@Ip.ip.ip.ip ssh -o StrictHostKeyChecking=no -o ConnectTimeout=2 surname@ip.ip.ip.ip /usr/local/mesos/sbin/mesos-daemon.sh mesos-master </dev/null >/dev/null masters@ip.ip.ip.ip's password: /usr/local/mesos/sbin/mesos-daemon.sh: 7: ulimit: error setting limit (Operation not permitted) -------------------------------------------------------------------------------------------------------------------------------- Starting mesos-slave on spark@ip.ip.ip.ip ssh -o StrictHostKeyChecking=no -o ConnectTimeout=2 spark@ip.ip.ip.ip /usr/local/mesos/sbin/mesos-daemon.sh mesos-slave </dev/null >/dev/null spark@ip.ip.ip.ip's password: /usr/local/mesos/sbin/mesos-daemon.sh: 7: ulimit: error setting limit (Operation not permitted) -------------------------------------------------------------------------------------------------------------------------------- Everything's started! Any words would be truly appreciated.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-187, originally reported by vince67", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980058"}}
{"id": "2c546134043eeda672267c4fc6402b71", "issue_key": "SPARK-538", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone", "description": "Hi Matei, Maybe I can't descibe it clearly. We run masters or slaves on different machines,it is success. But when we run spark.examples.SparkPi on the master , our process hangs,we have not got the result. Descirption like these: 12/09/02 16:47:54 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 339585269 12/09/02 16:47:54 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/09/02 16:47:54 INFO spark.CacheTrackerActor: Started slave cache (size 323.9MB) on vince67-ThinkCentre-XXXX 12/09/02 16:47:54 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/09/02 16:47:54 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-3e79b235-1b94-44d1-823b-0369f6698688/shuffle 12/09/02 16:47:54 INFO server.Server: jetty-7.5.3.v20111011 12/09/02 16:47:54 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:49578 STARTING 12/09/02 16:47:54 INFO spark.ShuffleManager: Local URI: http://ip.ip.ip.ip:49578 12/09/02 16:47:55 INFO server.Server: jetty-7.5.3.v20111011 12/09/02 16:47:55 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:49600 STARTING 12/09/02 16:47:55 INFO broadcast.HttpBroadcast: Broadcast server started at http://ip.ip.ip.ip:49600 12/09/02 16:47:55 INFO spark.MesosScheduler: Registered as framework ID 201209021640-74572372-5050-16898-0004 12/09/02 16:47:55 INFO spark.SparkContext: Starting job... 12/09/02 16:47:55 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/09/02 16:47:55 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/09/02 16:47:55 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/09/02 16:47:55 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/09/02 16:47:55 INFO spark.CacheTrackerActor: Asked for current cache locations 12/09/02 16:47:55 INFO spark.MesosScheduler: Final stage: Stage 0 12/09/02 16:47:55 INFO spark.MesosScheduler: Parents of final stage: List() 12/09/02 16:47:55 INFO spark.MesosScheduler: Missing parents: List() 12/09/02 16:47:55 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/09/02 16:47:55 INFO spark.MesosScheduler: Got a job with 2 tasks 12/09/02 16:47:55 INFO spark.MesosScheduler: Adding job with ID 0 12/09/02 16:47:55 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:55 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 151 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:55 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:55 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:56 INFO spark.SimpleJob: Lost TID 0 (task 0:0) 12/09/02 16:47:56 INFO spark.SimpleJob: Starting task 0:0 as TID 2 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:56 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:56 INFO spark.SimpleJob: Lost TID 1 (task 0:1) 12/09/02 16:47:56 INFO spark.SimpleJob: Starting task 0:1 as TID 3 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:56 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 5 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:57 INFO spark.SimpleJob: Lost TID 2 (task 0:0) 12/09/02 16:47:57 INFO spark.SimpleJob: Starting task 0:0 as TID 4 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:57 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:57 INFO spark.SimpleJob: Lost TID 3 (task 0:1) 12/09/02 16:47:57 INFO spark.SimpleJob: Starting task 0:1 as TID 5 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:57 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:58 INFO spark.SimpleJob: Lost TID 4 (task 0:0) 12/09/02 16:47:58 INFO spark.SimpleJob: Starting task 0:0 as TID 6 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:58 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:58 INFO spark.SimpleJob: Lost TID 5 (task 0:1) 12/09/02 16:47:58 INFO spark.SimpleJob: Starting task 0:1 as TID 7 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:58 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:59 INFO spark.SimpleJob: Lost TID 6 (task 0:0) 12/09/02 16:47:59 INFO spark.SimpleJob: Starting task 0:0 as TID 8 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:59 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:59 INFO spark.SimpleJob: Lost TID 7 (task 0:1) 12/09/02 16:47:59 INFO spark.SimpleJob: Starting task 0:1 as TID 9 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:59 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:48:00 INFO spark.SimpleJob: Lost TID 8 (task 0:0) 12/09/02 16:48:00 ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job 12/09/02 16:48:00 INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone Your help will be appreciate.", "reporter": "vince67", "assignee": null, "created": "0012-09-02T01:09:00.000+0000", "updated": "2014-09-21T15:28:37.000+0000", "resolved": "2014-09-21T15:28:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hi Vince, The problem is the messages earlier, which say \"lost TID\". They mean that a task crashed, and most likely, that is because Spark or Scala was not installed on your worker nodes. On lmrspark-G41MT-S2 for example, did you download and build Spark in the same directory as on the master? It needs to be in the same place on all of them. You can find a log of what happened by going to lmrspark-G41MT-S2 and looking in /tmp/mesos/slaveID/frameworkID/executors/0/stdout and stderr. (The frameworkID will be 201209021640-74572372-5050-16898-0004 for this job, which is printed towards the start of the log). It will likely say something like 'scala not found' or 'path ... does not exist'.", "created": "2012-09-02T09:49:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from vince67: I try to go to the directory to find the log stderr on lmrspark-G41MT-S2,but it dose not exist. In fact, when I run master and slave on the same machine, I can get the right result as well as the log stderr.", "created": "2012-09-03T00:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Do you have any log output from mesos-slave on that machine? Maybe /tmp is not writable and that's why it can't run the tasks.", "created": "2012-09-06T14:24:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way, you can pass a different work directory to mesos-slave with the --work-dir option. It will then run the tasks, and place the stdout/stderr files, in that directory.", "created": "2012-09-06T14:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-188, originally reported by vince67", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matthew Farrellee", "body": "this is a reasonable question for the user list, see http://spark.apache.org/community.html. i'm going to close this in favor of user list interaction. if you disagree, please re-open.", "created": "2014-09-21T15:28:16.606+0000"}], "num_comments": 6, "text": "Issue: SPARK-538\nSummary: INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone\nDescription: Hi Matei, Maybe I can't descibe it clearly. We run masters or slaves on different machines,it is success. But when we run spark.examples.SparkPi on the master , our process hangs,we have not got the result. Descirption like these: 12/09/02 16:47:54 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 339585269 12/09/02 16:47:54 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/09/02 16:47:54 INFO spark.CacheTrackerActor: Started slave cache (size 323.9MB) on vince67-ThinkCentre-XXXX 12/09/02 16:47:54 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/09/02 16:47:54 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-3e79b235-1b94-44d1-823b-0369f6698688/shuffle 12/09/02 16:47:54 INFO server.Server: jetty-7.5.3.v20111011 12/09/02 16:47:54 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:49578 STARTING 12/09/02 16:47:54 INFO spark.ShuffleManager: Local URI: http://ip.ip.ip.ip:49578 12/09/02 16:47:55 INFO server.Server: jetty-7.5.3.v20111011 12/09/02 16:47:55 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:49600 STARTING 12/09/02 16:47:55 INFO broadcast.HttpBroadcast: Broadcast server started at http://ip.ip.ip.ip:49600 12/09/02 16:47:55 INFO spark.MesosScheduler: Registered as framework ID 201209021640-74572372-5050-16898-0004 12/09/02 16:47:55 INFO spark.SparkContext: Starting job... 12/09/02 16:47:55 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/09/02 16:47:55 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/09/02 16:47:55 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/09/02 16:47:55 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/09/02 16:47:55 INFO spark.CacheTrackerActor: Asked for current cache locations 12/09/02 16:47:55 INFO spark.MesosScheduler: Final stage: Stage 0 12/09/02 16:47:55 INFO spark.MesosScheduler: Parents of final stage: List() 12/09/02 16:47:55 INFO spark.MesosScheduler: Missing parents: List() 12/09/02 16:47:55 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/09/02 16:47:55 INFO spark.MesosScheduler: Got a job with 2 tasks 12/09/02 16:47:55 INFO spark.MesosScheduler: Adding job with ID 0 12/09/02 16:47:55 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:55 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 151 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:55 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:55 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:56 INFO spark.SimpleJob: Lost TID 0 (task 0:0) 12/09/02 16:47:56 INFO spark.SimpleJob: Starting task 0:0 as TID 2 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:56 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:56 INFO spark.SimpleJob: Lost TID 1 (task 0:1) 12/09/02 16:47:56 INFO spark.SimpleJob: Starting task 0:1 as TID 3 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:56 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 5 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:57 INFO spark.SimpleJob: Lost TID 2 (task 0:0) 12/09/02 16:47:57 INFO spark.SimpleJob: Starting task 0:0 as TID 4 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:57 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:57 INFO spark.SimpleJob: Lost TID 3 (task 0:1) 12/09/02 16:47:57 INFO spark.SimpleJob: Starting task 0:1 as TID 5 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:57 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:58 INFO spark.SimpleJob: Lost TID 4 (task 0:0) 12/09/02 16:47:58 INFO spark.SimpleJob: Starting task 0:0 as TID 6 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:58 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:58 INFO spark.SimpleJob: Lost TID 5 (task 0:1) 12/09/02 16:47:58 INFO spark.SimpleJob: Starting task 0:1 as TID 7 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:58 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:59 INFO spark.SimpleJob: Lost TID 6 (task 0:0) 12/09/02 16:47:59 INFO spark.SimpleJob: Starting task 0:0 as TID 8 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:59 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:47:59 INFO spark.SimpleJob: Lost TID 7 (task 0:1) 12/09/02 16:47:59 INFO spark.SimpleJob: Starting task 0:1 as TID 9 on slave 201209021640-74572372-5050-16898-2: lmrspark-G41MT-S2 (preferred) 12/09/02 16:47:59 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/02 16:48:00 INFO spark.SimpleJob: Lost TID 8 (task 0:0) 12/09/02 16:48:00 ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job 12/09/02 16:48:00 INFO spark.MesosScheduler: Ignoring update from TID 9 because its job is gone Your help will be appreciate.\n\nComments (6):\n1. Patrick McFadin: Github comment from mateiz: Hi Vince, The problem is the messages earlier, which say \"lost TID\". They mean that a task crashed, and most likely, that is because Spark or Scala was not installed on your worker nodes. On lmrspark-G41MT-S2 for example, did you download and build Spark in the same directory as on the master? It needs to be in the same place on all of them. You can find a log of what happened by going to lmrspark-G41MT-S2 and looking in /tmp/mesos/slaveID/frameworkID/executors/0/stdout and stderr. (The frameworkID will be 201209021640-74572372-5050-16898-0004 for this job, which is printed towards the start of the log). It will likely say something like 'scala not found' or 'path ... does not exist'.\n2. Patrick McFadin: Github comment from vince67: I try to go to the directory to find the log stderr on lmrspark-G41MT-S2,but it dose not exist. In fact, when I run master and slave on the same machine, I can get the right result as well as the log stderr.\n3. Patrick McFadin: Github comment from mateiz: Do you have any log output from mesos-slave on that machine? Maybe /tmp is not writable and that's why it can't run the tasks.\n4. Patrick McFadin: Github comment from mateiz: By the way, you can pass a different work directory to mesos-slave with the --work-dir option. It will then run the tasks, and place the stdout/stderr files, in that directory.\n5. Patrick McFadin: Imported from Github issue spark-188, originally reported by vince67\n6. Matthew Farrellee: this is a reasonable question for the user list, see http://spark.apache.org/community.html. i'm going to close this in favor of user list interaction. if you disagree, please re-open.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "297cc56dcfb6ffd830def77f63e7d060", "issue_key": "SPARK-368", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Simulating a Spark standalone cluster locally", "description": "Title is pretty self-explanatory. The URL scheme is spark-cluster[N, coresPerSlave, memoryPerSlave]. There currently seems to be a problem with the Shutdown of the Executors, apparently they keep the JVM running when the user exits the Spark shell. I will fix that (let me know if you know a good way), but I submitted already because I wanted to get a quick code review.", "reporter": "Denny Britz", "assignee": null, "created": "0012-09-04T20:18:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: I think it is better to separate the master / slaves into multiple JVM processes so the local cluster setup better approximates a real cluster mode, which makes testing much easier. Updated: never mind. I thought this was for the executor. As long as the executor is launched as a separate JVM from the master node, I am happy.", "created": "2012-09-04T21:07:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I actually disagree with Reynold on that -- I think running the spark.deploy master and slaves in the same process is fine. They will still communicate over Akka and see the same issues they would in separate JVMs. I'd prefer to avoid spawning lots of JVMs so that we can use this effectively in unit tests. For the executor JVMs, is the problem just that they stay running when you Ctrl-C? You should add a shutdown hook (Runtime.addShutdownHook) inside the spark.deploy.Worker that stops its child processes when the JVM is stopped, or even add a way to explicitly stop a Worker from outside. I thought I'd actually already added such a shutdown hook.", "created": "2012-09-04T21:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Oops - updated my comment. I didn't realize the executors would've been launched as separate JVM processes.", "created": "2012-09-04T21:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I looked into the executor problem. They exit fine if I quit the repl via \"ctrl+c\", but not when I type \"exit\" or \":quit\". Any ideas? Maybe that's not even a problem with the new code.", "created": "2012-09-05T16:13:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: I think that Ctrl-C is sending SIGINT to the REPL and all of its children (see http://www.vidarholen.net/contents/blog/?p=34). If I send SIGINT directly to the REPL process using `kill -SIGINT`, then the executors keep running after the REPL exits.", "created": "2012-09-06T08:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: So shutdown hooks seem to work fine in the Scala and Spark shells: scala> val r = Runtime.getRuntime r: java.lang.Runtime = java.lang.Runtime@67439515 scala> r.addShutdownHook(new Thread() { override def run() {println(\"!!!\")} }) scala> [Ctrl-D] !!! Try printing some stuff in our shutdown hook and making sure it's called. You may also need to do a process.waitFor after killing it with kill().", "created": "2012-09-06T14:14:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Working now, see the commit above.", "created": "2012-09-07T10:40:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, but instead of having the shutdown hook kill the master actor system, can you just add a shutdown hook in ExecutorRunner that does process.destroy()? I don't see that here. Also, it would be nice if calling SparkContext.stop() when there is a local Spark cluster would shutdown that cluster. I think this would require some kind of wrapper or flag in SparkSchedulerBackend that lets you run some code when stop() is called, which in this case would stop the ActorSystems in the LocalSparkCluster, or send them a message. Basically the reason is that I'd like to use the local-cluster mode in unit tests, which will each start and stop a SparkContext.", "created": "2012-09-07T11:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good; thanks!", "created": "2012-09-07T14:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-189, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 10, "text": "Issue: SPARK-368\nSummary: Simulating a Spark standalone cluster locally\nDescription: Title is pretty self-explanatory. The URL scheme is spark-cluster[N, coresPerSlave, memoryPerSlave]. There currently seems to be a problem with the Shutdown of the Executors, apparently they keep the JVM running when the user exits the Spark shell. I will fix that (let me know if you know a good way), but I submitted already because I wanted to get a quick code review.\n\nComments (10):\n1. Patrick McFadin: Github comment from rxin: I think it is better to separate the master / slaves into multiple JVM processes so the local cluster setup better approximates a real cluster mode, which makes testing much easier. Updated: never mind. I thought this was for the executor. As long as the executor is launched as a separate JVM from the master node, I am happy.\n2. Patrick McFadin: Github comment from mateiz: I actually disagree with Reynold on that -- I think running the spark.deploy master and slaves in the same process is fine. They will still communicate over Akka and see the same issues they would in separate JVMs. I'd prefer to avoid spawning lots of JVMs so that we can use this effectively in unit tests. For the executor JVMs, is the problem just that they stay running when you Ctrl-C? You should add a shutdown hook (Runtime.addShutdownHook) inside the spark.deploy.Worker that stops its child processes when the JVM is stopped, or even add a way to explicitly stop a Worker from outside. I thought I'd actually already added such a shutdown hook.\n3. Patrick McFadin: Github comment from rxin: Oops - updated my comment. I didn't realize the executors would've been launched as separate JVM processes.\n4. Patrick McFadin: Github comment from dennybritz: I looked into the executor problem. They exit fine if I quit the repl via \"ctrl+c\", but not when I type \"exit\" or \":quit\". Any ideas? Maybe that's not even a problem with the new code.\n5. Patrick McFadin: Github comment from JoshRosen: I think that Ctrl-C is sending SIGINT to the REPL and all of its children (see http://www.vidarholen.net/contents/blog/?p=34). If I send SIGINT directly to the REPL process using `kill -SIGINT`, then the executors keep running after the REPL exits.\n6. Patrick McFadin: Github comment from mateiz: So shutdown hooks seem to work fine in the Scala and Spark shells: scala> val r = Runtime.getRuntime r: java.lang.Runtime = java.lang.Runtime@67439515 scala> r.addShutdownHook(new Thread() { override def run() {println(\"!!!\")} }) scala> [Ctrl-D] !!! Try printing some stuff in our shutdown hook and making sure it's called. You may also need to do a process.waitFor after killing it with kill().\n7. Patrick McFadin: Github comment from dennybritz: Working now, see the commit above.\n8. Patrick McFadin: Github comment from mateiz: Looks good, but instead of having the shutdown hook kill the master actor system, can you just add a shutdown hook in ExecutorRunner that does process.destroy()? I don't see that here. Also, it would be nice if calling SparkContext.stop() when there is a local Spark cluster would shutdown that cluster. I think this would require some kind of wrapper or flag in SparkSchedulerBackend that lets you run some code when stop() is called, which in this case would stop the ActorSystems in the LocalSparkCluster, or send them a message. Basically the reason is that I'd like to use the local-cluster mode in unit tests, which will each start and stop a SparkContext.\n9. Patrick McFadin: Github comment from mateiz: Looks good; thanks!\n10. Patrick McFadin: Imported from Github issue spark-189, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "dd67cd6db98935cb89f26c69057b48d7", "issue_key": "SPARK-367", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Log cache add/remove messages in block manager.", "description": "Removed cache add/remove log messages from CacheTracker. Added log messages on BlockManagerMaster to reflect block add/remove. Also did some minor cleanup of storage package code.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-09-05T15:02:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks.", "created": "2012-09-05T15:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-190, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-367\nSummary: Log cache add/remove messages in block manager.\nDescription: Removed cache add/remove log messages from CacheTracker. Added log messages on BlockManagerMaster to reflect block add/remove. Also did some minor cleanup of storage package code.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks.\n2. Patrick McFadin: Imported from Github issue spark-190, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "90c11705776cceafaf0c95c5f87b5d92", "issue_key": "SPARK-366", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Broken build after typesafe ivy repo changes", "description": "The build is broken since last night due to the following unresolved dependencies: [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: UNRESOLVED DEPENDENCIES :: [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: com.github.mpeltonen#sbt-idea;0.11.1: not found [warn] :: com.typesafe.sbteclipse#sbteclipse;2.0: not found [warn] :: com.eed3si9n#sbt-assembly;0.7.2: not found [warn] :::::::::::::::::::::::::::::::::::::::::::::: I noticed that typesafe did some changes to what was available at their repo and this caused the build to fail. I suppose finding the dependencies in other repos will be enogh.", "reporter": "ivantopo", "assignee": null, "created": "0012-09-06T06:59:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for pointing this out, but I can't reproduce the problem. I tried a build on a machine with a cleaned ~/.ivy2 and ~/.m2, from a fresh checkout of Spark. Are you sure that it's still happening? Maybe it was a transient thing. On Sep 6, 2012, at 7:59 AM, ivantopo wrote: > The build is broken since last night due to the following unresolved dependencies: > > [warn] :::::::::::::::::::::::::::::::::::::::::::::: > [warn] :: UNRESOLVED DEPENDENCIES :: > [warn] :::::::::::::::::::::::::::::::::::::::::::::: > [warn] :: com.github.mpeltonen#sbt-idea;0.11.1: not found > [warn] :: com.typesafe.sbteclipse#sbteclipse;2.0: not found > [warn] :: com.eed3si9n#sbt-assembly;0.7.2: not found > [warn] :::::::::::::::::::::::::::::::::::::::::::::: > > I noticed that typesafe did some changes to what was available at their repo and this caused the build to fail. I suppose finding the dependencies in other repos will be enogh. > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-09-06T14:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from ivantopo: Hi Matei, the issue is no longer happening! I confirmed that there was a problem with the typesafe repo and also confirmed they are back to normal, thanks for your quick feedback, best regards!", "created": "2012-09-06T17:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-191, originally reported by ivantopo", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-366\nSummary: Broken build after typesafe ivy repo changes\nDescription: The build is broken since last night due to the following unresolved dependencies: [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: UNRESOLVED DEPENDENCIES :: [warn] :::::::::::::::::::::::::::::::::::::::::::::: [warn] :: com.github.mpeltonen#sbt-idea;0.11.1: not found [warn] :: com.typesafe.sbteclipse#sbteclipse;2.0: not found [warn] :: com.eed3si9n#sbt-assembly;0.7.2: not found [warn] :::::::::::::::::::::::::::::::::::::::::::::: I noticed that typesafe did some changes to what was available at their repo and this caused the build to fail. I suppose finding the dependencies in other repos will be enogh.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Thanks for pointing this out, but I can't reproduce the problem. I tried a build on a machine with a cleaned ~/.ivy2 and ~/.m2, from a fresh checkout of Spark. Are you sure that it's still happening? Maybe it was a transient thing. On Sep 6, 2012, at 7:59 AM, ivantopo wrote: > The build is broken since last night due to the following unresolved dependencies: > > [warn] :::::::::::::::::::::::::::::::::::::::::::::: > [warn] :: UNRESOLVED DEPENDENCIES :: > [warn] :::::::::::::::::::::::::::::::::::::::::::::: > [warn] :: com.github.mpeltonen#sbt-idea;0.11.1: not found > [warn] :: com.typesafe.sbteclipse#sbteclipse;2.0: not found > [warn] :: com.eed3si9n#sbt-assembly;0.7.2: not found > [warn] :::::::::::::::::::::::::::::::::::::::::::::: > > I noticed that typesafe did some changes to what was available at their repo and this caused the build to fail. I suppose finding the dependencies in other repos will be enogh. > > — > Reply to this email directly or view it on GitHub. > >\n2. Patrick McFadin: Github comment from ivantopo: Hi Matei, the issue is no longer happening! I confirmed that there was a problem with the typesafe repo and also confirmed they are back to normal, thanks for your quick feedback, best regards!\n3. Patrick McFadin: Imported from Github issue spark-191, originally reported by ivantopo", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "b8c34abd041c00b498db967859190d6d", "issue_key": "SPARK-537", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "driver.run() returned with code DRIVER_ABORTED", "description": "Hi there, When I try to run Spark on Mesos as a cluster, some error happen like this: ``` ./run spark.examples.SparkPi *.*.*.*:5050 12/09/07 14:49:28 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 994836480 12/09/07 14:49:28 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/09/07 14:49:28 INFO spark.CacheTrackerActor: Started slave cache (size 948.8MB) on shawpc 12/09/07 14:49:28 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/09/07 14:49:28 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-81220c47-bc43-4809-ac48-5e3e8e023c8a/shuffle 12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011 12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:57595 STARTING 12/09/07 14:49:28 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:57595 12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011 12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:60113 STARTING 12/09/07 14:49:28 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:60113 12/09/07 14:49:28 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-d541f37c-ae35-476c-b2fc-9908b0739f50 12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011 12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:50511 STARTING 12/09/07 14:49:28 INFO spark.MesosScheduler: JAR server started at http://127.0.1.1:50511 12/09/07 14:49:28 INFO spark.MesosScheduler: Registered as framework ID 201209071448-846324308-5050-26925-0000 12/09/07 14:49:29 INFO spark.SparkContext: Starting job... 12/09/07 14:49:29 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/09/07 14:49:29 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/09/07 14:49:29 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/09/07 14:49:29 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/09/07 14:49:29 INFO spark.CacheTrackerActor: Asked for current cache locations 12/09/07 14:49:29 INFO spark.MesosScheduler: Final stage: Stage 0 12/09/07 14:49:29 INFO spark.MesosScheduler: Parents of final stage: List() 12/09/07 14:49:29 INFO spark.MesosScheduler: Missing parents: List() 12/09/07 14:49:29 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/09/07 14:49:29 INFO spark.MesosScheduler: Got a job with 2 tasks 12/09/07 14:49:29 INFO spark.MesosScheduler: Adding job with ID 0 12/09/07 14:49:29 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:29 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 52 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:29 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:29 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 0 (task 0:0) 12/09/07 14:49:30 INFO spark.SimpleJob: Starting task 0:0 as TID 2 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:30 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 1 (task 0:1) 12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 2 (task 0:0) 12/09/07 14:49:30 INFO spark.SimpleJob: Starting task 0:0 as TID 3 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:30 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:1 as TID 4 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 3 (task 0:0) 12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:0 as TID 5 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 4 (task 0:1) 12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 5 (task 0:0) 12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:0 as TID 6 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:34 INFO spark.SimpleJob: Starting task 0:1 as TID 7 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:34 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:34 INFO spark.SimpleJob: Lost TID 6 (task 0:0) 12/09/07 14:49:34 ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job Exception in thread \"Thread-50\" java.io.EOFException at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280) at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749) at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779) at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279) at spark.JavaSerializerInstance$$anon$2.<init>(JavaSerializer.scala:39) at spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:39) at spark.SimpleJob.taskLost(SimpleJob.scala:296) at spark.SimpleJob.statusUpdate(SimpleJob.scala:207) at spark.MesosScheduler.statusUpdate(MesosScheduler.scala:287) 12/09/07 14:49:34 INFO spark.SimpleJob: Starting task 0:0 as TID 8 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:34 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:34 INFO spark.SimpleJob: Lost TID 7 (task 0:1) 12/09/07 14:49:34 INFO spark.MesosScheduler: driver.run() returned with code DRIVER_ABORTED ```", "reporter": "yshaw", "assignee": null, "created": "0012-09-06T22:59:00.000+0000", "updated": "2014-09-21T15:25:42.000+0000", "resolved": "2014-09-21T15:25:42.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from yshaw: And the error information loged in the mesos slaves like this: ``` java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351) at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366) at java.net.Socket.connect(Socket.java:529) at java.net.Socket.connect(Socket.java:478) at sun.net.NetworkClient.doConnect(NetworkClient.java:163) at sun.net.www.http.HttpClient.openServer(HttpClient.java:388) at sun.net.www.http.HttpClient.openServer(HttpClient.java:523) at sun.net.www.http.HttpClient.<init>(HttpClient.java:227) at sun.net.www.http.HttpClient.New(HttpClient.java:300) at sun.net.www.http.HttpClient.New(HttpClient.java:317) at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:970) at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:911) at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:836) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1172) at java.net.URL.openStream(URL.java:1010) at spark.Executor.spark$Executor$$downloadFile(Executor.scala:161) at spark.Executor$$anonfun$createClassLoader$2.apply(Executor.scala:132) at spark.Executor$$anonfun$createClassLoader$2.apply(Executor.scala:129) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.Executor.createClassLoader(Executor.scala:129) at spark.Executor.registered(Executor.scala:42) Exception in thread \"Thread-0\" ``` And both in the master and the slave, I can't open the 0.0.0.0:8080 or 0.0.0.0:8081 exactly, the error in the web page is : ``` Error 500: Internal Server Error Sorry, the requested URL http://0.0.0.0:8081/ caused an error: Unhandled exception Exception: IOError('socket error', error(111, 'Connection refused')) Traceback: Traceback (most recent call last): File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 499, in handle return handler(**args) File \"/home/shaw/mesos/share/mesos/webui/slave/webui.py\", line 57, in index return template(\"index\", slave_port = slave_port, log_dir = log_dir) File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1796, in template return TEMPLATES[tpl].render(**kwargs) File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1775, in render self.execute(stdout, **args) File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1763, in execute eval(self.co, env) File \"/home/shaw/mesos/share/mesos/webui/slave/index.tpl\", line 10, in <module> % data = urllib.urlopen(url).read() File \"/usr/lib/python2.7/urllib.py\", line 86, in urlopen return opener.open(url) File \"/usr/lib/python2.7/urllib.py\", line 207, in open return getattr(self, name)(url) File \"/usr/lib/python2.7/urllib.py\", line 344, in open_http h.endheaders(data) File \"/usr/lib/python2.7/httplib.py\", line 954, in endheaders self._send_output(message_body) IOError: [Errno socket error] [Errno 111] Connection refused ```", "created": "2012-09-06T23:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-192, originally reported by yshaw", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matthew Farrellee", "body": "this should be resolved by a number of fixes in 1.0. please re-open if it still reproduces.", "created": "2014-09-21T15:25:18.141+0000"}], "num_comments": 3, "text": "Issue: SPARK-537\nSummary: driver.run() returned with code DRIVER_ABORTED\nDescription: Hi there, When I try to run Spark on Mesos as a cluster, some error happen like this: ``` ./run spark.examples.SparkPi *.*.*.*:5050 12/09/07 14:49:28 INFO spark.BoundedMemoryCache: BoundedMemoryCache.maxBytes = 994836480 12/09/07 14:49:28 INFO spark.CacheTrackerActor: Registered actor on port 7077 12/09/07 14:49:28 INFO spark.CacheTrackerActor: Started slave cache (size 948.8MB) on shawpc 12/09/07 14:49:28 INFO spark.MapOutputTrackerActor: Registered actor on port 7077 12/09/07 14:49:28 INFO spark.ShuffleManager: Shuffle dir: /tmp/spark-local-81220c47-bc43-4809-ac48-5e3e8e023c8a/shuffle 12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011 12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:57595 STARTING 12/09/07 14:49:28 INFO spark.ShuffleManager: Local URI: http://127.0.1.1:57595 12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011 12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:60113 STARTING 12/09/07 14:49:28 INFO broadcast.HttpBroadcast: Broadcast server started at http://127.0.1.1:60113 12/09/07 14:49:28 INFO spark.MesosScheduler: Temp directory for JARs: /tmp/spark-d541f37c-ae35-476c-b2fc-9908b0739f50 12/09/07 14:49:28 INFO server.Server: jetty-7.5.3.v20111011 12/09/07 14:49:28 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:50511 STARTING 12/09/07 14:49:28 INFO spark.MesosScheduler: JAR server started at http://127.0.1.1:50511 12/09/07 14:49:28 INFO spark.MesosScheduler: Registered as framework ID 201209071448-846324308-5050-26925-0000 12/09/07 14:49:29 INFO spark.SparkContext: Starting job... 12/09/07 14:49:29 INFO spark.CacheTracker: Registering RDD ID 1 with cache 12/09/07 14:49:29 INFO spark.CacheTrackerActor: Registering RDD 1 with 2 partitions 12/09/07 14:49:29 INFO spark.CacheTracker: Registering RDD ID 0 with cache 12/09/07 14:49:29 INFO spark.CacheTrackerActor: Registering RDD 0 with 2 partitions 12/09/07 14:49:29 INFO spark.CacheTrackerActor: Asked for current cache locations 12/09/07 14:49:29 INFO spark.MesosScheduler: Final stage: Stage 0 12/09/07 14:49:29 INFO spark.MesosScheduler: Parents of final stage: List() 12/09/07 14:49:29 INFO spark.MesosScheduler: Missing parents: List() 12/09/07 14:49:29 INFO spark.MesosScheduler: Submitting Stage 0, which has no missing parents 12/09/07 14:49:29 INFO spark.MesosScheduler: Got a job with 2 tasks 12/09/07 14:49:29 INFO spark.MesosScheduler: Adding job with ID 0 12/09/07 14:49:29 INFO spark.SimpleJob: Starting task 0:0 as TID 0 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:29 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 52 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:29 INFO spark.SimpleJob: Starting task 0:1 as TID 1 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:29 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 0 (task 0:0) 12/09/07 14:49:30 INFO spark.SimpleJob: Starting task 0:0 as TID 2 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:30 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 1 (task 0:1) 12/09/07 14:49:30 INFO spark.SimpleJob: Lost TID 2 (task 0:0) 12/09/07 14:49:30 INFO spark.SimpleJob: Starting task 0:0 as TID 3 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:30 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:1 as TID 4 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 3 (task 0:0) 12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:0 as TID 5 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 4 (task 0:1) 12/09/07 14:49:32 INFO spark.SimpleJob: Lost TID 5 (task 0:0) 12/09/07 14:49:32 INFO spark.SimpleJob: Starting task 0:0 as TID 6 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:32 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 0 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:34 INFO spark.SimpleJob: Starting task 0:1 as TID 7 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:34 INFO spark.SimpleJob: Size of task 0:1 is 1606 bytes and took 2 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:34 INFO spark.SimpleJob: Lost TID 6 (task 0:0) 12/09/07 14:49:34 ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job Exception in thread \"Thread-50\" java.io.EOFException at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2280) at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2749) at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:779) at java.io.ObjectInputStream.<init>(ObjectInputStream.java:279) at spark.JavaSerializerInstance$$anon$2.<init>(JavaSerializer.scala:39) at spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:39) at spark.SimpleJob.taskLost(SimpleJob.scala:296) at spark.SimpleJob.statusUpdate(SimpleJob.scala:207) at spark.MesosScheduler.statusUpdate(MesosScheduler.scala:287) 12/09/07 14:49:34 INFO spark.SimpleJob: Starting task 0:0 as TID 8 on slave 201209071448-846324308-5050-26925-0: shawpc (preferred) 12/09/07 14:49:34 INFO spark.SimpleJob: Size of task 0:0 is 1606 bytes and took 1 ms to serialize by spark.JavaSerializerInstance 12/09/07 14:49:34 INFO spark.SimpleJob: Lost TID 7 (task 0:1) 12/09/07 14:49:34 INFO spark.MesosScheduler: driver.run() returned with code DRIVER_ABORTED ```\n\nComments (3):\n1. Patrick McFadin: Github comment from yshaw: And the error information loged in the mesos slaves like this: ``` java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351) at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366) at java.net.Socket.connect(Socket.java:529) at java.net.Socket.connect(Socket.java:478) at sun.net.NetworkClient.doConnect(NetworkClient.java:163) at sun.net.www.http.HttpClient.openServer(HttpClient.java:388) at sun.net.www.http.HttpClient.openServer(HttpClient.java:523) at sun.net.www.http.HttpClient.<init>(HttpClient.java:227) at sun.net.www.http.HttpClient.New(HttpClient.java:300) at sun.net.www.http.HttpClient.New(HttpClient.java:317) at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:970) at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:911) at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:836) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1172) at java.net.URL.openStream(URL.java:1010) at spark.Executor.spark$Executor$$downloadFile(Executor.scala:161) at spark.Executor$$anonfun$createClassLoader$2.apply(Executor.scala:132) at spark.Executor$$anonfun$createClassLoader$2.apply(Executor.scala:129) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.Executor.createClassLoader(Executor.scala:129) at spark.Executor.registered(Executor.scala:42) Exception in thread \"Thread-0\" ``` And both in the master and the slave, I can't open the 0.0.0.0:8080 or 0.0.0.0:8081 exactly, the error in the web page is : ``` Error 500: Internal Server Error Sorry, the requested URL http://0.0.0.0:8081/ caused an error: Unhandled exception Exception: IOError('socket error', error(111, 'Connection refused')) Traceback: Traceback (most recent call last): File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 499, in handle return handler(**args) File \"/home/shaw/mesos/share/mesos/webui/slave/webui.py\", line 57, in index return template(\"index\", slave_port = slave_port, log_dir = log_dir) File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1796, in template return TEMPLATES[tpl].render(**kwargs) File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1775, in render self.execute(stdout, **args) File \"/home/shaw/mesos/share/mesos/webui/bottle-0.8.3/bottle.py\", line 1763, in execute eval(self.co, env) File \"/home/shaw/mesos/share/mesos/webui/slave/index.tpl\", line 10, in <module> % data = urllib.urlopen(url).read() File \"/usr/lib/python2.7/urllib.py\", line 86, in urlopen return opener.open(url) File \"/usr/lib/python2.7/urllib.py\", line 207, in open return getattr(self, name)(url) File \"/usr/lib/python2.7/urllib.py\", line 344, in open_http h.endheaders(data) File \"/usr/lib/python2.7/httplib.py\", line 954, in endheaders self._send_output(message_body) IOError: [Errno socket error] [Errno 111] Connection refused ```\n2. Patrick McFadin: Imported from Github issue spark-192, originally reported by yshaw\n3. Matthew Farrellee: this should be resolved by a number of fixes in 1.0. please re-open if it still reproduces.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "7ba14cca74c9dd0bd905442a96aa0d91", "issue_key": "SPARK-365", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Don't exit from the Examples since that stops the YARN ApplicationMaster.", "description": "....", "reporter": "Denny Britz", "assignee": null, "created": "0012-09-07T08:58:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks.", "created": "2012-09-07T09:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-193, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-365\nSummary: Don't exit from the Examples since that stops the YARN ApplicationMaster.\nDescription: ....\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Great, thanks.\n2. Patrick McFadin: Imported from Github issue spark-193, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "7df91c0d19f480a7f32d4dff6a795b0c", "issue_key": "SPARK-536", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Set SPARK_MEM based on instance type in EC2 scripts", "description": "Right now it's just 3 GB by default, which is often too small.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-07T11:45:00.000+0000", "updated": "2012-10-22T14:55:33.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-194, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Reynold Clone", "body": "Closing this one since it is done in Spark 0.6 AMI.", "created": "2012-10-19T21:35:40.455+0000"}], "num_comments": 2, "text": "Issue: SPARK-536\nSummary: Set SPARK_MEM based on instance type in EC2 scripts\nDescription: Right now it's just 3 GB by default, which is often too small.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-194, originally reported by mateiz\n2. Reynold Clone: Closing this one since it is done in Spark 0.6 AMI.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "7e7ad94018e0bd485988ae1edc4d0888", "issue_key": "SPARK-364", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark HTTP FileServer", "description": "A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver. - Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing. - Includes test with local-cluster and local mode. - Modified serialization of ShuffleMapTask - I wasn't quite sure about the caching in ShuffleMapTask, is using the hashCode safe? There may be a better way to do this. Also, it doesn't seem like the serializedInfoCache (and fileCache) is cleared except when the SparkContext shuts down? What if it's a really long running task?", "reporter": "Denny Britz", "assignee": null, "created": "0012-09-10T14:58:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from dennybritz: Thinking about it, computing the hash code probably takes a long time as well, should I index the cache with the stageID?", "created": "2012-09-10T15:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Yeah, do index it by shuffleID.", "created": "2012-09-11T13:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good; thanks!", "created": "2012-09-11T16:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-195, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-364\nSummary: Spark HTTP FileServer\nDescription: A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver. - Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing. - Includes test with local-cluster and local mode. - Modified serialization of ShuffleMapTask - I wasn't quite sure about the caching in ShuffleMapTask, is using the hashCode safe? There may be a better way to do this. Also, it doesn't seem like the serializedInfoCache (and fileCache) is cleared except when the SparkContext shuts down? What if it's a really long running task?\n\nComments (4):\n1. Patrick McFadin: Github comment from dennybritz: Thinking about it, computing the hash code probably takes a long time as well, should I index the cache with the stageID?\n2. Patrick McFadin: Github comment from mateiz: Yeah, do index it by shuffleID.\n3. Patrick McFadin: Github comment from mateiz: Looks good; thanks!\n4. Patrick McFadin: Imported from Github issue spark-195, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "2d999cbf639f18fd4e31d8b0a4bf6d84", "issue_key": "SPARK-363", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Log entire exception (including stack trace) in BlockManagerWorker.", "description": "Previously only the error message is logged. This is not super helpful when you see an error...", "reporter": "Reynold Xin", "assignee": null, "created": "0012-09-11T10:32:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: PTAL", "created": "2012-09-11T13:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks.", "created": "2012-09-11T13:41:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-196, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-363\nSummary: Log entire exception (including stack trace) in BlockManagerWorker.\nDescription: Previously only the error message is logged. This is not super helpful when you see an error...\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: PTAL\n2. Patrick McFadin: Github comment from mateiz: Great, thanks.\n3. Patrick McFadin: Imported from Github issue spark-196, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "c236d92e299575032dc39b8637998ca6", "issue_key": "SPARK-362", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adds a \"docs\" directory containing existing Spark documentation and doc build instructions", "description": "This change adds a directory called docs to the root Spark project directory. I used Jekyll as the framework for directory structure and compiling the docs, and used initializr.com for out-of-the-box generic look-and-feel (initializr generates pretty html wrapper leveraging Bootstrap/jquery, modernizr, ...). The Scaladoc API link in the global nav menu doesn't yet work. To build the docs (install and) run `jekyll` and look in docs/_site or run `jekyll --server` and look at localhost:4000", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-09-12T12:20:00.000+0000", "updated": "2012-10-19T22:50:21.000+0000", "resolved": "2012-10-19T22:50:21.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Andy!", "created": "2012-09-12T12:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way, is it possible to change the way this generates links so that they use relative paths? Right now if you open docs/_site/index.html in a browser, none of the links works because they have absolute paths.", "created": "2012-09-12T12:53:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-197, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-362\nSummary: Adds a \"docs\" directory containing existing Spark documentation and doc build instructions\nDescription: This change adds a directory called docs to the root Spark project directory. I used Jekyll as the framework for directory structure and compiling the docs, and used initializr.com for out-of-the-box generic look-and-feel (initializr generates pretty html wrapper leveraging Bootstrap/jquery, modernizr, ...). The Scaladoc API link in the global nav menu doesn't yet work. To build the docs (install and) run `jekyll` and look in docs/_site or run `jekyll --server` and look at localhost:4000\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Thanks Andy!\n2. Patrick McFadin: Github comment from mateiz: By the way, is it possible to change the way this generates links so that they use relative paths? Right now if you open docs/_site/index.html in a browser, none of the links works because they have absolute paths.\n3. Patrick McFadin: Imported from Github issue spark-197, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "77335245de7cd96842557e93dbf9ba4a", "issue_key": "SPARK-361", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fix links and make things a bit prettier.", "description": "Fixed links so that viewing html without running `jekyll --server` works better. Added code syntax highlighting via pygments. Updated the stylesheet a bit to make things a little easier to read. Removed a duplicate doc file (EC2-Scripts.html) which existed because of capitalization weirdness (was a duplicate of ec2-scripts.html).", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-09-12T18:34:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks!", "created": "2012-09-12T18:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-198, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-361\nSummary: Fix links and make things a bit prettier.\nDescription: Fixed links so that viewing html without running `jekyll --server` works better. Added code syntax highlighting via pygments. Updated the stylesheet a bit to make things a little easier to read. Removed a duplicate doc file (EC2-Scripts.html) which existed because of capitalization weirdness (was a duplicate of ec2-scripts.html).\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks!\n2. Patrick McFadin: Imported from Github issue spark-198, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "421430e487d224237ba931f982221c23", "issue_key": "SPARK-360", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "YARN and standalone documentation", "description": "YARN and standalone documentation", "reporter": "Denny Britz", "assignee": null, "created": "0012-09-13T08:48:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from dennybritz: Also, the navbar style was being messed up by the line-height property. I restricted the line-height to the body text.", "created": "2012-09-13T08:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks!", "created": "2012-09-13T09:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-199, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-360\nSummary: YARN and standalone documentation\nDescription: YARN and standalone documentation\n\nComments (3):\n1. Patrick McFadin: Github comment from dennybritz: Also, the navbar style was being messed up by the line-height property. I restricted the line-height to the body text.\n2. Patrick McFadin: Github comment from mateiz: Great, thanks!\n3. Patrick McFadin: Imported from Github issue spark-199, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "a1d8c8ebcacaba584aa2ad05452f5d16", "issue_key": "SPARK-359", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Updates to docs", "description": "You can see what the generated site currently looks like here: http://www.cs.berkeley.edu/~andyk/spark-docs - Added jekyll plugin (in _plugin directory, written in Ruby) that makes sure scaladoc is built and copies over Scaladoc from Spark subprojects into docs directory before generating site. Also updated links to the scaladoc in api.md (which generates api.html) to point to these scaladoc directories. - Make logo look nicer (created a vector copy of it using illustrator magic and then used that to get a small version that looks a lot nicer) - Changed navbar to not collapse when page gets narrower (which was not super helpful for a source code documentation site) but left responsive css in so that the page looks good when the window gets smaller (turning responsive off in bootstrap unfortunately makes things somewhat ugly with narrow windows) - Adds back a file (ec2-scripts.md) that was mistakenly removed in previous commit.", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-09-13T16:29:00.000+0000", "updated": "2015-06-14T00:09:58.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks!", "created": "2012-09-13T17:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-200, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-359\nSummary: Updates to docs\nDescription: You can see what the generated site currently looks like here: http://www.cs.berkeley.edu/~andyk/spark-docs - Added jekyll plugin (in _plugin directory, written in Ruby) that makes sure scaladoc is built and copies over Scaladoc from Spark subprojects into docs directory before generating site. Also updated links to the scaladoc in api.md (which generates api.html) to point to these scaladoc directories. - Make logo look nicer (created a vector copy of it using illustrator magic and then used that to get a small version that looks a lot nicer) - Changed navbar to not collapse when page gets narrower (which was not super helpful for a source code documentation site) but left responsive css in so that the page looks good when the window gets smaller (turning responsive off in bootstrap unfortunately makes things somewhat ugly with narrow windows) - Adds back a file (ec2-scripts.md) that was mistakenly removed in previous commit.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks!\n2. Patrick McFadin: Imported from Github issue spark-200, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "8fec38f17186bbe502c2c6a3e89e07bd", "issue_key": "SPARK-535", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Error with technique to find hostname in bin/start-slaves.sh in dev branch", "description": "I get an error when I run the new shell script to start slaves on my OS X Mountain Lion laptop (named Lucifer): $ bin/start-slaves.sh cd /Users/andyk/Development/spark/bin/.. ; /Users/andyk/Development/spark/bin/spark-daemon.sh start spark.deploy.worker.Worker spark://found::7077 It Looks like the lines getting hostname aren't working for me: ~~~~~~~~~~~~ Lucifer:spark andyk$ hostname=`hostname` Lucifer:spark andyk$ ip=`host \"$hostname\" | cut -d \" \" -f 4` Lucifer:spark andyk$ echo $ip found: Lucifer:spark andyk$ hostname Lucifer.local Lucifer:spark andyk$ host Lucifer.local Host Lucifer.local not found: 3(NXDOMAIN) ~~~~~~~~~~~~", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-09-16T13:11:00.000+0000", "updated": "2012-10-22T15:09:39.000+0000", "resolved": "2012-10-22T15:09:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from andyk: I poked around [1, 2], and one thing that worked for me that is probably somewhat portable but depends on having python is: <code>ip=\"\\`python -c \"import socket;print(socket.gethostbyname(socket.gethostname()))\"\\`\"</code> [1] http://stackoverflow.com/questions/2361709/efficient-way-to-get-your-ip-address-in-shell-scripts [2] http://stackoverflow.com/questions/166506/finding-local-ip-addresses-using-pythons-stdlib", "created": "2012-09-16T13:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I couldn't get this to happen on my Mac, so it might have to do with the way you've asked Mac OS X to set your hostname. Since our scripts are just based on Hadoop's though, my guess is that the same thing would happen for Hadoop, so it can't be super common. Maybe one thing you can try to fix it is to add export SPARK_MASTER_IP=<numeric IP> in your conf/spark-env.sh. Then you will force it to bind to a specific IP.", "created": "2012-09-26T22:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-201, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Reynold fixed this now.", "created": "2012-10-22T15:09:24.231+0000"}], "num_comments": 4, "text": "Issue: SPARK-535\nSummary: Error with technique to find hostname in bin/start-slaves.sh in dev branch\nDescription: I get an error when I run the new shell script to start slaves on my OS X Mountain Lion laptop (named Lucifer): $ bin/start-slaves.sh cd /Users/andyk/Development/spark/bin/.. ; /Users/andyk/Development/spark/bin/spark-daemon.sh start spark.deploy.worker.Worker spark://found::7077 It Looks like the lines getting hostname aren't working for me: ~~~~~~~~~~~~ Lucifer:spark andyk$ hostname=`hostname` Lucifer:spark andyk$ ip=`host \"$hostname\" | cut -d \" \" -f 4` Lucifer:spark andyk$ echo $ip found: Lucifer:spark andyk$ hostname Lucifer.local Lucifer:spark andyk$ host Lucifer.local Host Lucifer.local not found: 3(NXDOMAIN) ~~~~~~~~~~~~\n\nComments (4):\n1. Patrick McFadin: Github comment from andyk: I poked around [1, 2], and one thing that worked for me that is probably somewhat portable but depends on having python is: <code>ip=\"\\`python -c \"import socket;print(socket.gethostbyname(socket.gethostname()))\"\\`\"</code> [1] http://stackoverflow.com/questions/2361709/efficient-way-to-get-your-ip-address-in-shell-scripts [2] http://stackoverflow.com/questions/166506/finding-local-ip-addresses-using-pythons-stdlib\n2. Patrick McFadin: Github comment from mateiz: I couldn't get this to happen on my Mac, so it might have to do with the way you've asked Mac OS X to set your hostname. Since our scripts are just based on Hadoop's though, my guess is that the same thing would happen for Hadoop, so it can't be super common. Maybe one thing you can try to fix it is to add export SPARK_MASTER_IP=<numeric IP> in your conf/spark-env.sh. Then you will force it to bind to a specific IP.\n3. Patrick McFadin: Imported from Github issue spark-201, originally reported by andyk\n4. Matei Alexandru Zaharia: Reynold fixed this now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "2e04fd06f578ea86ea1ab7a6e724ec14", "issue_key": "SPARK-358", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Updates to docs (including nav structure)", "description": "See what the compiled docs look like as of this commit at http://www.cs.berkeley.edu/~andyk/spark-docs-8f7dfcf33 - Rework/expand the nav bar with more of the docs site - Removing parts of docs about EC2 and Mesos that differentiate between running 0.5 and before - Merged subheadings from running-on-amazon-ec2.html that are still relevant (i.e., \"Using a newer version of Spark\" and \"Accessing Data in S3\") into ec2-scripts.html and deleted running-on-amazon-ec2.html - Added some TODO comments to a few docs - Updated the blurb about AMP Camp - Renamed programming-guide to spark-programming-guide - Fixing typos/etc. in Standalone Spark doc - Add docs/api to .gitignore", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-09-16T14:48:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks!", "created": "2012-09-16T16:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-202, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-358\nSummary: Updates to docs (including nav structure)\nDescription: See what the compiled docs look like as of this commit at http://www.cs.berkeley.edu/~andyk/spark-docs-8f7dfcf33 - Rework/expand the nav bar with more of the docs site - Removing parts of docs about EC2 and Mesos that differentiate between running 0.5 and before - Merged subheadings from running-on-amazon-ec2.html that are still relevant (i.e., \"Using a newer version of Spark\" and \"Accessing Data in S3\") into ec2-scripts.html and deleted running-on-amazon-ec2.html - Added some TODO comments to a few docs - Updated the blurb about AMP Camp - Renamed programming-guide to spark-programming-guide - Fixing typos/etc. in Standalone Spark doc - Add docs/api to .gitignore\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks!\n2. Patrick McFadin: Imported from Github issue spark-202, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "1fe50096cddf6bb7aa04379b2670081e", "issue_key": "SPARK-357", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Java Programming Guide", "description": "Adds a Java Programing Guide and fixes a pair of broken links to the Scala Programming Guide.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-09-16T19:51:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks. I've merged this and we can add to it later.", "created": "2012-09-20T16:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-203, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-357\nSummary: Java Programming Guide\nDescription: Adds a Java Programing Guide and fixes a pair of broken links to the Scala Programming Guide.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks. I've merged this and we can add to it later.\n2. Patrick McFadin: Imported from Github issue spark-203, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "b17f8061420d33b43ef3867f8c05c4ec", "issue_key": "SPARK-356", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "When a file is downloaded, make it executable.", "description": "That's neccsary for scripts (e.g. in Shark).", "reporter": "Denny Britz", "assignee": null, "created": "0012-09-17T09:10:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Sounds like a good idea.", "created": "2012-09-17T09:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-204, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-356\nSummary: When a file is downloaded, make it executable.\nDescription: That's neccsary for scripts (e.g. in Shark).\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Sounds like a good idea.\n2. Patrick McFadin: Imported from Github issue spark-204, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "9e4ecb0cf5698459ea1656f4bfd32035", "issue_key": "SPARK-534", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Make SparkContext thread-safe", "description": "SparkEnv (used by SparkContext) is not thread-safe and it causes issues with scala's Futures and parrallel collections. For example, this will not work:  val f = Futures.future({ sc.textFile(\"hdfs://....\") }) f.apply()  Workaround for now:  val f = Futures.future({ SparkEnv.set(sc.env) sc.textFile(\"hdfs://....\") }) f.apply()", "reporter": "tjhunter", "assignee": null, "created": "0012-09-17T09:49:00.000+0000", "updated": "2014-10-18T21:41:08.000+0000", "resolved": "2014-10-18T21:40:51.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-205, originally reported by tjhunter", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Some users have reported problems with the workaround: https://groups.google.com/d/topic/spark-users/iyV6gzrcajM/discussion https://groups.google.com/d/topic/spark-users/ROeSaelkhnY/discussion We should try this ourselves and add a test case that documents the expected behavior.", "created": "2013-02-12T11:41:03.080+0000"}, {"author": "Josh Rosen", "body": "The thread safety problems stem from the fact that SparkEnv is a thread-local. There's a discussion of this at https://groups.google.com/d/msg/spark-developers/GLx8yunSj0A/qAygH3hXyOoJ From Matei: SparkEnv was thread-local mostly because our RDD.compute() object didn't receive it as a parameter. If we make it passed through as part of TaskContext, we should be able to eliminate that requirement. It's something I've been meaning to do eventually, but if someone wants to step in and do it, go ahead.", "created": "2013-05-24T15:57:18.820+0000"}, {"author": "Jason Dai", "body": "We will fix this as some of our use cases will require it.", "created": "2013-05-29T19:16:28.475+0000"}, {"author": "Hans Uhlig", "body": "Specific use case for threadsafe Context. Executing jobs from Tomcat or other web container based on user requests.", "created": "2014-06-25T20:49:15.369+0000"}, {"author": "Josh Rosen", "body": "I'm going to resolve this as \"invalid\" since it's a really old issue and its title / description don't really match up. The issue reported here isn't really thread-safety, it's that older versions of Spark required you to explicitly set SparkEnv before accessing SparkContext from a separate thread. I'm also going to unlink this as a blocker to SPARK-2243, since that issue is for supporting multiple active SparkContexts in the same JVM, whereas this is about concurrent use of the _same_ SparkContext. As far as I know, we've always supported multiple threads submitting jobs to the same shared SparkContext. The specific issue here of having to manually set SparkEnv has been fixed for a while (since at least Spark 1.0, I think).", "created": "2014-10-18T21:40:51.383+0000"}], "num_comments": 6, "text": "Issue: SPARK-534\nSummary: Make SparkContext thread-safe\nDescription: SparkEnv (used by SparkContext) is not thread-safe and it causes issues with scala's Futures and parrallel collections. For example, this will not work:  val f = Futures.future({ sc.textFile(\"hdfs://....\") }) f.apply()  Workaround for now:  val f = Futures.future({ SparkEnv.set(sc.env) sc.textFile(\"hdfs://....\") }) f.apply()\n\nComments (6):\n1. Patrick McFadin: Imported from Github issue spark-205, originally reported by tjhunter\n2. Josh Rosen: Some users have reported problems with the workaround: https://groups.google.com/d/topic/spark-users/iyV6gzrcajM/discussion https://groups.google.com/d/topic/spark-users/ROeSaelkhnY/discussion We should try this ourselves and add a test case that documents the expected behavior.\n3. Josh Rosen: The thread safety problems stem from the fact that SparkEnv is a thread-local. There's a discussion of this at https://groups.google.com/d/msg/spark-developers/GLx8yunSj0A/qAygH3hXyOoJ From Matei: SparkEnv was thread-local mostly because our RDD.compute() object didn't receive it as a parameter. If we make it passed through as part of TaskContext, we should be able to eliminate that requirement. It's something I've been meaning to do eventually, but if someone wants to step in and do it, go ahead.\n4. Jason Dai: We will fix this as some of our use cases will require it.\n5. Hans Uhlig: Specific use case for threadsafe Context. Executing jobs from Tomcat or other web container based on user requests.\n6. Josh Rosen: I'm going to resolve this as \"invalid\" since it's a really old issue and its title / description don't really match up. The issue reported here isn't really thread-safety, it's that older versions of Spark required you to explicitly set SparkEnv before accessing SparkContext from a separate thread. I'm also going to unlink this as a blocker to SPARK-2243, since that issue is for supporting multiple active SparkContexts in the same JVM, whereas this is about concurrent use of the _same_ SparkContext. As far as I know, we've always supported multiple threads submitting jobs to the same shared SparkContext. The specific issue here of having to manually set SparkEnv has been fixed for a while (since at least Spark 1.0, I think).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "f1294811aff0052e61900d78958ae651", "issue_key": "SPARK-355", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "SampledRDD produces incorrect samples when sampling with replacement", "description": "The algorithm for sampling with replacement in SampledRDD is broken. Each split is always guaranteed to get the same number of items in the sample output, so effectively it is performing a sample that is stratified by split. A simple reproducing example: // This part isn't important - it's just a way to get an RDD with 2 splits, ((1, 1)) and ((2, 1)). val rdd = new spark.PairRDDFunctions(sc.parallelize(Seq((1, 1), (2, 1)))) class MyPartitioner extends spark.Partitioner { override def numPartitions = 2 override def getPartition(key: Any) = key.asInstanceOf[Int] % 2 } val twoSplits = rdd.reduceByKey(new MyPartitioner(), (x: Int, y: Int) => x) // Try this as many times as you like - you'll always get ((1, 1), (2, 1)) or ((2, 1), (1, 1)). twoSplits.sample(true, 1, new java.util.Random().nextInt).collect A simple fix is to draw the number of samples for each split from a binomial(splitSize, desiredSampleFraction) distribution. I'd like to fix this along with some other improvements to sampling.", "reporter": "henryem", "assignee": null, "created": "0012-09-17T21:34:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This has been fixed now, though we still need an exact sampling algorithm too.", "created": "2012-09-27T16:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-206, originally reported by henryem", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-355\nSummary: SampledRDD produces incorrect samples when sampling with replacement\nDescription: The algorithm for sampling with replacement in SampledRDD is broken. Each split is always guaranteed to get the same number of items in the sample output, so effectively it is performing a sample that is stratified by split. A simple reproducing example: // This part isn't important - it's just a way to get an RDD with 2 splits, ((1, 1)) and ((2, 1)). val rdd = new spark.PairRDDFunctions(sc.parallelize(Seq((1, 1), (2, 1)))) class MyPartitioner extends spark.Partitioner { override def numPartitions = 2 override def getPartition(key: Any) = key.asInstanceOf[Int] % 2 } val twoSplits = rdd.reduceByKey(new MyPartitioner(), (x: Int, y: Int) => x) // Try this as many times as you like - you'll always get ((1, 1), (2, 1)) or ((2, 1), (1, 1)). twoSplits.sample(true, 1, new java.util.Random().nextInt).collect A simple fix is to draw the number of samples for each split from a binomial(splitSize, desiredSampleFraction) distribution. I'd like to fix this along with some other improvements to sampling.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This has been fixed now, though we still need an exact sampling algorithm too.\n2. Patrick McFadin: Imported from Github issue spark-206, originally reported by henryem", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "b82715b8a7aee3291021d27b70aaf4d0", "issue_key": "SPARK-354", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "RDD.takeSample() produces samples biased toward earlier splits", "description": "Since SampledRDD doesn't support samples of fixed sizes, RDD.takeSample(k) unions together samples until the union's size exceeds size k, then truncates the union to the first k. This truncation step biases the sample toward splits with lower indices. One fix would be to choose k values at random during the truncation step. Really, this should be fixed by implementing a good fixed-size sampling algorithm, which I plan to do soon.", "reporter": "henryem", "assignee": null, "created": "0012-09-17T21:43:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This has been fixed now, though we still need an exact sampling algorithm too.", "created": "2012-09-27T16:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-207, originally reported by henryem", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-354\nSummary: RDD.takeSample() produces samples biased toward earlier splits\nDescription: Since SampledRDD doesn't support samples of fixed sizes, RDD.takeSample(k) unions together samples until the union's size exceeds size k, then truncates the union to the first k. This truncation step biases the sample toward splits with lower indices. One fix would be to choose k values at random during the truncation step. Really, this should be fixed by implementing a good fixed-size sampling algorithm, which I plan to do soon.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This has been fixed now, though we still need an exact sampling algorithm too.\n2. Patrick McFadin: Imported from Github issue spark-207, originally reported by henryem", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "e7a114dfc2ce41441e0fb53bb87f0d4a", "issue_key": "SPARK-353", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Separated ShuffledRDD into multiple classes.", "description": "RepartitionShuffledRDD, ShuffledSortedRDD, and ShuffledAggregatedRDD. Mostly for performance reasons. In the future, we can also add Sort based aggregation to the code.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-09-19T11:38:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Reynold, This looks fine, but just to double-check, do we need new tests to verify all the new RDD subclasses? Or are they covered by the existing operations? Matei", "created": "2012-09-23T17:14:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: They are mostly covered by existing tests if you assume the sort test pretty much covers RepartitionShuffledRDD. Basically there are 3 RDDs now: RepartitionShuffledRDD: Used by ShuffledSortedRDD ShuffledSortedRDD: tested by sortByKey ShuffledAggregatedRDD: tested by anything combineByKey as before.", "created": "2012-09-23T17:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Alright, I've merged it. Let's see if we run into any issues.", "created": "2012-09-24T11:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-208, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-353\nSummary: Separated ShuffledRDD into multiple classes.\nDescription: RepartitionShuffledRDD, ShuffledSortedRDD, and ShuffledAggregatedRDD. Mostly for performance reasons. In the future, we can also add Sort based aggregation to the code.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Hey Reynold, This looks fine, but just to double-check, do we need new tests to verify all the new RDD subclasses? Or are they covered by the existing operations? Matei\n2. Patrick McFadin: Github comment from rxin: They are mostly covered by existing tests if you assume the sort test pretty much covers RepartitionShuffledRDD. Basically there are 3 RDDs now: RepartitionShuffledRDD: Used by ShuffledSortedRDD ShuffledSortedRDD: tested by sortByKey ShuffledAggregatedRDD: tested by anything combineByKey as before.\n3. Patrick McFadin: Github comment from mateiz: Alright, I've merged it. Let's see if we run into any issues.\n4. Patrick McFadin: Imported from Github issue spark-208, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "d518cabfdc4c5b969970e3a38964bde7", "issue_key": "SPARK-533", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Killing tasks in spark - request for comment", "description": "This patch is the first step towards introducing support for killing tasks in Spark. The higher-level goal is to support killing tasks that are speculatively executed or unnecessary (e.g. limit queries in Shark). The plan is to support task killing by interrupting threads that are executing the tasks to either generate an InterruptedException (if the task is waiting on a lock) or set the interrupted status for the thread. As tasks in Spark can be CPU-bound while working on in-memory data, we need to periodically check the interrupt bit from the thread. This is simple for ShuffleMap tasks where the user-defined function is invoked once per element. However as ResultTasks currently provide an RDD iterator to the user-defined-function, we have to change the interface to allow such tasks to be interrupted. This patch shows an example of a new interface `OutputFunction` that makes ResultTasks use functions that can be called for each-element of the RDD. Note: This is not a complete patch and only shows how OutputFunction will look for `collect` and `fold`. It would be great to get comments on the general approach and on the specific semantics used to create OutputFunctions. This work was done jointly with @apanda", "reporter": "Shivaram Venkataraman", "assignee": "Reynold Xin", "created": "0012-09-19T14:18:00.000+0000", "updated": "2013-11-17T14:45:09.000+0000", "resolved": "2013-11-17T14:45:09.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: So Shiv and I discussed this in person today, and we came up with a better proposal. We can define a special RDD that handles the interrupted checking. Let's call it ThreadStatusCheckRDD: ```scala class ThreadStatusCheckRDD(parent: RDD) extends RDD { def compute: Iterator = new Iterator { val threadStopped = false def hasNext = !threadStopped && parent.hasNext def next = { if (Thread.interrupted) { threadStopped = true } parent.next() } } } ``` And then we can inject instances of this RDD in the lineage. We need to be smart about where to inject these RDDs, one example is to inject before a blocking operator (e.g. sort, or assume for any mapPartitions), and at the end of a chain. We did a simple benchmark testing the cost of Thread.interrupted, and it is negligible.", "created": "2012-09-19T20:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-209, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Reynold Xin", "body": "Submitted a pull request at : https://github.com/mesos/spark/pull/935", "created": "2013-09-18T04:33:53.349+0000"}], "num_comments": 3, "text": "Issue: SPARK-533\nSummary: Killing tasks in spark - request for comment\nDescription: This patch is the first step towards introducing support for killing tasks in Spark. The higher-level goal is to support killing tasks that are speculatively executed or unnecessary (e.g. limit queries in Shark). The plan is to support task killing by interrupting threads that are executing the tasks to either generate an InterruptedException (if the task is waiting on a lock) or set the interrupted status for the thread. As tasks in Spark can be CPU-bound while working on in-memory data, we need to periodically check the interrupt bit from the thread. This is simple for ShuffleMap tasks where the user-defined function is invoked once per element. However as ResultTasks currently provide an RDD iterator to the user-defined-function, we have to change the interface to allow such tasks to be interrupted. This patch shows an example of a new interface `OutputFunction` that makes ResultTasks use functions that can be called for each-element of the RDD. Note: This is not a complete patch and only shows how OutputFunction will look for `collect` and `fold`. It would be great to get comments on the general approach and on the specific semantics used to create OutputFunctions. This work was done jointly with @apanda\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: So Shiv and I discussed this in person today, and we came up with a better proposal. We can define a special RDD that handles the interrupted checking. Let's call it ThreadStatusCheckRDD: ```scala class ThreadStatusCheckRDD(parent: RDD) extends RDD { def compute: Iterator = new Iterator { val threadStopped = false def hasNext = !threadStopped && parent.hasNext def next = { if (Thread.interrupted) { threadStopped = true } parent.next() } } } ``` And then we can inject instances of this RDD in the lineage. We need to be smart about where to inject these RDDs, one example is to inject before a blocking operator (e.g. sort, or assume for any mapPartitions), and at the end of a chain. We did a simple benchmark testing the cost of Thread.interrupted, and it is negligible.\n2. Patrick McFadin: Imported from Github issue spark-209, originally reported by shivaram\n3. Reynold Xin: Submitted a pull request at : https://github.com/mesos/spark/pull/935", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "3d725eb83f2a4fc19bb58ac1d1ace43e", "issue_key": "SPARK-352", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Set a limited number of retry in standalone deploy mode.", "description": "It took Denny and me 2 hrs today to figure out I had the wrong scala home set and the test (which uses the standalone cluster) goes into infinite retry and everything just hangs. This commit limits the number of retries allowed.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-09-19T14:44:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Good idea! Just ran into this with someone else actually.", "created": "2012-09-20T16:34:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-210, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-352\nSummary: Set a limited number of retry in standalone deploy mode.\nDescription: It took Denny and me 2 hrs today to figure out I had the wrong scala home set and the test (which uses the standalone cluster) goes into infinite retry and everything just hangs. This commit limits the number of retries allowed.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Good idea! Just ran into this with someone else actually.\n2. Patrick McFadin: Imported from Github issue spark-210, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "c88bd28bbc610345c116dd1d35f97136", "issue_key": "SPARK-351", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "run", "description": "", "reporter": "vince67", "assignee": null, "created": "0012-09-20T01:07:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-211, originally reported by vince67", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-351\nSummary: run\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-211, originally reported by vince67", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "8b52afa0ca1d4980383ec129c874bdec", "issue_key": "SPARK-350", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Log where in the user's code each RDD got created", "description": "Possible by just getting a stack trace of the current thread.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-20T16:08:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from ankurdave: Here's the solution currently in the debugger: https://github.com/ankurdave/spark/blob/dev/core/src/main/scala/spark/RDD.scala#L78", "created": "2012-09-20T19:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This is now fixed! Thanks to pull #225.", "created": "2012-09-28T18:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-213, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-350\nSummary: Log where in the user's code each RDD got created\nDescription: Possible by just getting a stack trace of the current thread.\n\nComments (3):\n1. Patrick McFadin: Github comment from ankurdave: Here's the solution currently in the debugger: https://github.com/ankurdave/spark/blob/dev/core/src/main/scala/spark/RDD.scala#L78\n2. Patrick McFadin: Github comment from mateiz: This is now fixed! Thanks to pull #225.\n3. Patrick McFadin: Imported from Github issue spark-213, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "3a3ba25f73d32d3103ccefb611c45abc", "issue_key": "SPARK-532", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Log something when no resources have been offered in the cluster", "description": "This is a behavior that's commonly confusing -- your job seems to hang because it didn't get offered any worker nodes, or it got them offered but they had too little RAM.", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick McFadin", "created": "0012-09-20T16:08:00.000+0000", "updated": "2013-03-05T16:11:24.000+0000", "resolved": "2013-03-05T16:11:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-212, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-532\nSummary: Log something when no resources have been offered in the cluster\nDescription: This is a behavior that's commonly confusing -- your job seems to hang because it didn't get offered any worker nodes, or it got them offered but they had too little RAM.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-212, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "a4140e629ff1b6dcb3e985a91964134d", "issue_key": "SPARK-349", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "The --ebs-vol-size option doesn't work with the current EC2 AMI", "description": "Seems to be because it already has a /vol directory on the image.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-20T16:32:00.000+0000", "updated": "2012-10-22T14:55:30.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tdas: To add more information, the only the Spark AMI in us-east-1 has this problem. The us-west-1 AMI (ami-17f4d052) does not have this problem.", "created": "2012-09-20T18:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This is now fixed.", "created": "2012-09-24T15:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-214, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-349\nSummary: The --ebs-vol-size option doesn't work with the current EC2 AMI\nDescription: Seems to be because it already has a /vol directory on the image.\n\nComments (3):\n1. Patrick McFadin: Github comment from tdas: To add more information, the only the Spark AMI in us-east-1 has this problem. The us-west-1 AMI (ami-17f4d052) does not have this problem.\n2. Patrick McFadin: Github comment from mateiz: This is now fixed.\n3. Patrick McFadin: Imported from Github issue spark-214, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "705698625ff6ac1802485b2e187cd970", "issue_key": "SPARK-348", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "HTTP File server fixes", "description": "The server returned the local file path instead of the URL, thus it was working when testing locally, but not on a cluster.", "reporter": "Denny Britz", "assignee": null, "created": "0012-09-21T10:02:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for the fix.", "created": "2012-09-21T16:39:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-215, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-348\nSummary: HTTP File server fixes\nDescription: The server returned the local file path instead of the URL, thus it was working when testing locally, but not on a cluster.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks for the fix.\n2. Patrick McFadin: Imported from Github issue spark-215, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "945e3e498f24c765d7f8fc23e381629d", "issue_key": "SPARK-347", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Logs from \"run\" disappear once you run the tests because test-classes are now on classpath", "description": "This is a slightly awkward side-effect of trying to run the distributed tests -- it places the test version of log4j.properties on the classpath.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-22T21:51:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: On a related note, we should make the tests redirect all log4j output to a file, instead of leaving only the errors on stdout. People get confused by the errors and think tests failed.", "created": "2012-10-01T09:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-216, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-347\nSummary: Logs from \"run\" disappear once you run the tests because test-classes are now on classpath\nDescription: This is a slightly awkward side-effect of trying to run the distributed tests -- it places the test version of log4j.properties on the classpath.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: On a related note, we should make the tests redirect all log4j output to a file, instead of leaving only the errors on stdout. People get confused by the errors and think tests failed.\n2. Patrick McFadin: Imported from Github issue spark-216, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "a093901d1faf8cea21e7356291140560", "issue_key": "SPARK-346", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added a method to RDD to expose the ClassManifest.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-09-24T15:57:00.000+0000", "updated": "2012-10-19T22:50:25.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Minor thing.. rename it to something like elementManifest to make it more obscure. It would've also been nice to make it private[spark] but I guess your Shark code is not in the Spark package, right?", "created": "2012-09-24T20:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Renamed it to elementClassManifest. PTAL.", "created": "2012-09-24T22:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: OK, thanks.", "created": "2012-09-24T22:52:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-217, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-346\nSummary: Added a method to RDD to expose the ClassManifest.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Minor thing.. rename it to something like elementManifest to make it more obscure. It would've also been nice to make it private[spark] but I guess your Shark code is not in the Spark package, right?\n2. Patrick McFadin: Github comment from rxin: Renamed it to elementClassManifest. PTAL.\n3. Patrick McFadin: Github comment from mateiz: OK, thanks.\n4. Patrick McFadin: Imported from Github issue spark-217, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "fa84a9d16d81e86c7bb6eb11117b3819", "issue_key": "SPARK-345", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Scripts to start Spark under windows", "description": "Windows Command Shell versions of run and sbt scripts", "reporter": "rnpandya", "assignee": null, "created": "0012-09-24T19:36:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Awesome, thanks Ravi!", "created": "2012-09-24T20:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-218, originally reported by rnpandya", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-345\nSummary: Scripts to start Spark under windows\nDescription: Windows Command Shell versions of run and sbt scripts\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Awesome, thanks Ravi!\n2. Patrick McFadin: Imported from Github issue spark-218, originally reported by rnpandya", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "5d868fb344293fb3758524c4432ecddc", "issue_key": "SPARK-344", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add spark-shell.cmd", "description": "", "reporter": "rnpandya", "assignee": null, "created": "0012-09-25T06:32:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Ravi!", "created": "2012-09-25T08:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-219, originally reported by rnpandya", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-344\nSummary: Add spark-shell.cmd\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Ravi!\n2. Patrick McFadin: Imported from Github issue spark-219, originally reported by rnpandya", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "618e4b830b325384790686d6dda8f0d8", "issue_key": "SPARK-343", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "One commit that makes nav dropdowns show on hover", "description": "One commit that makes nav dropdowns show on hover.", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-09-25T14:45:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-220, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-343\nSummary: One commit that makes nav dropdowns show on hover\nDescription: One commit that makes nav dropdowns show on hover.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-220, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "7101754f7996be72bf94e7dfc7c2860a", "issue_key": "SPARK-342", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Web UI should display memory in a nicer format", "description": "Can use Utils.memoryBytesToString for this.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-25T22:54:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Fixed now.", "created": "2012-09-26T22:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-221, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-342\nSummary: Web UI should display memory in a nicer format\nDescription: Can use Utils.memoryBytesToString for this.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Fixed now.\n2. Patrick McFadin: Imported from Github issue spark-221, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "debd7173882db228fe060112aa5a9b8c", "issue_key": "SPARK-341", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "Added MapPartitionsWithSplitRDD.", "description": "", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "0012-09-26T16:12:00.000+0000", "updated": "2019-05-28T05:28:21.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Awesome, thanks! One small thing I might change after is to rename this to just mapPartitions, since the compiler should be able to infer the type of function passed based on the number of arguments it expects.", "created": "2012-09-26T22:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Talked to Matei offline - it is best to keep the names separate because if they are the same name, scala compiler can be confused by situations like rdd.mapPartitions(myfunction) When the arguments are not specified to myfunction, it cannot reliably infer the type (whether it is a 2 arg closure or a 1 arg closure).", "created": "2012-09-26T22:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-222, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-341\nSummary: Added MapPartitionsWithSplitRDD.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Awesome, thanks! One small thing I might change after is to rename this to just mapPartitions, since the compiler should be able to infer the type of function passed based on the number of arguments it expects.\n2. Patrick McFadin: Github comment from rxin: Talked to Matei offline - it is best to keep the names separate because if they are the same name, scala compiler can be confused by situations like rdd.mapPartitions(myfunction) When the arguments are not specified to myfunction, it cannot reliably infer the type (whether it is a 2 arg closure or a 1 arg closure).\n3. Patrick McFadin: Imported from Github issue spark-222, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "3092f821619d10a9cf0b6beed86f4e02", "issue_key": "SPARK-340", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Rename StorageLevels to something easier to remember", "description": "Something like this: MEMORY_ONLY -> deserialized, in-memory MEMORY_ONLY_SER -> serialized, in-memory MEMORY_AND_DISK -> deserialized, spills to disk MEMORY_AND_DISK_SER -> serialized, spills to disk", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-26T22:32:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from jakajancar: To me, e.g. `.persist(disk=true, serialize=true)` would be even easier to remember (I don't see much point in named StorageLevels with different setting permutations).", "created": "2012-09-30T09:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-223, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-340\nSummary: Rename StorageLevels to something easier to remember\nDescription: Something like this: MEMORY_ONLY -> deserialized, in-memory MEMORY_ONLY_SER -> serialized, in-memory MEMORY_AND_DISK -> deserialized, spills to disk MEMORY_AND_DISK_SER -> serialized, spills to disk\n\nComments (2):\n1. Patrick McFadin: Github comment from jakajancar: To me, e.g. `.persist(disk=true, serialize=true)` would be even easier to remember (I don't see much point in named StorageLevels with different setting permutations).\n2. Patrick McFadin: Imported from Github issue spark-223, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "8f2001306d492e936fe632b23da2a65a", "issue_key": "SPARK-531", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Create more explicit logs for cache registration and task completion", "description": "The logs are pretty confusing when you are getting started with spark: RegisterRDD(0,1) ResultTask(0,0) These parenthetical indices are used in both cases with very different meanings. I think similar patterns may also be used elsewhere, but I'm not sure. Without increasing verbosity too much, I propose just having: RegisterRDD(id=0, partitions=1) ResultTask(stage=0, partition=0)", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-09-28T12:43:00.000+0000", "updated": "2013-01-24T13:09:36.000+0000", "resolved": "2013-01-24T13:09:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-224, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "This is old, deleting it.", "created": "2013-01-24T13:09:36.576+0000"}], "num_comments": 2, "text": "Issue: SPARK-531\nSummary: Create more explicit logs for cache registration and task completion\nDescription: The logs are pretty confusing when you are getting started with spark: RegisterRDD(0,1) ResultTask(0,0) These parenthetical indices are used in both cases with very different meanings. I think similar patterns may also be used elsewhere, but I'm not sure. Without increasing verbosity too much, I propose just having: RegisterRDD(id=0, partitions=1) ResultTask(stage=0, partition=0)\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-224, originally reported by pwendell\n2. Patrick McFadin: This is old, deleting it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "a4d098c394406da3ccab6912b001c3f2", "issue_key": "SPARK-339", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Log message which records RDD origin", "description": "This adds tracking to determine the \"origin\" of an RDD. Origin is defined by the boundary between the user's code and the spark code, during an RDD's instantiation. It is meant to help users understand where a Spark RDD is coming from in their code. This patch also logs origin data when stages are submitted to the scheduler. Finally, it adds a new log message to fix an inconsitency in the way that dependent stages (those missing parents) and independent stages (those without) are logged during submission.", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-09-28T14:52:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for the fixes!", "created": "2012-09-28T15:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-225, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-339\nSummary: Log message which records RDD origin\nDescription: This adds tracking to determine the \"origin\" of an RDD. Origin is defined by the boundary between the user's code and the spark code, during an RDD's instantiation. It is meant to help users understand where a Spark RDD is coming from in their code. This patch also logs origin data when stages are submitted to the scheduler. Finally, it adds a new log message to fix an inconsitency in the way that dependent stages (those missing parents) and independent stages (those without) are logged during submission.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks for the fixes!\n2. Patrick McFadin: Imported from Github issue spark-225, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "5ef0073701bc34eced38aa67f6632ee8", "issue_key": "SPARK-338", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add a CoalescedRDD for decreasing the number of partitions in a map phase", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-28T18:04:00.000+0000", "updated": "2013-05-20T20:00:02.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This has been done.", "created": "2012-10-06T22:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-226, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-338\nSummary: Add a CoalescedRDD for decreasing the number of partitions in a map phase\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This has been done.\n2. Patrick McFadin: Imported from Github issue spark-226, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "58a07bb86c32b6690c101b4d10bf4846", "issue_key": "SPARK-337", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Allow controlling number of splits in distinct().", "description": "Also documents `distinct()` in the Scala programming guide.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-09-28T22:47:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Alright, thanks!", "created": "2012-09-28T22:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-227, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-337\nSummary: Allow controlling number of splits in distinct().\nDescription: Also documents `distinct()` in the Scala programming guide.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Alright, thanks!\n2. Patrick McFadin: Imported from Github issue spark-227, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "107ff6793792493dc126ff40c7e9fa89", "issue_key": "SPARK-336", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added mapPartitionsWithSplit to the programming guide.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-09-29T00:32:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks!", "created": "2012-09-29T20:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-228, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-336\nSummary: Added mapPartitionsWithSplit to the programming guide.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks!\n2. Patrick McFadin: Imported from Github issue spark-228, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "57fcaf4ec48ca738b1e7d13626aa30b3", "issue_key": "SPARK-530", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark driver process doesn't exit after finishing", "description": "My driver program does not seem to exit after running. If I place System.exit(0) at the end of my main function, it exits. This is on the dev branch.", "reporter": "Jaka Jancar", "assignee": null, "created": "0012-09-29T04:50:00.000+0000", "updated": "2013-01-22T23:28:36.000+0000", "resolved": "2013-01-22T23:28:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks for mentioning this, I'll look into it. Does calling sc.stop() instead of System.exit(0) stop it by any chance?", "created": "2012-09-30T21:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from jakajancar: Yes, it does: 12/10/01 09:00:27 INFO analyzer.App: Done! 12/10/01 09:00:27 INFO mesos.MesosSchedulerBackend: driver.run() returned with code DRIVER_STOPPED 12/10/01 09:00:27 INFO spark.MapOutputTrackerActor: MapOutputTrackerActor stopped! 12/10/01 09:00:27 INFO spark.CacheTrackerActor: Stopping CacheTrackerActor 12/10/01 09:00:27 INFO network.ConnectionManager: Selector selected 0 of 1 keys 12/10/01 09:00:27 INFO network.ConnectionManager: Selector thread was interrupted! 12/10/01 09:00:27 INFO network.ConnectionManager: ConnectionManager stopped 12/10/01 09:00:27 INFO storage.MemoryStore: MemoryStore cleared 12/10/01 09:00:27 INFO storage.MemoryStore: Shutting down block dropper 12/10/01 09:00:27 INFO storage.BlockManager: BlockManager stopped 12/10/01 09:00:27 INFO storage.BlockManagerMasterActor: Stopping BlockManagerMaster 12/10/01 09:00:27 INFO storage.BlockManagerMaster: BlockManagerMaster stopped 12/10/01 09:00:27 INFO spark.SparkContext: Successfully stopped SparkContext (The first line is the \"last\" in my code. 0.5 would exit after that, 0.6 just hangs after it)", "created": "2012-10-01T01:06:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-229, originally reported by jakajancar", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Closing this because I believe it was fixed in 0.6 (the report is from before that).", "created": "2013-01-22T23:28:36.993+0000"}], "num_comments": 4, "text": "Issue: SPARK-530\nSummary: Spark driver process doesn't exit after finishing\nDescription: My driver program does not seem to exit after running. If I place System.exit(0) at the end of my main function, it exits. This is on the dev branch.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Thanks for mentioning this, I'll look into it. Does calling sc.stop() instead of System.exit(0) stop it by any chance?\n2. Patrick McFadin: Github comment from jakajancar: Yes, it does: 12/10/01 09:00:27 INFO analyzer.App: Done! 12/10/01 09:00:27 INFO mesos.MesosSchedulerBackend: driver.run() returned with code DRIVER_STOPPED 12/10/01 09:00:27 INFO spark.MapOutputTrackerActor: MapOutputTrackerActor stopped! 12/10/01 09:00:27 INFO spark.CacheTrackerActor: Stopping CacheTrackerActor 12/10/01 09:00:27 INFO network.ConnectionManager: Selector selected 0 of 1 keys 12/10/01 09:00:27 INFO network.ConnectionManager: Selector thread was interrupted! 12/10/01 09:00:27 INFO network.ConnectionManager: ConnectionManager stopped 12/10/01 09:00:27 INFO storage.MemoryStore: MemoryStore cleared 12/10/01 09:00:27 INFO storage.MemoryStore: Shutting down block dropper 12/10/01 09:00:27 INFO storage.BlockManager: BlockManager stopped 12/10/01 09:00:27 INFO storage.BlockManagerMasterActor: Stopping BlockManagerMaster 12/10/01 09:00:27 INFO storage.BlockManagerMaster: BlockManagerMaster stopped 12/10/01 09:00:27 INFO spark.SparkContext: Successfully stopped SparkContext (The first line is the \"last\" in my code. 0.5 would exit after that, 0.6 just hangs after it)\n3. Patrick McFadin: Imported from Github issue spark-229, originally reported by jakajancar\n4. Matei Alexandru Zaharia: Closing this because I believe it was fixed in 0.6 (the report is from before that).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "a52fbf42347a8085534ee633327db96e", "issue_key": "SPARK-335", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "SizeEstimator gives different sizes for Strings on Java 7", "description": "This is probably due to the fields of the String class changing somehow in Java 7. We should probably just not have tests with String, or maybe check for the JVM version too. Right now the current tests fail.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-09-30T21:01:00.000+0000", "updated": "2012-10-22T14:55:34.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from shivaram: Is this on 32-bit or 64-bit arch ? I'll try to take a look at this sometime today/tomorrow on openjdk7", "created": "2012-10-02T09:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: It was 64-bit, but it doesn't really matter because our tests set the arch and compressed oops deterministically anyway.", "created": "2012-10-02T10:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: FWIW, I am not able to reproduce the failure on the dev branch with jdk-7 on debian java version \"1.7.0_03\" OpenJDK Runtime Environment (IcedTea7 2.1.2) (7u3-2.1.2-2) OpenJDK 64-Bit Server VM (build 22.0-b10, mixed mode)", "created": "2012-10-02T11:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: So this was the environment: Scala version 2.9.2 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_07) Maybe one difference is that it was Oracle Java?", "created": "2012-10-04T14:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: I can reproduce the failure with the same Oracle Java version and digging deeper this is an unfortunate case of the String class being modified in the Oracle JDK. Details: The String.java file in open jdk has fields value, offset, count and hash. http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/lang/String.java However the Oracle version gets rid of the offset and count fields. I can see this in the source for the java classes shipped along with the JDK (http://www.oracle.com/technetwork/java/javase/jdk-7-readme-429198.html - Source Code Unfortunately I can't find an online version of this String.java to link to). In terms of solving this problem: Whenever we are estimating the size of classes defined outside our control, I don't think its possible to get a good test case. Testing the dummy classes works well as those are under our control. Strings are the only foreign class we test right now and we could remove those tests. One workaround could be to define a local String-like class and use that for testing. Thoughts ?", "created": "2012-10-04T16:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Can't we just add an \"OR\" condition to test string? I think it is important to have this test, and having it pass on sun + openjdk 1.6 / 1.7 are good enough.", "created": "2012-10-04T16:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I think we could keep one test with String, and maybe make the failure say (\"are you maybe not on OpenJDK or Oracle?\"). However, right now a lot of other tests use Strings, so we should replace those.", "created": "2012-10-04T16:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: Looks like longer term this should land on OpenJDK as well (at 7u6, I was using openjdk-7u3 http://mail.openjdk.java.net/pipermail/core-libs-dev/2012-May/010257.html). I like Reynold's idea of checking for both conditions. Will create a patch soon.", "created": "2012-10-04T16:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Cool, thanks!", "created": "2012-10-04T16:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-230, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 10, "text": "Issue: SPARK-335\nSummary: SizeEstimator gives different sizes for Strings on Java 7\nDescription: This is probably due to the fields of the String class changing somehow in Java 7. We should probably just not have tests with String, or maybe check for the JVM version too. Right now the current tests fail.\n\nComments (10):\n1. Patrick McFadin: Github comment from shivaram: Is this on 32-bit or 64-bit arch ? I'll try to take a look at this sometime today/tomorrow on openjdk7\n2. Patrick McFadin: Github comment from mateiz: It was 64-bit, but it doesn't really matter because our tests set the arch and compressed oops deterministically anyway.\n3. Patrick McFadin: Github comment from shivaram: FWIW, I am not able to reproduce the failure on the dev branch with jdk-7 on debian java version \"1.7.0_03\" OpenJDK Runtime Environment (IcedTea7 2.1.2) (7u3-2.1.2-2) OpenJDK 64-Bit Server VM (build 22.0-b10, mixed mode)\n4. Patrick McFadin: Github comment from mateiz: So this was the environment: Scala version 2.9.2 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_07) Maybe one difference is that it was Oracle Java?\n5. Patrick McFadin: Github comment from shivaram: I can reproduce the failure with the same Oracle Java version and digging deeper this is an unfortunate case of the String class being modified in the Oracle JDK. Details: The String.java file in open jdk has fields value, offset, count and hash. http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/lang/String.java However the Oracle version gets rid of the offset and count fields. I can see this in the source for the java classes shipped along with the JDK (http://www.oracle.com/technetwork/java/javase/jdk-7-readme-429198.html - Source Code Unfortunately I can't find an online version of this String.java to link to). In terms of solving this problem: Whenever we are estimating the size of classes defined outside our control, I don't think its possible to get a good test case. Testing the dummy classes works well as those are under our control. Strings are the only foreign class we test right now and we could remove those tests. One workaround could be to define a local String-like class and use that for testing. Thoughts ?\n6. Patrick McFadin: Github comment from rxin: Can't we just add an \"OR\" condition to test string? I think it is important to have this test, and having it pass on sun + openjdk 1.6 / 1.7 are good enough.\n7. Patrick McFadin: Github comment from mateiz: I think we could keep one test with String, and maybe make the failure say (\"are you maybe not on OpenJDK or Oracle?\"). However, right now a lot of other tests use Strings, so we should replace those.\n8. Patrick McFadin: Github comment from shivaram: Looks like longer term this should land on OpenJDK as well (at 7u6, I was using openjdk-7u3 http://mail.openjdk.java.net/pipermail/core-libs-dev/2012-May/010257.html). I like Reynold's idea of checking for both conditions. Will create a patch soon.\n9. Patrick McFadin: Github comment from mateiz: Cool, thanks!\n10. Patrick McFadin: Imported from Github issue spark-230, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "5c295711e53d4b98fdfe7845858cd375", "issue_key": "SPARK-334", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added a new command \"pl\" in sbt to publish to both Maven and Ivy.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-09-30T23:18:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Awesome, thanks!", "created": "2012-10-01T09:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-231, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-334\nSummary: Added a new command \"pl\" in sbt to publish to both Maven and Ivy.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Awesome, thanks!\n2. Patrick McFadin: Imported from Github issue spark-231, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "2191a69f2fde34c3931d1997b4227a73", "issue_key": "SPARK-333", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "0.6: NPE in spark.storage.BlockManager", "description": "I'm getting the following (it's seems to happen only when I'm working with little data): 12/10/01 16:10:32 ERROR local.LocalScheduler: Exception in task 0 java.lang.NullPointerException at spark.storage.BlockManager$.dispose(BlockManager.scala:709) at spark.util.ByteBufferInputStream.cleanUp(ByteBufferInputStream.scala:58) at spark.util.ByteBufferInputStream.read(ByteBufferInputStream.scala:30) at com.ning.compress.lzf.LZFDecoder.readHeader(LZFDecoder.java:363) at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:176) at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254) at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:111) at spark.ZigZag$.readInt(KryoSerializer.scala:57) at spark.KryoDeserializationStream.readObject(KryoSerializer.scala:91) at spark.DeserializationStream$$anon$1.getNext(Serializer.scala:84) at spark.DeserializationStream$$anon$1.hasNext(Serializer.scala:94) at spark.RDD$$anonfun$count$1.apply(RDD.scala:272) at spark.RDD$$anonfun$count$1.apply(RDD.scala:270) at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) at spark.scheduler.ResultTask.run(ResultTask.scala:18) at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74) at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at spark.DaemonThread.run(DaemonThreadFactory.scala:15) Exception in thread \"main\" spark.SparkException: Job failed: ResultTask(0, 0) failed: ExceptionFailure(java.lang.NullPointerException) at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:528) at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:526) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:526) at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:496) at spark.scheduler.DAGScheduler.run(DAGScheduler.scala:268) at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:89)", "reporter": "Jaka Jancar", "assignee": null, "created": "0012-10-01T12:12:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: Also seeing this once I upgraded to the latest dev. I believe this problem did not exist a few days ago. Must be a very recent commit. -- Reynold Xin AMPLab, UC Berkeley http://www.cs.berkeley.edu/~rxin/ On Mon, Oct 1, 2012 at 1:12 PM, Jaka Jančar <notifications@github.com>wrote: > I'm getting the following (it's seems to happen only when I'm working with > little data): > > 12/10/01 16:10:32 ERROR local.LocalScheduler: Exception in task 0 > java.lang.NullPointerException > at spark.storage.BlockManager$.dispose(BlockManager.scala:709) > at spark.util.ByteBufferInputStream.cleanUp(ByteBufferInputStream.scala:58) > at spark.util.ByteBufferInputStream.read(ByteBufferInputStream.scala:30) > at com.ning.compress.lzf.LZFDecoder.readHeader(LZFDecoder.java:363) > at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:176) > at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254) > at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:111) > at spark.ZigZag$.readInt(KryoSerializer.scala:57) > at spark.KryoDeserializationStream.readObject(KryoSerializer.scala:91) > at spark.DeserializationStream$$anon$1.getNext(Serializer.scala:84) > at spark.DeserializationStream$$anon$1.hasNext(Serializer.scala:94) > at spark.RDD$$anonfun$count$1.apply(RDD.scala:272) > at spark.RDD$$anonfun$count$1.apply(RDD.scala:270) > at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) > at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) > at spark.scheduler.ResultTask.run(ResultTask.scala:18) > at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74) > at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50) > at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) > at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) > at java.util.concurrent.FutureTask.run(FutureTask.java:166) > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) > at spark.DaemonThread.run(DaemonThreadFactory.scala:15) > Exception in thread \"main\" spark.SparkException: Job failed: ResultTask(0, 0) failed: ExceptionFailure(java.lang.NullPointerException) > at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:528) > at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:526) > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) > at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:526) > at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:496) > at spark.scheduler.DAGScheduler.run(DAGScheduler.scala:268) > at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:89) > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/232>. > >", "created": "2012-10-01T12:16:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: I've isolated the problem to the following commit: logcommit 9b326d01e9a9ec4a4a9abf293cf039c07d426293Author: Matei Zaharia <matei@eecs.berkeley.edu>Date: Sat Sep 29 20:21:54 2012 -0700 Made BlockManager unmap memory-mapped files when necessary to reduce the number of open files. Also optimized sending of disk-based blocks. commit 56dcad593641ef8de211fcb4303574a9f4509f89 The problem is in BlockManager.scala dispose, the cleaner is null. def dispose(buffer: ByteBuffer) { if (buffer != null && buffer.isInstanceOf[MappedByteBuffer]) { logDebug(\"Unmapping \" + buffer) buffer.asInstanceOf[DirectBuffer].cleaner().clean() } }", "created": "2012-10-01T12:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Submitted a patch in pull request #233.", "created": "2012-10-01T13:09:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-232, originally reported by jakajancar", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-333\nSummary: 0.6: NPE in spark.storage.BlockManager\nDescription: I'm getting the following (it's seems to happen only when I'm working with little data): 12/10/01 16:10:32 ERROR local.LocalScheduler: Exception in task 0 java.lang.NullPointerException at spark.storage.BlockManager$.dispose(BlockManager.scala:709) at spark.util.ByteBufferInputStream.cleanUp(ByteBufferInputStream.scala:58) at spark.util.ByteBufferInputStream.read(ByteBufferInputStream.scala:30) at com.ning.compress.lzf.LZFDecoder.readHeader(LZFDecoder.java:363) at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:176) at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254) at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:111) at spark.ZigZag$.readInt(KryoSerializer.scala:57) at spark.KryoDeserializationStream.readObject(KryoSerializer.scala:91) at spark.DeserializationStream$$anon$1.getNext(Serializer.scala:84) at spark.DeserializationStream$$anon$1.hasNext(Serializer.scala:94) at spark.RDD$$anonfun$count$1.apply(RDD.scala:272) at spark.RDD$$anonfun$count$1.apply(RDD.scala:270) at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) at spark.scheduler.ResultTask.run(ResultTask.scala:18) at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74) at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at spark.DaemonThread.run(DaemonThreadFactory.scala:15) Exception in thread \"main\" spark.SparkException: Job failed: ResultTask(0, 0) failed: ExceptionFailure(java.lang.NullPointerException) at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:528) at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:526) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:526) at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:496) at spark.scheduler.DAGScheduler.run(DAGScheduler.scala:268) at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:89)\n\nComments (4):\n1. Patrick McFadin: Github comment from rxin: Also seeing this once I upgraded to the latest dev. I believe this problem did not exist a few days ago. Must be a very recent commit. -- Reynold Xin AMPLab, UC Berkeley http://www.cs.berkeley.edu/~rxin/ On Mon, Oct 1, 2012 at 1:12 PM, Jaka Jančar <notifications@github.com>wrote: > I'm getting the following (it's seems to happen only when I'm working with > little data): > > 12/10/01 16:10:32 ERROR local.LocalScheduler: Exception in task 0 > java.lang.NullPointerException > at spark.storage.BlockManager$.dispose(BlockManager.scala:709) > at spark.util.ByteBufferInputStream.cleanUp(ByteBufferInputStream.scala:58) > at spark.util.ByteBufferInputStream.read(ByteBufferInputStream.scala:30) > at com.ning.compress.lzf.LZFDecoder.readHeader(LZFDecoder.java:363) > at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:176) > at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254) > at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:111) > at spark.ZigZag$.readInt(KryoSerializer.scala:57) > at spark.KryoDeserializationStream.readObject(KryoSerializer.scala:91) > at spark.DeserializationStream$$anon$1.getNext(Serializer.scala:84) > at spark.DeserializationStream$$anon$1.hasNext(Serializer.scala:94) > at spark.RDD$$anonfun$count$1.apply(RDD.scala:272) > at spark.RDD$$anonfun$count$1.apply(RDD.scala:270) > at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) > at spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:426) > at spark.scheduler.ResultTask.run(ResultTask.scala:18) > at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74) > at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50) > at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) > at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) > at java.util.concurrent.FutureTask.run(FutureTask.java:166) > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) > at spark.DaemonThread.run(DaemonThreadFactory.scala:15) > Exception in thread \"main\" spark.SparkException: Job failed: ResultTask(0, 0) failed: ExceptionFailure(java.lang.NullPointerException) > at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:528) > at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:526) > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) > at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:526) > at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:496) > at spark.scheduler.DAGScheduler.run(DAGScheduler.scala:268) > at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:89) > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/232>. > >\n2. Patrick McFadin: Github comment from rxin: I've isolated the problem to the following commit: logcommit 9b326d01e9a9ec4a4a9abf293cf039c07d426293Author: Matei Zaharia <matei@eecs.berkeley.edu>Date: Sat Sep 29 20:21:54 2012 -0700 Made BlockManager unmap memory-mapped files when necessary to reduce the number of open files. Also optimized sending of disk-based blocks. commit 56dcad593641ef8de211fcb4303574a9f4509f89 The problem is in BlockManager.scala dispose, the cleaner is null. def dispose(buffer: ByteBuffer) { if (buffer != null && buffer.isInstanceOf[MappedByteBuffer]) { logDebug(\"Unmapping \" + buffer) buffer.asInstanceOf[DirectBuffer].cleaner().clean() } }\n3. Patrick McFadin: Github comment from rxin: Submitted a patch in pull request #233.\n4. Patrick McFadin: Imported from Github issue spark-232, originally reported by jakajancar", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "080f30c63e711686042fc9a787e6d2d3", "issue_key": "SPARK-332", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fixed #232: DirectBuffer's cleaner was empty and Spark tried to invoke clean on it.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-01T13:08:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Reynold! I actually saw that check in some of the samples I read but never saw it happen on my box.", "created": "2012-10-01T13:31:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-233, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-332\nSummary: Fixed #232: DirectBuffer's cleaner was empty and Spark tried to invoke clean on it.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Reynold! I actually saw that check in some of the samples I read but never saw it happen on my box.\n2. Patrick McFadin: Imported from Github issue spark-233, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "b648f99cde81b60ec446d9402db8954c", "issue_key": "SPARK-331", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Publish local maven", "description": "This just over-writes the default behavior of publish-local rather than having a separate \"pl\" target. It worked for me when I ran from a clean build.", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-01T14:40:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-234, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-331\nSummary: Publish local maven\nDescription: This just over-writes the default behavior of publish-local rather than having a separate \"pl\" target. It worked for me when I ran from a clean build.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-234, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "667f0e505414c3dd099ca36bda939852", "issue_key": "SPARK-330", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "publish-local should go to maven + ivy by default", "description": "This makes publishing to ivy and maven the default behavior. Worked for me on a clean build.", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-01T14:44:00.000+0000", "updated": "2012-10-19T22:50:35.000+0000", "resolved": "2012-10-19T22:50:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks.", "created": "2012-10-01T14:49:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from RayRacine: FWIW, I don't see why the default should be to publish everything twice. For project builds SBT allows one to specify whether a particular dependency is M2 or Ivy. No particular need to double publish locally. Not arguing against having the build system support double local publishing, just making it the default behavior. I am not aware of any other SBT / Scala project that follows this approach.", "created": "2012-10-02T06:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Are you worried that the build will just take too long? I think that in that case we should do Maven only; the problem is that more tools seem to be aware of Maven than Ivy. But I'm not sure how easy it is to change the rules in sbt so that publish-local means Maven only. We'd probably have to give it a different name (e.g. maven-local).", "created": "2012-10-02T10:50:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pwendell: I think the ideal setup would be maven-local ivy-local publish-local (goes to both) With regards to publish-local going to both: For people who have expectations based on other projects, I don't see that they are particularly harmed by having it publish to maven as well - it takes only negligible time. For people who do not have expectations from other projects, I think it's most intuitive that publish-local would publish to any local repo's that are publish-able. This project supports a Java API, so we expect to have a lot of people building for the purpose of hooking into maven, including people who aren't active developers on several scala projects. - Patrick On Tue, Oct 2, 2012 at 11:50 AM, Matei Zaharia <notifications@github.com>wrote: > Are you worried that the build will just take too long? I think that in > that case we should do Maven only; the problem is that more tools seem to > be aware of Maven than Ivy. But I'm not sure how easy it is to change the > rules in sbt so that publish-local means Maven only. We'd probably have to > give it a different name (e.g. maven-local). > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/235#issuecomment-9082276>. > >", "created": "2012-10-02T10:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Patrick is best to respond to this, but I think the extra time is measured in seconds (just a few seconds) .... -- Reynold Xin AMPLab, UC Berkeley http://www.cs.berkeley.edu/~rxin/ On Tue, Oct 2, 2012 at 11:50 AM, Matei Zaharia <notifications@github.com>wrote: > Are you worried that the build will just take too long? I think that in > that case we should do Maven only; the problem is that more tools seem to > be aware of Maven than Ivy. But I'm not sure how easy it is to change the > rules in sbt so that publish-local means Maven only. We'd probably have to > give it a different name (e.g. maven-local). > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/235#issuecomment-9082276>. > >", "created": "2012-10-02T10:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from RayRacine: Sorry, guys. Did not intend to generate a tempest in a teapot on this one. Certainly there is a tacit expectation (principle of least surprise) that a standard Scala project built with SBT upon execution of publish-local will Ivy publish locally at a minimum. I don't think that should change, which it has not. Extending the behavior to double publish on the surface may affect those who already customize their local-publish command (which I do). In my case the double publishing had no deleterious impact. (Changing publish-local to maven only would have.) After that the question comes down to time and space considerations. In my case for example, I routinely disable ScalaDoc and Source(?) packaging in publish local to save time in my development cycle. All-in-all, this is a minor concern. I'd have left publish-local to default behavior and extended behavior with publish-local-maven. It's a one-liner to put back single Ivy local publishing and I've already made that change in my Spark clone. So all is well. I withdraw my concerns.", "created": "2012-10-02T11:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-235, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-330\nSummary: publish-local should go to maven + ivy by default\nDescription: This makes publishing to ivy and maven the default behavior. Worked for me on a clean build.\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: Great, thanks.\n2. Patrick McFadin: Github comment from RayRacine: FWIW, I don't see why the default should be to publish everything twice. For project builds SBT allows one to specify whether a particular dependency is M2 or Ivy. No particular need to double publish locally. Not arguing against having the build system support double local publishing, just making it the default behavior. I am not aware of any other SBT / Scala project that follows this approach.\n3. Patrick McFadin: Github comment from mateiz: Are you worried that the build will just take too long? I think that in that case we should do Maven only; the problem is that more tools seem to be aware of Maven than Ivy. But I'm not sure how easy it is to change the rules in sbt so that publish-local means Maven only. We'd probably have to give it a different name (e.g. maven-local).\n4. Patrick McFadin: Github comment from pwendell: I think the ideal setup would be maven-local ivy-local publish-local (goes to both) With regards to publish-local going to both: For people who have expectations based on other projects, I don't see that they are particularly harmed by having it publish to maven as well - it takes only negligible time. For people who do not have expectations from other projects, I think it's most intuitive that publish-local would publish to any local repo's that are publish-able. This project supports a Java API, so we expect to have a lot of people building for the purpose of hooking into maven, including people who aren't active developers on several scala projects. - Patrick On Tue, Oct 2, 2012 at 11:50 AM, Matei Zaharia <notifications@github.com>wrote: > Are you worried that the build will just take too long? I think that in > that case we should do Maven only; the problem is that more tools seem to > be aware of Maven than Ivy. But I'm not sure how easy it is to change the > rules in sbt so that publish-local means Maven only. We'd probably have to > give it a different name (e.g. maven-local). > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/235#issuecomment-9082276>. > >\n5. Patrick McFadin: Github comment from rxin: Patrick is best to respond to this, but I think the extra time is measured in seconds (just a few seconds) .... -- Reynold Xin AMPLab, UC Berkeley http://www.cs.berkeley.edu/~rxin/ On Tue, Oct 2, 2012 at 11:50 AM, Matei Zaharia <notifications@github.com>wrote: > Are you worried that the build will just take too long? I think that in > that case we should do Maven only; the problem is that more tools seem to > be aware of Maven than Ivy. But I'm not sure how easy it is to change the > rules in sbt so that publish-local means Maven only. We'd probably have to > give it a different name (e.g. maven-local). > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/235#issuecomment-9082276>. > >\n6. Patrick McFadin: Github comment from RayRacine: Sorry, guys. Did not intend to generate a tempest in a teapot on this one. Certainly there is a tacit expectation (principle of least surprise) that a standard Scala project built with SBT upon execution of publish-local will Ivy publish locally at a minimum. I don't think that should change, which it has not. Extending the behavior to double publish on the surface may affect those who already customize their local-publish command (which I do). In my case the double publishing had no deleterious impact. (Changing publish-local to maven only would have.) After that the question comes down to time and space considerations. In my case for example, I routinely disable ScalaDoc and Source(?) packaging in publish local to save time in my development cycle. All-in-all, this is a minor concern. I'd have left publish-local to default behavior and extended behavior with publish-local-maven. It's a one-liner to put back single Ivy local publishing and I've already made that change in my Spark clone. So all is well. I withdraw my concerns.\n7. Patrick McFadin: Imported from Github issue spark-235, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "52760f0d495a75975e3e3edb95814c12", "issue_key": "SPARK-329", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "A Spark \"Quick Start\" example", "description": "This commit includes a quick start example that covers: 1) Basic usage of the Spark shell 2) A simple Spark job in Scala 3) A simple Spark job in Java", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-01T16:24:00.000+0000", "updated": "2012-10-19T22:50:33.000+0000", "resolved": "2012-10-19T22:50:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks pretty good overall! A few comments though: * For the standalone jobs, show them how to package their job into a JAR and pass that to SparkContext. Don't use the two-arguent SparkContext constructor. In fact it might be good to end the guide with a section called \"running on a cluster\" that points them to the standalone cluster doc and tells them to try running the job against that. * At the beginning, include a line on how to compile Spark (it's just sbt package) * Link to the programming guide when describing the operations. It might also be good to add an example of reduceByKey that does a word count, without fully explaining it, and say \"look in the programming guide for details of these operations\". * Separate the \"A Spark Job\" into two top-level sections: Writing a Standalone Scala Job and Writing a Standalone Java Job", "created": "2012-10-02T17:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pwendell: Hey Matei, I want to make sure I understand the first point, since I haven't used that constructor yet. The job would look the same as it does now, except that it would pass a reference to a packaged jar file (containing the job itself) to the constructor. We'd walk through pacaking the jar file in a way that's consistent with the name passed in the constructor. - Patrick On Tue, Oct 2, 2012 at 6:15 PM, Matei Zaharia <notifications@github.com>wrote: > This looks pretty good overall! A few comments though: > > - > > For the standalone jobs, show them how to package their job into a JAR > and pass that to SparkContext. Don't use the two-arguent SparkContext > constructor. In fact it might be good to end the guide with a section > called \"running on a cluster\" that points them to the standalone cluster > doc and tells them to try running the job against that. > - > > At the beginning, include a line on how to compile Spark (it's just > sbt package) > - > > Link to the programming guide when describing the operations. It might > also be good to add an example of reduceByKey that does a word count, > without fully explaining it, and say \"look in the programming guide for > details of these operations\". > - > > Separate the \"A Spark Job\" into two top-level sections: Writing a > Standalone Scala Job and Writing a Standalone Java Job > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/236#issuecomment-9092831>. > >", "created": "2012-10-02T19:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Yes, exactly. Look at the part on Linking with Spark in the programing guide.", "created": "2012-10-02T20:19:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pwendell: This latest commit includes substantial changes based on Matei's feedback. It addresses all 4 comments.", "created": "2012-10-02T22:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Patrick! Might do a few minor edits to this but it looks good.", "created": "2012-10-03T07:31:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pwendell: Sounds good. In your court now. On Wed, Oct 3, 2012 at 8:31 AM, Matei Zaharia <notifications@github.com>wrote: > Thanks Patrick! Might do a few minor edits to this but it looks good. > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/236#issuecomment-9110467>. > >", "created": "2012-10-03T07:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-236, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-329\nSummary: A Spark \"Quick Start\" example\nDescription: This commit includes a quick start example that covers: 1) Basic usage of the Spark shell 2) A simple Spark job in Scala 3) A simple Spark job in Java\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: This looks pretty good overall! A few comments though: * For the standalone jobs, show them how to package their job into a JAR and pass that to SparkContext. Don't use the two-arguent SparkContext constructor. In fact it might be good to end the guide with a section called \"running on a cluster\" that points them to the standalone cluster doc and tells them to try running the job against that. * At the beginning, include a line on how to compile Spark (it's just sbt package) * Link to the programming guide when describing the operations. It might also be good to add an example of reduceByKey that does a word count, without fully explaining it, and say \"look in the programming guide for details of these operations\". * Separate the \"A Spark Job\" into two top-level sections: Writing a Standalone Scala Job and Writing a Standalone Java Job\n2. Patrick McFadin: Github comment from pwendell: Hey Matei, I want to make sure I understand the first point, since I haven't used that constructor yet. The job would look the same as it does now, except that it would pass a reference to a packaged jar file (containing the job itself) to the constructor. We'd walk through pacaking the jar file in a way that's consistent with the name passed in the constructor. - Patrick On Tue, Oct 2, 2012 at 6:15 PM, Matei Zaharia <notifications@github.com>wrote: > This looks pretty good overall! A few comments though: > > - > > For the standalone jobs, show them how to package their job into a JAR > and pass that to SparkContext. Don't use the two-arguent SparkContext > constructor. In fact it might be good to end the guide with a section > called \"running on a cluster\" that points them to the standalone cluster > doc and tells them to try running the job against that. > - > > At the beginning, include a line on how to compile Spark (it's just > sbt package) > - > > Link to the programming guide when describing the operations. It might > also be good to add an example of reduceByKey that does a word count, > without fully explaining it, and say \"look in the programming guide for > details of these operations\". > - > > Separate the \"A Spark Job\" into two top-level sections: Writing a > Standalone Scala Job and Writing a Standalone Java Job > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/236#issuecomment-9092831>. > >\n3. Patrick McFadin: Github comment from mateiz: Yes, exactly. Look at the part on Linking with Spark in the programing guide.\n4. Patrick McFadin: Github comment from pwendell: This latest commit includes substantial changes based on Matei's feedback. It addresses all 4 comments.\n5. Patrick McFadin: Github comment from mateiz: Thanks Patrick! Might do a few minor edits to this but it looks good.\n6. Patrick McFadin: Github comment from pwendell: Sounds good. In your court now. On Wed, Oct 3, 2012 at 8:31 AM, Matei Zaharia <notifications@github.com>wrote: > Thanks Patrick! Might do a few minor edits to this but it looks good. > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/pull/236#issuecomment-9110467>. > >\n7. Patrick McFadin: Imported from Github issue spark-236, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "8b3e5928a4bbcfe11de5782b9e3c232a", "issue_key": "SPARK-328", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Don't build spark-repl-assembly in the assembly target by default", "description": "I think we needed it for Shark a while back, but it should not be necessary if we make Shark find Spark through Maven instead. Also it takes a while to build.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-10-02T10:56:00.000+0000", "updated": "2012-10-22T14:55:35.000+0000", "resolved": "2012-10-19T22:50:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is now fixed.", "created": "2012-10-04T14:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-237, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-328\nSummary: Don't build spark-repl-assembly in the assembly target by default\nDescription: I think we needed it for Shark a while back, but it should not be necessary if we make Shark find Spark through Maven instead. Also it takes a while to build.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is now fixed.\n2. Patrick McFadin: Imported from Github issue spark-237, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "1b88a86e12668c60a9da7643f759173d", "issue_key": "SPARK-327", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Allow whitespaces in cluster URL configuration for local cluster.", "description": "E.g. allow me to say (note the whitespace) export MASTER=\"local-cluster[2, 1, 512]\"", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-02T10:58:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from rxin: I piggybacked another commit into this. The other one checks to make sure SPARK_MEM <= memoryPerSlave for local cluster mode.", "created": "2012-10-02T14:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Good idea, thanks.", "created": "2012-10-02T15:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-238, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-327\nSummary: Allow whitespaces in cluster URL configuration for local cluster.\nDescription: E.g. allow me to say (note the whitespace) export MASTER=\"local-cluster[2, 1, 512]\"\n\nComments (3):\n1. Patrick McFadin: Github comment from rxin: I piggybacked another commit into this. The other one checks to make sure SPARK_MEM <= memoryPerSlave for local cluster mode.\n2. Patrick McFadin: Github comment from mateiz: Good idea, thanks.\n3. Patrick McFadin: Imported from Github issue spark-238, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "923ab00d1152c40d48b368fe3ea40b06", "issue_key": "SPARK-529", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Have a single file that controls the environmental variables and spark config options", "description": "E.g. multiple places in the code base uses SPARK_MEM and has its own default set to 512. We need a central place to enforce default values as well as documenting the variables.", "reporter": "Reynold Xin", "assignee": "Marcelo Masiero Vanzin", "created": "0012-10-02T14:41:00.000+0000", "updated": "2016-04-05T22:21:01.000+0000", "resolved": "2016-04-05T22:21:01.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-239, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Sean R. Owen", "body": "This looks obsolete and/or fixed, as variables like SPARK_MEM are deprecated, and I suppose there is spark-env.sh too.", "created": "2014-09-04T08:54:08.730+0000"}, {"author": "Marcelo Masiero Vanzin", "body": "I'm reopening this since I believe it's a worthy addition; in fact, Spark SQL already has something similar, and I'm just refactoring that code a little for use in the other modules. (It's not a single file per-se, but the spirit is the same - one location where a particular config option is defined - name, type and default value).", "created": "2015-12-30T23:01:27.702+0000"}, {"author": "Apache Spark", "body": "User 'vanzin' has created a pull request for this issue: https://github.com/apache/spark/pull/10205", "created": "2015-12-31T04:02:07.471+0000"}, {"author": "Apache Spark", "body": "User 'vanzin' has created a pull request for this issue: https://github.com/apache/spark/pull/11570", "created": "2016-03-08T01:57:04.123+0000"}, {"author": "Marcelo Masiero Vanzin", "body": "The new infrastructure is in place, although not all configs are currently using it. Those can be dealt with separately.", "created": "2016-04-05T22:21:01.097+0000"}], "num_comments": 6, "text": "Issue: SPARK-529\nSummary: Have a single file that controls the environmental variables and spark config options\nDescription: E.g. multiple places in the code base uses SPARK_MEM and has its own default set to 512. We need a central place to enforce default values as well as documenting the variables.\n\nComments (6):\n1. Patrick McFadin: Imported from Github issue spark-239, originally reported by rxin\n2. Sean R. Owen: This looks obsolete and/or fixed, as variables like SPARK_MEM are deprecated, and I suppose there is spark-env.sh too.\n3. Marcelo Masiero Vanzin: I'm reopening this since I believe it's a worthy addition; in fact, Spark SQL already has something similar, and I'm just refactoring that code a little for use in the other modules. (It's not a single file per-se, but the spirit is the same - one location where a particular config option is defined - name, type and default value).\n4. Apache Spark: User 'vanzin' has created a pull request for this issue: https://github.com/apache/spark/pull/10205\n5. Apache Spark: User 'vanzin' has created a pull request for this issue: https://github.com/apache/spark/pull/11570\n6. Marcelo Masiero Vanzin: The new infrastructure is in place, although not all configs are currently using it. Those can be dealt with separately.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "19efc9d45dba216dd45b045350c515c0", "issue_key": "SPARK-326", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Package-Private Classes", "description": "Made all core classes except for *RDD and what's used by Shark package private. Haven't yet figured out how to make the HTML templates for the standalone UI private. I'll do the same for the other subprojects, just wanted to get comments.", "reporter": "Denny Britz", "assignee": null, "created": "0012-10-02T18:06:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Good start, but the following should *not* be private: * Accumulable, AccumulableParam, Accumulator, AccumulatorParam * Broadcast", "created": "2012-10-02T18:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Also PairRDDFunctions, DoubleRDDFunctions, etc (all the implicit conversions).", "created": "2012-10-02T18:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way, there's no need to do the same in repl and examples. We are not providing ScalaDoc for those. Maybe do it for internal classes in Bagel, but there aren't any of those AFAIK.", "created": "2012-10-02T18:14:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: One last thing: as a matter of style, maybe put the private[spark] on a separate line above the class name when the class name is long or has many parameters. Also, no need to include it for inner classes in a class that is already private[spark].", "created": "2012-10-02T18:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Made the changes. I thought I didn't mark any inner classes, at least not purposefully.", "created": "2012-10-02T18:31:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: Denny - can you check if Shark still compiles with this?", "created": "2012-10-02T18:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Yes, Shark compiles. There are some classes (like OneToOneDependency) that I left purposefully public for Shark even though they probably shouldn't be.", "created": "2012-10-02T18:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: What about the Java API's classes? The abstract traits `WrappedFunction1` and `WrappedFunction2` should probably be private, since they aren't part of the user-facing API. I think that `JavaRDDLike` should be private. It doesn't really function like a proper RDD interface/base class since some common methods like `filter()` and `cache()` aren't defined in it, so leaving it public could be confusing. This situation is due to a weird interaction between a Scala compiler bug and Java, which prevents me from completely following the [`TraversableLike` approach](http://docs.scala-lang.org/overviews/core/architecture-of-scala-collections.html) and having abstract methods that return RDDs of the instance's type.", "created": "2012-10-02T18:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Yeah, I wasn't sure about those. Seems like making them private is the right thing to do then.", "created": "2012-10-02T18:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from JoshRosen: On the other hand, `JavaRDDLike` might be useful when implementing other language APIs on top of the Java API; removing it would force you to have variables of type `Object` to hold Java RDDs of any type. Both approaches would require casting, but `JavaRDDLike` provides slightly more type safety. I suppose a third option would be to define an empty trait like `JavaRDDBase` for the sole purpose of being a superclass/interface for the Java RDDs.", "created": "2012-10-02T18:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Yeah, that makes sense. I left JavaRDDLike public for now.", "created": "2012-10-02T19:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good. I think we should leave the Dependency stuff public in case people want to subclass RDD themselves. We might instead make some of the RDD subclasses private (e.g. MappedRDD) but let's leave that for later. I'll merge this in now.", "created": "2012-10-02T20:22:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I just made a few more things package-private, so try Shark again. Doubt it's affected though. I left all the shuffle RDDs Reynold added in.", "created": "2012-10-02T21:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-240, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 14, "text": "Issue: SPARK-326\nSummary: Package-Private Classes\nDescription: Made all core classes except for *RDD and what's used by Shark package private. Haven't yet figured out how to make the HTML templates for the standalone UI private. I'll do the same for the other subprojects, just wanted to get comments.\n\nComments (14):\n1. Patrick McFadin: Github comment from mateiz: Good start, but the following should *not* be private: * Accumulable, AccumulableParam, Accumulator, AccumulatorParam * Broadcast\n2. Patrick McFadin: Github comment from mateiz: Also PairRDDFunctions, DoubleRDDFunctions, etc (all the implicit conversions).\n3. Patrick McFadin: Github comment from mateiz: By the way, there's no need to do the same in repl and examples. We are not providing ScalaDoc for those. Maybe do it for internal classes in Bagel, but there aren't any of those AFAIK.\n4. Patrick McFadin: Github comment from mateiz: One last thing: as a matter of style, maybe put the private[spark] on a separate line above the class name when the class name is long or has many parameters. Also, no need to include it for inner classes in a class that is already private[spark].\n5. Patrick McFadin: Github comment from dennybritz: Made the changes. I thought I didn't mark any inner classes, at least not purposefully.\n6. Patrick McFadin: Github comment from rxin: Denny - can you check if Shark still compiles with this?\n7. Patrick McFadin: Github comment from dennybritz: Yes, Shark compiles. There are some classes (like OneToOneDependency) that I left purposefully public for Shark even though they probably shouldn't be.\n8. Patrick McFadin: Github comment from JoshRosen: What about the Java API's classes? The abstract traits `WrappedFunction1` and `WrappedFunction2` should probably be private, since they aren't part of the user-facing API. I think that `JavaRDDLike` should be private. It doesn't really function like a proper RDD interface/base class since some common methods like `filter()` and `cache()` aren't defined in it, so leaving it public could be confusing. This situation is due to a weird interaction between a Scala compiler bug and Java, which prevents me from completely following the [`TraversableLike` approach](http://docs.scala-lang.org/overviews/core/architecture-of-scala-collections.html) and having abstract methods that return RDDs of the instance's type.\n9. Patrick McFadin: Github comment from dennybritz: Yeah, I wasn't sure about those. Seems like making them private is the right thing to do then.\n10. Patrick McFadin: Github comment from JoshRosen: On the other hand, `JavaRDDLike` might be useful when implementing other language APIs on top of the Java API; removing it would force you to have variables of type `Object` to hold Java RDDs of any type. Both approaches would require casting, but `JavaRDDLike` provides slightly more type safety. I suppose a third option would be to define an empty trait like `JavaRDDBase` for the sole purpose of being a superclass/interface for the Java RDDs.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "b6ea34d31596ff0a9bc78bafc2e93148", "issue_key": "SPARK-325", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "First cut at adding documentation for GC tuning", "description": "Created new sub-headings for cache-size tuning and GC tuning.", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-10-02T19:11:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Shivaram!", "created": "2012-10-02T20:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-241, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-325\nSummary: First cut at adding documentation for GC tuning\nDescription: Created new sub-headings for cache-size tuning and GC tuning.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Shivaram!\n2. Patrick McFadin: Imported from Github issue spark-241, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "72eb3a422efe32da81787d8a06e4bb0b", "issue_key": "SPARK-324", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Changing version of Scala in README", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-02T21:11:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Good catch!", "created": "2012-10-02T21:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-242, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-324\nSummary: Changing version of Scala in README\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Good catch!\n2. Patrick McFadin: Imported from Github issue spark-242, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "eede0de2b451e480c655601c67100cfe", "issue_key": "SPARK-323", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Removes the included mesos-0.9.0.jar and pulls it from Maven Central instead", "description": "Removes the included mesos-0.9.0.jar and adds a libraryDependency to the build file so that mesos-0.9.0-incubating.jar (which contains the same class files, but has a silightly different name) will be pulled down from Maven Central instead.", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-10-03T08:04:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Sweet! Thanks Andy.", "created": "2012-10-03T10:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-243, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-323\nSummary: Removes the included mesos-0.9.0.jar and pulls it from Maven Central instead\nDescription: Removes the included mesos-0.9.0.jar and adds a libraryDependency to the build file so that mesos-0.9.0-incubating.jar (which contains the same class files, but has a silightly different name) will be pulled down from Maven Central instead.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Sweet! Thanks Andy.\n2. Patrick McFadin: Imported from Github issue spark-243, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "77f8ba52c0bb84c5922ddd9a43685669", "issue_key": "SPARK-322", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Made Serializer and JavaSerializer non private.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-03T09:21:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-244, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-322\nSummary: Made Serializer and JavaSerializer non private.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-244, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "311f3b2658028163a6f2c308d401eb0e", "issue_key": "SPARK-321", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Some additions to the Tuning Guide.", "description": "1. Slight change in organization 2. Added pre-requisites 3. Made a new section about determining memory footprint of an RDD 4. Other small changes", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-03T13:08:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-245, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-321\nSummary: Some additions to the Tuning Guide.\nDescription: 1. Slight change in organization 2. Added pre-requisites 3. Made a new section about determining memory footprint of an RDD 4. Other small changes\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-245, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "0d97399d8e9a5732e2f6b5d1e31b1a6e", "issue_key": "SPARK-320", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fix SizeEstimator tests to work with String classes in JDK 6 and 7", "description": "Fixes issue #230", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-10-04T18:44:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Shivaram!", "created": "2012-10-05T09:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-246, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-320\nSummary: Fix SizeEstimator tests to work with String classes in JDK 6 and 7\nDescription: Fixes issue #230\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Shivaram!\n2. Patrick McFadin: Imported from Github issue spark-246, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "ded882374d6ad3829c6ae3cce4a93e4c", "issue_key": "SPARK-319", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Dev", "description": "This is just a repeat of https://github.com/mesos/spark/pull/173 that merge request didn't make it into the dev branch", "reporter": "squito", "assignee": null, "created": "0012-10-05T07:39:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Oh shoot, didn't realize that this didn't make it into dev.", "created": "2012-10-05T09:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-247, originally reported by squito", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-319\nSummary: Dev\nDescription: This is just a repeat of https://github.com/mesos/spark/pull/173 that merge request didn't make it into the dev branch\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Oh shoot, didn't realize that this didn't make it into dev.\n2. Patrick McFadin: Imported from Github issue spark-247, originally reported by squito", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "586a127ad4a27621f89d718dcae62661", "issue_key": "SPARK-528", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Provide a dist-like target that builds a binary distribution (JARs + scripts)", "description": "So that Spark can be deployed without copying the whole built source folder.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-10-05T12:23:00.000+0000", "updated": "2014-09-16T17:45:30.000+0000", "resolved": "2014-09-16T17:45:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-248, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Sean R. Owen", "body": "Browsing old issues: I think this is long since fixed, along with https://issues.apache.org/jira/browse/SPARK-547 , because the build produces assembly JARs?", "created": "2014-09-11T08:56:30.164+0000"}, {"author": "Patrick Wendell", "body": "Yeah thanks this was fixed a long time ago.", "created": "2014-09-16T17:45:30.938+0000"}], "num_comments": 3, "text": "Issue: SPARK-528\nSummary: Provide a dist-like target that builds a binary distribution (JARs + scripts)\nDescription: So that Spark can be deployed without copying the whole built source folder.\n\nComments (3):\n1. Patrick McFadin: Imported from Github issue spark-248, originally reported by mateiz\n2. Sean R. Owen: Browsing old issues: I think this is long since fixed, along with https://issues.apache.org/jira/browse/SPARK-547 , because the build produces assembly JARs?\n3. Patrick Wendell: Yeah thanks this was fixed a long time ago.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "ef40341d7b8219f3b3ed0e658cc1038a", "issue_key": "SPARK-318", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Move RDD classes/files to their own package/directory", "description": "Refactors RDD classes/files to their own package/directory. All RDD files except for RDD.scala are now in src/main/scala/spark/rdd. Also fixes a bug in AccumulatorSuite.scala that was causing the test named \"localValue readable in tasks\" to fail.", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-10-05T19:08:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Awesome, thanks!", "created": "2012-10-05T21:00:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way just FYI, your Eclipse seemed to have added one tab, so you might want to change it to spaces only. I'll fix that in another commit.", "created": "2012-10-05T21:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-249, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-318\nSummary: Move RDD classes/files to their own package/directory\nDescription: Refactors RDD classes/files to their own package/directory. All RDD files except for RDD.scala are now in src/main/scala/spark/rdd. Also fixes a bug in AccumulatorSuite.scala that was causing the test named \"localValue readable in tasks\" to fail.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Awesome, thanks!\n2. Patrick McFadin: Github comment from mateiz: By the way just FYI, your Eclipse seemed to have added one tab, so you might want to change it to spaces only. I'll fix that in another commit.\n3. Patrick McFadin: Imported from Github issue spark-249, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "bde3ef9a493299d6ecda8c8f7c892940", "issue_key": "SPARK-317", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fixed a bug in addFile that if the file is specified as \"file:///\", the symlink is created incorrectly for local mode.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-06T23:55:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks.", "created": "2012-10-06T23:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-250, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-317\nSummary: Fixed a bug in addFile that if the file is specified as \"file:///\", the symlink is created incorrectly for local mode.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, thanks.\n2. Patrick McFadin: Imported from Github issue spark-250, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "28c4b23d01074f859ac07b4ce9afc459", "issue_key": "SPARK-316", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Document Dependency classes and make minor interface improvements", "description": "These commits add API documentation for the `Dependency` classes and make two interface changes: - Remove the unused `isShuffle` field from `Dependency` - Change the type of `ShuffleDependency.aggregator` to an `Option` to communicate that the aggregator is optional. Currently, the absence of an aggregator is expressed using `Aggregator[K, V, V](null, null, null, false)`, which is a bit of a hack. Will these two changes introduce backwards-compatibility problems in the API?", "reporter": "Josh Rosen", "assignee": null, "created": "0012-10-07T00:19:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Cool, thanks!", "created": "2012-10-07T08:56:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I'm okay changing the RDD internal API for now, though this might affect Shark. We will lock it down more when we get to a version 1.0.", "created": "2012-10-07T08:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-251, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-316\nSummary: Document Dependency classes and make minor interface improvements\nDescription: These commits add API documentation for the `Dependency` classes and make two interface changes: - Remove the unused `isShuffle` field from `Dependency` - Change the type of `ShuffleDependency.aggregator` to an `Option` to communicate that the aggregator is optional. Currently, the absence of an aggregator is expressed using `Aggregator[K, V, V](null, null, null, false)`, which is a bit of a hack. Will these two changes introduce backwards-compatibility problems in the API?\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Cool, thanks!\n2. Patrick McFadin: Github comment from mateiz: I'm okay changing the RDD internal API for now, though this might affect Shark. We will lock it down more when we get to a version 1.0.\n3. Patrick McFadin: Imported from Github issue spark-251, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "d66374c25820421b774606c618d5582b", "issue_key": "SPARK-315", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adding Sonatype releases to SBT.", "description": "This does a few things to get this branch ready for release: 1. Upgrades the sbt and Scala version 2. Sets the release number to 0.5.1 3. Adds the Sonatype publishing target 4. Installs the PGP signing plugin 5. Removes the Mesos jar dependency", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-07T00:21:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, but let me edit a few other files that point to Scala 2.9.1 before publishing. I'll let you know when I'm done.", "created": "2012-10-07T08:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-252, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-315\nSummary: Adding Sonatype releases to SBT.\nDescription: This does a few things to get this branch ready for release: 1. Upgrades the sbt and Scala version 2. Sets the release number to 0.5.1 3. Adds the Sonatype publishing target 4. Installs the PGP signing plugin 5. Removes the Mesos jar dependency\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Looks good, but let me edit a few other files that point to Scala 2.9.1 before publishing. I'll let you know when I'm done.\n2. Patrick McFadin: Imported from Github issue spark-252, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "894d58d5b69e647b154f73ee95e0ceec", "issue_key": "SPARK-314", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Changed the println to logInfo in Utils.fetchFile.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-07T00:54:00.000+0000", "updated": "2012-10-19T22:50:39.000+0000", "resolved": "2012-10-19T22:50:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-253, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-314\nSummary: Changed the println to logInfo in Utils.fetchFile.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-253, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "eb6074d306c1fa9233245883e0becce1", "issue_key": "SPARK-313", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Removing one link in quickstart", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-08T07:55:00.000+0000", "updated": "2012-10-19T22:50:38.000+0000", "resolved": "2012-10-19T22:50:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-254, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-313\nSummary: Removing one link in quickstart\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-254, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "9737b1b5554643a621d49ab3c0c591bb", "issue_key": "SPARK-312", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Removes the annoying small gap above the nav menu dropdown boxes", "description": "Fixes the small gap above the nav menu dropdown boxes and the hoverable menu items that was causing the dropdowns to go away when the user moved their mouse down towards them.", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-10-08T08:35:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks!", "created": "2012-10-08T08:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-255, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-312\nSummary: Removes the annoying small gap above the nav menu dropdown boxes\nDescription: Fixes the small gap above the nav menu dropdown boxes and the hoverable menu items that was causing the dropdowns to go away when the user moved their mouse down towards them.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Great, thanks!\n2. Patrick McFadin: Imported from Github issue spark-255, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "7858b6738cabc9eb735d2e1888d536f9", "issue_key": "SPARK-311", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adding new download instructions", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-08T08:44:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from pwendell: I updated this just now to reflect the templated versions. I also changed the wording a bit.", "created": "2012-10-08T15:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks but I think this is too long for an overview page. Just say \"this documentation corresponds to version X\" and they can figure out how to get others. Also, cut the part about Git.", "created": "2012-10-08T15:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from pwendell: sure now this just says the version and the downloads page.", "created": "2012-10-08T16:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: OK, thanks.", "created": "2012-10-08T16:25:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-256, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-311\nSummary: Adding new download instructions\n\nComments (5):\n1. Patrick McFadin: Github comment from pwendell: I updated this just now to reflect the templated versions. I also changed the wording a bit.\n2. Patrick McFadin: Github comment from mateiz: Thanks but I think this is too long for an overview page. Just say \"this documentation corresponds to version X\" and they can figure out how to get others. Also, cut the part about Git.\n3. Patrick McFadin: Github comment from pwendell: sure now this just says the version and the downloads page.\n4. Patrick McFadin: Github comment from mateiz: OK, thanks.\n5. Patrick McFadin: Imported from Github issue spark-256, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "b5b0617a3f0452c9327dbd36c73f55a8", "issue_key": "SPARK-310", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adds special version variables to docs templating system", "description": "Adds liquid variables to docs templating system so that they can be used throughout the docs: SPARK_VERSION, SCALA_VERSION, and MESOS_VERSION. To use them, e.g. use {{site.SPARK_VERSION}}. Also removes uses of {{HOME_PATH}} which were being resolved to \"\" by the templating system anyway.", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-10-08T09:32:00.000+0000", "updated": "2012-10-19T22:50:36.000+0000", "resolved": "2012-10-19T22:50:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Can you pull the latest change from dev into your branch to make it mergeable? One of the earlier commits made it incompatible.", "created": "2012-10-08T10:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from andyk: Ok, I think I've fixed the conflict. i also updated the README with instructions for building without scaladoc (because this feature can save people lots of time).", "created": "2012-10-08T11:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Great, thanks!", "created": "2012-10-08T11:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-257, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-310\nSummary: Adds special version variables to docs templating system\nDescription: Adds liquid variables to docs templating system so that they can be used throughout the docs: SPARK_VERSION, SCALA_VERSION, and MESOS_VERSION. To use them, e.g. use {{site.SPARK_VERSION}}. Also removes uses of {{HOME_PATH}} which were being resolved to \"\" by the templating system anyway.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Can you pull the latest change from dev into your branch to make it mergeable? One of the earlier commits made it incompatible.\n2. Patrick McFadin: Github comment from andyk: Ok, I think I've fixed the conflict. i also updated the README with instructions for building without scaladoc (because this feature can save people lots of time).\n3. Patrick McFadin: Github comment from mateiz: Great, thanks!\n4. Patrick McFadin: Imported from Github issue spark-257, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "612101f86349ee184290f684ca19521c", "issue_key": "SPARK-309", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Readding yarn-standalone scheduler scheme", "description": "Also merged the latest dev branch in.", "reporter": "Denny Britz", "assignee": null, "created": "0012-10-08T14:01:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks. Please reply to that thread on spark-developers too.", "created": "2012-10-08T14:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-258, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-309\nSummary: Readding yarn-standalone scheduler scheme\nDescription: Also merged the latest dev branch in.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks. Please reply to that thread on spark-developers too.\n2. Patrick McFadin: Imported from Github issue spark-258, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "879b1a80c985ba58581694bedf27e008", "issue_key": "SPARK-308", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Synchronization bug fix in broadcast implementations", "description": "- Synchronization bugs found during debugging the broadcast problem faced by one of the users (their problem couldn't be reproduced and still remain unresolved) - Removed some logging comments - Changed the broadcast example to call broadcast on multiple consecutive iterations, instead of just one.", "reporter": "Mosharaf Chowdhury", "assignee": null, "created": "0012-10-08T15:34:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Mosharaf. It would still be good to investigate and fix the full bug sometime though.", "created": "2012-10-08T15:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-259, originally reported by mosharaf", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-308\nSummary: Synchronization bug fix in broadcast implementations\nDescription: - Synchronization bugs found during debugging the broadcast problem faced by one of the users (their problem couldn't be reproduced and still remain unresolved) - Removed some logging comments - Changed the broadcast example to call broadcast on multiple consecutive iterations, instead of just one.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Mosharaf. It would still be good to investigate and fix the full bug sometime though.\n2. Patrick McFadin: Imported from Github issue spark-259, originally reported by mosharaf", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "a008d91f6909480e44b2950cb0f47834", "issue_key": "SPARK-307", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Updates docs to use the new version num vars and adds Spark version in nav bar", "description": "Updating lots of docs to use the new special version number variables, and adding the version to the navbar so it is easy to tell which version of Spark these docs were compiled for. See what the generated docs look like after this commit at: http://www.cs.berkeley.edu/~andyk/spark-docs-e1a724f39/", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-10-08T16:25:00.000+0000", "updated": "2012-10-19T22:50:28.000+0000", "resolved": "2012-10-19T22:50:28.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Andy.", "created": "2012-10-09T08:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-260, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-307\nSummary: Updates docs to use the new version num vars and adds Spark version in nav bar\nDescription: Updating lots of docs to use the new special version number variables, and adding the version to the navbar so it is easy to tell which version of Spark these docs were compiled for. See what the generated docs look like after this commit at: http://www.cs.berkeley.edu/~andyk/spark-docs-e1a724f39/\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Andy.\n2. Patrick McFadin: Imported from Github issue spark-260, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "046d2a3a5c8d6b4affa134cd0e14f6e2", "issue_key": "SPARK-306", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adding documentation to public API's.", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-08T21:27:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-261, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-306\nSummary: Adding documentation to public API's.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-261, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "f625ea209282ae5faf9bd72586a66b76", "issue_key": "SPARK-305", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Document RDD api (i.e. RDD.scala)", "description": "Adds scaladoc public API in RDD.scala", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-10-08T21:52:00.000+0000", "updated": "2012-10-19T22:50:22.000+0000", "resolved": "2012-10-19T22:50:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks, looks good.", "created": "2012-10-09T08:48:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-262, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-305\nSummary: Document RDD api (i.e. RDD.scala)\nDescription: Adds scaladoc public API in RDD.scala\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks, looks good.\n2. Patrick McFadin: Imported from Github issue spark-262, originally reported by andyk", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "e2c386ab905e82277bc080170abfca43", "issue_key": "SPARK-304", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Add m1.medium node option to cluster management script.", "description": "EC2 m1.medium node option is missing in the spark_ec2.py script.", "reporter": "RayRacine", "assignee": null, "created": "0012-10-09T07:08:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Awesome, thanks.", "created": "2012-10-09T08:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-263, originally reported by RayRacine", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-304\nSummary: Add m1.medium node option to cluster management script.\nDescription: EC2 m1.medium node option is missing in the spark_ec2.py script.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Awesome, thanks.\n2. Patrick McFadin: Imported from Github issue spark-263, originally reported by RayRacine", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "2674c52c006c597059ed560683b10a15", "issue_key": "SPARK-303", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support for Hadoop 2 distributions such as cdh4", "description": "Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore. Instead, you are now required to use the `JobContextImpl` and `TaskAttemptContextImpl` classes, respectively. This pull requests extracts the code in Spark where it uses these, into separate traits that are located in Hadoop major version specific source folders that the sbt build then includes/excludes as necessary. In addition, for Hadoop 2, Spark now also needs to depend on the `hadoop-client` artifact.", "reporter": "Thomas Dudziak", "assignee": null, "created": "0012-10-09T14:29:00.000+0000", "updated": "2012-10-19T22:50:23.000+0000", "resolved": "2012-10-19T22:50:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Tom, but we actually already have these fixes in the `yarn` branch. I don't think we want to merge them into master because that still relies on Hadoop 0.20.205. You can't use a Hadoop 2 client to connect to an older cluster, right? I think we'll just recommend using the `yarn` branch to people who use Hadoop 2, and we'll merge them when more people have moved to Hadoop 2.", "created": "2012-10-09T17:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tomdz: I don't know much about the yarn branch, but the name implies that it requires yarn, which we're not using with our cdh4 installation (we only use MRv1). The changes in this pull request seem to be sufficient to run current spark against the hdfs in our installation.", "created": "2012-10-09T21:38:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: My question is just, do these changes work when connecting to HDFS clusters on Hadoop 1.0 or 0.20.x? Since there's an API change in Hadoop, it seems that they wouldn't work. We should probably just rename the \"yarn\" branch to \"hadoop2\". You're right that the name of that branch makes it sound like it requires YARN, but actually it runs with Mesos, the standalone deploy mode, etc as well.", "created": "2012-10-10T10:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tomdz: The change are supposed to make Spark work with the Hadoop/hdfs version that it is compiled against. I.e. if I use `2.0.0-mr1-cdh4.1.0` as the Hadoop version (and set the `HADOOP_MAJOR_VERSION` to 2 since cdh4 is a Hadoop 2 distribution), then the resulting Spark artifact will work with cdh4 (and presumably everything that is API compatible to that Hadoop version). The mechanism would allow to handle other Hadoop backwards-incompatible changes as well. On a related note, we have a Maven POM that defines profiles for each of the popular Hadoop distributions (Hadoop 1 & 2) and generates Spark jars with corresponding classifiers. The benefit of that is that in my project I can depend on the right version of Spark (using a classifier in the dependency). If there is interest in that, I could create a pull request for it.", "created": "2012-10-10T11:10:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks, I didn't see that you're building differently based on the major version. I'll merge this in but probably after the release of 0.6.0 (in the next minor release after that). Maybe we can use the same mechanism to bring the YARN stuff into the main branch and only compile it conditionally.", "created": "2012-10-14T11:09:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from tomdz: Regenerated the pull request against 0.6 dev branch in #285.", "created": "2012-10-18T15:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-264, originally reported by tomdz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 7, "text": "Issue: SPARK-303\nSummary: Support for Hadoop 2 distributions such as cdh4\nDescription: Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore. Instead, you are now required to use the `JobContextImpl` and `TaskAttemptContextImpl` classes, respectively. This pull requests extracts the code in Spark where it uses these, into separate traits that are located in Hadoop major version specific source folders that the sbt build then includes/excludes as necessary. In addition, for Hadoop 2, Spark now also needs to depend on the `hadoop-client` artifact.\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: Thanks Tom, but we actually already have these fixes in the `yarn` branch. I don't think we want to merge them into master because that still relies on Hadoop 0.20.205. You can't use a Hadoop 2 client to connect to an older cluster, right? I think we'll just recommend using the `yarn` branch to people who use Hadoop 2, and we'll merge them when more people have moved to Hadoop 2.\n2. Patrick McFadin: Github comment from tomdz: I don't know much about the yarn branch, but the name implies that it requires yarn, which we're not using with our cdh4 installation (we only use MRv1). The changes in this pull request seem to be sufficient to run current spark against the hdfs in our installation.\n3. Patrick McFadin: Github comment from mateiz: My question is just, do these changes work when connecting to HDFS clusters on Hadoop 1.0 or 0.20.x? Since there's an API change in Hadoop, it seems that they wouldn't work. We should probably just rename the \"yarn\" branch to \"hadoop2\". You're right that the name of that branch makes it sound like it requires YARN, but actually it runs with Mesos, the standalone deploy mode, etc as well.\n4. Patrick McFadin: Github comment from tomdz: The change are supposed to make Spark work with the Hadoop/hdfs version that it is compiled against. I.e. if I use `2.0.0-mr1-cdh4.1.0` as the Hadoop version (and set the `HADOOP_MAJOR_VERSION` to 2 since cdh4 is a Hadoop 2 distribution), then the resulting Spark artifact will work with cdh4 (and presumably everything that is API compatible to that Hadoop version). The mechanism would allow to handle other Hadoop backwards-incompatible changes as well. On a related note, we have a Maven POM that defines profiles for each of the popular Hadoop distributions (Hadoop 1 & 2) and generates Spark jars with corresponding classifiers. The benefit of that is that in my project I can depend on the right version of Spark (using a classifier in the dependency). If there is interest in that, I could create a pull request for it.\n5. Patrick McFadin: Github comment from mateiz: Thanks, I didn't see that you're building differently based on the major version. I'll merge this in but probably after the release of 0.6.0 (in the next minor release after that). Maybe we can use the same mechanism to bring the YARN stuff into the main branch and only compile it conditionally.\n6. Patrick McFadin: Github comment from tomdz: Regenerated the pull request against 0.6 dev branch in #285.\n7. Patrick McFadin: Imported from Github issue spark-264, originally reported by tomdz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.995517"}}
{"id": "36268c73bb96a97a89305910e8997593", "issue_key": "SPARK-302", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Making Spark version configurable in docs and updating Bagel doc", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-09T21:41:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks Patrick!", "created": "2012-10-10T10:02:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-265, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-302\nSummary: Making Spark version configurable in docs and updating Bagel doc\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Thanks Patrick!\n2. Patrick McFadin: Imported from Github issue spark-265, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.995517"}}
{"id": "e988fd7dc5d7d23a88e64350acba5125", "issue_key": "SPARK-527", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support spark-shell when running on YARN", "description": "Right now the YARN mode only allows standalone jobs.", "reporter": "Matei Alexandru Zaharia", "assignee": "Raymond Valencia", "created": "0012-10-10T10:35:00.000+0000", "updated": "2014-01-23T23:29:37.000+0000", "resolved": "2014-01-23T23:29:20.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-266, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Raymond Valencia", "body": "Hi I have send a pull request on this one at : https://github.com/mesos/spark/pull/868 , please help to take a review", "created": "2013-08-28T00:02:49.558+0000"}, {"author": "Josh Rosen", "body": "Support for spark-shell under YARN was added in 0.8.1 using yarn-client mode: https://github.com/apache/incubator-spark/pull/101", "created": "2014-01-23T23:29:20.417+0000"}], "num_comments": 3, "text": "Issue: SPARK-527\nSummary: Support spark-shell when running on YARN\nDescription: Right now the YARN mode only allows standalone jobs.\n\nComments (3):\n1. Patrick McFadin: Imported from Github issue spark-266, originally reported by mateiz\n2. Raymond Valencia: Hi I have send a pull request on this one at : https://github.com/mesos/spark/pull/868 , please help to take a review\n3. Josh Rosen: Support for spark-shell under YARN was added in 0.8.1 using yarn-client mode: https://github.com/apache/incubator-spark/pull/101", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.995517"}}
{"id": "f405e68a8cfdf558f1f7a2bd7440cf71", "issue_key": "SPARK-301", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Fixed bug when fetching Jar dependencies.", "description": "Instead of checking currentFiles check currentJars. Came across this when testing some Shark queries.", "reporter": "Denny Britz", "assignee": null, "created": "0012-10-10T15:11:00.000+0000", "updated": "2012-10-19T22:50:22.000+0000", "resolved": "2012-10-19T22:50:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Good catch; I probably miscopied this when I moved this code around to fetch JARs before deserializing the task.", "created": "2012-10-10T17:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-267, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-301\nSummary: Fixed bug when fetching Jar dependencies.\nDescription: Instead of checking currentFiles check currentJars. Came across this when testing some Shark queries.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Good catch; I probably miscopied this when I moved this code around to fetch JARs before deserializing the task.\n2. Patrick McFadin: Imported from Github issue spark-267, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.995517"}}
{"id": "702c5328d07abb092a25995d0ff91ade", "issue_key": "SPARK-300", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adding code for publishing to Sonatype.", "description": "By default - I'm leaving this commented out. This is because there is a bug in the PGP signing plugin which causes it to active even duing a publish-local. So we'll just uncomment when we decide to publish.", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-10T16:27:00.000+0000", "updated": "2012-10-19T22:50:31.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Okay, thanks.", "created": "2012-10-10T17:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way, the same issue seems to affect publish-local on branch 0.5. I'll fix it there too.", "created": "2012-10-10T17:58:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-268, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-300\nSummary: Adding code for publishing to Sonatype.\nDescription: By default - I'm leaving this commented out. This is because there is a bug in the PGP signing plugin which causes it to active even duing a publish-local. So we'll just uncomment when we decide to publish.\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Okay, thanks.\n2. Patrick McFadin: Github comment from mateiz: By the way, the same issue seems to affect publish-local on branch 0.5. I'll fix it there too.\n3. Patrick McFadin: Imported from Github issue spark-268, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.995517"}}
{"id": "d505267cf8dea8fcd906ad739883a6ec", "issue_key": "SPARK-299", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Null pointer exception when RDD size is larger than cache size", "description": "I got the null pointer exception today when I marked an rdd to be cached but the RDD size ended up being larger than memory available. What I think is happening is that the CacheTracker calls blockManager.put and expects the put to succeed. But if a block is marked as memory only and if we don't have enough space, the block manager will compute the split but fail to put it in the cache. I think the right thing to do is to stream through the computation like we would do a non-cached RDD, but I am not exactly sure what the right code fix is. --------------------------------------- 12/10/10 23:57:33 INFO storage.MemoryStore: ensureFreeSpace(419262667) called with curMem=7152168890, maxMem=7470573158 12/10/10 23:57:33 INFO storage.MemoryStore: Will not store rdd_2_230 as it would require dropping another block from the same RDD 12/10/10 23:57:33 INFO storage.BlockManager: Dropping block rdd_2_230 from memory 12/10/10 23:57:33 WARN storage.MemoryStore: Block rdd_2_230 could not be removed as it does not exist 12/10/10 23:57:33 WARN spark.CacheTracker: loading partition failed after computing it rdd_2_230 12/10/10 23:57:33 ERROR executor.Executor: Exception in task ID 233 java.lang.NullPointerException at spark.scheduler.OutputTask.run(OutputTask.scala:29) at spark.executor.Executor$TaskRunner.run(Executor.scala:99) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:679)", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-10-10T20:05:00.000+0000", "updated": "2012-10-19T22:50:27.000+0000", "resolved": "2012-10-19T22:50:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I think we should not be calling BlockManager.get / put there. The reason it happened is probably because we didn't want to create an ArrayBuffer and pass that to the BlockManager (which would copy it), but the current way is silly; for example it will serialize and then deserialize the data if you're using serialized caching. We should probably just add a way for the block manager to take an ArrayBuffer and put it without cloning, or just give it one that it will copy. I'm not too worried about supporting cases where a single partition is bigger than the RAM right now so I think it's fine to materialize an ArrayBuffer even if we don't end up keeping it in the cache (e.g. due to the same-RDD replacement rule).", "created": "2012-10-10T21:55:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Anyway let me know if you want to fix it, or I should. We should get it fixed before releasing 0.6.", "created": "2012-10-10T21:57:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: I agree with your take that the block manager should be able to take an array buffer rather than an iterator. And yes, this is not a case where a single partition is larger than memory available, but a case where I had 235 partitions and it happened that I didn't have memory for the last 4 or 5 partitions. I'll take a shot at fixing this tonight, but if I either don't get too far or run out of time, I'll let you by tomorrow for you to go ahead with a fix.", "created": "2012-10-10T22:04:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-269, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-299\nSummary: Null pointer exception when RDD size is larger than cache size\nDescription: I got the null pointer exception today when I marked an rdd to be cached but the RDD size ended up being larger than memory available. What I think is happening is that the CacheTracker calls blockManager.put and expects the put to succeed. But if a block is marked as memory only and if we don't have enough space, the block manager will compute the split but fail to put it in the cache. I think the right thing to do is to stream through the computation like we would do a non-cached RDD, but I am not exactly sure what the right code fix is. --------------------------------------- 12/10/10 23:57:33 INFO storage.MemoryStore: ensureFreeSpace(419262667) called with curMem=7152168890, maxMem=7470573158 12/10/10 23:57:33 INFO storage.MemoryStore: Will not store rdd_2_230 as it would require dropping another block from the same RDD 12/10/10 23:57:33 INFO storage.BlockManager: Dropping block rdd_2_230 from memory 12/10/10 23:57:33 WARN storage.MemoryStore: Block rdd_2_230 could not be removed as it does not exist 12/10/10 23:57:33 WARN spark.CacheTracker: loading partition failed after computing it rdd_2_230 12/10/10 23:57:33 ERROR executor.Executor: Exception in task ID 233 java.lang.NullPointerException at spark.scheduler.OutputTask.run(OutputTask.scala:29) at spark.executor.Executor$TaskRunner.run(Executor.scala:99) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:679)\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: I think we should not be calling BlockManager.get / put there. The reason it happened is probably because we didn't want to create an ArrayBuffer and pass that to the BlockManager (which would copy it), but the current way is silly; for example it will serialize and then deserialize the data if you're using serialized caching. We should probably just add a way for the block manager to take an ArrayBuffer and put it without cloning, or just give it one that it will copy. I'm not too worried about supporting cases where a single partition is bigger than the RAM right now so I think it's fine to materialize an ArrayBuffer even if we don't end up keeping it in the cache (e.g. due to the same-RDD replacement rule).\n2. Patrick McFadin: Github comment from mateiz: Anyway let me know if you want to fix it, or I should. We should get it fixed before releasing 0.6.\n3. Patrick McFadin: Github comment from shivaram: I agree with your take that the block manager should be able to take an array buffer rather than an iterator. And yes, this is not a case where a single partition is larger than memory available, but a case where I had 235 partitions and it happened that I didn't have memory for the last 4 or 5 partitions. I'll take a shot at fixing this tonight, but if I either don't get too far or run out of time, I'll let you by tomorrow for you to go ahead with a fix.\n4. Patrick McFadin: Imported from Github issue spark-269, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.996656"}}
{"id": "bd21e1954e41e91ce806f7af6ac3e561", "issue_key": "SPARK-298", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adding Java documentation", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-10T23:50:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Awesome, thanks a lot for doing this.", "created": "2012-10-11T11:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-270, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-298\nSummary: Adding Java documentation\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Awesome, thanks a lot for doing this.\n2. Patrick McFadin: Imported from Github issue spark-270, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.996656"}}
{"id": "5ce3bccdc263838308b65b6aedc8c0d9", "issue_key": "SPARK-297", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Change block manager to accept a ArrayBuffer", "description": "Attempt to fix #269. Existing tests pass, but I haven't added a new test to make sure things are fixed. This is partly because I was not sure if I should extend CacheTrackerSuite or add something to the Block Manager test cases.", "reporter": "Shivaram Venkataraman", "assignee": null, "created": "0012-10-10T23:56:00.000+0000", "updated": "2012-10-19T22:50:29.000+0000", "resolved": "2012-10-19T22:50:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks good to me. I would add a test in DistributedSuite actually, to just launch a cluster where the memory limit per worker is low (e.g. set spark.storage.memoryFraction = 0.01) and try caching and using some data on it.", "created": "2012-10-12T12:44:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from shivaram: Added the test with the assumption that 512MB of memory times 0.0001 is around 50KB and the array should be larger than that.", "created": "2012-10-12T13:17:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Thanks. It's a good bet because we do set the JVMs to exactly 512 MB of memory. I might also add a test where half the data fits and half doesn't.", "created": "2012-10-12T13:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-271, originally reported by shivaram", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-297\nSummary: Change block manager to accept a ArrayBuffer\nDescription: Attempt to fix #269. Existing tests pass, but I haven't added a new test to make sure things are fixed. This is partly because I was not sure if I should extend CacheTrackerSuite or add something to the Block Manager test cases.\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: This looks good to me. I would add a test in DistributedSuite actually, to just launch a cluster where the memory limit per worker is low (e.g. set spark.storage.memoryFraction = 0.01) and try caching and using some data on it.\n2. Patrick McFadin: Github comment from shivaram: Added the test with the assumption that 512MB of memory times 0.0001 is around 50KB and the array should be larger than that.\n3. Patrick McFadin: Github comment from mateiz: Thanks. It's a good bet because we do set the JVMs to exactly 512 MB of memory. I might also add a test where half the data fits and half doesn't.\n4. Patrick McFadin: Imported from Github issue spark-271, originally reported by shivaram", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.996656"}}
{"id": "e83eaf98d7b450415f38e9c30e6ce09f", "issue_key": "SPARK-526", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "sbt/sbt --> eclipse won't work now", "description": "after a fresh clone of the repository, if you do sbt/sbt and > eclipse [error] Not a valid command: eclipse (similar: help, alias) ... ...", "reporter": "tewr05", "assignee": null, "created": "0012-10-11T22:14:00.000+0000", "updated": "2013-01-20T12:33:47.000+0000", "resolved": "2013-01-20T12:33:47.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Interesting; see if you can find a version of the SBT Eclipse plugin for sbt 0.11.3 -- we recently updated our version of sbt. Or maybe the name of the command changed (e.g. it might be gen-eclipse or something). Matei On Oct 11, 2012, at 11:14 PM, George Li wrote: > after a fresh clone of the repository, if you do sbt/sbt and > > eclipse > [error] Not a valid command: eclipse (similar: help, alias) > ... > ... > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-10-11T22:19:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-272, originally reported by tewr05", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "\"sbt/sbt eclipse\" works for me on both the master branch and 0.6.1, so I'm marking this as resolved.", "created": "2013-01-20T12:33:47.666+0000"}], "num_comments": 3, "text": "Issue: SPARK-526\nSummary: sbt/sbt --> eclipse won't work now\nDescription: after a fresh clone of the repository, if you do sbt/sbt and > eclipse [error] Not a valid command: eclipse (similar: help, alias) ... ...\n\nComments (3):\n1. Patrick McFadin: Github comment from mateiz: Interesting; see if you can find a version of the SBT Eclipse plugin for sbt 0.11.3 -- we recently updated our version of sbt. Or maybe the name of the command changed (e.g. it might be gen-eclipse or something). Matei On Oct 11, 2012, at 11:14 PM, George Li wrote: > after a fresh clone of the repository, if you do sbt/sbt and > > eclipse > [error] Not a valid command: eclipse (similar: help, alias) > ... > ... > > — > Reply to this email directly or view it on GitHub. > >\n2. Patrick McFadin: Imported from Github issue spark-272, originally reported by tewr05\n3. Josh Rosen: \"sbt/sbt eclipse\" works for me on both the master branch and 0.6.1, so I'm marking this as resolved.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.996656"}}
{"id": "0dfa33c687054762cc1a06135a1112f1", "issue_key": "SPARK-296", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Let the user specify environment variables to be passed to the Executors", "description": "", "reporter": "Denny Britz", "assignee": null, "created": "0012-10-13T13:02:00.000+0000", "updated": "2012-10-19T22:50:24.000+0000", "resolved": "2012-10-19T22:50:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Hey Denny, Are you sure this will work for the standalone and Mesos coarse-grain modes? In those cases, it the Scheduler will launch executors *before* you get the chance to call putExecutorEnv on the SparkContext, so they won't see the new variables. I suggest that, instead, you make SparkContext's constructor take an `env` map as a fifth argument (and add a separate 4-argument constructor). That will also make it clear that you can't add or change the env after you start a job. Matei", "created": "2012-10-13T13:19:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: By the way if you do this, change JavaSparkContext in the same way.", "created": "2012-10-13T13:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Ah, you're right. I won't be back before tonight though so if you want to release today please feel free to make the changes yourself. If not I will change it tomorrow morning.", "created": "2012-10-13T13:29:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Alright, I'll merge it and fix this myself; thanks.", "created": "2012-10-13T13:36:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-273, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 5, "text": "Issue: SPARK-296\nSummary: Let the user specify environment variables to be passed to the Executors\n\nComments (5):\n1. Patrick McFadin: Github comment from mateiz: Hey Denny, Are you sure this will work for the standalone and Mesos coarse-grain modes? In those cases, it the Scheduler will launch executors *before* you get the chance to call putExecutorEnv on the SparkContext, so they won't see the new variables. I suggest that, instead, you make SparkContext's constructor take an `env` map as a fifth argument (and add a separate 4-argument constructor). That will also make it clear that you can't add or change the env after you start a job. Matei\n2. Patrick McFadin: Github comment from mateiz: By the way if you do this, change JavaSparkContext in the same way.\n3. Patrick McFadin: Github comment from dennybritz: Ah, you're right. I won't be back before tonight though so if you want to release today please feel free to make the changes yourself. If not I will change it tomorrow morning.\n4. Patrick McFadin: Github comment from mateiz: Alright, I'll merge it and fix this myself; thanks.\n5. Patrick McFadin: Imported from Github issue spark-273, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.996656"}}
{"id": "aebb51e67db3ea38a1c7056e0c136ec5", "issue_key": "SPARK-295", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Access denied for LATEST_AMI_URL on 0.6 EC2 scripts", "description": "When running the Spark 0.6 EC2 script, I received an \"Access Denied\" message when trying to fetch LATEST_AMI_URL = https://s3.amazonaws.com/mesos-images/ids/latest-spark-0.6. The old URL, https://s3.amazonaws.com/mesos-images/ids/latest-spark-0.5 works fine, so this is probably a permissions issue.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-10-13T14:26:00.000+0000", "updated": "2012-10-19T22:50:21.000+0000", "resolved": "2012-10-19T22:50:21.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Yup, it's because there's no AMI for 0.6 yet. You should use the 0.5 one manually for now, or use the 0.5 deploy script.", "created": "2012-10-13T14:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-274, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-295\nSummary: Access denied for LATEST_AMI_URL on 0.6 EC2 scripts\nDescription: When running the Spark 0.6 EC2 script, I received an \"Access Denied\" message when trying to fetch LATEST_AMI_URL = https://s3.amazonaws.com/mesos-images/ids/latest-spark-0.6. The old URL, https://s3.amazonaws.com/mesos-images/ids/latest-spark-0.5 works fine, so this is probably a permissions issue.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Yup, it's because there's no AMI for 0.6 yet. You should use the 0.5 one manually for now, or use the 0.5 deploy script.\n2. Patrick McFadin: Imported from Github issue spark-274, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.996656"}}
{"id": "ea8d2e6f9356c881041455118c3ec3d3", "issue_key": "SPARK-525", "issue_type": "Improvement", "status": "Closed", "priority": null, "resolution": null, "summary": "Refactoring of shuffling-related classes", "description": "These commits refactor interfaces and classes related to shuffling. The main change is to remove map-side combining and aggregation from the shuffling interfaces. Map-side combining can be performed by calling `mapPartitions()` on an RDD, then passing that transformed RDD to `ShuffledRDD`. The reduce-side combining of the combiners can be performed in a second `mapPartitions()` call. In this branch, - `ShuffleMapTask` only performs shuffling, not map-side combining. - All of the specialized `ShuffledRDD` subclasses were removed and replaced with a single `ShuffledRDD` class, which is only responsible for fetching a shuffled RDD. - `ShuffleFetcher` returns an iterator over its fetched partitions, rather than accepting a function to apply to those partitions. - The map-side combiner tests were removed, since map-side combining no longer uses a special mechanism. - A `preservesPartitioning` option was added to `mapPartitions()` to allow users to specify that a transformation preserves the RDD's partitioning; this is used when implementing map-side combiners and reduce-side aggregation. - RDD methods were modified to use these new interfaces. For some methods, like `sort()`, this had the effect of grouping the method's code into a single file rather than splitting it between the method and a special RDD subclass. These changes haven't had a noticeable performance impact in the tests that I've run, but I'm still in the process of gathering benchmarks. I'd appreciate any feedback on these changes, particularly the changes to `cogroup()` and `CoGroupedRDD`.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-10-13T16:14:00.000+0000", "updated": "2012-10-23T23:58:58.000+0000", "resolved": "2012-10-23T23:58:58.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-275, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Closing since this pull request was merged.", "created": "2012-10-23T23:58:58.626+0000"}], "num_comments": 2, "text": "Issue: SPARK-525\nSummary: Refactoring of shuffling-related classes\nDescription: These commits refactor interfaces and classes related to shuffling. The main change is to remove map-side combining and aggregation from the shuffling interfaces. Map-side combining can be performed by calling `mapPartitions()` on an RDD, then passing that transformed RDD to `ShuffledRDD`. The reduce-side combining of the combiners can be performed in a second `mapPartitions()` call. In this branch, - `ShuffleMapTask` only performs shuffling, not map-side combining. - All of the specialized `ShuffledRDD` subclasses were removed and replaced with a single `ShuffledRDD` class, which is only responsible for fetching a shuffled RDD. - `ShuffleFetcher` returns an iterator over its fetched partitions, rather than accepting a function to apply to those partitions. - The map-side combiner tests were removed, since map-side combining no longer uses a special mechanism. - A `preservesPartitioning` option was added to `mapPartitions()` to allow users to specify that a transformation preserves the RDD's partitioning; this is used when implementing map-side combiners and reduce-side aggregation. - RDD methods were modified to use these new interfaces. For some methods, like `sort()`, this had the effect of grouping the method's code into a single file rather than splitting it between the method and a special RDD subclass. These changes haven't had a noticeable performance impact in the tests that I've run, but I'm still in the process of gathering benchmarks. I'd appreciate any feedback on these changes, particularly the changes to `cogroup()` and `CoGroupedRDD`.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-275, originally reported by JoshRosen\n2. Josh Rosen: Closing since this pull request was merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.996656"}}
{"id": "fa6043dbd8217a01db34d0ec5f50d06d", "issue_key": "SPARK-294", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Disable gpg by default.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-13T22:10:00.000+0000", "updated": "2012-10-19T22:50:26.000+0000", "resolved": "2012-10-19T22:50:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I've already done this separately, so closing the pull request, but thanks!.", "created": "2012-10-14T21:08:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-276, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-294\nSummary: Disable gpg by default.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: I've already done this separately, so closing the pull request, but thanks!.\n2. Patrick McFadin: Imported from Github issue spark-276, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.997640"}}
{"id": "60dd739d0b851ae8472dbd6060c7b244", "issue_key": "SPARK-293", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "SparkContext.newShuffleId should be public or ShuffleDependency should set its own id", "description": "The `SparkContext.newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`: ```scala class ShuffleDependency[K, V, C]( val shuffleId: Int, @transient rdd: RDD[(K, V)], val aggregator: Option[Aggregator[K, V, C]], val partitioner: Partitioner) extends Dependency(rdd) ``` `SparkContext.newShuffleId` is currently `private[spark]`, which prevents me from defining custom RDD subclasses in Shark that create their own `ShuffleDependency` instances. In the current dev branch, `ShuffleDependency` is always supplied with `SparkContext.newShuffleId`. If it's not important to be able to set a `ShuffleDependency`'s id to an arbitrary value, then perhaps the `shuffleId` field could be moved from its constructor into the class body, where it could be set to `rdd.context.newShuffleId`.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-10-14T00:45:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Good catch; I've fixed this now, by making the shuffle ID not be a constructor argument.", "created": "2012-10-14T09:00:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-277, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-293\nSummary: SparkContext.newShuffleId should be public or ShuffleDependency should set its own id\nDescription: The `SparkContext.newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`: ```scala class ShuffleDependency[K, V, C]( val shuffleId: Int, @transient rdd: RDD[(K, V)], val aggregator: Option[Aggregator[K, V, C]], val partitioner: Partitioner) extends Dependency(rdd) ``` `SparkContext.newShuffleId` is currently `private[spark]`, which prevents me from defining custom RDD subclasses in Shark that create their own `ShuffleDependency` instances. In the current dev branch, `ShuffleDependency` is always supplied with `SparkContext.newShuffleId`. If it's not important to be able to set a `ShuffleDependency`'s id to an arbitrary value, then perhaps the `shuffleId` field could be moved from its constructor into the class body, where it could be set to `rdd.context.newShuffleId`.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Good catch; I've fixed this now, by making the shuffle ID not be a constructor argument.\n2. Patrick McFadin: Imported from Github issue spark-277, originally reported by JoshRosen", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.997640"}}
{"id": "b9b64784a9166b765f33e7e65da52e9d", "issue_key": "SPARK-292", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Adding dependency repos in quickstart example", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-14T10:50:00.000+0000", "updated": "2012-10-19T22:50:30.000+0000", "resolved": "2012-10-19T22:50:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-278, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-292\nSummary: Adding dependency repos in quickstart example\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-278, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.997640"}}
{"id": "df4dc166a5b209a5839a701ba47d2895", "issue_key": "SPARK-291", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Removing credentials line in build.", "description": "This line isn't needed - in fact it messes with the right way of resolving credentials automatically.", "reporter": "Patrick Wendell", "assignee": null, "created": "0012-10-14T18:35:00.000+0000", "updated": "2012-10-19T22:50:32.000+0000", "resolved": "2012-10-19T22:50:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-279, originally reported by pwendell", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-291\nSummary: Removing credentials line in build.\nDescription: This line isn't needed - in fact it messes with the right way of resolving credentials automatically.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-279, originally reported by pwendell", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.997640"}}
{"id": "962d5028311ebfd636e2d41654fcaa49", "issue_key": "SPARK-524", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "spark integration issue with Cloudera hadoop", "description": "Hi, 1. I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM) 2. Follow instruction on https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some tweaks. 3. I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack of document) 4. ./spartk-shell.sh import spark._ val sc = new SparkContext(\"localhost:5050\",\"passwd\") val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") I am getting error Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version mismatch. (client = 61, server = 63) IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = \"0.20.2-cdh3u3\" I am getting error at ec2.count() ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job like the one reported at http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E Please let me know if you cannot replicate this error, and give more instruction on how Spark integrate with Cloudera Hadoop Thanks -QH", "reporter": "openreserach", "assignee": null, "created": "0012-10-15T19:17:00.000+0000", "updated": "2014-09-02T13:02:07.000+0000", "resolved": "2014-09-02T13:02:07.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Did you copy the code you built to the worker nodes too? If so, look in /mnt/mesos-work on the worker to find the stdout and stderr output for your job, and let me know what's in there. But generally \"lost TID\" means that the JVM crashed, which likely means that it didn't have the right code or otherwise couldn't start Spark. Matei On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > Hi, > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM) > > Follow instruction on https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some tweaks. > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack of document) > > ./spartk-shell.sh > import spark._ > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > I am getting error > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version mismatch. (client = 61, server = 63) > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = \"0.20.2-cdh3u3\" > I am getting error at ec2.count() > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > like the one reported at http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > Please let me know if you cannot replicate this error, and give more instruction on how Spark integrate with Cloudera Hadoop > > Thanks > > -QH > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-10-15T19:30:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from openreserach: As stated in the issue description, it is a dummy *single* instance cluster, i.e., master and slave are co-hosted in one box. Where should I copy code to? -QH On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <notifications@github.com>wrote: > Did you copy the code you built to the worker nodes too? > > If so, look in /mnt/mesos-work on the worker to find the stdout and stderr > output for your job, and let me know what's in there. But generally \"lost > TID\" means that the JVM crashed, which likely means that it didn't have the > right code or otherwise couldn't start Spark. > > Matei > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > Hi, > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same > issue if I build mesos from source code in locall VM) > > > > Follow instruction on > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > tweaks. > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack > of document) > > > > ./spartk-shell.sh > > import spark._ > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > I am getting error > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > mismatch. (client = 61, server = 63) > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = > \"0.20.2-cdh3u3\" > > I am getting error at ec2.count() > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > like the one reported at > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > Please let me know if you cannot replicate this error, and give more > instruction on how Spark integrate with Cloudera Hadoop > > > > Thanks > > > > -QH > > > > — > > Reply to this email directly or view it on GitHub. > > > > > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > -- Dr. Qiming He Qiming.He@openresearchinc.com 301-525-6612 (Phone) 815-327-2122 (Fax)", "created": "2012-10-15T19:40:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Ah, got it. So how exactly did you start the cluster -- did you not use our EC2 script? In that case you probably need to configure some settings in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try starting a cluster with our script (https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for you. Again, to figure out the problem, you'd need to look at the logs of the worker process. Mesos places them in a \"work\" directory for the job. This is configured to be /mnt/mesos-work when you launch Mesos with our EC2 scripts, but if you launched it manually then it likely be in /tmp. Matei On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > As stated in the issue description, it is a dummy *single* instance > cluster, i.e., master and slave are co-hosted in one box. Where should I > copy code to? -QH > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <notifications@github.com>wrote: > > > Did you copy the code you built to the worker nodes too? > > > > If so, look in /mnt/mesos-work on the worker to find the stdout and stderr > > output for your job, and let me know what's in there. But generally \"lost > > TID\" means that the JVM crashed, which likely means that it didn't have the > > right code or otherwise couldn't start Spark. > > > > Matei > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > Hi, > > > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same > > issue if I build mesos from source code in locall VM) > > > > > > Follow instruction on > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > > tweaks. > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack > > of document) > > > > > > ./spartk-shell.sh > > > import spark._ > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > I am getting error > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > mismatch. (client = 61, server = 63) > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = > > \"0.20.2-cdh3u3\" > > > I am getting error at ec2.count() > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > > like the one reported at > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > Please let me know if you cannot replicate this error, and give more > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > Thanks > > > > > > -QH > > > > > > — > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > — > > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > -- > Dr. Qiming He > Qiming.He@openresearchinc.com > 301-525-6612 (Phone) > 815-327-2122 (Fax) > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-10-15T19:45:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from openreserach: Hi, 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it is just dummy single instance, running in AWS (should be the same as local) 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala No client mismatch problem, but still get error: ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job Here is tail of /tmp/mesos-slave.INFO I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:27.461669 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/0 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] Forked executor at 21143 I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 Sent signal to 21143 I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 for deletion I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status update for task 0 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.444704 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/1 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] Forked executor at 21190 I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 Sent signal to 21190 I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 for deletion I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status update for task 1 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.518450 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/2 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] Forked executor at 21237 I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 Sent signal to 21237 I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 for deletion I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status update for task 2 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.441154 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/3 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] Forked executor at 21284 I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 Sent signal to 21284 I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 for deletion I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status update for task 3 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.451181 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/4 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] Forked executor at 21331 I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 Sent signal to 21331 I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 for deletion I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status update for task 4 of framework 20121017 0241-1056562698-5050-1405-0005 On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <notifications@github.com>wrote: > Ah, got it. So how exactly did you start the cluster -- did you not use > our EC2 script? In that case you probably need to configure some settings > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try > starting a cluster with our script ( > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for > you. > > Again, to figure out the problem, you'd need to look at the logs of the > worker process. Mesos places them in a \"work\" directory for the job. This > is configured to be /mnt/mesos-work when you launch Mesos with our EC2 > scripts, but if you launched it manually then it likely be in /tmp. > > Matei > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > As stated in the issue description, it is a dummy *single* instance > > cluster, i.e., master and slave are co-hosted in one box. Where should I > > copy code to? -QH > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > notifications@github.com>wrote: > > > > > Did you copy the code you built to the worker nodes too? > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout and > stderr > > > output for your job, and let me know what's in there. But generally > \"lost > > > TID\" means that the JVM crashed, which likely means that it didn't > have the > > > right code or otherwise couldn't start Spark. > > > > > > Matei > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > Hi, > > > > > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) > (Same > > > issue if I build mesos from source code in locall VM) > > > > > > > > Follow instruction on > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > > > tweaks. > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to > lack > > > of document) > > > > > > > > ./spartk-shell.sh > > > > import spark._ > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > project/SparkBuild.scala > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > I am getting error > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > > mismatch. (client = 61, server = 63) > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION > = > > > \"0.20.2-cdh3u3\" > > > > I am getting error at ec2.count() > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting > job > > > > like the one reported at > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > Please let me know if you cannot replicate this error, and give more > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > Thanks > > > > > > > > -QH > > > > > > > >  > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > >  > > > Reply to this email directly or view it on GitHub< > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > -- > > Dr. Qiming He > > Qiming.He@openresearchinc.com > > 301-525-6612 (Phone) > > 815-327-2122 (Fax) > >  > > Reply to this email directly or view it on GitHub. > > > > > >  > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > -- Dr. Qiming He Qiming.He@openresearchinc.com 301-525-6612 (Phone) 815-327-2122 (Fax)", "created": "2012-10-16T20:23:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Got it. Actually the place to look for the worker process's output is /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 In there you'll find two files, stdout and stderr, with the output of the process. My guess is that it couldn't launch scala because you have not set up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it. Matei On Oct 16, 2012, at 9:23 PM, Qiming He wrote: > Hi, > > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it is > just dummy single instance, running in AWS (should be the same as local) > > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala > > No client mismatch problem, but still get error: > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > Here is tail of /tmp/mesos-slave.INFO > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:27.461669 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/0 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] Forked > executor at 21143 > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 > Sent signal to 21143 > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > for deletion > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status > update for task 0 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.444704 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/1 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] Forked > executor at 21190 > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 > Sent signal to 21190 > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 > for deletion > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status > update for task 1 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.518450 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/2 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] Forked > executor at 21237 > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 > Sent signal to 21237 > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 > for deletion > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status > update for task 2 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.441154 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/3 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] Forked > executor at 21284 > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 > Sent signal to 21284 > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 > for deletion > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status > update for task 3 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.451181 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/4 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] Forked > executor at 21331 > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 > Sent signal to 21331 > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 > for deletion > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status > update for task 4 of framework 20121017 > 0241-1056562698-5050-1405-0005 > > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <notifications@github.com>wrote: > > > Ah, got it. So how exactly did you start the cluster -- did you not use > > our EC2 script? In that case you probably need to configure some settings > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try > > starting a cluster with our script ( > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for > > you. > > > > Again, to figure out the problem, you'd need to look at the logs of the > > worker process. Mesos places them in a \"work\" directory for the job. This > > is configured to be /mnt/mesos-work when you launch Mesos with our EC2 > > scripts, but if you launched it manually then it likely be in /tmp. > > > > Matei > > > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > > > As stated in the issue description, it is a dummy *single* instance > > > cluster, i.e., master and slave are co-hosted in one box. Where should I > > > copy code to? -QH > > > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > > notifications@github.com>wrote: > > > > > > > Did you copy the code you built to the worker nodes too? > > > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout and > > stderr > > > > output for your job, and let me know what's in there. But generally > > \"lost > > > > TID\" means that the JVM crashed, which likely means that it didn't > > have the > > > > right code or otherwise couldn't start Spark. > > > > > > > > Matei > > > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > > > Hi, > > > > > > > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) > > (Same > > > > issue if I build mesos from source code in locall VM) > > > > > > > > > > Follow instruction on > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > > > > tweaks. > > > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to > > lack > > > > of document) > > > > > > > > > > ./spartk-shell.sh > > > > > import spark._ > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > > project/SparkBuild.scala > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > I am getting error > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > > > mismatch. (client = 61, server = 63) > > > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION > > = > > > > \"0.20.2-cdh3u3\" > > > > > I am getting error at ec2.count() > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting > > job > > > > > like the one reported at > > > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > > > Please let me know if you cannot replicate this error, and give more > > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > > > Thanks > > > > > > > > > > -QH > > > > > > > > > >  > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > >  > > > > Reply to this email directly or view it on GitHub< > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > > > > > > > -- > > > Dr. Qiming He > > > Qiming.He@openresearchinc.com > > > 301-525-6612 (Phone) > > > 815-327-2122 (Fax) > > >  > > > Reply to this email directly or view it on GitHub. > > > > > > > > > >  > > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > > > > > > > -- > Dr. Qiming He > Qiming.He@openresearchinc.com > 301-525-6612 (Phone) > 815-327-2122 (Fax) > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-10-16T20:27:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from openreserach: YES. That is the problem! After I set *export SCALA_HOME=/root/scala-2.9.1.final* it works perfectly. Last question: if scala is installed by package manager like yum or apt-get under /usr/bin/scala. what is SCALA_HOME then? Thanks -Qiming On Wed, Oct 17, 2012 at 12:27 AM, Matei Zaharia <notifications@github.com>wrote: > Got it. Actually the place to look for the worker process's output is > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > In there you'll find two files, stdout and stderr, with the output of the > process. My guess is that it couldn't launch scala because you have not set > up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it. > > Matei > > On Oct 16, 2012, at 9:23 PM, Qiming He wrote: > > > Hi, > > > > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it > is > > just dummy single instance, running in AWS (should be the same as local) > > > > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is > > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val > > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala > > > > No client mismatch problem, but still get error: > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > > > Here is tail of /tmp/mesos-slave.INFO > > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:27.461669 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/0 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21143 > > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 > > Sent signal to 21143 > > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > for deletion > > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status > > update for task 0 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.444704 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/1 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21190 > > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 > > Sent signal to 21190 > > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 > > > for deletion > > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status > > update for task 1 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.518450 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/2 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21237 > > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 > > Sent signal to 21237 > > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 > > > for deletion > > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status > > update for task 2 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.441154 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/3 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21284 > > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 > > Sent signal to 21284 > > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 > > > for deletion > > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status > > update for task 3 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.451181 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/4 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21331 > > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 > > Sent signal to 21331 > > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 > > > for deletion > > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status > > update for task 4 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > > > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia < > notifications@github.com>wrote: > > > > > Ah, got it. So how exactly did you start the cluster -- did you not > use > > > our EC2 script? In that case you probably need to configure some > settings > > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try > > > starting a cluster with our script ( > > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up > for > > > you. > > > > > > Again, to figure out the problem, you'd need to look at the logs of > the > > > worker process. Mesos places them in a \"work\" directory for the job. > This > > > is configured to be /mnt/mesos-work when you launch Mesos with our EC2 > > > scripts, but if you launched it manually then it likely be in /tmp. > > > > > > Matei > > > > > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > > > > > As stated in the issue description, it is a dummy *single* instance > > > > cluster, i.e., master and slave are co-hosted in one box. Where > should I > > > > copy code to? -QH > > > > > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > > > notifications@github.com>wrote: > > > > > > > > > Did you copy the code you built to the worker nodes too? > > > > > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout > and > > > stderr > > > > > output for your job, and let me know what's in there. But > generally > > > \"lost > > > > > TID\" means that the JVM crashed, which likely means that it didn't > > > have the > > > > > right code or otherwise couldn't start Spark. > > > > > > > > > > Matei > > > > > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > > > > > Hi, > > > > > > > > > > > > I am using single EC2 instance with pre-built mesos > (ami-0fcb7966) > > > (Same > > > > > issue if I build mesos from source code in locall VM) > > > > > > > > > > > > Follow instruction on > > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with > some > > > > > tweaks. > > > > > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due > to > > > lack > > > > > of document) > > > > > > > > > > > > ./spartk-shell.sh > > > > > > import spark._ > > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > > > project/SparkBuild.scala > > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > I am getting error > > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > > > > mismatch. (client = 61, server = 63) > > > > > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val > HADOOP_VERSION > > > = > > > > > \"0.20.2-cdh3u3\" > > > > > > I am getting error at ec2.count() > > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; > aborting > > > job > > > > > > like the one reported at > > > > > > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > > > > > Please let me know if you cannot replicate this error, and give > more > > > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > > > > > Thanks > > > > > > > > > > > > -QH > > > > > > > > > > > >  > > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > > > > > >  > > > > > Reply to this email directly or view it on GitHub< > > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > > > > > > > > > > > > > -- > > > > Dr. Qiming He > > > > Qiming.He@openresearchinc.com > > > > 301-525-6612 (Phone) > > > > 815-327-2122 (Fax) > > > >  > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > >  > > > Reply to this email directly or view it on GitHub< > https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > > > > > > > > > > > > > -- > > Dr. Qiming He > > Qiming.He@openresearchinc.com > > 301-525-6612 (Phone) > > 815-327-2122 (Fax) > >  > > Reply to this email directly or view it on GitHub. > > > > > >  > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9515690>. > > -- Dr. Qiming He Qiming.He@openresearchinc.com 301-525-6612 (Phone) 815-327-2122 (Fax)", "created": "2012-10-17T17:46:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: It is usually just a symlink in /usr/bin/scala. Do a ls -l to figure out where the symlink points to. E.g. for mine: ~ $ ls -l `which scala` lrwxr-xr-x 1 root wheel 39 Sep 26 15:31 /usr/local/bin/scala -> /usr/local/Cellar/scala/2.9.2/bin/scala In this case, SCALA_HOME should be /usr/local/Cellar/scala/2.9.2/libexec On Wed, Oct 17, 2012 at 6:46 PM, Qiming He <notifications@github.com> wrote: > YES. That is the problem! After I set > *export SCALA_HOME=/root/scala-2.9.1.final* > it works perfectly. > > Last question: if scala is installed by package manager like yum or > apt-get > under /usr/bin/scala. > what is SCALA_HOME then? > > Thanks > > -Qiming > > > On Wed, Oct 17, 2012 at 12:27 AM, Matei Zaharia <notifications@github.com>wrote: > > > > Got it. Actually the place to look for the worker process's output is > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > > > > > In there you'll find two files, stdout and stderr, with the output of > the > > process. My guess is that it couldn't launch scala because you have not > set > > up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it. > > > > Matei > > > > On Oct 16, 2012, at 9:23 PM, Qiming He wrote: > > > > > Hi, > > > > > > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So > it > > is > > > just dummy single instance, running in AWS (should be the same as > local) > > > > > > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is > > > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val > > > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala > > > > > > No client mismatch problem, but still get error: > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > > > > > Here is tail of /tmp/mesos-slave.INFO > > > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:27.461669 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/0 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21143 > > > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 > > > Sent signal to 21143 > > > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > > > > for deletion > > > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 0 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.444704 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/1 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21190 > > > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 > > > Sent signal to 21190 > > > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 > > > > > > for deletion > > > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 1 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.518450 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/2 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21237 > > > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 > > > Sent signal to 21237 > > > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 > > > > > > for deletion > > > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 2 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.441154 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/3 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21284 > > > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 > > > Sent signal to 21284 > > > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 > > > > > > for deletion > > > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 3 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.451181 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/4 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21331 > > > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 > > > Sent signal to 21331 > > > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 > > > > > > for deletion > > > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 4 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > > > > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia < > > notifications@github.com>wrote: > > > > > > > Ah, got it. So how exactly did you start the cluster -- did you not > > use > > > > our EC2 script? In that case you probably need to configure some > > settings > > > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. > Try > > > > starting a cluster with our script ( > > > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set > up > > for > > > > you. > > > > > > > > Again, to figure out the problem, you'd need to look at the logs of > > the > > > > worker process. Mesos places them in a \"work\" directory for the job. > > This > > > > is configured to be /mnt/mesos-work when you launch Mesos with our > EC2 > > > > scripts, but if you launched it manually then it likely be in /tmp. > > > > > > > > Matei > > > > > > > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > > > > > > > As stated in the issue description, it is a dummy *single* > instance > > > > > cluster, i.e., master and slave are co-hosted in one box. Where > > should I > > > > > copy code to? -QH > > > > > > > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > > > > notifications@github.com>wrote: > > > > > > > > > > > Did you copy the code you built to the worker nodes too? > > > > > > > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout > > and > > > > stderr > > > > > > output for your job, and let me know what's in there. But > > generally > > > > \"lost > > > > > > TID\" means that the JVM crashed, which likely means that it > didn't > > > > have the > > > > > > right code or otherwise couldn't start Spark. > > > > > > > > > > > > Matei > > > > > > > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > > > > > > > Hi, > > > > > > > > > > > > > > I am using single EC2 instance with pre-built mesos > > (ami-0fcb7966) > > > > (Same > > > > > > issue if I build mesos from source code in locall VM) > > > > > > > > > > > > > > Follow instruction on > > > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with > > some > > > > > > tweaks. > > > > > > > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop > due > > to > > > > lack > > > > > > of document) > > > > > > > > > > > > > > ./spartk-shell.sh > > > > > > > import spark._ > > > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > > > > project/SparkBuild.scala > > > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > I am getting error > > > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol > version > > > > > > mismatch. (client = 61, server = 63) > > > > > > > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val > > HADOOP_VERSION > > > > = > > > > > > \"0.20.2-cdh3u3\" > > > > > > > I am getting error at ec2.count() > > > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; > > aborting > > > > job > > > > > > > like the one reported at > > > > > > > > > > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > > > > > > > Please let me know if you cannot replicate this error, and > give > > more > > > > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > > > > > > > Thanks > > > > > > > > > > > > > > -QH > > > > > > > > > > > > > > — > > > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > > > > > > > > > > — > > > > > > Reply to this email directly or view it on GitHub< > > > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > -- > > > > > Dr. Qiming He > > > > > Qiming.He@openresearchinc.com > > > > > 301-525-6612 (Phone) > > > > > 815-327-2122 (Fax) > > > > > — > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > > — > > > > Reply to this email directly or view it on GitHub< > > https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > > > > > > > > > > > > > > > > > > > -- > > > Dr. Qiming He > > > Qiming.He@openresearchinc.com > > > 301-525-6612 (Phone) > > > 815-327-2122 (Fax) > > > — > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > — > > Reply to this email directly or view it on GitHub< > https://github.com/mesos/spark/issues/280#issuecomment-9515690>. > > > > > > > > -- > Dr. Qiming He > Qiming.He@openresearchinc.com > 301-525-6612 (Phone) > 815-327-2122 (Fax) > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9550652>. > >", "created": "2012-10-17T21:05:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-280, originally reported by openreserach", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Sean R. Owen", "body": "Can I ask a meta-question? This JIRA is an example, but just one. I see hundreds of JIRAs that likely have no further action. Some are likely obsoleted by time and subsequent changes, like this one -- CDH integration is much different now and presumably fixes this. Some are feature requests or changes that de facto don't have support and therefore won't be committed. These seem like they should be closed, for clarity. Bugs are riskier to close in case they identify a real issue that still exists. Is there any momentum for, or anything I can do, to help clean up things like this just to start?", "created": "2014-07-13T10:21:43.558+0000"}, {"author": "Nicholas Chammas", "body": "+1 for cleanup of issues that likely have no further action.", "created": "2014-07-13T15:32:06.793+0000"}, {"author": "Sean R. Owen", "body": "I'm testing whether contributors can in fact close others JIRAs. It seems clear this one is stale and possibly already resolved. I'm also interested in how comfortable people are with people like me modifying JIRAs where it seems like there is a clear update or status change that needs to happen, like, it's clearly already resolved and needs to be marked as such?", "created": "2014-09-02T13:02:07.881+0000"}], "num_comments": 11, "text": "Issue: SPARK-524\nSummary: spark integration issue with Cloudera hadoop\nDescription: Hi, 1. I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM) 2. Follow instruction on https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some tweaks. 3. I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack of document) 4. ./spartk-shell.sh import spark._ val sc = new SparkContext(\"localhost:5050\",\"passwd\") val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") I am getting error Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version mismatch. (client = 61, server = 63) IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = \"0.20.2-cdh3u3\" I am getting error at ec2.count() ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job like the one reported at http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E Please let me know if you cannot replicate this error, and give more instruction on how Spark integrate with Cloudera Hadoop Thanks -QH\n\nComments (11):\n1. Patrick McFadin: Github comment from mateiz: Did you copy the code you built to the worker nodes too? If so, look in /mnt/mesos-work on the worker to find the stdout and stderr output for your job, and let me know what's in there. But generally \"lost TID\" means that the JVM crashed, which likely means that it didn't have the right code or otherwise couldn't start Spark. Matei On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > Hi, > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same issue if I build mesos from source code in locall VM) > > Follow instruction on https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some tweaks. > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack of document) > > ./spartk-shell.sh > import spark._ > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > I am getting error > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version mismatch. (client = 61, server = 63) > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = \"0.20.2-cdh3u3\" > I am getting error at ec2.count() > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > like the one reported at http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > Please let me know if you cannot replicate this error, and give more instruction on how Spark integrate with Cloudera Hadoop > > Thanks > > -QH > > — > Reply to this email directly or view it on GitHub. > >\n2. Patrick McFadin: Github comment from openreserach: As stated in the issue description, it is a dummy *single* instance cluster, i.e., master and slave are co-hosted in one box. Where should I copy code to? -QH On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <notifications@github.com>wrote: > Did you copy the code you built to the worker nodes too? > > If so, look in /mnt/mesos-work on the worker to find the stdout and stderr > output for your job, and let me know what's in there. But generally \"lost > TID\" means that the JVM crashed, which likely means that it didn't have the > right code or otherwise couldn't start Spark. > > Matei > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > Hi, > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same > issue if I build mesos from source code in locall VM) > > > > Follow instruction on > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > tweaks. > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack > of document) > > > > ./spartk-shell.sh > > import spark._ > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > I am getting error > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > mismatch. (client = 61, server = 63) > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = > \"0.20.2-cdh3u3\" > > I am getting error at ec2.count() > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > like the one reported at > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > Please let me know if you cannot replicate this error, and give more > instruction on how Spark integrate with Cloudera Hadoop > > > > Thanks > > > > -QH > > > > — > > Reply to this email directly or view it on GitHub. > > > > > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > -- Dr. Qiming He Qiming.He@openresearchinc.com 301-525-6612 (Phone) 815-327-2122 (Fax)\n3. Patrick McFadin: Github comment from mateiz: Ah, got it. So how exactly did you start the cluster -- did you not use our EC2 script? In that case you probably need to configure some settings in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try starting a cluster with our script (https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for you. Again, to figure out the problem, you'd need to look at the logs of the worker process. Mesos places them in a \"work\" directory for the job. This is configured to be /mnt/mesos-work when you launch Mesos with our EC2 scripts, but if you launched it manually then it likely be in /tmp. Matei On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > As stated in the issue description, it is a dummy *single* instance > cluster, i.e., master and slave are co-hosted in one box. Where should I > copy code to? -QH > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia <notifications@github.com>wrote: > > > Did you copy the code you built to the worker nodes too? > > > > If so, look in /mnt/mesos-work on the worker to find the stdout and stderr > > output for your job, and let me know what's in there. But generally \"lost > > TID\" means that the JVM crashed, which likely means that it didn't have the > > right code or otherwise couldn't start Spark. > > > > Matei > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > Hi, > > > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) (Same > > issue if I build mesos from source code in locall VM) > > > > > > Follow instruction on > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > > tweaks. > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to lack > > of document) > > > > > > ./spartk-shell.sh > > > import spark._ > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in project/SparkBuild.scala > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > I am getting error > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > mismatch. (client = 61, server = 63) > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION = > > \"0.20.2-cdh3u3\" > > > I am getting error at ec2.count() > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > > like the one reported at > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > Please let me know if you cannot replicate this error, and give more > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > Thanks > > > > > > -QH > > > > > > — > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > — > > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > -- > Dr. Qiming He > Qiming.He@openresearchinc.com > 301-525-6612 (Phone) > 815-327-2122 (Fax) > — > Reply to this email directly or view it on GitHub. > >\n4. Patrick McFadin: Github comment from openreserach: Hi, 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it is just dummy single instance, running in AWS (should be the same as local) 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala No client mismatch problem, but still get error: ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job Here is tail of /tmp/mesos-slave.INFO I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:27.461669 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/0 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] Forked executor at 21143 I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 Sent signal to 21143 I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 for deletion I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status update for task 0 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.444704 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/1 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] Forked executor at 21190 I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 Sent signal to 21190 I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 for deletion I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status update for task 1 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.518450 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/2 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] Forked executor at 21237 I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 Sent signal to 21237 I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 for deletion I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status update for task 2 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.441154 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/3 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] Forked executor at 21284 I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 Sent signal to 21284 I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 for deletion I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status update for task 3 of framework 20121017 0241-1056562698-5050-1405-0005 I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for framework 201210170241-1056562698-5050-1405-0 005 I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work directory for executor 'default' of framewo rk 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.451181 1421 slave.cpp:522] Using '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as work directory for executor 'default' of framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] Launching default (/root/spark/spark-executor ) in /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe cutors/default/runs/4 with resources mem=512' for framework 201210170241-1056562698-5050-1405-0005 I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] Forked executor at 21331 I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] Telling slave of lost executor default of fr amework 201210170241-1056562698-5050-1405-0005 I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 Sent signal to 21331 I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of framework 201210170241-1056562698-5050-1405-00 05 has exited with status 127 I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of framework 201210170241-1056562698-5050-1405- 0005 is now in state TASK_LOST I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory /tmp/mesos/slaves/201210170241-1056562 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 for deletion I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status update for task 4 of framework 20121017 0241-1056562698-5050-1405-0005 On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <notifications@github.com>wrote: > Ah, got it. So how exactly did you start the cluster -- did you not use > our EC2 script? In that case you probably need to configure some settings > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try > starting a cluster with our script ( > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for > you. > > Again, to figure out the problem, you'd need to look at the logs of the > worker process. Mesos places them in a \"work\" directory for the job. This > is configured to be /mnt/mesos-work when you launch Mesos with our EC2 > scripts, but if you launched it manually then it likely be in /tmp. > > Matei > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > As stated in the issue description, it is a dummy *single* instance > > cluster, i.e., master and slave are co-hosted in one box. Where should I > > copy code to? -QH > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > notifications@github.com>wrote: > > > > > Did you copy the code you built to the worker nodes too? > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout and > stderr > > > output for your job, and let me know what's in there. But generally > \"lost > > > TID\" means that the JVM crashed, which likely means that it didn't > have the > > > right code or otherwise couldn't start Spark. > > > > > > Matei > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > Hi, > > > > > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) > (Same > > > issue if I build mesos from source code in locall VM) > > > > > > > > Follow instruction on > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > > > tweaks. > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to > lack > > > of document) > > > > > > > > ./spartk-shell.sh > > > > import spark._ > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > project/SparkBuild.scala > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > I am getting error > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > > mismatch. (client = 61, server = 63) > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION > = > > > \"0.20.2-cdh3u3\" > > > > I am getting error at ec2.count() > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting > job > > > > like the one reported at > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > Please let me know if you cannot replicate this error, and give more > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > Thanks > > > > > > > > -QH > > > > > > > >  > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > >  > > > Reply to this email directly or view it on GitHub< > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > -- > > Dr. Qiming He > > Qiming.He@openresearchinc.com > > 301-525-6612 (Phone) > > 815-327-2122 (Fax) > >  > > Reply to this email directly or view it on GitHub. > > > > > >  > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > -- Dr. Qiming He Qiming.He@openresearchinc.com 301-525-6612 (Phone) 815-327-2122 (Fax)\n5. Patrick McFadin: Github comment from mateiz: Got it. Actually the place to look for the worker process's output is /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 In there you'll find two files, stdout and stderr, with the output of the process. My guess is that it couldn't launch scala because you have not set up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it. Matei On Oct 16, 2012, at 9:23 PM, Qiming He wrote: > Hi, > > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it is > just dummy single instance, running in AWS (should be the same as local) > > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala > > No client mismatch problem, but still get error: > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > Here is tail of /tmp/mesos-slave.INFO > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:27.461669 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/0 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] Forked > executor at 21143 > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 > Sent signal to 21143 > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > for deletion > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status > update for task 0 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.444704 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/1 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] Forked > executor at 21190 > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 > Sent signal to 21190 > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 > for deletion > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status > update for task 1 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.518450 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/2 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] Forked > executor at 21237 > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 > Sent signal to 21237 > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 > for deletion > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status > update for task 2 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.441154 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/3 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] Forked > executor at 21284 > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 > Sent signal to 21284 > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 > for deletion > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status > update for task 3 of framework 20121017 > 0241-1056562698-5050-1405-0005 > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for > framework 201210170241-1056562698-5050-1405-0 > 005 > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work > directory for executor 'default' of framewo > rk 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.451181 1421 slave.cpp:522] Using > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as > work directory for executor 'default' of > framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] > Launching default (/root/spark/spark-executor > ) in > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > cutors/default/runs/4 with resources > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] Forked > executor at 21331 > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] Telling > slave of lost executor default of fr > amework 201210170241-1056562698-5050-1405-0005 > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 > Sent signal to 21331 > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of framework > 201210170241-1056562698-5050-1405-00 > 05 has exited with status 127 > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of > framework 201210170241-1056562698-5050-1405- > 0005 is now in state TASK_LOST > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory > /tmp/mesos/slaves/201210170241-1056562 > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 > for deletion > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status > update for task 4 of framework 20121017 > 0241-1056562698-5050-1405-0005 > > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia <notifications@github.com>wrote: > > > Ah, got it. So how exactly did you start the cluster -- did you not use > > our EC2 script? In that case you probably need to configure some settings > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try > > starting a cluster with our script ( > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up for > > you. > > > > Again, to figure out the problem, you'd need to look at the logs of the > > worker process. Mesos places them in a \"work\" directory for the job. This > > is configured to be /mnt/mesos-work when you launch Mesos with our EC2 > > scripts, but if you launched it manually then it likely be in /tmp. > > > > Matei > > > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > > > As stated in the issue description, it is a dummy *single* instance > > > cluster, i.e., master and slave are co-hosted in one box. Where should I > > > copy code to? -QH > > > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > > notifications@github.com>wrote: > > > > > > > Did you copy the code you built to the worker nodes too? > > > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout and > > stderr > > > > output for your job, and let me know what's in there. But generally > > \"lost > > > > TID\" means that the JVM crashed, which likely means that it didn't > > have the > > > > right code or otherwise couldn't start Spark. > > > > > > > > Matei > > > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > > > Hi, > > > > > > > > > > I am using single EC2 instance with pre-built mesos (ami-0fcb7966) > > (Same > > > > issue if I build mesos from source code in locall VM) > > > > > > > > > > Follow instruction on > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with some > > > > tweaks. > > > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due to > > lack > > > > of document) > > > > > > > > > > ./spartk-shell.sh > > > > > import spark._ > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > > project/SparkBuild.scala > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > I am getting error > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > > > mismatch. (client = 61, server = 63) > > > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val HADOOP_VERSION > > = > > > > \"0.20.2-cdh3u3\" > > > > > I am getting error at ec2.count() > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting > > job > > > > > like the one reported at > > > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > > > Please let me know if you cannot replicate this error, and give more > > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > > > Thanks > > > > > > > > > > -QH > > > > > > > > > >  > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > >  > > > > Reply to this email directly or view it on GitHub< > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > > > > > > > -- > > > Dr. Qiming He > > > Qiming.He@openresearchinc.com > > > 301-525-6612 (Phone) > > > 815-327-2122 (Fax) > > >  > > > Reply to this email directly or view it on GitHub. > > > > > > > > > >  > > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > > > > > > > -- > Dr. Qiming He > Qiming.He@openresearchinc.com > 301-525-6612 (Phone) > 815-327-2122 (Fax) > — > Reply to this email directly or view it on GitHub. > >\n6. Patrick McFadin: Github comment from openreserach: YES. That is the problem! After I set *export SCALA_HOME=/root/scala-2.9.1.final* it works perfectly. Last question: if scala is installed by package manager like yum or apt-get under /usr/bin/scala. what is SCALA_HOME then? Thanks -Qiming On Wed, Oct 17, 2012 at 12:27 AM, Matei Zaharia <notifications@github.com>wrote: > Got it. Actually the place to look for the worker process's output is > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > In there you'll find two files, stdout and stderr, with the output of the > process. My guess is that it couldn't launch scala because you have not set > up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it. > > Matei > > On Oct 16, 2012, at 9:23 PM, Qiming He wrote: > > > Hi, > > > > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So it > is > > just dummy single instance, running in AWS (should be the same as local) > > > > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is > > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val > > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala > > > > No client mismatch problem, but still get error: > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > > > Here is tail of /tmp/mesos-slave.INFO > > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:27.461669 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/0 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21143 > > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 > > Sent signal to 21143 > > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > for deletion > > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of status > > update for task 0 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.444704 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/1 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21190 > > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 > > Sent signal to 21190 > > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 > > > for deletion > > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of status > > update for task 1 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.518450 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/2 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21237 > > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 > > Sent signal to 21237 > > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 > > > for deletion > > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of status > > update for task 2 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.441154 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/3 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21284 > > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 > > Sent signal to 21284 > > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 > > > for deletion > > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of status > > update for task 3 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for > > framework 201210170241-1056562698-5050-1405-0 > > 005 > > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work > > directory for executor 'default' of framewo > > rk 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.451181 1421 slave.cpp:522] Using > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as > > work directory for executor 'default' of > > framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] > > Launching default (/root/spark/spark-executor > > ) in > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > cutors/default/runs/4 with resources > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] > Forked > > executor at 21331 > > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] > Telling > > slave of lost executor default of fr > > amework 201210170241-1056562698-5050-1405-0005 > > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 > > Sent signal to 21331 > > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of > framework > > 201210170241-1056562698-5050-1405-00 > > 05 has exited with status 127 > > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of > > framework 201210170241-1056562698-5050-1405- > > 0005 is now in state TASK_LOST > > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor directory > > /tmp/mesos/slaves/201210170241-1056562 > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 > > > for deletion > > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of status > > update for task 4 of framework 20121017 > > 0241-1056562698-5050-1405-0005 > > > > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia < > notifications@github.com>wrote: > > > > > Ah, got it. So how exactly did you start the cluster -- did you not > use > > > our EC2 script? In that case you probably need to configure some > settings > > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. Try > > > starting a cluster with our script ( > > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set up > for > > > you. > > > > > > Again, to figure out the problem, you'd need to look at the logs of > the > > > worker process. Mesos places them in a \"work\" directory for the job. > This > > > is configured to be /mnt/mesos-work when you launch Mesos with our EC2 > > > scripts, but if you launched it manually then it likely be in /tmp. > > > > > > Matei > > > > > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > > > > > As stated in the issue description, it is a dummy *single* instance > > > > cluster, i.e., master and slave are co-hosted in one box. Where > should I > > > > copy code to? -QH > > > > > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > > > notifications@github.com>wrote: > > > > > > > > > Did you copy the code you built to the worker nodes too? > > > > > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout > and > > > stderr > > > > > output for your job, and let me know what's in there. But > generally > > > \"lost > > > > > TID\" means that the JVM crashed, which likely means that it didn't > > > have the > > > > > right code or otherwise couldn't start Spark. > > > > > > > > > > Matei > > > > > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > > > > > Hi, > > > > > > > > > > > > I am using single EC2 instance with pre-built mesos > (ami-0fcb7966) > > > (Same > > > > > issue if I build mesos from source code in locall VM) > > > > > > > > > > > > Follow instruction on > > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with > some > > > > > tweaks. > > > > > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop due > to > > > lack > > > > > of document) > > > > > > > > > > > > ./spartk-shell.sh > > > > > > import spark._ > > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > > > project/SparkBuild.scala > > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > I am getting error > > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol version > > > > > mismatch. (client = 61, server = 63) > > > > > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val > HADOOP_VERSION > > > = > > > > > \"0.20.2-cdh3u3\" > > > > > > I am getting error at ec2.count() > > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; > aborting > > > job > > > > > > like the one reported at > > > > > > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > > > > > Please let me know if you cannot replicate this error, and give > more > > > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > > > > > Thanks > > > > > > > > > > > > -QH > > > > > > > > > > > >  > > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > > > > > >  > > > > > Reply to this email directly or view it on GitHub< > > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > > > > > > > > > > > > > -- > > > > Dr. Qiming He > > > > Qiming.He@openresearchinc.com > > > > 301-525-6612 (Phone) > > > > 815-327-2122 (Fax) > > > >  > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > >  > > > Reply to this email directly or view it on GitHub< > https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > > > > > > > > > > > > > -- > > Dr. Qiming He > > Qiming.He@openresearchinc.com > > 301-525-6612 (Phone) > > 815-327-2122 (Fax) > >  > > Reply to this email directly or view it on GitHub. > > > > > >  > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9515690>. > > -- Dr. Qiming He Qiming.He@openresearchinc.com 301-525-6612 (Phone) 815-327-2122 (Fax)\n7. Patrick McFadin: Github comment from rxin: It is usually just a symlink in /usr/bin/scala. Do a ls -l to figure out where the symlink points to. E.g. for mine: ~ $ ls -l `which scala` lrwxr-xr-x 1 root wheel 39 Sep 26 15:31 /usr/local/bin/scala -> /usr/local/Cellar/scala/2.9.2/bin/scala In this case, SCALA_HOME should be /usr/local/Cellar/scala/2.9.2/libexec On Wed, Oct 17, 2012 at 6:46 PM, Qiming He <notifications@github.com> wrote: > YES. That is the problem! After I set > *export SCALA_HOME=/root/scala-2.9.1.final* > it works perfectly. > > Last question: if scala is installed by package manager like yum or > apt-get > under /usr/bin/scala. > what is SCALA_HOME then? > > Thanks > > -Qiming > > > On Wed, Oct 17, 2012 at 12:27 AM, Matei Zaharia <notifications@github.com>wrote: > > > > Got it. Actually the place to look for the worker process's output is > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > > > > > In there you'll find two files, stdout and stderr, with the output of > the > > process. My guess is that it couldn't launch scala because you have not > set > > up a conf/spark-env.sh file in your Spark folder with SCALA_HOME in it. > > > > Matei > > > > On Oct 16, 2012, at 9:23 PM, Qiming He wrote: > > > > > Hi, > > > > > > 1st-ly, I just use EC2 ami instance, I did not try EC2 script yet. So > it > > is > > > just dummy single instance, running in AWS (should be the same as > local) > > > > > > 2nd-ly, I cannot find /mnt/mesos-work. I am assuming it is > > > /tmp/mesos-slave.INFO. I try Apache hadoop 1.0.4, setting val > > > HADOOP_VERSION = \"1.0.4\" in project/SparkBuild.scala > > > > > > No client mismatch problem, but still get error: > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; aborting job > > > > > > Here is tail of /tmp/mesos-slave.INFO > > > I1017 04:15:27.429769 1421 slave.cpp:457] Got assigned task 0 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:27.429924 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:27.461669 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:27.463573 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/0 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:27.469944 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21143 > > > I1017 04:15:28.339529 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.339673 1421 process_utils.hpp:64] Stopping ... 21143 > > > Sent signal to 21143 > > > I1017 04:15:28.425079 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:28.427103 1421 slave.cpp:989] Status update: task 0 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:28.430907 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/0 > > > > > > for deletion > > > I1017 04:15:28.434944 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 0 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:28.442648 1421 slave.cpp:457] Got assigned task 1 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:28.442703 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.444704 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.447643 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/1 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:28.449476 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21190 > > > I1017 04:15:29.340275 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.340428 1421 process_utils.hpp:64] Stopping ... 21190 > > > Sent signal to 21190 > > > I1017 04:15:29.432467 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:29.434921 1421 slave.cpp:989] Status update: task 1 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:29.501458 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/1 > > > > > > for deletion > > > I1017 04:15:29.515281 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 1 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:29.516788 1421 slave.cpp:457] Got assigned task 2 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:29.516840 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.518450 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.520694 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/2 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:29.522667 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21237 > > > I1017 04:15:30.341961 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.342100 1421 process_utils.hpp:64] Stopping ... 21237 > > > Sent signal to 21237 > > > I1017 04:15:30.427222 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:30.428863 1421 slave.cpp:989] Status update: task 2 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:30.432663 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/2 > > > > > > for deletion > > > I1017 04:15:30.432873 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 2 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:30.440033 1421 slave.cpp:457] Got assigned task 3 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:30.440099 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.441154 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.442700 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/3 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:30.444150 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21284 > > > I1017 04:15:31.343088 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.343229 1421 process_utils.hpp:64] Stopping ... 21284 > > > Sent signal to 21284 > > > I1017 04:15:31.430294 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:31.432476 1421 slave.cpp:989] Status update: task 3 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:31.434686 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/3 > > > > > > for deletion > > > I1017 04:15:31.434814 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 3 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > I1017 04:15:31.449017 1421 slave.cpp:457] Got assigned task 4 for > > > framework 201210170241-1056562698-5050-1405-0 > > > 005 > > > I1017 04:15:31.449122 1421 slave.cpp:1559] Generating a unique work > > > directory for executor 'default' of framewo > > > rk 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.451181 1421 slave.cpp:522] Using > > > '/tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/framewor > > > > > > ks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4' as > > > work directory for executor 'default' of > > > framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.452993 1421 process_based_isolation_module.cpp:93] > > > Launching default (/root/spark/spark-executor > > > ) in > > > > > > /tmp/mesos/slaves/201210170241-1056562698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/exe > > > > > > cutors/default/runs/4 with resources > > > mem=512' for framework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:31.455411 1421 process_based_isolation_module.cpp:129] > > Forked > > > executor at 21331 > > > I1017 04:15:32.344189 1421 process_based_isolation_module.cpp:255] > > Telling > > > slave of lost executor default of fr > > > amework 201210170241-1056562698-5050-1405-0005 > > > I1017 04:15:32.344326 1421 process_utils.hpp:64] Stopping ... 21331 > > > Sent signal to 21331 > > > I1017 04:15:32.431380 1421 slave.cpp:1383] Executor 'default' of > > framework > > > 201210170241-1056562698-5050-1405-00 > > > 05 has exited with status 127 > > > I1017 04:15:32.433527 1421 slave.cpp:989] Status update: task 4 of > > > framework 201210170241-1056562698-5050-1405- > > > 0005 is now in state TASK_LOST > > > I1017 04:15:32.437247 1421 slave.cpp:1507] Scheduling executor > directory > > > /tmp/mesos/slaves/201210170241-1056562 > > > > > > > > > 698-5050-1405-0/frameworks/201210170241-1056562698-5050-1405-0005/executors/default/runs/4 > > > > > > for deletion > > > I1017 04:15:32.441869 1421 slave.cpp:699] Got acknowledgement of > status > > > update for task 4 of framework 20121017 > > > 0241-1056562698-5050-1405-0005 > > > > > > On Mon, Oct 15, 2012 at 11:45 PM, Matei Zaharia < > > notifications@github.com>wrote: > > > > > > > Ah, got it. So how exactly did you start the cluster -- did you not > > use > > > > our EC2 script? In that case you probably need to configure some > > settings > > > > in conf/spark-env.sh, such as SCALA_HOME and MESOS_NATIVE_LIBRARY. > Try > > > > starting a cluster with our script ( > > > > https://github.com/mesos/spark/wiki/EC2-Scripts) to have them set > up > > for > > > > you. > > > > > > > > Again, to figure out the problem, you'd need to look at the logs of > > the > > > > worker process. Mesos places them in a \"work\" directory for the job. > > This > > > > is configured to be /mnt/mesos-work when you launch Mesos with our > EC2 > > > > scripts, but if you launched it manually then it likely be in /tmp. > > > > > > > > Matei > > > > > > > > On Oct 15, 2012, at 8:40 PM, Qiming He wrote: > > > > > > > > > As stated in the issue description, it is a dummy *single* > instance > > > > > cluster, i.e., master and slave are co-hosted in one box. Where > > should I > > > > > copy code to? -QH > > > > > > > > > > On Mon, Oct 15, 2012 at 11:30 PM, Matei Zaharia < > > > > notifications@github.com>wrote: > > > > > > > > > > > Did you copy the code you built to the worker nodes too? > > > > > > > > > > > > If so, look in /mnt/mesos-work on the worker to find the stdout > > and > > > > stderr > > > > > > output for your job, and let me know what's in there. But > > generally > > > > \"lost > > > > > > TID\" means that the JVM crashed, which likely means that it > didn't > > > > have the > > > > > > right code or otherwise couldn't start Spark. > > > > > > > > > > > > Matei > > > > > > > > > > > > On Oct 15, 2012, at 8:17 PM, Qiming He wrote: > > > > > > > > > > > > > Hi, > > > > > > > > > > > > > > I am using single EC2 instance with pre-built mesos > > (ami-0fcb7966) > > > > (Same > > > > > > issue if I build mesos from source code in locall VM) > > > > > > > > > > > > > > Follow instruction on > > > > > > https://github.com/mesos/spark/wiki/Running-spark-on-mesos with > > some > > > > > > tweaks. > > > > > > > > > > > > > > I install Cloudera cdhu5 by yum (not using pre-built hadoop > due > > to > > > > lack > > > > > > of document) > > > > > > > > > > > > > > ./spartk-shell.sh > > > > > > > import spark._ > > > > > > > val sc = new SparkContext(\"localhost:5050\",\"passwd\") > > > > > > > val ec2 = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > > > > > > > > IF I keep val HADOOP_VERSION = \"0.20.205.0\" in > > > > project/SparkBuild.scala > > > > > > > at val file = sc.textFile(\"hdfs://localhost:8020/tmp/passwd\") > > > > > > > I am getting error > > > > > > > Protocol org.apache.hadoop.hdfs.protocol.ClientProtocol > version > > > > > > mismatch. (client = 61, server = 63) > > > > > > > > > > > > > > IF I set val HADOOP_VERSION = \"0.20.2-cdh3u5\" or val > > HADOOP_VERSION > > > > = > > > > > > \"0.20.2-cdh3u3\" > > > > > > > I am getting error at ec2.count() > > > > > > > ERROR spark.SimpleJob: Task 0:0 failed more than 4 times; > > aborting > > > > job > > > > > > > like the one reported at > > > > > > > > > > > > > http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201108.mbox/%3CBD25AE7A-C9DC-4020-AD40-41C66DCAA56E@eecs.berkeley.edu%3E > > > > > > > > > > > > > > Please let me know if you cannot replicate this error, and > give > > more > > > > > > instruction on how Spark integrate with Cloudera Hadoop > > > > > > > > > > > > > > Thanks > > > > > > > > > > > > > > -QH > > > > > > > > > > > > > > — > > > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > > > > > > > > > > — > > > > > > Reply to this email directly or view it on GitHub< > > > > https://github.com/mesos/spark/issues/280#issuecomment-9472032>. > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > -- > > > > > Dr. Qiming He > > > > > Qiming.He@openresearchinc.com > > > > > 301-525-6612 (Phone) > > > > > 815-327-2122 (Fax) > > > > > — > > > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > > > > > > > > > — > > > > Reply to this email directly or view it on GitHub< > > https://github.com/mesos/spark/issues/280#issuecomment-9472214>. > > > > > > > > > > > > > > > > > > > > -- > > > Dr. Qiming He > > > Qiming.He@openresearchinc.com > > > 301-525-6612 (Phone) > > > 815-327-2122 (Fax) > > > — > > > Reply to this email directly or view it on GitHub. > > > > > > > > > > — > > Reply to this email directly or view it on GitHub< > https://github.com/mesos/spark/issues/280#issuecomment-9515690>. > > > > > > > > -- > Dr. Qiming He > Qiming.He@openresearchinc.com > 301-525-6612 (Phone) > 815-327-2122 (Fax) > > — > Reply to this email directly or view it on GitHub<https://github.com/mesos/spark/issues/280#issuecomment-9550652>. > >\n8. Patrick McFadin: Imported from Github issue spark-280, originally reported by openreserach\n9. Sean R. Owen: Can I ask a meta-question? This JIRA is an example, but just one. I see hundreds of JIRAs that likely have no further action. Some are likely obsoleted by time and subsequent changes, like this one -- CDH integration is much different now and presumably fixes this. Some are feature requests or changes that de facto don't have support and therefore won't be committed. These seem like they should be closed, for clarity. Bugs are riskier to close in case they identify a real issue that still exists. Is there any momentum for, or anything I can do, to help clean up things like this just to start?\n10. Nicholas Chammas: +1 for cleanup of issues that likely have no further action.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.998713"}}
{"id": "67faaf7615e151c11a91d52aa53a8ec6", "issue_key": "SPARK-523", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Added a method to report slave memory status; force serialize accumulator update in local mode.", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-15T20:40:00.000+0000", "updated": "2013-04-01T22:05:03.000+0000", "resolved": "2013-04-01T22:05:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-281, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "This was an old pull request that was merged in https://github.com/mesos/spark/pull/281; closing.", "created": "2013-04-01T22:05:03.972+0000"}], "num_comments": 2, "text": "Issue: SPARK-523\nSummary: Added a method to report slave memory status; force serialize accumulator update in local mode.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-281, originally reported by rxin\n2. Josh Rosen: This was an old pull request that was merged in https://github.com/mesos/spark/pull/281; closing.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.000708"}}
{"id": "1b9105f6cd212935acd1c145b6b16027", "issue_key": "SPARK-522", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Updated Kryo to version 2.20", "description": "sbt/sbt test pass. Tried it on Shark using a join query that produces 60 million rows and it worked fine. Saw a negligible performance increase in the join query (very hard to tell the difference - if anything, ~ 5% - could well be just system variability even though I tested 20 times).", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "0012-10-16T00:59:00.000+0000", "updated": "2013-01-20T12:22:49.000+0000", "resolved": "2013-01-20T12:22:49.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Cool, that was fast! Let me figure out the branches a bit before merging this though. I want to keep branch 0.6 on Kryo 1.x, and then merge dev into master and have that take in this stuff.", "created": "2012-10-16T09:32:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: BTW - I run Shark with Kryo 2.0 through all 700 tests, and only 4 of them failed because of the following reason: spark.SparkException: Job failed: ShuffleMapTask(2236, 0) failed: ExceptionFailure(com.esotericsoftware.kryo.KryoException: java.lang .IllegalArgumentException: Can not set java.sql.Timestamp field org.apache.hadoop.hive.serde2.io.TimestampWritable.timestamp to java. util.Date Serialization trace: timestamp (org.apache.hadoop.hive.serde2.io.TimestampWritable) value (org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector) fieldObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector$MyField) fields (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector) array (scala.collection.mutable.ArrayBuffer)) I will look into this. Other than that, this looks pretty solid.", "created": "2012-10-18T11:28:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Not bad. I assume the Spark tests pass too? Matei On Oct 18, 2012, at 12:28 PM, Reynold Xin wrote: > BTW - I run Shark with Kryo 2.0 through all 700 tests, and only 4 of them failed because of the following reason: > > spark.SparkException: Job failed: ShuffleMapTask(2236, 0) failed: ExceptionFailure(com.esotericsoftware.kryo.KryoException: java.lang > .IllegalArgumentException: Can not set java.sql.Timestamp field org.apache.hadoop.hive.serde2.io.TimestampWritable.timestamp to java. > util.Date > Serialization trace: > timestamp (org.apache.hadoop.hive.serde2.io.TimestampWritable) > value (org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector) > fieldObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector$MyField) > fields (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector) > array (scala.collection.mutable.ArrayBuffer)) > > I will look into this. Other than that, this looks pretty solid. > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-10-18T13:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: That was in the original comment: sbt/sbt test passes :)", "created": "2012-10-18T13:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from rxin: I was able to fix the problem with TimestampWritable in Shark by creating a new specialized serializer for writables: ```scala kryo.register(classOf[org.apache.hadoop.hive.serde2.io.TimestampWritable], new KryoWritableSerializer[org.apache.hadoop.hive.serde2.io.TimestampWritable]) /** A Kryo serializer for Hadoop writables. */ class KryoWritableSerializer[T <: Writable] extends KSerializer[T] { override def write(kryo: Kryo, output: KryoOutput, writable: T) { val ouputStream = new DataOutputStream(output) writable.write(ouputStream) } override def read(kryo: Kryo, input: KryoInput, cls: java.lang.Class[T]): T = { val writable = cls.newInstance() val inputStream = new DataInputStream(input) writable.readFields(inputStream) writable } } ```", "created": "2012-10-18T14:33:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-282, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Resolving since this pull request was merged.", "created": "2013-01-20T12:22:49.211+0000"}], "num_comments": 7, "text": "Issue: SPARK-522\nSummary: Updated Kryo to version 2.20\nDescription: sbt/sbt test pass. Tried it on Shark using a join query that produces 60 million rows and it worked fine. Saw a negligible performance increase in the join query (very hard to tell the difference - if anything, ~ 5% - could well be just system variability even though I tested 20 times).\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: Cool, that was fast! Let me figure out the branches a bit before merging this though. I want to keep branch 0.6 on Kryo 1.x, and then merge dev into master and have that take in this stuff.\n2. Patrick McFadin: Github comment from rxin: BTW - I run Shark with Kryo 2.0 through all 700 tests, and only 4 of them failed because of the following reason: spark.SparkException: Job failed: ShuffleMapTask(2236, 0) failed: ExceptionFailure(com.esotericsoftware.kryo.KryoException: java.lang .IllegalArgumentException: Can not set java.sql.Timestamp field org.apache.hadoop.hive.serde2.io.TimestampWritable.timestamp to java. util.Date Serialization trace: timestamp (org.apache.hadoop.hive.serde2.io.TimestampWritable) value (org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector) fieldObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector$MyField) fields (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector) array (scala.collection.mutable.ArrayBuffer)) I will look into this. Other than that, this looks pretty solid.\n3. Patrick McFadin: Github comment from mateiz: Not bad. I assume the Spark tests pass too? Matei On Oct 18, 2012, at 12:28 PM, Reynold Xin wrote: > BTW - I run Shark with Kryo 2.0 through all 700 tests, and only 4 of them failed because of the following reason: > > spark.SparkException: Job failed: ShuffleMapTask(2236, 0) failed: ExceptionFailure(com.esotericsoftware.kryo.KryoException: java.lang > .IllegalArgumentException: Can not set java.sql.Timestamp field org.apache.hadoop.hive.serde2.io.TimestampWritable.timestamp to java. > util.Date > Serialization trace: > timestamp (org.apache.hadoop.hive.serde2.io.TimestampWritable) > value (org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector) > fieldObjectInspector (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector$MyField) > fields (org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector) > array (scala.collection.mutable.ArrayBuffer)) > > I will look into this. Other than that, this looks pretty solid. > > — > Reply to this email directly or view it on GitHub. > >\n4. Patrick McFadin: Github comment from rxin: That was in the original comment: sbt/sbt test passes :)\n5. Patrick McFadin: Github comment from rxin: I was able to fix the problem with TimestampWritable in Shark by creating a new specialized serializer for writables: ```scala kryo.register(classOf[org.apache.hadoop.hive.serde2.io.TimestampWritable], new KryoWritableSerializer[org.apache.hadoop.hive.serde2.io.TimestampWritable]) /** A Kryo serializer for Hadoop writables. */ class KryoWritableSerializer[T <: Writable] extends KSerializer[T] { override def write(kryo: Kryo, output: KryoOutput, writable: T) { val ouputStream = new DataOutputStream(output) writable.write(ouputStream) } override def read(kryo: Kryo, input: KryoInput, cls: java.lang.Class[T]): T = { val writable = cls.newInstance() val inputStream = new DataInputStream(input) writable.readFields(inputStream) writable } } ```\n6. Patrick McFadin: Imported from Github issue spark-282, originally reported by rxin\n7. Josh Rosen: Resolving since this pull request was merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.001718"}}
{"id": "c844180b303f2d26bbf3e772c6e990cd", "issue_key": "SPARK-520", "issue_type": "Improvement", "status": "Resolved", "priority": null, "resolution": null, "summary": "Structure SBT build file into modules.", "description": "*Build Changes* This does a bit of housekeeping for the SparkBuild SBT file. It should be considered a first cut and not an ideal. Please feel free to modify to taste. - It is intended to be \"neutral\" in effect in the sense no build semantics were meant to be changed. - No dependency versions were changed. - It does publish-local a Spark build successfully on my machine. *Breakage* - The Maven stuff is certainly not working and I hope this is something that can be repaired quickly. Sorry about that. *Modifications* - I added an additional file ShellPrompt.scala This modifies the SBT prompt to show the prompt as project:branch:project-version> - As a result of that, I changed the root projects name from \"root\" to \"spark\" as seeing a console prompt starting with the word \"root\" gives me palpitations. - I took the liberty of adding two additional scalac compiler settings -Xlint and -Xcheckinit. Ray", "reporter": "RayRacine", "assignee": null, "created": "0012-10-18T08:30:00.000+0000", "updated": "2014-09-03T11:38:39.000+0000", "resolved": "2014-09-03T11:38:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from RayRacine: I added a comment about something I noted in the build file. /* Workaround for issue #206 (fixed after SBT 0.11.0) */ // Is this still applicable? Remove? RPR watchTransitiveSources <<= Defaults.inDependencies[Task[Seq[File]]] (watchSources.task, const (std.TaskExtra.constant (Nil)), aggregate = true, includeRoot = true) apply { _.join.map (_.flatten) }, Since Spark is now on SBT 11.3 (maybe heading to 12.2) does anyone recall if this is still necessary?", "created": "2012-10-18T08:40:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-284, originally reported by RayRacine", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Sean R. Owen", "body": "Given subsequent reorganization of SBT and Maven build, I think this is obsolete.", "created": "2014-09-03T11:38:39.980+0000"}], "num_comments": 3, "text": "Issue: SPARK-520\nSummary: Structure SBT build file into modules.\nDescription: *Build Changes* This does a bit of housekeeping for the SparkBuild SBT file. It should be considered a first cut and not an ideal. Please feel free to modify to taste. - It is intended to be \"neutral\" in effect in the sense no build semantics were meant to be changed. - No dependency versions were changed. - It does publish-local a Spark build successfully on my machine. *Breakage* - The Maven stuff is certainly not working and I hope this is something that can be repaired quickly. Sorry about that. *Modifications* - I added an additional file ShellPrompt.scala This modifies the SBT prompt to show the prompt as project:branch:project-version> - As a result of that, I changed the root projects name from \"root\" to \"spark\" as seeing a console prompt starting with the word \"root\" gives me palpitations. - I took the liberty of adding two additional scalac compiler settings -Xlint and -Xcheckinit. Ray\n\nComments (3):\n1. Patrick McFadin: Github comment from RayRacine: I added a comment about something I noted in the build file. /* Workaround for issue #206 (fixed after SBT 0.11.0) */ // Is this still applicable? Remove? RPR watchTransitiveSources <<= Defaults.inDependencies[Task[Seq[File]]] (watchSources.task, const (std.TaskExtra.constant (Nil)), aggregate = true, includeRoot = true) apply { _.join.map (_.flatten) }, Since Spark is now on SBT 11.3 (maybe heading to 12.2) does anyone recall if this is still necessary?\n2. Patrick McFadin: Imported from Github issue spark-284, originally reported by RayRacine\n3. Sean R. Owen: Given subsequent reorganization of SBT and Maven build, I think this is obsolete.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.001718"}}
{"id": "dfad860c3be729b9989055823de58fe8", "issue_key": "SPARK-521", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Create repository for /root/mesos-ec2 scripts", "description": "The Spark AMI contains scripts in `/root/mesos-ec2` which do not appear to be part of the standard Mesos distribution and are not in any public repositories. For example, `/root/mesos-ec2/compute_cluster_url.py` and `/root/mesos-ec2/deploy_templates.py`. It looks like there are also some modifications to the standard mesos-ec2 scripts to call these new scripts. These should probably in a public repository and the EC2 script README should have a link to it. The current EC2 scripts can be hard to understand since these files aren't included. For example, the 0.5.1 release notes mention that the launch script automatically configures Spark's memory limit; this feature appears to be implemented in `/root/mesos-ec2/deploy_templates.py`, so I couldn't find a commit for it in Spark.", "reporter": "Josh Rosen", "assignee": "Shivaram Venkataraman", "created": "0012-10-18T08:30:00.000+0000", "updated": "2013-04-05T19:47:03.000+0000", "resolved": "2013-04-05T19:47:03.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I like this idea, and it should be a accompanied by a guide to making your own AMIs. On Oct 18, 2012, at 9:30 AM, Josh Rosen wrote: > The Spark AMI contains scripts in /root/mesos-ec2 which do not appear to be part of the standard Mesos distribution and are not in any public repositories. > > For example, /root/mesos-ec2/compute_cluster_url.py and /root/mesos-ec2/deploy_templates.py. It looks like there are also some modifications to the standard mesos-ec2 scripts to call these new scripts. > > These should probably in a public repository and the EC2 script README should have a link to it. The current EC2 scripts can be hard to understand since these files aren't included. For example, the 0.5.1 release notes mention that the launch script automatically configures Spark's memory limit; this feature appears to be implemented in /root/mesos-ec2/deploy_templates.py, so I couldn't find a commit for it in Spark. > > — > Reply to this email directly or view it on GitHub. > >", "created": "2012-10-18T13:16:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-283, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Peter Sankauskas", "body": "Should these scripts be copied from the AMI and placed into here instead of a new repo? https://github.com/mesos/spark/tree/master/ec2/deploy.generic/root/mesos-ec2 I am happy to do this if that is all that is required. Please assign to me.", "created": "2012-11-19T11:39:11.027+0000"}, {"author": "Matei Alexandru Zaharia", "body": "No, I think it's best to keep them in a separate Git repo that is checked out on the AMI. The deploy.generic setup is there so that the spark-ec2 scripts can work with any version of the AMI, including possibly future versions. We don't want to require users to update their local Spark version in order to be able to launch a fixed AMI. We'll move the stuff that is on the AMI into a separate repo, but I think that's something we have to do on this end, along with maybe a better way to package the AMIs in the first place.", "created": "2012-11-19T13:48:14.244+0000"}, {"author": "Peter Sankauskas", "body": "I am happy to help out with the AMI creation scripts. Have done plenty in the past, including using Ansible do make it easy to maintain and rerun.", "created": "2012-11-19T14:40:49.896+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Great! As I discussed on the mailing list, an automatic script would be awesome. Just make sure it works with the way spark-ec2 deploys stuff, which is basically to copy a file containing some environment variables to ~/mesos-ec2 and then to run a script that's on the machine.", "created": "2012-11-19T15:06:53.133+0000"}, {"author": "Josh Rosen", "body": "A repository for these scripts was created when we released 0.7; it's at https://github.com/mesos/spark-ec2/", "created": "2013-04-05T19:47:03.759+0000"}], "num_comments": 7, "text": "Issue: SPARK-521\nSummary: Create repository for /root/mesos-ec2 scripts\nDescription: The Spark AMI contains scripts in `/root/mesos-ec2` which do not appear to be part of the standard Mesos distribution and are not in any public repositories. For example, `/root/mesos-ec2/compute_cluster_url.py` and `/root/mesos-ec2/deploy_templates.py`. It looks like there are also some modifications to the standard mesos-ec2 scripts to call these new scripts. These should probably in a public repository and the EC2 script README should have a link to it. The current EC2 scripts can be hard to understand since these files aren't included. For example, the 0.5.1 release notes mention that the launch script automatically configures Spark's memory limit; this feature appears to be implemented in `/root/mesos-ec2/deploy_templates.py`, so I couldn't find a commit for it in Spark.\n\nComments (7):\n1. Patrick McFadin: Github comment from mateiz: I like this idea, and it should be a accompanied by a guide to making your own AMIs. On Oct 18, 2012, at 9:30 AM, Josh Rosen wrote: > The Spark AMI contains scripts in /root/mesos-ec2 which do not appear to be part of the standard Mesos distribution and are not in any public repositories. > > For example, /root/mesos-ec2/compute_cluster_url.py and /root/mesos-ec2/deploy_templates.py. It looks like there are also some modifications to the standard mesos-ec2 scripts to call these new scripts. > > These should probably in a public repository and the EC2 script README should have a link to it. The current EC2 scripts can be hard to understand since these files aren't included. For example, the 0.5.1 release notes mention that the launch script automatically configures Spark's memory limit; this feature appears to be implemented in /root/mesos-ec2/deploy_templates.py, so I couldn't find a commit for it in Spark. > > — > Reply to this email directly or view it on GitHub. > >\n2. Patrick McFadin: Imported from Github issue spark-283, originally reported by JoshRosen\n3. Peter Sankauskas: Should these scripts be copied from the AMI and placed into here instead of a new repo? https://github.com/mesos/spark/tree/master/ec2/deploy.generic/root/mesos-ec2 I am happy to do this if that is all that is required. Please assign to me.\n4. Matei Alexandru Zaharia: No, I think it's best to keep them in a separate Git repo that is checked out on the AMI. The deploy.generic setup is there so that the spark-ec2 scripts can work with any version of the AMI, including possibly future versions. We don't want to require users to update their local Spark version in order to be able to launch a fixed AMI. We'll move the stuff that is on the AMI into a separate repo, but I think that's something we have to do on this end, along with maybe a better way to package the AMIs in the first place.\n5. Peter Sankauskas: I am happy to help out with the AMI creation scripts. Have done plenty in the past, including using Ansible do make it easy to maintain and rerun.\n6. Matei Alexandru Zaharia: Great! As I discussed on the mailing list, an automatic script would be awesome. Just make sure it works with the way spark-ec2 deploys stuff, which is basically to copy a file containing some environment variables to ~/mesos-ec2 and then to run a script that's on the machine.\n7. Josh Rosen: A repository for these scripts was created when we released 0.7; it's at https://github.com/mesos/spark-ec2/", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.001718"}}
{"id": "dd5b5db0d716ae4b2c25dd81260fa5a3", "issue_key": "SPARK-519", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Support for Hadoop 2 distributions such as cdh4", "description": "Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore. Instead, you are now required to use the `JobContextImpl` and `TaskAttemptContextImpl` classes, respectively. This pull requests extracts the code in Spark where it uses these, into separate traits that are located in Hadoop major version specific source folders that the sbt build then includes/excludes as necessary. In addition, for Hadoop 2, Spark now also needs to depend on the `hadoop-client` artifact. This pull request is a new version of #264 against the 0.6 dev branch.", "reporter": "Thomas Dudziak", "assignee": "Thomas Dudziak", "created": "0012-10-18T15:11:00.000+0000", "updated": "2013-05-04T19:12:39.000+0000", "resolved": "2013-05-04T19:12:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tomdz: This approach will require that there are two Spark artifacts published to the maven repo (one for Hadoop 1 and one for Hadoop 2). I have not made those changes to the `SparkBuild` file primarily because I'm not very familar with sbt (we use Maven to generate our Spark build). Also worth noting that we played around with an alternate approach where Spark deals with the Hadoop incompatibilities at runtime by using reflection. The benefit of this would be a single Spark artifact. However this introduces a runtime overhead for every task/job that spark generates in HDFS, plus it removes type safety and overall feels less clean.", "created": "2012-10-18T15:15:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-285, originally reported by tomdz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "[~tomdzk]'s pull request adding Hadoop 2 support was merged and was first released in v0.6.1 (https://github.com/mesos/spark/commit/15e95be2fd26161416736a323d114ff1fc98f941). Resolving this as fixed.", "created": "2013-05-04T19:12:39.224+0000"}], "num_comments": 3, "text": "Issue: SPARK-519\nSummary: Support for Hadoop 2 distributions such as cdh4\nDescription: Hadoop 2 unfortunately introduced backwards-incompatible changes in classes used by Spark, `JobContext` and `TaskAttemptContext` in both the `mapred` and `mapreduce` packages. In the new codebase, these classes are now interfaces and thus cannot be instantiated directly anymore. Instead, you are now required to use the `JobContextImpl` and `TaskAttemptContextImpl` classes, respectively. This pull requests extracts the code in Spark where it uses these, into separate traits that are located in Hadoop major version specific source folders that the sbt build then includes/excludes as necessary. In addition, for Hadoop 2, Spark now also needs to depend on the `hadoop-client` artifact. This pull request is a new version of #264 against the 0.6 dev branch.\n\nComments (3):\n1. Patrick McFadin: Github comment from tomdz: This approach will require that there are two Spark artifacts published to the maven repo (one for Hadoop 1 and one for Hadoop 2). I have not made those changes to the `SparkBuild` file primarily because I'm not very familar with sbt (we use Maven to generate our Spark build). Also worth noting that we played around with an alternate approach where Spark deals with the Hadoop incompatibilities at runtime by using reflection. The benefit of this would be a single Spark artifact. However this introduces a runtime overhead for every task/job that spark generates in HDFS, plus it removes type safety and overall feels less clean.\n2. Patrick McFadin: Imported from Github issue spark-285, originally reported by tomdz\n3. Josh Rosen: [~tomdzk]'s pull request adding Hadoop 2 support was merged and was first released in v0.6.1 (https://github.com/mesos/spark/commit/15e95be2fd26161416736a323d114ff1fc98f941). Resolving this as fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "06b59caee72a634c892fc4c0e13f855f", "issue_key": "SPARK-518", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "Allow EC2 script to stop/destroy cluster after master/slave failures", "description": "This commit solves a problem where `spark-ec2` could not destroy or stop a cluster if its master or all of its slaves had been destroyed. It also prints the total count of master and slave nodes before printing error messages.", "reporter": "Josh Rosen", "assignee": null, "created": "0012-10-18T16:00:00.000+0000", "updated": "2012-10-24T00:00:22.000+0000", "resolved": "2012-10-24T00:00:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-286, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Closing, since this pull request was merged.", "created": "2012-10-24T00:00:22.185+0000"}], "num_comments": 2, "text": "Issue: SPARK-518\nSummary: Allow EC2 script to stop/destroy cluster after master/slave failures\nDescription: This commit solves a problem where `spark-ec2` could not destroy or stop a cluster if its master or all of its slaves had been destroyed. It also prints the total count of master and slave nodes before printing error messages.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-286, originally reported by JoshRosen\n2. Josh Rosen: Closing, since this pull request was merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "e3ec6a33dac5da68bf22e0a0f4207914", "issue_key": "SPARK-290", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Use SPARK_MASTER_IP if it is set in start-slaves.sh.", "description": "Also check to prompt user if it is not set and the script cannot figure out the master's ip.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-19T00:09:00.000+0000", "updated": "2012-10-19T22:50:34.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-287, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-290\nSummary: Use SPARK_MASTER_IP if it is set in start-slaves.sh.\nDescription: Also check to prompt user if it is not set and the script cannot figure out the master's ip.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-287, originally reported by rxin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "84b93b8802233ac711a4eb5c46cba488", "issue_key": "SPARK-517", "issue_type": "Bug", "status": "Closed", "priority": null, "resolution": null, "summary": "Exiting spark-ec2 with unfulfilled spot instance request does not cancel request", "description": "Exiting the `spark-ec2` script after placing a spot instance request will not cancel that request, which may cause extra instances to be launched. The script should probably register an `atexit` or `SIGINT` handler to perform cleanup. There are some potential race conditions here, so canceling the spot instance request might not work if its instances are just beginning to launch; we may want to print a warning message about this.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "0012-10-19T08:37:00.000+0000", "updated": "2012-11-01T11:36:04.000+0000", "resolved": "2012-11-01T11:36:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-288, originally reported by JoshRosen", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/297", "created": "2012-11-01T11:36:03.985+0000"}], "num_comments": 2, "text": "Issue: SPARK-517\nSummary: Exiting spark-ec2 with unfulfilled spot instance request does not cancel request\nDescription: Exiting the `spark-ec2` script after placing a spot instance request will not cancel that request, which may cause extra instances to be launched. The script should probably register an `atexit` or `SIGINT` handler to perform cleanup. There are some potential race conditions here, so canceling the spot instance request might not work if its instances are just beginning to launch; we may want to print a warning message about this.\n\nComments (2):\n1. Patrick McFadin: Imported from Github issue spark-288, originally reported by JoshRosen\n2. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/297", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "ff7e2ae382c204655b5e960ce45bc2ad", "issue_key": "SPARK-516", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Improve error reporting when slaves fail to start", "description": "Currently Spark just hangs waiting for resources and slaves to respond. This behavior is very confusing to users, especially first time users. If an error message is generated, it should be propagated back to the master so the user is aware of it.", "reporter": "Reynold Xin", "assignee": null, "created": "0012-10-19T10:49:00.000+0000", "updated": "2015-11-08T20:24:16.000+0000", "resolved": "2015-11-08T20:24:16.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-289, originally reported by rxin", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Neelesh Srinivas Salian", "body": "[~rxin] is this still relevant?", "created": "2015-11-08T20:20:38.289+0000"}, {"author": "Reynold Xin", "body": "Marking as not a problem since it's been a really long time.", "created": "2015-11-08T20:24:16.935+0000"}], "num_comments": 3, "text": "Issue: SPARK-516\nSummary: Improve error reporting when slaves fail to start\nDescription: Currently Spark just hangs waiting for resources and slaves to respond. This behavior is very confusing to users, especially first time users. If an error message is generated, it should be propagated back to the master so the user is aware of it.\n\nComments (3):\n1. Patrick McFadin: Imported from Github issue spark-289, originally reported by rxin\n2. Neelesh Srinivas Salian: [~rxin] is this still relevant?\n3. Reynold Xin: Marking as not a problem since it's been a really long time.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "ae6578ab5b275bdc09b3f6bedb5c6ae2", "issue_key": "SPARK-579", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "An anonymous submission to Spark", "description": "I found a bug! I am going to anonymously report it :)", "reporter": null, "assignee": "Patrick McFadin", "created": "2012-10-19T21:28:15.000+0000", "updated": "2012-10-19T21:28:41.000+0000", "resolved": "2012-10-19T21:28:41.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "This was a test, I am resolving.", "created": "2012-10-19T21:28:41.208+0000"}], "num_comments": 1, "text": "Issue: SPARK-579\nSummary: An anonymous submission to Spark\nDescription: I found a bug! I am going to anonymously report it :)\n\nComments (1):\n1. Patrick McFadin: This was a test, I am resolving.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "fe3f4e5074fcec8bf666e770ec213510", "issue_key": "SPARK-580", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Use Spark local directory as PySpark tmp directory", "description": "", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-10-19T23:01:52.000+0000", "updated": "2013-02-01T12:20:43.000+0000", "resolved": "2013-02-01T12:20:43.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/439", "created": "2013-02-01T12:20:43.893+0000"}], "num_comments": 1, "text": "Issue: SPARK-580\nSummary: Use Spark local directory as PySpark tmp directory\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/439", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "8d1da820035e9fb868aa3baa72466d39", "issue_key": "SPARK-581", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Try using a smart commit and seeing if it works", "description": "", "reporter": "Patrick McFadin", "assignee": null, "created": "2012-10-19T23:12:33.000+0000", "updated": "2012-10-19T23:40:05.000+0000", "resolved": "2012-10-19T23:19:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Worked! cool.", "created": "2012-10-19T23:40:05.790+0000"}], "num_comments": 1, "text": "Issue: SPARK-581\nSummary: Try using a smart commit and seeing if it works\n\nComments (1):\n1. Patrick McFadin: Worked! cool.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.002615"}}
{"id": "d55df148426eb971324397c8e18835bd", "issue_key": "SPARK-582", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Add unit test for complex column (lazy column)", "description": "46e4baba281dc994666d21773d6151eea01c/src/test/scala/shark/memstore/ColumnSuite.scala", "reporter": "Reynold Xin", "assignee": null, "created": "2012-10-20T02:21:48.000+0000", "updated": "2012-10-20T11:09:39.000+0000", "resolved": "2012-10-20T11:09:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Should this JIRA be under SHARK?", "created": "2012-10-20T10:46:59.099+0000"}, {"author": "Reynold Xin", "body": "This is a Shark issue created in the wrong project.", "created": "2012-10-20T11:09:39.002+0000"}], "num_comments": 2, "text": "Issue: SPARK-582\nSummary: Add unit test for complex column (lazy column)\nDescription: 46e4baba281dc994666d21773d6151eea01c/src/test/scala/shark/memstore/ColumnSuite.scala\n\nComments (2):\n1. Patrick McFadin: Should this JIRA be under SHARK?\n2. Reynold Xin: This is a Shark issue created in the wrong project.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.003709"}}
{"id": "918df0c934e8acbeaed10c601c10ee98", "issue_key": "SPARK-583", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Failures in BlockStore may lead to infinite loops of task failures", "description": "Summary: failures in BlockStore may lead to infinite loops of task failures. I ran into a situation where a block manager operation failed:  12/10/20 21:25:13 ERROR storage.BlockManagerWorker: Exception handling buffer message com.esotericsoftware.kryo.SerializationException: Buffer limit exceeded writing object of type: shark.ColumnarWritable at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:492) at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:78) at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:58) at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:73) at spark.storage.BlockManager.dataSerialize(BlockManager.scala:834) at spark.storage.MemoryStore.getBytes(MemoryStore.scala:72) at spark.storage.BlockManager.getLocalBytes(BlockManager.scala:311) at spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:79) at spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:58) at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33) at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.IndexedSeqLike$Elements.foreach(IndexedSeqLike.scala:54) at scala.collection.IterableLike$class.foreach(IterableLike.scala:73) at spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:12) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at spark.storage.BlockMessageArray.map(BlockMessageArray.scala:12) at spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:33) at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23) at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23) at spark.network.ConnectionManager.spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:276) at spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:242) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:679)  This failure appears to have been detected via a fetch failure in the following stage:  12/10/20 21:25:12 INFO scheduler.DAGScheduler: Marking Stage 2 (mapPartitions at Operator.scala:197) for resubmision due to a fetch failure 12/10/20 21:25:12 INFO scheduler.DAGScheduler: The failed fetch was from Stage 3 (mapPartitions at Operator.scala:197); marking it for resubmission  The failed task was retried on the same machine, and it executed without exceptions. However, the job is unable to make forward progress; the scheduler gets stuck in an infinite loop of the form  12/10/20 22:23:08 INFO spark.CacheTrackerActor: Asked for current cache locations 12/10/20 22:23:08 INFO scheduler.DAGScheduler: Resubmitting Stage 3 (mapPartitions at Operator.scala:197) because some of its tasks had failed: 220 12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting Stage 3 (mapPartitions at Operator.scala:197), which has no missing parents 12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 3 12/10/20 22:23:08 INFO cluster.ClusterScheduler: Adding task set 3.4080 with 1 tasks 12/10/20 22:23:08 INFO cluster.TaskSetManager: Starting task 3.4080:0 as TID 5484 on slave 201210202106-1093469194-5050-5222-43: domU-12-31-39-14-5E-5E.compute-1.internal (preferred) 12/10/20 22:23:08 INFO cluster.TaskSetManager: Serialized task 3.4080:0 as 7605 bytes in 0 ms 12/10/20 22:23:09 INFO cluster.TaskSetManager: Finished TID 5484 in 820 ms (progress: 1/1) 12/10/20 22:23:09 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(3, 220) 12/10/20 22:23:09 INFO scheduler.DAGScheduler: ShuffleMapTask finished with host domU-12-31-39-14-5E-5E.compute-1.internal 12/10/20 22:23:09 INFO scheduler.DAGScheduler: Stage 3 (mapPartitions at Operator.scala:197) finished; looking for newly runnable stages 12/10/20 22:23:09 INFO scheduler.DAGScheduler: running: Set() 12/10/20 22:23:09 INFO scheduler.DAGScheduler: waiting: Set(Stage 2, Stage 1) 12/10/20 22:23:09 INFO scheduler.DAGScheduler: failed: Set() 12/10/20 22:23:09 INFO spark.CacheTrackerActor: Asked for current cache locations  If I look at the worker that is running these tasks, I see infinite loop of the form  12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_0 already exists on this machine; not re-adding it 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_1 already exists on this machine; not re-adding it 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_2 already exists on this machine; not re-adding it [..] 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_199 already exists on this machine; not re-adding it 12/10/20 21:29:29 INFO executor.Executor: Serialized size of result for 1677 is 350 12/10/20 21:29:29 INFO executor.Executor: Finished task ID 1677 12/10/20 21:29:29 INFO executor.Executor: Running task ID 1678 12/10/20 21:29:29 INFO executor.Executor: Its generation is 3 12/10/20 21:29:29 INFO spark.CacheTracker: Cache key is rdd_4_220 12/10/20 21:29:29 INFO spark.CacheTracker: Found partition in cache! 12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Running Pre-Shuffle Group-By 12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Mapside hash aggregation enabled 12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5  I'm not sure of the exact cause of this behavior, but I have a guess: During the original failed execution, the task's output blocks were not stored but their block ids were added to the BlockManager's {{blockInfo}} map. This prevents these blocks from being recomputed and causes the \"{{Block shuffle_*_*_* already exists on this machine; not re-adding it}}\" warnings. As a result, the block is never stored and the master is never informed of its location. This causes the DAG scheduler to repeatedly launch the same task in an infinite loop, saying that it is \"{{Resubmitting Stage * because some of its tasks had failed: *}}\".", "reporter": "Josh Rosen", "assignee": "Charles Reiss", "created": "2012-10-20T15:54:00.000+0000", "updated": "2013-05-19T13:31:09.000+0000", "resolved": "2013-05-19T13:31:08.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "I haven't been able to reproduce the infinite loop of task retrials, but I did try modifying the {{DiskStore}} to randomly fail with {{IOException}} while trying to write blocks. On a local standalone cluster, these failures were detected but the job hung after trying to recompute the failed tasks. If I catch the exception and remove the block's {{blockId}} from the {{BlockManager}}'s {{blockInfo}} map, then the job is eventually able to succeed. This suggests that we should add exception handling inside the {{BlockManager}}'s {{put()}} and {{putBytes()}} calls. There is one subtle race condition to watch for, though: between the time that we store the {{blockId}} and the time that the write fails, another task may have retrieved that block's {{BlockInfo}} object and began to wait by calling {{waitForReady()}}. To avoid deadlock, this waiter needs to notified of the failure.", "created": "2012-10-26T15:54:42.291+0000"}, {"author": "Josh Rosen", "body": "I managed to run across this issue again today, using a branch based on Spark 0.6.1. Even though no exceptions occurred, I saw the same infinite loop of task resubmissions and \"Block already exists\" messages. Two machines exhibited loops, and both loops were preceded by a block being dropped from the memory store to free up memory. This is a bit strange, since it looks like the machine had plenty of free memory:  12/12/14 19:43:36 INFO storage.MemoryStore: Block rdd_323_43 of size 199481138 dropped from memory (free 8579469803) 12/12/14 19:43:36 INFO executor.StandaloneExecutorBackend: Got assigned task 76480 12/12/14 19:43:36 INFO executor.Executor: Running task ID 76480 12/12/14 19:43:36 INFO executor.Executor: Its generation is 325 12/12/14 19:43:36 INFO spark.CacheTracker: Cache key is rdd_645_43 12/12/14 19:43:36 INFO storage.BlockManager: Getting local block rdd_645_43 12/12/14 19:43:36 INFO storage.BlockManager: Getting remote block rdd_645_43 12/12/14 19:43:36 INFO spark.CacheTracker: Computing partition spark.ParallelCollectionSplit@6de0 12/12/14 19:43:39 INFO storage.MemoryStore: ensureFreeSpace(199481138) called with curMem=0, maxMem=8579469803 12/12/14 19:43:39 INFO storage.MemoryStore: Block rdd_645_43 stored as values to memory (estimated size 190.2 MB, free 7.8 GB)  My job looked something like this:  val samples = sc.parallelize(1 to numMachines, numMachines).flatMap(_ => generate_pairs) samples.cache() samples.count() // Force evaluation // In a loop: samples.groupByKey().count()  The real code used preshuffle() and the CoalescedRDD class in my skew-handling branch instead of performing the regular groupByKey(). It looks like a partition of {{samples}} was lost and recomputed. Following this, the resubmitted tasks were ShuffleMapTasks; the master log said that they correspond to the flatMap() line, so they are shuffling the recomputed {{samples}} partition. However, it looks like these shuffle blocks are already in the BlockManager:  12/12/14 19:43:39 INFO executor.StandaloneExecutorBackend: Got assigned task 76530 12/12/14 19:43:39 INFO executor.Executor: Running task ID 76530 12/12/14 19:43:39 INFO executor.Executor: Its generation is 325 12/12/14 19:43:39 INFO spark.CacheTracker: Cache key is rdd_645_43 12/12/14 19:43:39 INFO storage.BlockManager: Getting local block rdd_645_43 12/12/14 19:43:39 INFO spark.CacheTracker: Found partition in cache! 12/12/14 19:43:43 INFO executor.Executor: Serialized size of result for 76530 is 184 12/12/14 19:43:43 INFO executor.Executor: Finished task ID 76530 12/12/14 19:43:43 INFO executor.StandaloneExecutorBackend: Got assigned task 76538 12/12/14 19:43:43 INFO executor.Executor: Running task ID 76538 12/12/14 19:43:43 INFO executor.Executor: Its generation is 325 12/12/14 19:43:43 INFO spark.CacheTracker: Cache key is rdd_645_43 12/12/14 19:43:43 INFO storage.BlockManager: Getting local block rdd_645_43 12/12/14 19:43:43 INFO spark.CacheTracker: Found partition in cache! 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_0 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_1 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_2 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_3 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_4 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_5 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_6 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_7 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_8 already exists on this machine; not re-adding it ...  My theory: * A block being dropped somehow results in shuffle blocks on the same machine being marked as missing in the master's MapOutputTracker (or some other bookkeeping structure on the master). * This causes the master to resubmit tasks, because blocks required by the next phase seem to be missing. * The blocks were never actually lost, so they cannot be recomputed. * The master is not notified of the existence of these blocks, because put() returns before updating the master if the block already exists. * This leads to an infinite loop.", "created": "2012-12-14T16:03:52.106+0000"}, {"author": "Josh Rosen", "body": "On closer inspection of the master logs, it looks like the looping machines were marked as dead hosts, causing the master CacheTracker to mark their caches as lost. It looks like the dead host comes back, causing tasks to be scheduled on it:  12/12/14 19:23:37 INFO cluster.TaskSetManager: Lost TID 38348 (task 484.0:37) 12/12/14 19:23:37 INFO cluster.TaskSetManager: Loss was due to fetch failure from BlockManagerId(ip-10-12-129-38, 52252) taskLaunchOverhead is 0 milliseconds taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Marking Stage 484 (CoalescedShuffleFetcherRDD at SkewBenchmark.scala:77) for resubmision due to a fetch failure 12/12/14 19:23:37 INFO scheduler.DAGScheduler: The failed fetch was from Stage 483 (flatMap at SkewBenchmark.scala:43); marking it for resubmission 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Host lost: ip-10-12-129-38 12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Trying to remove the host: ip-10-12-129-38 from BlockManagerMaster. 12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Previous hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-12-129-38, 52252), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063)) 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38350 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Current hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063)) 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38329 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO storage.BlockManagerMaster: Removed ip-10-12-129-38 successfully in notifyADeadHost 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38341 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38360 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38351 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO spark.CacheTrackerActor: Memory cache lost on ip-10-12-129-38 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38334 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38323 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO spark.CacheTracker: CacheTracker successfully removed entries on ip-10-12-129-38 12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38327 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Resubmitting failed stages 12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting Stage 483 (flatMap at SkewBenchmark.scala:43), which has no missing parents 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 483 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Adding task set 483.1 with 1 tasks 12/12/14 19:23:37 INFO cluster.TaskSetManager: Starting task 483.1:0 as TID 38361 on slave worker-20121214185650-ip-10-144-65-37-40073: ip-10-144-65-37 (preferred) 12/12/14 19:23:37 INFO cluster.TaskSetManager: Serialized task 483.1:0 as 1398 bytes in 0 ms taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38346 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38332 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38352 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Registering block manager ip-10-12-129-38:52252 with 8.0 GB RAM 12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Added rdd_323_39 in memory on ip-10-12-129-38:52252 (size: 190.2 MB, free: 7.8 GB)  When the DAGScheduler handles task completion, it does not store records of output locations that are on dead hosts:  case smt: ShuffleMapTask => val stage = idToStage(smt.stageId) val status = event.result.asInstanceOf[MapStatus] val host = status.address.ip logInfo(\"ShuffleMapTask finished with host \" + host) if (!deadHosts.contains(host)) { // TODO: Make sure hostnames are consistent with Mesos stage.addOutputLoc(smt.partition, status) }  Hosts are permanently marked as dead, so this leads to an infinite loop as tasks scheduled on those machines can never register their output locations. It looks like the solution is to properly handle the return of dead hosts.", "created": "2012-12-14T17:04:06.962+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Charles may have recently fixed this in https://github.com/mesos/spark/pull/317. Talk with him to see whether it handles this case.", "created": "2012-12-14T18:28:15.281+0000"}, {"author": "Josh Rosen", "body": "My branch already includes Charles' pull request. His code allows the dead node's block manager to reconnect (which actually takes place in the above log), but doesn't address my specific problem. The problem is that the dead node's hostname is never removed from DAGScheduler's {{deadHosts}} set, even if that node returns.", "created": "2012-12-14T18:42:45.684+0000"}, {"author": "Josh Rosen", "body": "It looks like Charles fixed this in https://github.com/mesos/spark/pull/408, which made it into 0.7.0.", "created": "2013-05-19T13:31:08.981+0000"}], "num_comments": 6, "text": "Issue: SPARK-583\nSummary: Failures in BlockStore may lead to infinite loops of task failures\nDescription: Summary: failures in BlockStore may lead to infinite loops of task failures. I ran into a situation where a block manager operation failed:  12/10/20 21:25:13 ERROR storage.BlockManagerWorker: Exception handling buffer message com.esotericsoftware.kryo.SerializationException: Buffer limit exceeded writing object of type: shark.ColumnarWritable at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:492) at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:78) at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:58) at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:73) at spark.storage.BlockManager.dataSerialize(BlockManager.scala:834) at spark.storage.MemoryStore.getBytes(MemoryStore.scala:72) at spark.storage.BlockManager.getLocalBytes(BlockManager.scala:311) at spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:79) at spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:58) at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33) at spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:33) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.IndexedSeqLike$Elements.foreach(IndexedSeqLike.scala:54) at scala.collection.IterableLike$class.foreach(IterableLike.scala:73) at spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:12) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at spark.storage.BlockMessageArray.map(BlockMessageArray.scala:12) at spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:33) at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23) at spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:23) at spark.network.ConnectionManager.spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:276) at spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:242) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:679)  This failure appears to have been detected via a fetch failure in the following stage:  12/10/20 21:25:12 INFO scheduler.DAGScheduler: Marking Stage 2 (mapPartitions at Operator.scala:197) for resubmision due to a fetch failure 12/10/20 21:25:12 INFO scheduler.DAGScheduler: The failed fetch was from Stage 3 (mapPartitions at Operator.scala:197); marking it for resubmission  The failed task was retried on the same machine, and it executed without exceptions. However, the job is unable to make forward progress; the scheduler gets stuck in an infinite loop of the form  12/10/20 22:23:08 INFO spark.CacheTrackerActor: Asked for current cache locations 12/10/20 22:23:08 INFO scheduler.DAGScheduler: Resubmitting Stage 3 (mapPartitions at Operator.scala:197) because some of its tasks had failed: 220 12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting Stage 3 (mapPartitions at Operator.scala:197), which has no missing parents 12/10/20 22:23:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 3 12/10/20 22:23:08 INFO cluster.ClusterScheduler: Adding task set 3.4080 with 1 tasks 12/10/20 22:23:08 INFO cluster.TaskSetManager: Starting task 3.4080:0 as TID 5484 on slave 201210202106-1093469194-5050-5222-43: domU-12-31-39-14-5E-5E.compute-1.internal (preferred) 12/10/20 22:23:08 INFO cluster.TaskSetManager: Serialized task 3.4080:0 as 7605 bytes in 0 ms 12/10/20 22:23:09 INFO cluster.TaskSetManager: Finished TID 5484 in 820 ms (progress: 1/1) 12/10/20 22:23:09 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(3, 220) 12/10/20 22:23:09 INFO scheduler.DAGScheduler: ShuffleMapTask finished with host domU-12-31-39-14-5E-5E.compute-1.internal 12/10/20 22:23:09 INFO scheduler.DAGScheduler: Stage 3 (mapPartitions at Operator.scala:197) finished; looking for newly runnable stages 12/10/20 22:23:09 INFO scheduler.DAGScheduler: running: Set() 12/10/20 22:23:09 INFO scheduler.DAGScheduler: waiting: Set(Stage 2, Stage 1) 12/10/20 22:23:09 INFO scheduler.DAGScheduler: failed: Set() 12/10/20 22:23:09 INFO spark.CacheTrackerActor: Asked for current cache locations  If I look at the worker that is running these tasks, I see infinite loop of the form  12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_0 already exists on this machine; not re-adding it 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_1 already exists on this machine; not re-adding it 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_2 already exists on this machine; not re-adding it [..] 12/10/20 21:29:29 WARN storage.BlockManager: Block shuffle_0_220_199 already exists on this machine; not re-adding it 12/10/20 21:29:29 INFO executor.Executor: Serialized size of result for 1677 is 350 12/10/20 21:29:29 INFO executor.Executor: Finished task ID 1677 12/10/20 21:29:29 INFO executor.Executor: Running task ID 1678 12/10/20 21:29:29 INFO executor.Executor: Its generation is 3 12/10/20 21:29:29 INFO spark.CacheTracker: Cache key is rdd_4_220 12/10/20 21:29:29 INFO spark.CacheTracker: Found partition in cache! 12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Running Pre-Shuffle Group-By 12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: Mapside hash aggregation enabled 12/10/20 21:29:29 INFO exec.GroupByPreShuffleOperator: #hash table=24918 #rows=100000 reduction=0.24918 minReduction=0.5  I'm not sure of the exact cause of this behavior, but I have a guess: During the original failed execution, the task's output blocks were not stored but their block ids were added to the BlockManager's {{blockInfo}} map. This prevents these blocks from being recomputed and causes the \"{{Block shuffle_*_*_* already exists on this machine; not re-adding it}}\" warnings. As a result, the block is never stored and the master is never informed of its location. This causes the DAG scheduler to repeatedly launch the same task in an infinite loop, saying that it is \"{{Resubmitting Stage * because some of its tasks had failed: *}}\".\n\nComments (6):\n1. Josh Rosen: I haven't been able to reproduce the infinite loop of task retrials, but I did try modifying the {{DiskStore}} to randomly fail with {{IOException}} while trying to write blocks. On a local standalone cluster, these failures were detected but the job hung after trying to recompute the failed tasks. If I catch the exception and remove the block's {{blockId}} from the {{BlockManager}}'s {{blockInfo}} map, then the job is eventually able to succeed. This suggests that we should add exception handling inside the {{BlockManager}}'s {{put()}} and {{putBytes()}} calls. There is one subtle race condition to watch for, though: between the time that we store the {{blockId}} and the time that the write fails, another task may have retrieved that block's {{BlockInfo}} object and began to wait by calling {{waitForReady()}}. To avoid deadlock, this waiter needs to notified of the failure.\n2. Josh Rosen: I managed to run across this issue again today, using a branch based on Spark 0.6.1. Even though no exceptions occurred, I saw the same infinite loop of task resubmissions and \"Block already exists\" messages. Two machines exhibited loops, and both loops were preceded by a block being dropped from the memory store to free up memory. This is a bit strange, since it looks like the machine had plenty of free memory:  12/12/14 19:43:36 INFO storage.MemoryStore: Block rdd_323_43 of size 199481138 dropped from memory (free 8579469803) 12/12/14 19:43:36 INFO executor.StandaloneExecutorBackend: Got assigned task 76480 12/12/14 19:43:36 INFO executor.Executor: Running task ID 76480 12/12/14 19:43:36 INFO executor.Executor: Its generation is 325 12/12/14 19:43:36 INFO spark.CacheTracker: Cache key is rdd_645_43 12/12/14 19:43:36 INFO storage.BlockManager: Getting local block rdd_645_43 12/12/14 19:43:36 INFO storage.BlockManager: Getting remote block rdd_645_43 12/12/14 19:43:36 INFO spark.CacheTracker: Computing partition spark.ParallelCollectionSplit@6de0 12/12/14 19:43:39 INFO storage.MemoryStore: ensureFreeSpace(199481138) called with curMem=0, maxMem=8579469803 12/12/14 19:43:39 INFO storage.MemoryStore: Block rdd_645_43 stored as values to memory (estimated size 190.2 MB, free 7.8 GB)  My job looked something like this:  val samples = sc.parallelize(1 to numMachines, numMachines).flatMap(_ => generate_pairs) samples.cache() samples.count() // Force evaluation // In a loop: samples.groupByKey().count()  The real code used preshuffle() and the CoalescedRDD class in my skew-handling branch instead of performing the regular groupByKey(). It looks like a partition of {{samples}} was lost and recomputed. Following this, the resubmitted tasks were ShuffleMapTasks; the master log said that they correspond to the flatMap() line, so they are shuffling the recomputed {{samples}} partition. However, it looks like these shuffle blocks are already in the BlockManager:  12/12/14 19:43:39 INFO executor.StandaloneExecutorBackend: Got assigned task 76530 12/12/14 19:43:39 INFO executor.Executor: Running task ID 76530 12/12/14 19:43:39 INFO executor.Executor: Its generation is 325 12/12/14 19:43:39 INFO spark.CacheTracker: Cache key is rdd_645_43 12/12/14 19:43:39 INFO storage.BlockManager: Getting local block rdd_645_43 12/12/14 19:43:39 INFO spark.CacheTracker: Found partition in cache! 12/12/14 19:43:43 INFO executor.Executor: Serialized size of result for 76530 is 184 12/12/14 19:43:43 INFO executor.Executor: Finished task ID 76530 12/12/14 19:43:43 INFO executor.StandaloneExecutorBackend: Got assigned task 76538 12/12/14 19:43:43 INFO executor.Executor: Running task ID 76538 12/12/14 19:43:43 INFO executor.Executor: Its generation is 325 12/12/14 19:43:43 INFO spark.CacheTracker: Cache key is rdd_645_43 12/12/14 19:43:43 INFO storage.BlockManager: Getting local block rdd_645_43 12/12/14 19:43:43 INFO spark.CacheTracker: Found partition in cache! 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_0 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_1 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_2 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_3 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_4 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_5 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_6 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_7 already exists on this machine; not re-adding it 12/12/14 19:43:43 WARN storage.BlockManager: Block shuffle_320_43_8 already exists on this machine; not re-adding it ...  My theory: * A block being dropped somehow results in shuffle blocks on the same machine being marked as missing in the master's MapOutputTracker (or some other bookkeeping structure on the master). * This causes the master to resubmit tasks, because blocks required by the next phase seem to be missing. * The blocks were never actually lost, so they cannot be recomputed. * The master is not notified of the existence of these blocks, because put() returns before updating the master if the block already exists. * This leads to an infinite loop.\n3. Josh Rosen: On closer inspection of the master logs, it looks like the looping machines were marked as dead hosts, causing the master CacheTracker to mark their caches as lost. It looks like the dead host comes back, causing tasks to be scheduled on it:  12/12/14 19:23:37 INFO cluster.TaskSetManager: Lost TID 38348 (task 484.0:37) 12/12/14 19:23:37 INFO cluster.TaskSetManager: Loss was due to fetch failure from BlockManagerId(ip-10-12-129-38, 52252) taskLaunchOverhead is 0 milliseconds taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Marking Stage 484 (CoalescedShuffleFetcherRDD at SkewBenchmark.scala:77) for resubmision due to a fetch failure 12/12/14 19:23:37 INFO scheduler.DAGScheduler: The failed fetch was from Stage 483 (flatMap at SkewBenchmark.scala:43); marking it for resubmission 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Host lost: ip-10-12-129-38 12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Trying to remove the host: ip-10-12-129-38 from BlockManagerMaster. 12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Previous hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-12-129-38, 52252), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063)) 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38350 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO storage.BlockManagerMasterActor: Current hosts: ArrayBuffer(BlockManagerId(ip-10-144-65-13, 57952), BlockManagerId(ip-10-12-130-28, 38283), BlockManagerId(ip-10-152-157-13, 57580), BlockManagerId(ip-10-60-97-229, 60271), BlockManagerId(ip-10-152-161-81, 51219), BlockManagerId(ip-10-12-130-82, 37488), BlockManagerId(ip-10-144-141-107, 51761), BlockManagerId(ip-10-152-155-24, 44270), BlockManagerId(ip-10-152-146-166, 44224), BlockManagerId(ip-10-152-150-78, 34278), BlockManagerId(ip-10-145-205-182, 50214), BlockManagerId(ip-10-152-151-66, 59577), BlockManagerId(ip-10-145-205-217, 47570), BlockManagerId(ip-10-145-179-146, 55921), BlockManagerId(ip-10-144-72-168, 37861), BlockManagerId(ip-10-152-166-209, 35915), BlockManagerId(ip-10-145-203-221, 60217), BlockManagerId(ip-10-144-140-91, 48968), BlockManagerId(ip-10-152-148-80, 57024), BlockManagerId(ip-10-144-132-152, 42273), BlockManagerId(ip-10-147-128-53, 44072), BlockManagerId(ip-10-152-146-72, 50587), BlockManagerId(ip-10-60-99-32, 53933), BlockManagerId(ip-10-145-212-30, 58755), BlockManagerId(ip-10-147-130-32, 34711), BlockManagerId(ip-10-152-150-104, 60430), BlockManagerId(ip-10-152-156-131, 40720), BlockManagerId(ip-10-152-148-92, 43706), BlockManagerId(ip-10-147-129-249, 45660), BlockManagerId(ip-10-152-151-151, 51417), BlockManagerId(ip-10-145-203-198, 44413), BlockManagerId(ip-10-152-159-35, 41803), BlockManagerId(ip-10-145-203-17, 53531), BlockManagerId(ip-10-147-129-235, 40164), BlockManagerId(ip-10-60-99-168, 54819), BlockManagerId(ip-10-147-129-219, 38611), BlockManagerId(ip-10-152-165-130, 46365), BlockManagerId(ip-10-144-135-87, 50616), BlockManagerId(ip-10-152-138-241, 45156), BlockManagerId(ip-10-152-146-74, 45560), BlockManagerId(ip-10-145-230-152, 39402), BlockManagerId(ip-10-152-157-159, 58826), BlockManagerId(ip-10-10-189-14, 39648), BlockManagerId(ip-10-152-156-104, 42281), BlockManagerId(ip-10-144-65-37, 36194), BlockManagerId(ip-10-152-148-69, 48816), BlockManagerId(ip-10-62-94-62, 41136), BlockManagerId(ip-10-60-71-25, 42670), BlockManagerId(ip-10-144-65-33, 36063)) 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38329 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO storage.BlockManagerMaster: Removed ip-10-12-129-38 successfully in notifyADeadHost 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38341 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38360 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38351 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO spark.CacheTrackerActor: Memory cache lost on ip-10-12-129-38 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38334 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38323 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO spark.CacheTracker: CacheTracker successfully removed entries on ip-10-12-129-38 12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38327 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Ignoring update from TID 38333 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Resubmitting failed stages 12/12/14 19:23:37 INFO spark.CacheTrackerActor: Asked for current cache locations 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting Stage 483 (flatMap at SkewBenchmark.scala:43), which has no missing parents 12/12/14 19:23:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 483 12/12/14 19:23:37 INFO cluster.ClusterScheduler: Adding task set 483.1 with 1 tasks 12/12/14 19:23:37 INFO cluster.TaskSetManager: Starting task 483.1:0 as TID 38361 on slave worker-20121214185650-ip-10-144-65-37-40073: ip-10-144-65-37 (preferred) 12/12/14 19:23:37 INFO cluster.TaskSetManager: Serialized task 483.1:0 as 1398 bytes in 0 ms taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38346 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38332 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO cluster.ClusterScheduler: Ignoring update from TID 38352 because its task set is gone taskLaunchOverhead is 0 milliseconds 12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Registering block manager ip-10-12-129-38:52252 with 8.0 GB RAM 12/12/14 19:23:38 INFO storage.BlockManagerMasterActor: Added rdd_323_39 in memory on ip-10-12-129-38:52252 (size: 190.2 MB, free: 7.8 GB)  When the DAGScheduler handles task completion, it does not store records of output locations that are on dead hosts:  case smt: ShuffleMapTask => val stage = idToStage(smt.stageId) val status = event.result.asInstanceOf[MapStatus] val host = status.address.ip logInfo(\"ShuffleMapTask finished with host \" + host) if (!deadHosts.contains(host)) { // TODO: Make sure hostnames are consistent with Mesos stage.addOutputLoc(smt.partition, status) }  Hosts are permanently marked as dead, so this leads to an infinite loop as tasks scheduled on those machines can never register their output locations. It looks like the solution is to properly handle the return of dead hosts.\n4. Matei Alexandru Zaharia: Charles may have recently fixed this in https://github.com/mesos/spark/pull/317. Talk with him to see whether it handles this case.\n5. Josh Rosen: My branch already includes Charles' pull request. His code allows the dead node's block manager to reconnect (which actually takes place in the above log), but doesn't address my specific problem. The problem is that the dead node's hostname is never removed from DAGScheduler's {{deadHosts}} set, even if that node returns.\n6. Josh Rosen: It looks like Charles fixed this in https://github.com/mesos/spark/pull/408, which made it into 0.7.0.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.003709"}}
{"id": "7ea6de5afb9cf509da215fd0e78f099a", "issue_key": "SPARK-584", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Pass slave ip address when starting a cluster", "description": "Pass slave ip address from conf while starting a cluster: bin/start-slaves.sh is used to start all the slaves in the cluster. While the slave class takes a --ip argument, we don't pass the ip address from the conf/slaves.", "reporter": null, "assignee": null, "created": "2012-10-22T14:39:08.000+0000", "updated": "2016-01-05T21:13:57.000+0000", "resolved": "2016-01-05T21:13:56.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Lv, Qi", "body": "I've created a patch to fix this issue. A brief explanation: I modified \"slaves.sh\" to support a special placeholder string of \"\\_\\_SLAVE\\_IP\\_\\_\", which will be convert to the real slave ip when execute cmd on each slaves. With such enhancement, the rest works should be trivial, just add \"--ip \\_\\_SLAVE\\_IP\\_\\_\" into the execution cmdline of \"start-salves.sh\".", "created": "2012-12-12T23:34:40.778+0000"}, {"author": "Matthew Farrellee", "body": "what's the use case for this?", "created": "2014-09-21T18:22:03.692+0000"}, {"author": "Josh Rosen", "body": "Resolving as \"incomplete.\" Please submit a PR if this is still an issue.", "created": "2016-01-05T21:13:56.946+0000"}], "num_comments": 3, "text": "Issue: SPARK-584\nSummary: Pass slave ip address when starting a cluster\nDescription: Pass slave ip address from conf while starting a cluster: bin/start-slaves.sh is used to start all the slaves in the cluster. While the slave class takes a --ip argument, we don't pass the ip address from the conf/slaves.\n\nComments (3):\n1. Lv, Qi: I've created a patch to fix this issue. A brief explanation: I modified \"slaves.sh\" to support a special placeholder string of \"\\_\\_SLAVE\\_IP\\_\\_\", which will be convert to the real slave ip when execute cmd on each slaves. With such enhancement, the rest works should be trivial, just add \"--ip \\_\\_SLAVE\\_IP\\_\\_\" into the execution cmdline of \"start-salves.sh\".\n2. Matthew Farrellee: what's the use case for this?\n3. Josh Rosen: Resolving as \"incomplete.\" Please submit a PR if this is still an issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.004614"}}
{"id": "a9cba2b80d5600813640fd4d2efd3a72", "issue_key": "SPARK-585", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Mesos may not work with mesos:// URLs", "description": "According to some users. I guess we should strip the mesos:// at the front.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2012-10-23T12:01:53.000+0000", "updated": "2012-11-21T11:43:07.000+0000", "resolved": "2012-11-21T11:43:07.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-585\nSummary: Mesos may not work with mesos:// URLs\nDescription: According to some users. I guess we should strip the mesos:// at the front.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.004614"}}
{"id": "1ac42f4af114ff982aa574fb05b216c8", "issue_key": "SPARK-586", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Update docs to say that you need to set MESOS_NATIVE_LIBRARY when running standalone apps on Mesos", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2012-10-23T12:02:27.000+0000", "updated": "2012-10-24T10:01:52.000+0000", "resolved": "2012-10-24T10:01:52.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Duplicates SPARK-589", "created": "2012-10-24T10:01:52.764+0000"}], "num_comments": 1, "text": "Issue: SPARK-586\nSummary: Update docs to say that you need to set MESOS_NATIVE_LIBRARY when running standalone apps on Mesos\n\nComments (1):\n1. Patrick McFadin: Duplicates SPARK-589", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.004614"}}
{"id": "d27219d86e0834d9f0189c85f11cc66d", "issue_key": "SPARK-587", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Test issue", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2012-10-23T21:53:27.000+0000", "updated": "2012-10-23T21:56:54.000+0000", "resolved": "2012-10-23T21:56:54.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Test comment", "created": "2012-10-23T21:54:53.364+0000"}], "num_comments": 1, "text": "Issue: SPARK-587\nSummary: Test issue\n\nComments (1):\n1. Matei Alexandru Zaharia: Test comment", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.004614"}}
{"id": "227b21adff4a06aedd36bf01340eca4b", "issue_key": "SPARK-588", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Have assembly example in quick start, or elsewhere in docs", "description": "", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2012-10-24T09:47:53.000+0000", "updated": "2012-12-12T14:02:09.000+0000", "resolved": "2012-12-12T14:02:09.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Patrick McFadin", "body": "This already exists in \"linking with Spark\" I think that's probably the best place for this (might be too much for the quickstart). So I'm closing.", "created": "2012-12-12T14:01:56.626+0000"}], "num_comments": 1, "text": "Issue: SPARK-588\nSummary: Have assembly example in quick start, or elsewhere in docs\n\nComments (1):\n1. Patrick McFadin: This already exists in \"linking with Spark\" I think that's probably the best place for this (might be too much for the quickstart). So I'm closing.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.004614"}}
{"id": "5323b52f9dcca654eec3a3b316cf0497", "issue_key": "SPARK-589", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos", "description": "", "reporter": "Patrick McFadin", "assignee": null, "created": "2012-10-24T09:57:13.000+0000", "updated": "2013-08-10T16:31:04.000+0000", "resolved": "2013-08-10T16:31:04.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Patrick McFadin", "body": "This was fixed a while ago.", "created": "2013-08-10T16:31:04.281+0000"}], "num_comments": 1, "text": "Issue: SPARK-589\nSummary: MESOS_NATIVE_LIBRARY env var needs to be set when running on Mesos\n\nComments (1):\n1. Patrick McFadin: This was fixed a while ago.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.004614"}}
{"id": "4b957c920a6b5cdfd3f84e218d92d363", "issue_key": "SPARK-590", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Log task size when it's too large on master", "description": "For now, log once per job if the closure is > 100Kb.", "reporter": "Patrick McFadin", "assignee": null, "created": "2012-10-24T09:58:27.000+0000", "updated": "2013-12-07T13:04:53.000+0000", "resolved": "2013-12-07T13:04:53.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/apache/incubator-spark/pull/207", "created": "2013-12-07T13:04:53.144+0000"}], "num_comments": 1, "text": "Issue: SPARK-590\nSummary: Log task size when it's too large on master\nDescription: For now, log once per job if the closure is > 100Kb.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/apache/incubator-spark/pull/207", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.005614"}}
{"id": "e8b2cf864577afe8a288210506a4d2e0", "issue_key": "SPARK-591", "issue_type": "New Feature", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "Configuration System", "description": "Use a “SparkConf” object instead of system properties. Also, don’t rely on SPARK_MEM environment variable.", "reporter": "Denny Britz", "assignee": "Denny Britz", "created": "2012-10-24T15:57:16.000+0000", "updated": "2012-10-24T16:20:29.000+0000", "resolved": "2012-10-24T16:20:29.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Denny Britz", "body": "Duplicate of https://spark-project.atlassian.net/browse/SPARK-544.", "created": "2012-10-24T16:20:22.932+0000"}], "num_comments": 1, "text": "Issue: SPARK-591\nSummary: Configuration System\nDescription: Use a “SparkConf” object instead of system properties. Also, don’t rely on SPARK_MEM environment variable.\n\nComments (1):\n1. Denny Britz: Duplicate of https://spark-project.atlassian.net/browse/SPARK-544.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.005614"}}
{"id": "81defc8202881a9321239109f3564c6e", "issue_key": "SPARK-592", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Total memory size on per-RDD basis", "description": "Expose total memory size on a per-RDD basis.", "reporter": "Denny Britz", "assignee": null, "created": "2012-10-24T15:58:10.000+0000", "updated": "2014-09-21T18:18:53.000+0000", "resolved": "2013-08-06T23:20:37.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-592\nSummary: Total memory size on per-RDD basis\nDescription: Expose total memory size on a per-RDD basis.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.005614"}}
{"id": "7e58433ad4a597a1a069a515ae6e6ad5", "issue_key": "SPARK-593", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Send last few lines of failed standalone mode or Mesos task to master", "description": "Send last few lines of failed standalone mode or Mesos task to master.", "reporter": "Denny Britz", "assignee": "Denny Britz", "created": "2012-10-24T15:58:43.000+0000", "updated": "2016-01-16T13:00:13.000+0000", "resolved": "2016-01-16T13:00:13.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-593\nSummary: Send last few lines of failed standalone mode or Mesos task to master\nDescription: Send last few lines of failed standalone mode or Mesos task to master.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.005614"}}
{"id": "bab44dc2e6dfca6f41c63d9a968282e4", "issue_key": "SPARK-594", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Update examples to pass JARs when building a SparkContext in 0.6 and master", "description": "The \"right\" way to create a SparkContext is to pass a list of JARs and a sparkHome to the constructor, but our examples still use the older 2-parameter constructor that doesn't take these parameters, and rely on the spark.examples code being present on every worker node's classpath. For some reason a commit fixing this was in branch 0.5 but never made it into 0.6 and master.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2012-10-24T21:54:17.000+0000", "updated": "2013-02-25T20:30:12.000+0000", "resolved": "2013-02-25T20:30:12.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Fixed in https://github.com/mesos/spark/commit/5d7b591cfe14177f083814fe3e81745c5d279810", "created": "2013-02-25T20:30:12.453+0000"}], "num_comments": 1, "text": "Issue: SPARK-594\nSummary: Update examples to pass JARs when building a SparkContext in 0.6 and master\nDescription: The \"right\" way to create a SparkContext is to pass a list of JARs and a sparkHome to the constructor, but our examples still use the older 2-parameter constructor that doesn't take these parameters, and rely on the spark.examples code being present on every worker node's classpath. For some reason a commit fixing this was in branch 0.5 but never made it into 0.6 and master.\n\nComments (1):\n1. Matei Alexandru Zaharia: Fixed in https://github.com/mesos/spark/commit/5d7b591cfe14177f083814fe3e81745c5d279810", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.005614"}}
{"id": "4ed3987dc5f6be6a1e7eb4b820b12781", "issue_key": "SPARK-595", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Document \"local-cluster\" mode", "description": "The 'Spark Standalone Mode' guide describes how to manually launch a standalone cluster, which can be done locally for testing, but it does not mention SparkContext's `local-cluster` option. What are the differences between these approaches? Which one should I prefer for local testing? Can I still use the standalone web interface if I use 'local-cluster' mode? It would be useful to document this.", "reporter": "Josh Rosen", "assignee": "Yuto Akutsu", "created": "2012-10-26T15:26:45.000+0000", "updated": "2021-08-06T14:29:22.000+0000", "resolved": "2021-08-06T14:28:21.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "local-cluster is only for unit tests, to emulate a distributed cluster in a single JVM (it will still launch executors in separate ones, but at least you won't need one JVM per slave). For user applications there's no reason not to run the normal \"local\". So this is actually why I didn't include it in the doc, because the name can confuse people.", "created": "2012-10-26T19:12:41.290+0000"}, {"author": "Josh Rosen", "body": "Thanks for the explanation; I'm closing this as \"Won't Fix\".", "created": "2012-10-26T19:55:03.512+0000"}, {"author": "Josh Rosen", "body": "Maybe we should revisit this and actually document this; it could be useful for users who run Spark on a single machine with a large amount of memory (say 512 gigabytes, for example). I think it's also been recommended on the mailing list a few times.", "created": "2013-12-03T12:54:06.368+0000"}, {"author": "Evan Chan", "body": "I agree with Josh. This is really useful for running tests and simulating conditions that cannot be simulated in the local[n] node, namely serialization issues and multi-JVM issues.", "created": "2013-12-12T10:23:00.807+0000"}, {"author": "Vladimir Grigor", "body": "+1 for reopen", "created": "2015-01-26T16:47:01.938+0000"}, {"author": "Josh Rosen", "body": "I've re-opened this issue. Folks are using the API in the wild and we're not going to break compatibility for it, so we should document it.", "created": "2015-01-26T19:47:19.966+0000"}, {"author": "Justin Uang", "body": "+1 We are using for internal testing to ensure that our kryo serialization works.", "created": "2015-06-17T20:12:13.310+0000"}, {"author": "Josh Rosen", "body": "Hey Justin, do you want to submit a PR for this? I'll help review.", "created": "2015-06-17T20:17:17.889+0000"}, {"author": "Justin Uang", "body": "Sure, I'll get to it after I finish some tasks for work =)", "created": "2015-06-18T13:53:41.392+0000"}, {"author": "Apache Spark", "body": "User 'yutoacts' has created a pull request for this issue: https://github.com/apache/spark/pull/33537", "created": "2021-07-27T09:17:22.927+0000"}], "num_comments": 10, "text": "Issue: SPARK-595\nSummary: Document \"local-cluster\" mode\nDescription: The 'Spark Standalone Mode' guide describes how to manually launch a standalone cluster, which can be done locally for testing, but it does not mention SparkContext's `local-cluster` option. What are the differences between these approaches? Which one should I prefer for local testing? Can I still use the standalone web interface if I use 'local-cluster' mode? It would be useful to document this.\n\nComments (10):\n1. Matei Alexandru Zaharia: local-cluster is only for unit tests, to emulate a distributed cluster in a single JVM (it will still launch executors in separate ones, but at least you won't need one JVM per slave). For user applications there's no reason not to run the normal \"local\". So this is actually why I didn't include it in the doc, because the name can confuse people.\n2. Josh Rosen: Thanks for the explanation; I'm closing this as \"Won't Fix\".\n3. Josh Rosen: Maybe we should revisit this and actually document this; it could be useful for users who run Spark on a single machine with a large amount of memory (say 512 gigabytes, for example). I think it's also been recommended on the mailing list a few times.\n4. Evan Chan: I agree with Josh. This is really useful for running tests and simulating conditions that cannot be simulated in the local[n] node, namely serialization issues and multi-JVM issues.\n5. Vladimir Grigor: +1 for reopen\n6. Josh Rosen: I've re-opened this issue. Folks are using the API in the wild and we're not going to break compatibility for it, so we should document it.\n7. Justin Uang: +1 We are using for internal testing to ensure that our kryo serialization works.\n8. Josh Rosen: Hey Justin, do you want to submit a PR for this? I'll help review.\n9. Justin Uang: Sure, I'll get to it after I finish some tasks for work =)\n10. Apache Spark: User 'yutoacts' has created a pull request for this issue: https://github.com/apache/spark/pull/33537", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.006617"}}
{"id": "2cd3d6c012f3695799ff674cfeea90ba", "issue_key": "SPARK-596", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Memory Dashboard", "description": "Provide a dashboard that gives a view into the BlockManager and RDD storage.", "reporter": "Denny Britz", "assignee": "Denny Britz", "created": "2012-10-27T11:52:39.000+0000", "updated": "2013-05-04T22:59:40.000+0000", "resolved": "2013-05-04T22:59:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "This was added in https://github.com/mesos/spark/pull/401 and released in 0.7.0.", "created": "2013-05-04T22:59:40.103+0000"}], "num_comments": 1, "text": "Issue: SPARK-596\nSummary: Memory Dashboard\nDescription: Provide a dashboard that gives a view into the BlockManager and RDD storage.\n\nComments (1):\n1. Josh Rosen: This was added in https://github.com/mesos/spark/pull/401 and released in 0.7.0.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.006617"}}
{"id": "4c20c8425bfbeb7ce3dfee23acab70e9", "issue_key": "SPARK-597", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "HashPartitioner incorrectly partitions RDD[Array[_]]", "description": "Java arrays have {{hashCodes}} that are based on the arrays' identities rather than their contents [1]. As a result, attempting to partition an {{RDD[Array[_]]}} using a {{HashPartitioner}} will produce an unexpected/incorrect result. This was the cause of a bug in PySpark, where I hash partition PairRDDs with {{Array[Byte]}} keys. In PySpark, I fixed this by using a custom {{Partitioner}} that calls {{Arrays.hashCode(byte[])}} when passed an {{Array[Byte]}} [2]. I would like to address this issue more generally in Spark. We could add logic to {{HashPartitioner}} to perform special handling for arrays, but I'm not sure whether the additional branching would add a significant performance overhead. The logic could become messy because the {{Arrays}} module defines {{Arrays.hashCode()}} for primitive arrays and {{Arrays.deepHashCode()}} for Object arrays. Perhaps Guava or Apache Commons has an implementation of this. An alternative would be to keep the current {{HashPartitioner}} and add logic to print warnings (or to fail with an error) when shuffling an {{RDD[Array[_]]}} using the default {{HashPartitioner}}. [1] http://stackoverflow.com/questions/744735/java-array-hashcode-implementation [2] https://github.com/JoshRosen/spark/commit/2ccf3b665280bf5b0919e3801d028126cb070dbd", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-10-28T22:51:45.000+0000", "updated": "2014-10-08T22:27:50.000+0000", "resolved": "2013-01-17T10:43:12.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "There might be similar problems if arrays are used as {{Map}} keys or are stored in a {{Set}}.", "created": "2012-10-28T22:59:40.934+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This is a good catch, but it seems tough to do the right thing automatically in HashPartitioner, as you said. I think one interim fix is to add an ArrayPartitioner (or even ByteArrayPartitioner) and ask people to use that when their keys are byte arrays. I guess this is what you did in PySpark? We might be able to warn people of this condition in the PairRDDFunctions constructor, where we have a ClassManifest for the key K. Another option would be to choose the partitioner automatically based on the type of K, but let's try the other approach first and see whether there are any implications.", "created": "2012-10-29T14:14:11.534+0000"}, {"author": "Josh Rosen", "body": "If we add ArrayPartitioner or ByteArrayPartitioner, then it might be useful to perform type checking to prevent mistakes like using a {{ByteArrayPartitioner}} on an RDD with integer keys. We might do this by adding a type parameter {{K}} to {{Partitioner}}. The {{partitioner}} field is defined for all RDDs, not just key-value RDDs, so its type would have to be {{Partitioner[Any]}}. However, {{partitioner}} is a {{val}}, so it only matters that a partitioner of the right type is passed in the RDD's constructor. I would declare {{Partitioner[-K]}} to be contravariant in the key type, so that a {{Partitioner[Any]}} can be used with an {{RDD[Array[Byte]]}} but a {{Partitioner[Array[Byte]]}} cannot be used with an {{RDD[Int]}}. If we take this approach, then we might want to split this into two commits: one that adds a warning when using {{HashPartitioner}} with arrays of any kind, and another that changes {{Partition}} and adds the type parameters. This would retain backwards-compatibility for the existing releases, while allowing us to provide better type checking in future releases. Should I take a stab at this and submit a pull request?", "created": "2012-10-29T15:58:18.414+0000"}, {"author": "Josh Rosen", "body": "Also, array keys will cause problems with map-side combiners, which use RDD keys as {{HashMap}} keys. Is it worth supporting map-side combiners with array keys? If not, an easy solution would be to fail with an error. We could probably detect the error before the job ever runs. This would require separate patches for the 0.5/0.6 and 0.7 branches, due to my shuffle refactoring changes in 0.7.", "created": "2012-10-29T16:08:02.609+0000"}, {"author": "Josh Rosen", "body": "On reflection, it's probably a bad idea to use contravariant types in user-facing APIs because they could cause issues in the Java API. I submitted a pull request that causes Spark to throw exceptions in cases where we know that hashing arrays will produce incorrect results: https://github.com/mesos/spark/pull/348", "created": "2012-12-31T20:35:31.841+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Merged this into 0.6 as well.", "created": "2013-01-17T10:43:12.768+0000"}], "num_comments": 6, "text": "Issue: SPARK-597\nSummary: HashPartitioner incorrectly partitions RDD[Array[_]]\nDescription: Java arrays have {{hashCodes}} that are based on the arrays' identities rather than their contents [1]. As a result, attempting to partition an {{RDD[Array[_]]}} using a {{HashPartitioner}} will produce an unexpected/incorrect result. This was the cause of a bug in PySpark, where I hash partition PairRDDs with {{Array[Byte]}} keys. In PySpark, I fixed this by using a custom {{Partitioner}} that calls {{Arrays.hashCode(byte[])}} when passed an {{Array[Byte]}} [2]. I would like to address this issue more generally in Spark. We could add logic to {{HashPartitioner}} to perform special handling for arrays, but I'm not sure whether the additional branching would add a significant performance overhead. The logic could become messy because the {{Arrays}} module defines {{Arrays.hashCode()}} for primitive arrays and {{Arrays.deepHashCode()}} for Object arrays. Perhaps Guava or Apache Commons has an implementation of this. An alternative would be to keep the current {{HashPartitioner}} and add logic to print warnings (or to fail with an error) when shuffling an {{RDD[Array[_]]}} using the default {{HashPartitioner}}. [1] http://stackoverflow.com/questions/744735/java-array-hashcode-implementation [2] https://github.com/JoshRosen/spark/commit/2ccf3b665280bf5b0919e3801d028126cb070dbd\n\nComments (6):\n1. Josh Rosen: There might be similar problems if arrays are used as {{Map}} keys or are stored in a {{Set}}.\n2. Matei Alexandru Zaharia: This is a good catch, but it seems tough to do the right thing automatically in HashPartitioner, as you said. I think one interim fix is to add an ArrayPartitioner (or even ByteArrayPartitioner) and ask people to use that when their keys are byte arrays. I guess this is what you did in PySpark? We might be able to warn people of this condition in the PairRDDFunctions constructor, where we have a ClassManifest for the key K. Another option would be to choose the partitioner automatically based on the type of K, but let's try the other approach first and see whether there are any implications.\n3. Josh Rosen: If we add ArrayPartitioner or ByteArrayPartitioner, then it might be useful to perform type checking to prevent mistakes like using a {{ByteArrayPartitioner}} on an RDD with integer keys. We might do this by adding a type parameter {{K}} to {{Partitioner}}. The {{partitioner}} field is defined for all RDDs, not just key-value RDDs, so its type would have to be {{Partitioner[Any]}}. However, {{partitioner}} is a {{val}}, so it only matters that a partitioner of the right type is passed in the RDD's constructor. I would declare {{Partitioner[-K]}} to be contravariant in the key type, so that a {{Partitioner[Any]}} can be used with an {{RDD[Array[Byte]]}} but a {{Partitioner[Array[Byte]]}} cannot be used with an {{RDD[Int]}}. If we take this approach, then we might want to split this into two commits: one that adds a warning when using {{HashPartitioner}} with arrays of any kind, and another that changes {{Partition}} and adds the type parameters. This would retain backwards-compatibility for the existing releases, while allowing us to provide better type checking in future releases. Should I take a stab at this and submit a pull request?\n4. Josh Rosen: Also, array keys will cause problems with map-side combiners, which use RDD keys as {{HashMap}} keys. Is it worth supporting map-side combiners with array keys? If not, an easy solution would be to fail with an error. We could probably detect the error before the job ever runs. This would require separate patches for the 0.5/0.6 and 0.7 branches, due to my shuffle refactoring changes in 0.7.\n5. Josh Rosen: On reflection, it's probably a bad idea to use contravariant types in user-facing APIs because they could cause issues in the Java API. I submitted a pull request that causes Spark to throw exceptions in cases where we know that hashing arrays will produce incorrect results: https://github.com/mesos/spark/pull/348\n6. Matei Alexandru Zaharia: Merged this into 0.6 as well.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.006617"}}
{"id": "704030a6c614d41f1fc280a817e327fc", "issue_key": "SPARK-598", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Test PySpark's Java-side pickling of arrays of key-value pairs", "description": "There may be a bug in the PySpark code that packs {{Array[(Array[Byte], Array[Byte]]}} into a pickled object. This particular code path isn't exercised by the current PySpark implementation, but it may lead to strange results when collecting certain internal RDDs in Python (while debugging PySpark itself, for example).", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-10-29T23:53:45.000+0000", "updated": "2013-02-04T10:45:47.000+0000", "resolved": "2013-02-04T10:45:47.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "PySpark's Java-side pickling code has changed significantly and the code affected by this issue was removed, so I'm closing this.", "created": "2013-02-04T10:45:47.553+0000"}], "num_comments": 1, "text": "Issue: SPARK-598\nSummary: Test PySpark's Java-side pickling of arrays of key-value pairs\nDescription: There may be a bug in the PySpark code that packs {{Array[(Array[Byte], Array[Byte]]}} into a pickled object. This particular code path isn't exercised by the current PySpark implementation, but it may lead to strange results when collecting certain internal RDDs in Python (while debugging PySpark itself, for example).\n\nComments (1):\n1. Josh Rosen: PySpark's Java-side pickling code has changed significantly and the code affected by this issue was removed, so I'm closing this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.006617"}}
{"id": "00bd19c9231ca0dd19d1ec5850bf4d16", "issue_key": "SPARK-599", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "OutOfMemoryErrors can cause workers to hang indefinitely", "description": "While running Shark with an insufficient number of reduce tasks, an overloaded worker machine raised {{java.lang.OutOfMemoryError : GC overhead limit exceeded}}. This caused that Java process to hang at 100% CPU, spending all of its time in the garbage collector. This failure wasn't detected by the master, causing the entire job to hang. Handling and reporting failures due to {{OutOfMemoryError}} can be complicated because the {{OutOfMemoryError}} exception can be raised at many different locations, depending on which allocation caused the error. I'm not sure that it's safe to recover from {{OutOfMemoryError}}, so worker processes should probably die once they raise that error. We might be able to do this in an uncaught exception handler.", "reporter": "Josh Rosen", "assignee": null, "created": "2012-10-30T22:06:44.000+0000", "updated": "2012-11-26T12:32:39.000+0000", "resolved": "2012-11-26T12:32:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I think we might want to deal with this using a timeout instead of trying to catch the error. Do you know whether any threads continue running at all when there's an OOM? Somehow I doubt it.", "created": "2012-10-31T14:53:46.237+0000"}, {"author": "Josh Rosen", "body": "Here's a forced stack trace from the hanging worker: https://gist.github.com/3984822. Looks like all threads are blocked, but I don't know if this always happens. Could we take both approaches, killing the worker if we catch OOM while using a watchdog timeout in case that fails?", "created": "2012-10-31T14:59:34.464+0000"}, {"author": "Josh Rosen", "body": "Did https://github.com/mesos/spark/pull/305 fix this?", "created": "2012-11-26T09:46:03.714+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Good point, I think it does. Going to close it.", "created": "2012-11-26T12:32:12.029+0000"}], "num_comments": 4, "text": "Issue: SPARK-599\nSummary: OutOfMemoryErrors can cause workers to hang indefinitely\nDescription: While running Shark with an insufficient number of reduce tasks, an overloaded worker machine raised {{java.lang.OutOfMemoryError : GC overhead limit exceeded}}. This caused that Java process to hang at 100% CPU, spending all of its time in the garbage collector. This failure wasn't detected by the master, causing the entire job to hang. Handling and reporting failures due to {{OutOfMemoryError}} can be complicated because the {{OutOfMemoryError}} exception can be raised at many different locations, depending on which allocation caused the error. I'm not sure that it's safe to recover from {{OutOfMemoryError}}, so worker processes should probably die once they raise that error. We might be able to do this in an uncaught exception handler.\n\nComments (4):\n1. Matei Alexandru Zaharia: I think we might want to deal with this using a timeout instead of trying to catch the error. Do you know whether any threads continue running at all when there's an OOM? Somehow I doubt it.\n2. Josh Rosen: Here's a forced stack trace from the hanging worker: https://gist.github.com/3984822. Looks like all threads are blocked, but I don't know if this always happens. Could we take both approaches, killing the worker if we catch OOM while using a watchdog timeout in case that fails?\n3. Josh Rosen: Did https://github.com/mesos/spark/pull/305 fix this?\n4. Matei Alexandru Zaharia: Good point, I think it does. Going to close it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.007615"}}
{"id": "065f90fa15bbda6a08b0c25e9da00ab1", "issue_key": "SPARK-600", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "SparkContext.stop and clearJars delete local JAR files", "description": "If you happen to pass a JAR that's in your current working directory to SparkContext, clearJars() will delete it. I'm not exactly sure why it's deleting files to begin with (maybe it was meant to deal with JAR files that are somehow copied in local mode?) but it's certainly not something that should be done in SparkContext.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2012-10-31T14:43:25.000+0000", "updated": "2014-11-06T06:58:50.000+0000", "resolved": "2014-11-06T06:58:50.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Should no longer be a problem since 1.0", "created": "2014-11-06T06:58:50.598+0000"}], "num_comments": 1, "text": "Issue: SPARK-600\nSummary: SparkContext.stop and clearJars delete local JAR files\nDescription: If you happen to pass a JAR that's in your current working directory to SparkContext, clearJars() will delete it. I'm not exactly sure why it's deleting files to begin with (maybe it was meant to deal with JAR files that are somehow copied in local mode?) but it's certainly not something that should be done in SparkContext.\n\nComments (1):\n1. Matei Alexandru Zaharia: Should no longer be a problem since 1.0", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.007615"}}
{"id": "80b6424fc794ada258ff4a16cd168675", "issue_key": "SPARK-601", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None", "description": "If a lookup(k) is attempted on an RDD[(K, V)] with no partitioner, an UnsupportedOperationException is thrown even when the operation should succeed: scala> val rdd = sc.parallelize(List((1, 'a'), (1, 'b'), (2, 'c'))) rdd: spark.RDD[(Int, Char)] = spark.ParallelCollection@73f4117b scala> rdd.lookup(1) java.lang.UnsupportedOperationException: lookup() called on an RDD without a partitioner at spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:315) At a minimum, a filter.map.collect over the whole RDD works when the optimized path using a partitioner is not available: case None => - throw new UnsupportedOperationException(\"lookup() called on an RDD without a partitioner\") + self.filter(kv => kv._1 == key).map(kv => kv._2).collect", "reporter": null, "assignee": "Mark Hamstra", "created": "2012-10-31T16:02:31.000+0000", "updated": "2013-08-06T23:20:00.000+0000", "resolved": "2013-08-06T23:20:00.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-601\nSummary: PairRDDFunctions.lookup fails unnecessarily when self.partitioner is None\nDescription: If a lookup(k) is attempted on an RDD[(K, V)] with no partitioner, an UnsupportedOperationException is thrown even when the operation should succeed: scala> val rdd = sc.parallelize(List((1, 'a'), (1, 'b'), (2, 'c'))) rdd: spark.RDD[(Int, Char)] = spark.ParallelCollection@73f4117b scala> rdd.lookup(1) java.lang.UnsupportedOperationException: lookup() called on an RDD without a partitioner at spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:315) At a minimum, a filter.map.collect over the whole RDD works when the optimized path using a partitioner is not available: case None => - throw new UnsupportedOperationException(\"lookup() called on an RDD without a partitioner\") + self.filter(kv => kv._1 == key).map(kv => kv._2).collect", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.007615"}}
{"id": "99ab8666f2f5d9ef452f685a6cb46e0c", "issue_key": "SPARK-602", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Pass integer hashcodes to Java during PySpark hash partitioning", "description": "PySpark's current hash partitioning works by returning a sequence of serialized keys and values. The resulting {{JavaRDD[Array[Byte]]}} is grouped into pairs, treating the serialized Python keys as the pairs' keys. This hashes each key twice (once in Python and once in Java), but we could return integers from Python and avoid extra hashing (using a custom partitioner to treat the integer key itself as the hash bucket number, since we know the proper number of buckets in Python). This would avoid hashing byte arrays. This could be implemented cleanly by restructuring the Python code around two execution methods that resemble {{ShuffleMapTask}} and {{ResultTask}}.", "reporter": "Josh Rosen", "assignee": null, "created": "2012-11-01T09:50:27.000+0000", "updated": "2013-10-07T12:15:17.000+0000", "resolved": "2013-10-07T12:15:17.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Andre Schumacher", "body": "Should be solved by PR #33 so I am closing this.", "created": "2013-10-07T12:14:52.676+0000"}], "num_comments": 1, "text": "Issue: SPARK-602\nSummary: Pass integer hashcodes to Java during PySpark hash partitioning\nDescription: PySpark's current hash partitioning works by returning a sequence of serialized keys and values. The resulting {{JavaRDD[Array[Byte]]}} is grouped into pairs, treating the serialized Python keys as the pairs' keys. This hashes each key twice (once in Python and once in Java), but we could return integers from Python and avoid extra hashing (using a custom partitioner to treat the integer key itself as the hash bucket number, since we know the proper number of buckets in Python). This would avoid hashing byte arrays. This could be implemented cleanly by restructuring the Python code around two execution methods that resemble {{ShuffleMapTask}} and {{ResultTask}}.\n\nComments (1):\n1. Andre Schumacher: Should be solved by PR #33 so I am closing this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.007615"}}
{"id": "b892cd1f3899ee3562394014b0d353ec", "issue_key": "SPARK-603", "issue_type": "New Feature", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "add simple Counter API", "description": "Users need a very simple way to create counters in their jobs. Accumulators provide a way to do this, but are a little clunky, for two reasons: 1) the setup is a nuisance 2) w/ delayed evaluation, you don't know when it will actually run, so its hard to look at the values consider this code:  def filterBogus(rdd:RDD[MyCustomClass], sc: SparkContext) = { val filterCount = sc.accumulator(0) val filtered = rdd.filter{r => if (isOK(r)) true else {filterCount += 1; false} } println(\"removed \" + filterCount.value + \" records) filtered }  The println will always say 0 records were filtered, because its printed before anything has actually run. I could print out the value later on, but note that it would destroy the modularity of the method -- kinda ugly to return the accumulator just so that it can get printed later on. (and of course, the caller in turn might not know when the filter is going to get applied, and would have to pass the accumulator up even further ...) I'd like to have Counters which just automatically get printed out whenever a stage has been run, and also with some api to get them back. I realize this is tricky b/c a stage can get re-computed, so maybe you should only increment the counters once. Maybe a more general way to do this is to provide some callback for whenever an RDD is computed -- by default, you would just print the counters, but the user could replace w/ a custom handler.", "reporter": "Imran Rashid", "assignee": null, "created": "2012-11-01T12:46:28.000+0000", "updated": "2015-09-09T04:06:27.000+0000", "resolved": "2015-03-03T15:30:51.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "Hi Anonymous (not sure how you can read this but...), I'm not sure how a counters API could solve the delayed evaluation problem and be fundamentally different from the existing Accumulator feature. You'd have to make accessing a counter force evaluation of every RDD it's accessed from, and you'd have issues with recomputing the RDD later on when it's actually needed, causing a duplicate access. I can see some kind of API that offers a hook into the RDD evaluation DAG, but that operates at a stage level rather than an operation level (for example multiple maps are pipelined together) so the mapping for available hook points wouldn't be 1:1 with RDD operations, which would be quite tricky. I don't see a way to implement what you propose with Counters without compromising major parts of the Spark API contract (RDD laziness) so propose to close. Especially given that I haven no way to contact you given your information doesn't appear on the prior Atlassian Jira either: https://spark-project.atlassian.net/browse/SPARK-603 [~rxin] are you ok with closing this?", "created": "2014-11-14T08:54:59.765+0000"}, {"author": "Reynold Xin", "body": "Closing this one as part of [~aash]'s cleanup. I think this problem is being fixed as we add accumulator / metrics values to the web ui.", "created": "2014-11-14T17:55:02.610+0000"}, {"author": "Imran Rashid", "body": "Hey, this was originally reported by me too (probably I messed up when creating it on the old Jira, not sure if there is a way to change the reporter now?) I think perhaps the original issue was a little unclear, I'll try to clarify a little bit: I do *not* think we need to support something at the \"operation\" level -- having it work at the stage level (or even job level) is fine. I'm not even sure what it would mean to work at the operation level, since individual records are pushed through all the operations of a stage in one go. But the operation level is still a useful abstraction for the *developer*. Its nice for them to be able to write methods which are eg., just a {{filter}}. For normal RDD operations, this works just fine of course -- you can have a bunch of util methods that take in an RDD and output an RDD, maybe some {{filter}}, some {{map}}, etc., they can get combined however you like, everything remains lazy until there is some action. All wonderful. Things get messy as soon as you start to include accumulators, however -- you've got include them in your return values and then the outside logic has to know when they actual contain valid data. Rather than trying to solve this problem in general, I'm proposing that we do something dead-simple for basic counters, which might even live outside of accumulators completely. Putting accumulator values in the web UI is not bad for just this purpose, but overall I don't think its the right solution: 1. It limits what we can do with accumulators (see my comments on SPARK-664) 2. The api is more complicated than it needs to be. If the only point of accumulators is counters, then we can get away with something as simple as:  rdd.map{x => if (isFoobar(x)) { Counters(\"foobar\") += 1 } ... }  (eg., no need to even declare the counter up front.) 3. Having the value in the UI is nice, but its not the same as programmatic access. eg. it can be useful to have them in the job logs, the actual values might be used in other computation (eg., gives the size of a datastructure for a later step), etc. Even with the simpler counter api, this is tricky b/c of lazy evaluation. But maybe that is a reason you create a call-back up front:  Counters.addCallback(\"foobar\"){counts => ...} rdd.map{x => if (isFoobar(x)) { Counters(\"foobar\") += 1 } ... }  4. If you have long-running tasks, it might be nice to get incremental feedback from counters *during* the task. (There was a real need for long-running tasks before sort-based shuffle, when you couldn't have too many tasks in a shuffle ... perhaps its not anymore, I'm not sure.) We can get a little further with accumulators, eg. a SparkListener could do something with accumulator values when the stages finish. But I think we're stuck on the other points. I feel like right now accumulators are trapped between just being counters, and being a more general method of computation, and not quite doing either one very well.", "created": "2014-11-17T00:32:48.198+0000"}, {"author": "Sean R. Owen", "body": "Is this still live? I also kind of think this is accomplishable pretty easily with accumulators and adding another abstraction on top with different semantics might be more confusing than it's worth. FWIW.", "created": "2015-03-02T15:13:56.480+0000"}, {"author": "Imran Rashid", "body": "Hi [~srowen] I don't think anyone is actively working on this, and probably won't for a while -- I suppose that means it should be closed for now. I disagree that its easy to do this with accumulators. Its certainly possible, but it makes it quite complicated to do something that is use very common and should be dead-simple. (or at least, its harder than most people realize to use accumulators to do this *correctly*.) i guess it will be confusing to have counters & accumulators in the api, but it might only serve to highlight some of the intricacies of the accumulator api which aren't obvious (and can't be fixed w/out breaking changes).", "created": "2015-03-03T15:30:15.443+0000"}, {"author": "koert kuipers", "body": "we use counters a lot in scalding (to verify records counts mostly at different stages, for certain criteria). i do not think it is easy at all to recreate counters with accumulators. in fact with the current behavior of accumulators (they do not account for task failure, leading to double counting) i think its nearly impossible to implement counters.", "created": "2015-09-09T04:06:27.416+0000"}], "num_comments": 6, "text": "Issue: SPARK-603\nSummary: add simple Counter API\nDescription: Users need a very simple way to create counters in their jobs. Accumulators provide a way to do this, but are a little clunky, for two reasons: 1) the setup is a nuisance 2) w/ delayed evaluation, you don't know when it will actually run, so its hard to look at the values consider this code:  def filterBogus(rdd:RDD[MyCustomClass], sc: SparkContext) = { val filterCount = sc.accumulator(0) val filtered = rdd.filter{r => if (isOK(r)) true else {filterCount += 1; false} } println(\"removed \" + filterCount.value + \" records) filtered }  The println will always say 0 records were filtered, because its printed before anything has actually run. I could print out the value later on, but note that it would destroy the modularity of the method -- kinda ugly to return the accumulator just so that it can get printed later on. (and of course, the caller in turn might not know when the filter is going to get applied, and would have to pass the accumulator up even further ...) I'd like to have Counters which just automatically get printed out whenever a stage has been run, and also with some api to get them back. I realize this is tricky b/c a stage can get re-computed, so maybe you should only increment the counters once. Maybe a more general way to do this is to provide some callback for whenever an RDD is computed -- by default, you would just print the counters, but the user could replace w/ a custom handler.\n\nComments (6):\n1. Andrew Ash: Hi Anonymous (not sure how you can read this but...), I'm not sure how a counters API could solve the delayed evaluation problem and be fundamentally different from the existing Accumulator feature. You'd have to make accessing a counter force evaluation of every RDD it's accessed from, and you'd have issues with recomputing the RDD later on when it's actually needed, causing a duplicate access. I can see some kind of API that offers a hook into the RDD evaluation DAG, but that operates at a stage level rather than an operation level (for example multiple maps are pipelined together) so the mapping for available hook points wouldn't be 1:1 with RDD operations, which would be quite tricky. I don't see a way to implement what you propose with Counters without compromising major parts of the Spark API contract (RDD laziness) so propose to close. Especially given that I haven no way to contact you given your information doesn't appear on the prior Atlassian Jira either: https://spark-project.atlassian.net/browse/SPARK-603 [~rxin] are you ok with closing this?\n2. Reynold Xin: Closing this one as part of [~aash]'s cleanup. I think this problem is being fixed as we add accumulator / metrics values to the web ui.\n3. Imran Rashid: Hey, this was originally reported by me too (probably I messed up when creating it on the old Jira, not sure if there is a way to change the reporter now?) I think perhaps the original issue was a little unclear, I'll try to clarify a little bit: I do *not* think we need to support something at the \"operation\" level -- having it work at the stage level (or even job level) is fine. I'm not even sure what it would mean to work at the operation level, since individual records are pushed through all the operations of a stage in one go. But the operation level is still a useful abstraction for the *developer*. Its nice for them to be able to write methods which are eg., just a {{filter}}. For normal RDD operations, this works just fine of course -- you can have a bunch of util methods that take in an RDD and output an RDD, maybe some {{filter}}, some {{map}}, etc., they can get combined however you like, everything remains lazy until there is some action. All wonderful. Things get messy as soon as you start to include accumulators, however -- you've got include them in your return values and then the outside logic has to know when they actual contain valid data. Rather than trying to solve this problem in general, I'm proposing that we do something dead-simple for basic counters, which might even live outside of accumulators completely. Putting accumulator values in the web UI is not bad for just this purpose, but overall I don't think its the right solution: 1. It limits what we can do with accumulators (see my comments on SPARK-664) 2. The api is more complicated than it needs to be. If the only point of accumulators is counters, then we can get away with something as simple as:  rdd.map{x => if (isFoobar(x)) { Counters(\"foobar\") += 1 } ... }  (eg., no need to even declare the counter up front.) 3. Having the value in the UI is nice, but its not the same as programmatic access. eg. it can be useful to have them in the job logs, the actual values might be used in other computation (eg., gives the size of a datastructure for a later step), etc. Even with the simpler counter api, this is tricky b/c of lazy evaluation. But maybe that is a reason you create a call-back up front:  Counters.addCallback(\"foobar\"){counts => ...} rdd.map{x => if (isFoobar(x)) { Counters(\"foobar\") += 1 } ... }  4. If you have long-running tasks, it might be nice to get incremental feedback from counters *during* the task. (There was a real need for long-running tasks before sort-based shuffle, when you couldn't have too many tasks in a shuffle ... perhaps its not anymore, I'm not sure.) We can get a little further with accumulators, eg. a SparkListener could do something with accumulator values when the stages finish. But I think we're stuck on the other points. I feel like right now accumulators are trapped between just being counters, and being a more general method of computation, and not quite doing either one very well.\n4. Sean R. Owen: Is this still live? I also kind of think this is accomplishable pretty easily with accumulators and adding another abstraction on top with different semantics might be more confusing than it's worth. FWIW.\n5. Imran Rashid: Hi [~srowen] I don't think anyone is actively working on this, and probably won't for a while -- I suppose that means it should be closed for now. I disagree that its easy to do this with accumulators. Its certainly possible, but it makes it quite complicated to do something that is use very common and should be dead-simple. (or at least, its harder than most people realize to use accumulators to do this *correctly*.) i guess it will be confusing to have counters & accumulators in the api, but it might only serve to highlight some of the intricacies of the accumulator api which aren't obvious (and can't be fixed w/out breaking changes).\n6. koert kuipers: we use counters a lot in scalding (to verify records counts mostly at different stages, for certain criteria). i do not think it is easy at all to recreate counters with accumulators. in fact with the current behavior of accumulators (they do not account for task failure, leading to double counting) i think its nearly impossible to implement counters.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.007615"}}
{"id": "d586454aaa9fadef111e7bbe3f844fc1", "issue_key": "SPARK-604", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "reconnect if mesos slaves dies", "description": "when running on mesos, if a slave goes down, spark doesn't try to reassign the work to another machine. Even if the slave comes back up, the job is doomed. Currently when this happens, we just see this in the driver logs: 12/11/01 16:48:56 INFO mesos.MesosSchedulerBackend: Mesos slave lost: 201210312057-1560611338-5050-24091-52 Exception in thread \"Thread-346\" java.util.NoSuchElementException: key not found: value: \"201210312057-1560611338-5050-24091-52\" at scala.collection.MapLike$class.default(MapLike.scala:224) at scala.collection.mutable.HashMap.default(HashMap.scala:43) at scala.collection.MapLike$class.apply(MapLike.scala:135) at scala.collection.mutable.HashMap.apply(HashMap.scala:43) at spark.scheduler.cluster.ClusterScheduler.slaveLost(ClusterScheduler.scala:255) at spark.scheduler.mesos.MesosSchedulerBackend.slaveLost(MesosSchedulerBackend.scala:275) 12/11/01 16:48:56 INFO mesos.MesosSchedulerBackend: driver.run() returned with code DRIVER_ABORTED", "reporter": null, "assignee": null, "created": "2012-11-01T16:55:24.000+0000", "updated": "2015-05-15T13:50:33.000+0000", "resolved": "2015-05-15T13:50:33.000+0000", "labels": [], "components": ["Mesos"], "comments": [{"author": "Sean R. Owen", "body": "Stale at this point, without similar findings recently.", "created": "2015-05-15T13:50:33.189+0000"}], "num_comments": 1, "text": "Issue: SPARK-604\nSummary: reconnect if mesos slaves dies\nDescription: when running on mesos, if a slave goes down, spark doesn't try to reassign the work to another machine. Even if the slave comes back up, the job is doomed. Currently when this happens, we just see this in the driver logs: 12/11/01 16:48:56 INFO mesos.MesosSchedulerBackend: Mesos slave lost: 201210312057-1560611338-5050-24091-52 Exception in thread \"Thread-346\" java.util.NoSuchElementException: key not found: value: \"201210312057-1560611338-5050-24091-52\" at scala.collection.MapLike$class.default(MapLike.scala:224) at scala.collection.mutable.HashMap.default(HashMap.scala:43) at scala.collection.MapLike$class.apply(MapLike.scala:135) at scala.collection.mutable.HashMap.apply(HashMap.scala:43) at spark.scheduler.cluster.ClusterScheduler.slaveLost(ClusterScheduler.scala:255) at spark.scheduler.mesos.MesosSchedulerBackend.slaveLost(MesosSchedulerBackend.scala:275) 12/11/01 16:48:56 INFO mesos.MesosSchedulerBackend: driver.run() returned with code DRIVER_ABORTED\n\nComments (1):\n1. Sean R. Owen: Stale at this point, without similar findings recently.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.008698"}}
{"id": "6b3d052dd44a5ddae2d55b0e4d88a99d", "issue_key": "SPARK-605", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add support for new m3.xlarge and m3.2xlarge EC2 instance types", "description": "Today, Amazon introduced two new EC2 instance types, {{m3.xlarge}} and {{m3.2xlarge}} (http://aws.typepad.com/aws/2012/10/new-ec2-second-generation-standard-instances-and-price-reductions-1.html). These instances do not have local disks, which may require special handling in {{spark-ec2}}'s {{get_num_disks()}} method.", "reporter": "Josh Rosen", "assignee": "Patrick McFadin", "created": "2012-11-02T09:31:24.000+0000", "updated": "2014-01-23T19:57:39.000+0000", "resolved": "2014-01-23T19:57:39.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Josh Rosen", "body": "It looks like https://github.com/mesos/spark/pull/603 might fix this, but are there any other places in the spark-ec2 toolchain where not having a local physical disk will cause problems?", "created": "2013-05-10T14:20:30.058+0000"}, {"author": "Patrick McFadin", "body": "SPARK-727", "created": "2013-08-20T16:11:48.619+0000"}, {"author": "Josh Rosen", "body": "It looks like this was fixed as of Spark 0.8.0.", "created": "2014-01-23T19:57:39.717+0000"}], "num_comments": 3, "text": "Issue: SPARK-605\nSummary: Add support for new m3.xlarge and m3.2xlarge EC2 instance types\nDescription: Today, Amazon introduced two new EC2 instance types, {{m3.xlarge}} and {{m3.2xlarge}} (http://aws.typepad.com/aws/2012/10/new-ec2-second-generation-standard-instances-and-price-reductions-1.html). These instances do not have local disks, which may require special handling in {{spark-ec2}}'s {{get_num_disks()}} method.\n\nComments (3):\n1. Josh Rosen: It looks like https://github.com/mesos/spark/pull/603 might fix this, but are there any other places in the spark-ec2 toolchain where not having a local physical disk will cause problems?\n2. Patrick McFadin: SPARK-727\n3. Josh Rosen: It looks like this was fixed as of Spark 0.8.0.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.008698"}}
{"id": "7a310106772fc5cec07d7fd79cbd329d", "issue_key": "SPARK-606", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add mapSideCombine setting to Java API partitionBy() method.", "description": "Add mapSideCombine setting to Java API partitionBy() method.", "reporter": "Josh Rosen", "assignee": "Reynold Xin", "created": "2012-11-05T15:09:56.000+0000", "updated": "2014-07-26T20:02:29.000+0000", "resolved": "2014-07-26T20:02:29.000+0000", "labels": ["Starter"], "components": ["Java API"], "comments": [{"author": "Piyush Kansal", "body": "I am interested in working on this issue. Please assign it to me.", "created": "2014-03-05T00:12:41.600+0000"}, {"author": "Reynold Xin", "body": "Actually I feel we should just deprecate mapSideCombine in partitionBy ... it provides almost no benefit in the case of partitionBy.", "created": "2014-03-05T00:26:47.251+0000"}, {"author": "Josh Rosen", "body": "mapSideCombine was removed from partitionBy in https://github.com/apache/spark/commit/0e84fee76b529089fb52f15151202e9a7b847ed5", "created": "2014-07-26T20:02:29.921+0000"}], "num_comments": 3, "text": "Issue: SPARK-606\nSummary: Add mapSideCombine setting to Java API partitionBy() method.\nDescription: Add mapSideCombine setting to Java API partitionBy() method.\n\nComments (3):\n1. Piyush Kansal: I am interested in working on this issue. Please assign it to me.\n2. Reynold Xin: Actually I feel we should just deprecate mapSideCombine in partitionBy ... it provides almost no benefit in the case of partitionBy.\n3. Josh Rosen: mapSideCombine was removed from partitionBy in https://github.com/apache/spark/commit/0e84fee76b529089fb52f15151202e9a7b847ed5", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.008698"}}
{"id": "3c080add6181a6779da21d99846d4ebf", "issue_key": "SPARK-607", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Timeout while fetching map statuses may cause job to hang", "description": "Jobs may hang if workers time out while fetching map output locations from the MapOutputTracker. I ran into this issue while running under Mesos on EC2, but I was able to reproduce it on my own machine using a 1-node standalone cluster. After applying the attached patch to generate random timeout failures, my groupByKey job lost a task due to the timeout. It looks like the master is notified of the failure, since it appears in its log:  12/11/06 10:19:39 INFO TaskSetManager: Serialized task 0.0:7 as 3095 bytes in 1 ms 12/11/06 10:19:39 INFO TaskSetManager: Lost TID 10 (task 0.0:2) 12/11/06 10:19:40 INFO TaskSetManager: Loss was due to spark.SparkException: Error communicating with MapOutputTracker at spark.MapOutputTracker.askTracker(MapOutputTracker.scala:78) at spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:154) at spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:14) at spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:38) at spark.RDD.iterator(RDD.scala:161) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:18) at spark.RDD.iterator(RDD.scala:161) at spark.scheduler.ResultTask.run(ResultTask.scala:18) at spark.executor.Executor$TaskRunner.run(Executor.scala:76) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:680) 12/11/06 10:19:40 INFO TaskSetManager: Starting task 0.0:2 as TID 16 on slave worker-20121106101845-128.32.130.156-50931: 128.32.130.156 (preferred) 12/11/06 10:19:40 INFO TaskSetManager: Serialized task 0.0:2 as 3095 bytes in 1 ms  The job hangs here; perhaps the failure leaves some inconsistent state on the worker.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-11-06T10:32:47.000+0000", "updated": "2013-01-16T21:54:26.000+0000", "resolved": "2013-01-16T21:53:58.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Avanesov Valeriy", "body": "I've got the same problem. Is there any workaround?", "created": "2012-11-23T05:05:34.035+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I believe this was fixed in 0.6.1, when we increased the Akka message timeout. Can you try that?", "created": "2012-11-23T10:19:08.459+0000"}, {"author": "Josh Rosen", "body": "My test case patch can still produce a hang under 0.6.1. Increasing the Akka timeout addresses the cause of the TimeoutException but doesn't fix the hang itself. The exception should cause the task to fail rather than hanging.", "created": "2012-11-23T13:47:18.839+0000"}, {"author": "Josh Rosen", "body": "I found a potential cause for the freeze: If an exception is throwing while communicating with the MapOutputTracker, the requested shuffleId will never be removed from the {{fetching}} set, which tracks in-progress requests. As a result, subsequent fetches for the block will block while waiting for the failed fetch to finish. What's the right fix here? We could add a try-finally block around the call to the MapOutputTracker to ensure that the failed fetch removes the shuffleId from the {{fetching}} set, but this will just lead to a NullPointerException when the waiting thread tries to read the missing value. I suppose that this is okay, since the chain of failures will eventually be caught when the RDD partitions that were being computed are discovered to be missing. Does this sound reasonable?", "created": "2012-12-13T21:24:28.562+0000"}, {"author": "Josh Rosen", "body": "I tested that fix locally and it works, so I submitted a pull request: https://github.com/mesos/spark/pull/332", "created": "2012-12-13T22:04:49.743+0000"}, {"author": "Josh Rosen", "body": "Pull request merged into master and branch-0.6, so I'm marking this as resolved.", "created": "2013-01-16T21:54:26.456+0000"}], "num_comments": 6, "text": "Issue: SPARK-607\nSummary: Timeout while fetching map statuses may cause job to hang\nDescription: Jobs may hang if workers time out while fetching map output locations from the MapOutputTracker. I ran into this issue while running under Mesos on EC2, but I was able to reproduce it on my own machine using a 1-node standalone cluster. After applying the attached patch to generate random timeout failures, my groupByKey job lost a task due to the timeout. It looks like the master is notified of the failure, since it appears in its log:  12/11/06 10:19:39 INFO TaskSetManager: Serialized task 0.0:7 as 3095 bytes in 1 ms 12/11/06 10:19:39 INFO TaskSetManager: Lost TID 10 (task 0.0:2) 12/11/06 10:19:40 INFO TaskSetManager: Loss was due to spark.SparkException: Error communicating with MapOutputTracker at spark.MapOutputTracker.askTracker(MapOutputTracker.scala:78) at spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:154) at spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:14) at spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:38) at spark.RDD.iterator(RDD.scala:161) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:18) at spark.RDD.iterator(RDD.scala:161) at spark.scheduler.ResultTask.run(ResultTask.scala:18) at spark.executor.Executor$TaskRunner.run(Executor.scala:76) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:680) 12/11/06 10:19:40 INFO TaskSetManager: Starting task 0.0:2 as TID 16 on slave worker-20121106101845-128.32.130.156-50931: 128.32.130.156 (preferred) 12/11/06 10:19:40 INFO TaskSetManager: Serialized task 0.0:2 as 3095 bytes in 1 ms  The job hangs here; perhaps the failure leaves some inconsistent state on the worker.\n\nComments (6):\n1. Avanesov Valeriy: I've got the same problem. Is there any workaround?\n2. Matei Alexandru Zaharia: I believe this was fixed in 0.6.1, when we increased the Akka message timeout. Can you try that?\n3. Josh Rosen: My test case patch can still produce a hang under 0.6.1. Increasing the Akka timeout addresses the cause of the TimeoutException but doesn't fix the hang itself. The exception should cause the task to fail rather than hanging.\n4. Josh Rosen: I found a potential cause for the freeze: If an exception is throwing while communicating with the MapOutputTracker, the requested shuffleId will never be removed from the {{fetching}} set, which tracks in-progress requests. As a result, subsequent fetches for the block will block while waiting for the failed fetch to finish. What's the right fix here? We could add a try-finally block around the call to the MapOutputTracker to ensure that the failed fetch removes the shuffleId from the {{fetching}} set, but this will just lead to a NullPointerException when the waiting thread tries to read the missing value. I suppose that this is okay, since the chain of failures will eventually be caught when the RDD partitions that were being computed are discovered to be missing. Does this sound reasonable?\n5. Josh Rosen: I tested that fix locally and it works, so I submitted a pull request: https://github.com/mesos/spark/pull/332\n6. Josh Rosen: Pull request merged into master and branch-0.6, so I'm marking this as resolved.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.008698"}}
{"id": "adb8fd41973b814997f675a71fd5d128", "issue_key": "SPARK-608", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Allow showing worker STDOUT/STDERR log tail (e.g. last 512KB)", "description": "When a job is long running, log grows large and it becomes impossible to download the entire log. The standalone console should support showing the tail of the logs with a user specified tail size.", "reporter": "Reynold Xin", "assignee": null, "created": "2012-11-06T14:02:05.000+0000", "updated": "2013-07-12T20:31:46.000+0000", "resolved": "2013-07-12T20:31:46.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "This is still an issue. When I try to view large logs through the web UI, I receive the error \"The server could not handle the request in the appropriate time frame (async timeout)\". Why don't we implement a general mechanism that allows an offset and length to be specified when requesting a log? This would support more general use-cases than just {{tail}} and might also allow us to use long-polling to implement a live-updating log viewer.", "created": "2013-05-05T14:57:53.167+0000"}], "num_comments": 1, "text": "Issue: SPARK-608\nSummary: Allow showing worker STDOUT/STDERR log tail (e.g. last 512KB)\nDescription: When a job is long running, log grows large and it becomes impossible to download the entire log. The standalone console should support showing the tail of the logs with a user specified tail size.\n\nComments (1):\n1. Josh Rosen: This is still an issue. When I try to view large logs through the web UI, I receive the error \"The server could not handle the request in the appropriate time frame (async timeout)\". Why don't we implement a general mechanism that allows an offset and length to be specified when requesting a log? This would support more general use-cases than just {{tail}} and might also allow us to use long-polling to implement a live-updating log viewer.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.009655"}}
{"id": "50f24add673f8ab1a0c1bdc3f35f51ae", "issue_key": "SPARK-609", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add instructions for enabling Akka debug logging", "description": "How can I enable Akka debug logging in Spark? I tried setting {{akka.loglevel = \"DEBUG\"}} in the configuration in {{AkkaUtils}}, and I also tried setting properties in a {{log4j.conf}} file, but neither approach worked. It might be helpful to have instructions for this in a \"Spark Internals Debugging\" guide.", "reporter": "Josh Rosen", "assignee": null, "created": "2012-11-06T15:05:17.000+0000", "updated": "2015-01-03T22:43:59.000+0000", "resolved": "2015-01-03T22:43:59.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Josh Rosen", "body": "I'm going to close out this issue for now, since I think it's no longer an issue in recent versions of Spark. Please comment / reopen if you think there's still something that needs fixing.", "created": "2015-01-03T22:43:59.786+0000"}], "num_comments": 1, "text": "Issue: SPARK-609\nSummary: Add instructions for enabling Akka debug logging\nDescription: How can I enable Akka debug logging in Spark? I tried setting {{akka.loglevel = \"DEBUG\"}} in the configuration in {{AkkaUtils}}, and I also tried setting properties in a {{log4j.conf}} file, but neither approach worked. It might be helpful to have instructions for this in a \"Spark Internals Debugging\" guide.\n\nComments (1):\n1. Josh Rosen: I'm going to close out this issue for now, since I think it's no longer an issue in recent versions of Spark. Please comment / reopen if you think there's still something that needs fixing.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.009655"}}
{"id": "5f7f259a9e41b593f5f892dbee523fd3", "issue_key": "SPARK-610", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support master failover in standalone mode", "description": "The standalone deploy mode is quite simple, which shouldn't make it too bad to add support for master failover using ZooKeeper or something similar. This would really up its usefulness.", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Davidson", "created": "2012-11-06T16:04:19.000+0000", "updated": "2014-11-06T06:59:46.000+0000", "resolved": "2014-11-06T06:59:46.000+0000", "labels": [], "components": [], "comments": [{"author": "Matthew Farrellee", "body": "[~matei] given YARN and Mesos implementations, is this something the standalone mode should strive to do?", "created": "2014-09-21T17:45:54.207+0000"}], "num_comments": 1, "text": "Issue: SPARK-610\nSummary: Support master failover in standalone mode\nDescription: The standalone deploy mode is quite simple, which shouldn't make it too bad to add support for master failover using ZooKeeper or something similar. This would really up its usefulness.\n\nComments (1):\n1. Matthew Farrellee: [~matei] given YARN and Mesos implementations, is this something the standalone mode should strive to do?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.009655"}}
{"id": "c9b37e89438b4fc6498e00d995dc422a", "issue_key": "SPARK-611", "issue_type": "New Feature", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Allow JStack to be run from web UI", "description": "Huge debugging improvement if the standalone mode dashboard can run jstack and show it on the web page for a slave.", "reporter": "Reynold Xin", "assignee": "Josh Rosen", "created": "2012-11-07T13:43:39.000+0000", "updated": "2014-11-04T02:19:21.000+0000", "resolved": "2014-11-04T02:19:21.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "Note for my own reference: http://nadeausoftware.com/articles/2008/04/java_tip_how_list_and_find_threads_and_thread_groups#Gettingalistofallthreads", "created": "2012-12-17T23:19:09.107+0000"}, {"author": "Josh Rosen", "body": "Patrick and I discussed this yesterday and I'm going to take a shot at implementing it this afternoon. There are two different design considerations here: _how_ do we obtain information on threads and _when_ do we do it? One approach would be to literally invoke {{jstack}} on the executor JVM from the executor JVM by obtaining its own process ID and using a process builder to fork out to {{jstack}} (this approach is described [in this blog post|http://www.takipiblog.com/supercharged-jstack-how-to-debug-your-servers-at-100mph/]). Unfortunately, this could be prohibitively expensive due to the high cost of forking from JVMs with large heaps. Another approach, which I'm planning to use, is to call {{Thread.getAllStackTraces()}} and format the result into a string. I think that this loses a bit of information compared to {{jstack}} but it's much lower overhead in the worst case. As for the \"when\", one approach would be an \"on demand\" jstack button that triggers a thread dump on an executor. This approach would require a large amount of new RPC plumbing / machinery in several places. A lighter-weight, first-cut approach, which we plan to explore, is to piggyback periodic thread dumps on the Executor -> Driver heartbeat messages. I'm going to try to hack this together and submit a PR tomorrow.", "created": "2014-10-25T07:21:08.248+0000"}, {"author": "Apache Spark", "body": "User 'JoshRosen' has created a pull request for this issue: https://github.com/apache/spark/pull/2944", "created": "2014-10-26T03:10:14.592+0000"}], "num_comments": 3, "text": "Issue: SPARK-611\nSummary: Allow JStack to be run from web UI\nDescription: Huge debugging improvement if the standalone mode dashboard can run jstack and show it on the web page for a slave.\n\nComments (3):\n1. Patrick McFadin: Note for my own reference: http://nadeausoftware.com/articles/2008/04/java_tip_how_list_and_find_threads_and_thread_groups#Gettingalistofallthreads\n2. Josh Rosen: Patrick and I discussed this yesterday and I'm going to take a shot at implementing it this afternoon. There are two different design considerations here: _how_ do we obtain information on threads and _when_ do we do it? One approach would be to literally invoke {{jstack}} on the executor JVM from the executor JVM by obtaining its own process ID and using a process builder to fork out to {{jstack}} (this approach is described [in this blog post|http://www.takipiblog.com/supercharged-jstack-how-to-debug-your-servers-at-100mph/]). Unfortunately, this could be prohibitively expensive due to the high cost of forking from JVMs with large heaps. Another approach, which I'm planning to use, is to call {{Thread.getAllStackTraces()}} and format the result into a string. I think that this loses a bit of information compared to {{jstack}} but it's much lower overhead in the worst case. As for the \"when\", one approach would be an \"on demand\" jstack button that triggers a thread dump on an executor. This approach would require a large amount of new RPC plumbing / machinery in several places. A lighter-weight, first-cut approach, which we plan to explore, is to piggyback periodic thread dumps on the Executor -> Driver heartbeat messages. I'm going to try to hack this together and submit a PR tomorrow.\n3. Apache Spark: User 'JoshRosen' has created a pull request for this issue: https://github.com/apache/spark/pull/2944", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.010614"}}
{"id": "1f37af891951abe0edb7db0e485bd277", "issue_key": "SPARK-612", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Update examples to pass JAR file to SparkContext in master and 0.6 branches", "description": "We did this in 0.5 but somehow lost it in master and 0.6. It's confusing to users.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2012-11-07T15:42:34.000+0000", "updated": "2013-01-20T12:50:20.000+0000", "resolved": "2013-01-20T12:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Duplicates SPARK-594", "created": "2013-01-20T12:50:20.789+0000"}], "num_comments": 1, "text": "Issue: SPARK-612\nSummary: Update examples to pass JAR file to SparkContext in master and 0.6 branches\nDescription: We did this in 0.5 but somehow lost it in master and 0.6. It's confusing to users.\n\nComments (1):\n1. Josh Rosen: Duplicates SPARK-594", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.010614"}}
{"id": "66d92afd9b1566939c1dd3581ffa0a8c", "issue_key": "SPARK-613", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Standalone web UI links to internal IPs when running on EC2", "description": "When I visit the standalone cluster web UI on EC2, the links to worker UIs are based on internal addresses (e.g. http://10.159.2.115:8081/) instead of externally-accessible addresses (e.g. http://ec2-*-*-*-*.compute-1.amazonaws.com/). This makes it hard to view the worker logs.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-11-07T23:01:09.000+0000", "updated": "2013-05-25T23:42:28.000+0000", "resolved": "2013-05-25T23:42:28.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Josh Rosen", "body": "Fixed by https://github.com/mesos/spark/pull/316", "created": "2012-12-04T15:40:55.442+0000"}, {"author": "Josh Rosen", "body": "It looks like there are hostnames like domU-12-31-39-09-F5-02.compute-1.internal and ip-10-226-87-193 which aren't recognized by the pattern used in that pull request. We should probably add an environment variable in spark-env.sh to specify that the machines are running on EC2, and automatically set this variable in the EC2 scripts.", "created": "2012-12-14T10:38:47.536+0000"}, {"author": "Shivaram Venkataraman", "body": "Fixed for Spark AMI by https://github.com/mesos/spark/pull/419", "created": "2013-01-28T10:30:39.552+0000"}, {"author": "Josh Rosen", "body": "Can https://github.com/mesos/spark/pull/419 be backported to 0.6?", "created": "2013-02-08T14:54:18.685+0000"}, {"author": "Josh Rosen", "body": "This may still be broken. I noticed a link to a worker with domu-*-*-*-*=*.compute-1.internal:8081, an internal address, when running the latest Spark master on the 0.7 AMI.", "created": "2013-05-05T12:27:18.561+0000"}, {"author": "Josh Rosen", "body": "Found what's hopefully the last issue here and opened a new PR: https://github.com/mesos/spark/pull/621", "created": "2013-05-24T13:10:49.493+0000"}, {"author": "Josh Rosen", "body": "Merged and cherry-picked that PR into branch-0.7, so this is resolved.", "created": "2013-05-25T23:42:28.336+0000"}], "num_comments": 7, "text": "Issue: SPARK-613\nSummary: Standalone web UI links to internal IPs when running on EC2\nDescription: When I visit the standalone cluster web UI on EC2, the links to worker UIs are based on internal addresses (e.g. http://10.159.2.115:8081/) instead of externally-accessible addresses (e.g. http://ec2-*-*-*-*.compute-1.amazonaws.com/). This makes it hard to view the worker logs.\n\nComments (7):\n1. Josh Rosen: Fixed by https://github.com/mesos/spark/pull/316\n2. Josh Rosen: It looks like there are hostnames like domU-12-31-39-09-F5-02.compute-1.internal and ip-10-226-87-193 which aren't recognized by the pattern used in that pull request. We should probably add an environment variable in spark-env.sh to specify that the machines are running on EC2, and automatically set this variable in the EC2 scripts.\n3. Shivaram Venkataraman: Fixed for Spark AMI by https://github.com/mesos/spark/pull/419\n4. Josh Rosen: Can https://github.com/mesos/spark/pull/419 be backported to 0.6?\n5. Josh Rosen: This may still be broken. I noticed a link to a worker with domu-*-*-*-*=*.compute-1.internal:8081, an internal address, when running the latest Spark master on the 0.7 AMI.\n6. Josh Rosen: Found what's hopefully the last issue here and opened a new PR: https://github.com/mesos/spark/pull/621\n7. Josh Rosen: Merged and cherry-picked that PR into branch-0.7, so this is resolved.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.010614"}}
{"id": "3dcbdbd825ac6c0b2f186cb373dfbf4d", "issue_key": "SPARK-614", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Make last 4 digits of framework id in standalone mode logging monotonically increasing", "description": "In mesos mode, the work log directories are monotonically increasing, and makes it very easy to spot a folder and go into it (e.g. only need to type *[last4digit]). We lost this in the standalone mode, as seen in this example. The last four digits would go up and down .... drwxr-xr-x 3 root root 4096 Nov 8 08:03 job-20121108080355-0000 drwxr-xr-x 3 root root 4096 Nov 8 08:04 job-20121108080450-0001 drwxr-xr-x 3 root root 4096 Nov 8 08:07 job-20121108080757-0002 drwxr-xr-x 3 root root 4096 Nov 8 08:10 job-20121108081014-0003 drwxr-xr-x 3 root root 4096 Nov 8 08:23 job-20121108082316-0004 drwxr-xr-x 3 root root 4096 Nov 8 08:26 job-20121108082616-0005 drwxr-xr-x 3 root root 4096 Nov 8 08:30 job-20121108083034-0006 drwxr-xr-x 3 root root 4096 Nov 8 08:35 job-20121108083514-0007 drwxr-xr-x 3 root root 4096 Nov 8 08:38 job-20121108083807-0008 drwxr-xr-x 3 root root 4096 Nov 8 08:41 job-20121108084105-0009 drwxr-xr-x 3 root root 4096 Nov 8 08:42 job-20121108084242-0010 drwxr-xr-x 3 root root 4096 Nov 8 08:45 job-20121108084512-0011 drwxr-xr-x 3 root root 4096 Nov 8 09:01 job-20121108090113-0000 drwxr-xr-x 3 root root 4096 Nov 8 09:15 job-20121108091536-0001 drwxr-xr-x 3 root root 4096 Nov 8 09:24 job-20121108092341-0003 drwxr-xr-x 3 root root 4096 Nov 8 09:27 job-20121108092703-0000 drwxr-xr-x 3 root root 4096 Nov 8 09:46 job-20121108094629-0001 drwxr-xr-x 3 root root 4096 Nov 8 09:48 job-20121108094809-0002 drwxr-xr-x 3 root root 4096 Nov 8 10:04 job-20121108100418-0003 drwxr-xr-x 3 root root 4096 Nov 8 10:18 job-20121108101814-0004 drwxr-xr-x 3 root root 4096 Nov 8 10:22 job-20121108102207-0005 drwxr-xr-x 3 root root 4096 Nov 8 18:48 job-20121108184842-0006 drwxr-xr-x 3 root root 4096 Nov 8 18:49 job-20121108184932-0007 drwxr-xr-x 3 root root 4096 Nov 8 18:50 job-20121108185007-0008 drwxr-xr-x 3 root root 4096 Nov 8 18:50 job-20121108185040-0009 drwxr-xr-x 3 root root 4096 Nov 8 18:51 job-20121108185127-0010 drwxr-xr-x 3 root root 4096 Nov 8 18:54 job-20121108185428-0011 drwxr-xr-x 3 root root 4096 Nov 8 18:58 job-20121108185837-0012 drwxr-xr-x 3 root root 4096 Nov 8 18:58 job-20121108185854-0013 drwxr-xr-x 3 root root 4096 Nov 8 19:00 job-20121108190005-0014 drwxr-xr-x 3 root root 4096 Nov 8 19:00 job-20121108190059-0015 drwxr-xr-x 3 root root 4096 Nov 8 19:10 job-20121108191010-0016 drwxr-xr-x 3 root root 4096 Nov 8 19:15 job-20121108191508-0017 drwxr-xr-x 3 root root 4096 Nov 8 19:21 job-20121108192125-0018 drwxr-xr-x 3 root root 4096 Nov 8 19:23 job-20121108192329-0019 drwxr-xr-x 3 root root 4096 Nov 8 19:26 job-20121108192638-0020 drwxr-xr-x 3 root root 4096 Nov 8 19:35 job-20121108193554-0022", "reporter": "Reynold Xin", "assignee": "Denny Britz", "created": "2012-11-08T11:41:33.000+0000", "updated": "2014-09-21T14:21:16.000+0000", "resolved": "2014-09-21T14:21:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "This only happens if you restart the standalone cluster between jobs. If you leave it running (the same as in Mesos), it will not happen.", "created": "2012-11-08T13:19:28.000+0000"}, {"author": "Reynold Xin", "body": "I've seen a few times the workers crashed, and that's why I had to restart the server. That said, any particular reason we follow the current directory naming scheme? How about making the first 4 digits monotonically increasing?", "created": "2012-11-08T13:23:41.122+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yes, the reason was to avoid race conditions that would happen if we tried to figure out the last job ID. In particular, the job ID is assigned by the master, so the master would need to remember the last job ID it gave when it restarted, which is kind of ugly. You could restart a worker without restarting the master.. no need to restart the whole cluster.", "created": "2012-11-08T13:42:23.616+0000"}, {"author": "Matthew Farrellee", "body": "it looks like nothing has happened with this in the past 23 months. i'm going to close this, but feel free to re-open.", "created": "2014-09-21T14:20:43.264+0000"}], "num_comments": 4, "text": "Issue: SPARK-614\nSummary: Make last 4 digits of framework id in standalone mode logging monotonically increasing\nDescription: In mesos mode, the work log directories are monotonically increasing, and makes it very easy to spot a folder and go into it (e.g. only need to type *[last4digit]). We lost this in the standalone mode, as seen in this example. The last four digits would go up and down .... drwxr-xr-x 3 root root 4096 Nov 8 08:03 job-20121108080355-0000 drwxr-xr-x 3 root root 4096 Nov 8 08:04 job-20121108080450-0001 drwxr-xr-x 3 root root 4096 Nov 8 08:07 job-20121108080757-0002 drwxr-xr-x 3 root root 4096 Nov 8 08:10 job-20121108081014-0003 drwxr-xr-x 3 root root 4096 Nov 8 08:23 job-20121108082316-0004 drwxr-xr-x 3 root root 4096 Nov 8 08:26 job-20121108082616-0005 drwxr-xr-x 3 root root 4096 Nov 8 08:30 job-20121108083034-0006 drwxr-xr-x 3 root root 4096 Nov 8 08:35 job-20121108083514-0007 drwxr-xr-x 3 root root 4096 Nov 8 08:38 job-20121108083807-0008 drwxr-xr-x 3 root root 4096 Nov 8 08:41 job-20121108084105-0009 drwxr-xr-x 3 root root 4096 Nov 8 08:42 job-20121108084242-0010 drwxr-xr-x 3 root root 4096 Nov 8 08:45 job-20121108084512-0011 drwxr-xr-x 3 root root 4096 Nov 8 09:01 job-20121108090113-0000 drwxr-xr-x 3 root root 4096 Nov 8 09:15 job-20121108091536-0001 drwxr-xr-x 3 root root 4096 Nov 8 09:24 job-20121108092341-0003 drwxr-xr-x 3 root root 4096 Nov 8 09:27 job-20121108092703-0000 drwxr-xr-x 3 root root 4096 Nov 8 09:46 job-20121108094629-0001 drwxr-xr-x 3 root root 4096 Nov 8 09:48 job-20121108094809-0002 drwxr-xr-x 3 root root 4096 Nov 8 10:04 job-20121108100418-0003 drwxr-xr-x 3 root root 4096 Nov 8 10:18 job-20121108101814-0004 drwxr-xr-x 3 root root 4096 Nov 8 10:22 job-20121108102207-0005 drwxr-xr-x 3 root root 4096 Nov 8 18:48 job-20121108184842-0006 drwxr-xr-x 3 root root 4096 Nov 8 18:49 job-20121108184932-0007 drwxr-xr-x 3 root root 4096 Nov 8 18:50 job-20121108185007-0008 drwxr-xr-x 3 root root 4096 Nov 8 18:50 job-20121108185040-0009 drwxr-xr-x 3 root root 4096 Nov 8 18:51 job-20121108185127-0010 drwxr-xr-x 3 root root 4096 Nov 8 18:54 job-20121108185428-0011 drwxr-xr-x 3 root root 4096 Nov 8 18:58 job-20121108185837-0012 drwxr-xr-x 3 root root 4096 Nov 8 18:58 job-20121108185854-0013 drwxr-xr-x 3 root root 4096 Nov 8 19:00 job-20121108190005-0014 drwxr-xr-x 3 root root 4096 Nov 8 19:00 job-20121108190059-0015 drwxr-xr-x 3 root root 4096 Nov 8 19:10 job-20121108191010-0016 drwxr-xr-x 3 root root 4096 Nov 8 19:15 job-20121108191508-0017 drwxr-xr-x 3 root root 4096 Nov 8 19:21 job-20121108192125-0018 drwxr-xr-x 3 root root 4096 Nov 8 19:23 job-20121108192329-0019 drwxr-xr-x 3 root root 4096 Nov 8 19:26 job-20121108192638-0020 drwxr-xr-x 3 root root 4096 Nov 8 19:35 job-20121108193554-0022\n\nComments (4):\n1. Matei Alexandru Zaharia: This only happens if you restart the standalone cluster between jobs. If you leave it running (the same as in Mesos), it will not happen.\n2. Reynold Xin: I've seen a few times the workers crashed, and that's why I had to restart the server. That said, any particular reason we follow the current directory naming scheme? How about making the first 4 digits monotonically increasing?\n3. Matei Alexandru Zaharia: Yes, the reason was to avoid race conditions that would happen if we tried to figure out the last job ID. In particular, the job ID is assigned by the master, so the master would need to remember the last job ID it gave when it restarted, which is kind of ugly. You could restart a worker without restarting the master.. no need to restart the whole cluster.\n4. Matthew Farrellee: it looks like nothing has happened with this in the past 23 months. i'm going to close this, but feel free to re-open.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.010614"}}
{"id": "e18013bf681d5a617772354034663acf", "issue_key": "SPARK-615", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add mapPartitionsWithIndex() to the Java API", "description": "We should add {{mapPartitionsWithIndex()}} to the Java API. What should the interface for this look like? We could require the user to pass in a {{FlatMapFunction[(Int, Iterator[T]))}}, but this requires them to unpack the tuple from Java. It would be nice if the UDF had a signature like {{f(int partition, Iterator[T] iterator)}}, but this will require defining a new set of {{Function}} classes.", "reporter": "Josh Rosen", "assignee": "Holden Karau", "created": "2012-11-09T20:46:04.000+0000", "updated": "2014-04-30T00:40:19.000+0000", "resolved": "2014-04-30T00:40:19.000+0000", "labels": ["Starter"], "components": ["Java API"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Sorry, thought I had replied to this earlier, but I think we should just create another subclass of Function.", "created": "2013-01-05T19:16:57.342+0000"}, {"author": "Josh Rosen", "body": "Holden Karau has a pull request for this at https://github.com/mesos/spark/pull/930/", "created": "2013-09-27T14:26:23.885+0000"}, {"author": "Josh Rosen", "body": "This still needs unit tests, since I don't think that it's actually callable from Java without passing ClassManifest; see my comment at https://github.com/mesos/spark/pull/930/files#r6640056", "created": "2013-10-20T14:25:22.906+0000"}], "num_comments": 3, "text": "Issue: SPARK-615\nSummary: Add mapPartitionsWithIndex() to the Java API\nDescription: We should add {{mapPartitionsWithIndex()}} to the Java API. What should the interface for this look like? We could require the user to pass in a {{FlatMapFunction[(Int, Iterator[T]))}}, but this requires them to unpack the tuple from Java. It would be nice if the UDF had a signature like {{f(int partition, Iterator[T] iterator)}}, but this will require defining a new set of {{Function}} classes.\n\nComments (3):\n1. Matei Alexandru Zaharia: Sorry, thought I had replied to this earlier, but I think we should just create another subclass of Function.\n2. Josh Rosen: Holden Karau has a pull request for this at https://github.com/mesos/spark/pull/930/\n3. Josh Rosen: This still needs unit tests, since I don't think that it's actually callable from Java without passing ClassManifest; see my comment at https://github.com/mesos/spark/pull/930/files#r6640056", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.011668"}}
{"id": "4cc844367c56cafe8c8b340958715344", "issue_key": "SPARK-616", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Show dead workers in the standalone web UI", "description": "It would be helpful if the standalone web UI listed dead workers; currently, dead workers disappear from the cluster summary. Perhaps they could be listed in a separate section or be highlighted in red.", "reporter": "Josh Rosen", "assignee": "Patrick McFadin", "created": "2012-11-10T21:30:22.000+0000", "updated": "2012-12-20T16:23:02.000+0000", "resolved": "2012-12-20T16:23:02.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Pull request up for this.", "created": "2012-12-17T23:12:59.250+0000"}], "num_comments": 1, "text": "Issue: SPARK-616\nSummary: Show dead workers in the standalone web UI\nDescription: It would be helpful if the standalone web UI listed dead workers; currently, dead workers disappear from the cluster summary. Perhaps they could be listed in a separate section or be highlighted in red.\n\nComments (1):\n1. Patrick McFadin: Pull request up for this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.011668"}}
{"id": "b9986286f277120029aca649a8cb9672", "issue_key": "SPARK-617", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Driver program can crash when a standalone worker is lost", "description": "Seems to be due to an uncaught communication timeout in Akka.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2012-11-10T22:46:56.000+0000", "updated": "2012-11-11T21:21:56.000+0000", "resolved": "2012-11-11T21:21:52.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Fixed in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7", "created": "2012-11-11T21:21:41.460+0000"}], "num_comments": 1, "text": "Issue: SPARK-617\nSummary: Driver program can crash when a standalone worker is lost\nDescription: Seems to be due to an uncaught communication timeout in Akka.\n\nComments (1):\n1. Matei Alexandru Zaharia: Fixed in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.012503"}}
{"id": "1e91b6a9f79ffb4e1a1554bc72bff481", "issue_key": "SPARK-618", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "start-mesos and stop-mesos scripts should check for running processes", "description": "The {{start-mesos}} script in the EC2 AMI should log warnings if Mesos processes are already running. Currently, running the script multiple times will launch multiple Mesos workers on each slave. This is blocked by [SPARK-521].", "reporter": "Josh Rosen", "assignee": null, "created": "2012-11-11T02:16:16.000+0000", "updated": "2013-09-27T14:30:17.000+0000", "resolved": "2013-09-27T14:30:17.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Josh Rosen", "body": "Our EC2 images no longer include Mesos, so I'm going to close this.", "created": "2013-09-27T14:30:17.267+0000"}], "num_comments": 1, "text": "Issue: SPARK-618\nSummary: start-mesos and stop-mesos scripts should check for running processes\nDescription: The {{start-mesos}} script in the EC2 AMI should log warnings if Mesos processes are already running. Currently, running the script multiple times will launch multiple Mesos workers on each slave. This is blocked by [SPARK-521].\n\nComments (1):\n1. Josh Rosen: Our EC2 images no longer include Mesos, so I'm going to close this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.012503"}}
{"id": "3092e77db8da574c892c4af38fdfa012", "issue_key": "SPARK-619", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Hadoop MapReduce should be configured to use all local disks for shuffle on AMI", "description": "It used to be, but that got lost at some point.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2012-11-11T11:22:40.000+0000", "updated": "2014-11-06T07:00:29.000+0000", "resolved": "2014-11-06T07:00:29.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-619\nSummary: Hadoop MapReduce should be configured to use all local disks for shuffle on AMI\nDescription: It used to be, but that got lost at some point.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.012503"}}
{"id": "9787651c9a29121fa41981e1cbb8154f", "issue_key": "SPARK-620", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Default SPARK_MEM on AMI too high", "description": "When I use the 0.6 AMI, start the Spark shell and try to load a dataset from S3 I get an out of memory error. It worked after I reduced the SPARK_MEM setting in the config.", "reporter": "Denny Britz", "assignee": null, "created": "2012-11-11T12:05:57.000+0000", "updated": "2013-05-04T19:19:00.000+0000", "resolved": "2013-05-04T19:18:59.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "The spell occasionally forks some commands, and each fork would require starting a new heap that's the same size as the old heap. To get rid of the problem, the spark_mem on the master cannot be greater than half of the ram available on the master node.", "created": "2012-11-11T12:07:46.146+0000"}, {"author": "Denny Britz", "body": "I only reduced the memory by about 1g and it got rid of the problem. It definitely was more than half of the memory available on the master (12g on m1.xlarge instance).", "created": "2012-11-11T12:12:17.378+0000"}, {"author": "Reynold Xin", "body": "There are two problems here really.", "created": "2012-11-11T12:16:27.385+0000"}], "num_comments": 3, "text": "Issue: SPARK-620\nSummary: Default SPARK_MEM on AMI too high\nDescription: When I use the 0.6 AMI, start the Spark shell and try to load a dataset from S3 I get an out of memory error. It worked after I reduced the SPARK_MEM setting in the config.\n\nComments (3):\n1. Reynold Xin: The spell occasionally forks some commands, and each fork would require starting a new heap that's the same size as the old heap. To get rid of the problem, the spark_mem on the master cannot be greater than half of the ram available on the master node.\n2. Denny Britz: I only reduced the memory by about 1g and it got rid of the problem. It definitely was more than half of the memory available on the master (12g on m1.xlarge instance).\n3. Reynold Xin: There are two problems here really.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.012503"}}
{"id": "583385f3439551260b91e12b2b41a7e0", "issue_key": "SPARK-621", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Provide an API to manually throw RDDs out of the cache", "description": "I want this.", "reporter": "Denny Britz", "assignee": "Reynold Xin", "created": "2012-11-11T12:14:34.000+0000", "updated": "2013-05-06T16:10:18.000+0000", "resolved": "2013-05-06T16:10:18.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "If this is a useful feature, it shouldn't be that hard to implement now that 0.7 added support for dropping blocks in the block manager: https://github.com/mesos/spark/pull/327", "created": "2013-04-06T13:26:57.668+0000"}, {"author": "Reynold Xin", "body": "In pull request: https://github.com/mesos/spark/pull/591", "created": "2013-05-01T17:47:54.464+0000"}], "num_comments": 2, "text": "Issue: SPARK-621\nSummary: Provide an API to manually throw RDDs out of the cache\nDescription: I want this.\n\nComments (2):\n1. Josh Rosen: If this is a useful feature, it shouldn't be that hard to implement now that 0.7 added support for dropping blocks in the block manager: https://github.com/mesos/spark/pull/327\n2. Reynold Xin: In pull request: https://github.com/mesos/spark/pull/591", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.012503"}}
{"id": "38d24010e61e4afb954234371539099e", "issue_key": "SPARK-622", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "spark-ec2 launch command hangs if instances fail while starting up.", "description": "The {{spark-ec2}} script should detect if instances fail while launching, rather than waiting indefinitely for the instances to start up.", "reporter": "Josh Rosen", "assignee": null, "created": "2012-11-11T12:59:07.000+0000", "updated": "2012-11-11T13:03:46.000+0000", "resolved": "2012-11-11T13:03:46.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Actually, it looks there is already code to handle this; I observed both a failure and a node that became stuck in \"pending\", and the pending node lead to the hang. Closing this.", "created": "2012-11-11T13:03:46.792+0000"}], "num_comments": 1, "text": "Issue: SPARK-622\nSummary: spark-ec2 launch command hangs if instances fail while starting up.\nDescription: The {{spark-ec2}} script should detect if instances fail while launching, rather than waiting indefinitely for the instances to start up.\n\nComments (1):\n1. Josh Rosen: Actually, it looks there is already code to handle this; I observed both a failure and a node that became stuck in \"pending\", and the pending node lead to the hang. Closing this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.012503"}}
{"id": "d06b93ba5eca90e48000854b357dfa97", "issue_key": "SPARK-623", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Don't hardcode log location for standalone UI", "description": "", "reporter": "Denny Britz", "assignee": "Christoph Grothaus", "created": "2012-11-14T15:41:13.000+0000", "updated": "2013-05-13T14:37:09.000+0000", "resolved": "2013-05-13T14:37:09.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Does anyone know which log / directory this issue is talking about?", "created": "2013-05-13T14:22:22.285+0000"}, {"author": "Reynold Xin", "body": "In WorkerArguments.scala, I do see  if (System.getenv(\"SPARK_WORKER_DIR\") != null) { workDir = System.getenv(\"SPARK_WORKER_DIR\") }", "created": "2013-05-13T14:25:13.107+0000"}, {"author": "Josh Rosen", "body": "I think that this issue referred to the WorkerWebUI assuming that worker logs would be in $SPARK_HOME/work. This was fixed by https://github.com/mesos/spark/pull/539", "created": "2013-05-13T14:37:09.355+0000"}], "num_comments": 3, "text": "Issue: SPARK-623\nSummary: Don't hardcode log location for standalone UI\n\nComments (3):\n1. Josh Rosen: Does anyone know which log / directory this issue is talking about?\n2. Reynold Xin: In WorkerArguments.scala, I do see  if (System.getenv(\"SPARK_WORKER_DIR\") != null) { workDir = System.getenv(\"SPARK_WORKER_DIR\") }\n3. Josh Rosen: I think that this issue referred to the WorkerWebUI assuming that worker logs would be in $SPARK_HOME/work. This was fixed by https://github.com/mesos/spark/pull/539", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.012503"}}
{"id": "894a06509235189d75fdea99e66ead19", "issue_key": "SPARK-624", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "The local IP address to bind to should be configurable", "description": "Local IP address that Spark uses by default to bind server sockets should be configurable. This is essential on systems where InetAddress.getLocalHost.getHostAddress does not return the correct interface, e.g. the default configuration of Debian/Ubuntu, where /etc/hosts resolves host name as 127.0.1.1. Spark should not have to rely on hostnames, /etc/hosts and DNS, and the proposed solution is to add an environment variable, e.g. SPARK_DEFAULT_LOCAL_IP, that would take precedence over InetAddress.getLocalHost.getHostAddress in Utils.localIpAddress.", "reporter": null, "assignee": null, "created": "2012-11-15T13:40:45.000+0000", "updated": "2012-11-21T11:38:19.000+0000", "resolved": "2012-11-21T11:38:19.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "An environment variable is good, but let me see first if we can just ask Java for all the IP addresses of the local host and ignore ones that start with 127.*. Ideally users shouldn't need to configure anything to bind to the right IP address. It looks like it may be possible to enumerate all IPs for the local hostname: http://stackoverflow.com/questions/494465/how-to-enumerate-ip-addresses-of-all-enabled-nic-cards-from-java. Let me know if you'd like to look into this.", "created": "2012-11-15T22:32:44.168+0000"}, {"author": "Mikhail Bautin", "body": "Yes, I can try to implement the general-case NIC IP detection. This would help users that only have one real NIC per host to get up and running out of the box. However, this would not help in our case, because we have two legitimate interfaces per host (one for communication within cluster and one for accessing the internet) and we have to tell Spark which one to use explicitly, so the environment variable would still be very useful for us.", "created": "2012-11-16T16:06:54.639+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I see, that makes sense. In that case I'll merge your pull request, but just beware that there will still be places where we use the hostname. For example, we found a bug where HDFS data locality didn't work on the standalone cluster because the workers reported IP addresses instead of hostnames, but HDFS uses hostnames to determine data locality. So we might need to make sure we use this variable whenever we bind a server socket in more places (but report the hostname to the master and hope it resolves to the right IP). If you try the latest master or 0.6 branches, they have this fix to the standalone mode.", "created": "2012-11-16T21:00:41.028+0000"}], "num_comments": 3, "text": "Issue: SPARK-624\nSummary: The local IP address to bind to should be configurable\nDescription: Local IP address that Spark uses by default to bind server sockets should be configurable. This is essential on systems where InetAddress.getLocalHost.getHostAddress does not return the correct interface, e.g. the default configuration of Debian/Ubuntu, where /etc/hosts resolves host name as 127.0.1.1. Spark should not have to rely on hostnames, /etc/hosts and DNS, and the proposed solution is to add an environment variable, e.g. SPARK_DEFAULT_LOCAL_IP, that would take precedence over InetAddress.getLocalHost.getHostAddress in Utils.localIpAddress.\n\nComments (3):\n1. Matei Alexandru Zaharia: An environment variable is good, but let me see first if we can just ask Java for all the IP addresses of the local host and ignore ones that start with 127.*. Ideally users shouldn't need to configure anything to bind to the right IP address. It looks like it may be possible to enumerate all IPs for the local hostname: http://stackoverflow.com/questions/494465/how-to-enumerate-ip-addresses-of-all-enabled-nic-cards-from-java. Let me know if you'd like to look into this.\n2. Mikhail Bautin: Yes, I can try to implement the general-case NIC IP detection. This would help users that only have one real NIC per host to get up and running out of the box. However, this would not help in our case, because we have two legitimate interfaces per host (one for communication within cluster and one for accessing the internet) and we have to tell Spark which one to use explicitly, so the environment variable would still be very useful for us.\n3. Matei Alexandru Zaharia: I see, that makes sense. In that case I'll merge your pull request, but just beware that there will still be places where we use the hostname. For example, we found a bug where HDFS data locality didn't work on the standalone cluster because the workers reported IP addresses instead of hostnames, but HDFS uses hostnames to determine data locality. So we might need to make sure we use this variable whenever we bind a server socket in more places (but report the hostname to the master and hope it resolves to the right IP). If you try the latest master or 0.6 branches, they have this fix to the standalone mode.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.013657"}}
{"id": "309fb85492ab7b1df9af5f501f0281a4", "issue_key": "SPARK-625", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Client hangs when connecting to standalone cluster using wrong address", "description": "I launched a standalone cluster on my laptop, connecting the workers to the master using my machine's public IP address (128.32.*.*:7077). If I try to connect spark-shell to the master using \"spark://0.0.0.0:7077\", it successfully brings up a Scala prompt but hangs when I try to run a job. From the standalone master's log, it looks like the client's messages are being dropped without the client discovering that the connection has failed:  12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message RegisterJob(JobDescription(Spark shell)) for non-local recipient akka://spark@0.0.0.0:7077/user/Master at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077 12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message DaemonMsgWatch(Actor[akka://spark@128.32.*.*:57518/user/$a],Actor[akka://spark@0.0.0.0:7077/user/Master]) for non-local recipient akka://spark@0.0.0.0:7077/remote at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077", "reporter": "Josh Rosen", "assignee": null, "created": "2012-11-27T14:13:26.000+0000", "updated": "2015-02-07T22:48:32.000+0000", "resolved": "2015-02-07T22:48:32.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "SeanM", "body": "I have run into this as well. From what it appears is that akka is just very sensitive to hostnames. Since these are different, you got that error: akka://spark@0.0.0.0:7077 akka://spark@128.32.*.*:7077 I ran into this because my slaves file was using fqdn, while akka was expecting just hostname. As soon as I switched my slaves file to just using hostnames, things started working great for me.", "created": "2013-01-17T22:35:27.674+0000"}, {"author": "Josh Rosen", "body": "Fixed by Matei in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7, which also fixed SPARK-617", "created": "2013-05-19T13:44:35.675+0000"}, {"author": "Josh Rosen", "body": "Actually, I take that back: If I run  MASTER=spark://0.0.0.0:7077 ./spark-shell  on my laptop, I see the same hang. This is with the current (0.8) master branch.", "created": "2013-05-19T13:47:02.618+0000"}, {"author": "Ian O Connell", "body": "What OS is your laptop Josh? something like http://stackoverflow.com/questions/11982562/socket-connect-to-0-0-0-0-windows-vs-mac suggests that java just won't like connecting to it. Chrome on windows for me says 0.0.0.0 is an invalid address, it works on OS X however. so you could just be hitting that?", "created": "2013-06-17T16:29:43.094+0000"}, {"author": "Andrew Ash", "body": "Spark is very sensitive to hostnames in Spark URLs, and that comes from Akka being very sensitive. I've personally been bitten by hostnames vs FQDNs vs external IP address vs loopback IP address, and it's really a pain. On current master branch (1.2) with the Spark standalone master listening on {{spark://aash-mbp.local:7077}} as confirmed by the master web UI, and the spark shell attempting to connect to {{spark://127.0.01:7077}} with the {{--master}} parameter, the driver tries 3 attempts and then fails with this message:  14/11/14 01:37:56 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077... 14/11/14 01:37:56 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077 14/11/14 01:37:56 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077 14/11/14 01:38:16 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077... 14/11/14 01:38:16 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077 14/11/14 01:38:16 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077 14/11/14 01:38:36 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077... 14/11/14 01:38:36 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077 14/11/14 01:38:36 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077 14/11/14 01:38:56 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up. 14/11/14 01:38:56 WARN SparkDeploySchedulerBackend: Application ID is not initialized yet. 14/11/14 01:38:56 ERROR TaskSchedulerImpl: Exiting due to error from cluster scheduler: All masters are unresponsive! Giving up.  So the hang seems to be gone and replaced with a reasonable 3x attempts and fail. [~joshrosen], short of changing Akka ourselves to make it less strict on exact URL matches, is there anything else we can do for this ticket? I think we can reasonably close as fixed.", "created": "2014-11-14T09:43:30.967+0000"}, {"author": "Josh Rosen", "body": "Let's resolve this as \"Fixed\" for now. Reducing Akka's sensitivity to hostnames is a more general issue and we may have a fix for this in the future by either upgrading to a version of Akka that differentiates between bound and advertised addressed or by replacing Akka with a different communications layer. I don't think we've observed the \"hang indefinitely\" behavior described in this ticket for many versions, so I think this should be safe to close.", "created": "2015-02-07T22:48:32.847+0000"}], "num_comments": 6, "text": "Issue: SPARK-625\nSummary: Client hangs when connecting to standalone cluster using wrong address\nDescription: I launched a standalone cluster on my laptop, connecting the workers to the master using my machine's public IP address (128.32.*.*:7077). If I try to connect spark-shell to the master using \"spark://0.0.0.0:7077\", it successfully brings up a Scala prompt but hangs when I try to run a job. From the standalone master's log, it looks like the client's messages are being dropped without the client discovering that the connection has failed:  12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message RegisterJob(JobDescription(Spark shell)) for non-local recipient akka://spark@0.0.0.0:7077/user/Master at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077 12/11/27 14:00:52 ERROR NettyRemoteTransport(null): dropping message DaemonMsgWatch(Actor[akka://spark@128.32.*.*:57518/user/$a],Actor[akka://spark@0.0.0.0:7077/user/Master]) for non-local recipient akka://spark@0.0.0.0:7077/remote at akka://spark@128.32.*.*:7077 local is akka://spark@128.32.*.*:7077\n\nComments (6):\n1. SeanM: I have run into this as well. From what it appears is that akka is just very sensitive to hostnames. Since these are different, you got that error: akka://spark@0.0.0.0:7077 akka://spark@128.32.*.*:7077 I ran into this because my slaves file was using fqdn, while akka was expecting just hostname. As soon as I switched my slaves file to just using hostnames, things started working great for me.\n2. Josh Rosen: Fixed by Matei in https://github.com/mesos/spark/commit/173e0354c0fc95d63112c7ff7121d8ff39f961b7, which also fixed SPARK-617\n3. Josh Rosen: Actually, I take that back: If I run  MASTER=spark://0.0.0.0:7077 ./spark-shell  on my laptop, I see the same hang. This is with the current (0.8) master branch.\n4. Ian O Connell: What OS is your laptop Josh? something like http://stackoverflow.com/questions/11982562/socket-connect-to-0-0-0-0-windows-vs-mac suggests that java just won't like connecting to it. Chrome on windows for me says 0.0.0.0 is an invalid address, it works on OS X however. so you could just be hitting that?\n5. Andrew Ash: Spark is very sensitive to hostnames in Spark URLs, and that comes from Akka being very sensitive. I've personally been bitten by hostnames vs FQDNs vs external IP address vs loopback IP address, and it's really a pain. On current master branch (1.2) with the Spark standalone master listening on {{spark://aash-mbp.local:7077}} as confirmed by the master web UI, and the spark shell attempting to connect to {{spark://127.0.01:7077}} with the {{--master}} parameter, the driver tries 3 attempts and then fails with this message:  14/11/14 01:37:56 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077... 14/11/14 01:37:56 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077 14/11/14 01:37:56 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077 14/11/14 01:38:16 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077... 14/11/14 01:38:16 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077 14/11/14 01:38:16 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077 14/11/14 01:38:36 INFO AppClient$ClientActor: Connecting to master spark://127.0.0.1:7077... 14/11/14 01:38:36 WARN Remoting: Tried to associate with unreachable remote address [akka.tcp://sparkMaster@127.0.0.1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /127.0.0.1:7077 14/11/14 01:38:36 WARN AppClient$ClientActor: Could not connect to akka.tcp://sparkMaster@127.0.0.1:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@127.0.0.1:7077 14/11/14 01:38:56 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up. 14/11/14 01:38:56 WARN SparkDeploySchedulerBackend: Application ID is not initialized yet. 14/11/14 01:38:56 ERROR TaskSchedulerImpl: Exiting due to error from cluster scheduler: All masters are unresponsive! Giving up.  So the hang seems to be gone and replaced with a reasonable 3x attempts and fail. [~joshrosen], short of changing Akka ourselves to make it less strict on exact URL matches, is there anything else we can do for this ticket? I think we can reasonably close as fixed.\n6. Josh Rosen: Let's resolve this as \"Fixed\" for now. Reducing Akka's sensitivity to hostnames is a more general issue and we may have a fix for this in the future by either upgrading to a version of Akka that differentiates between bound and advertised addressed or by replacing Akka with a different communications layer. I don't think we've observed the \"hang indefinitely\" behavior described in this ticket for many versions, so I think this should be safe to close.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.013657"}}
{"id": "02ce89774df2c7390130bb9f21cd2a8f", "issue_key": "SPARK-626", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "deleting security groups gives me a 400 error", "description": "Filing on behalf of Shivaram: Deleting security group tinytasks-test-zoo ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 637, in <module> main() File \"./spark_ec2.py\", line 571, in main conn.delete_security_group(group.name) File \"/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2039, in delete_security_group File \"/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>", "reporter": "Reynold Xin", "assignee": "Peter Sankauskas", "created": "2012-11-29T21:40:22.000+0000", "updated": "2012-12-11T12:13:57.000+0000", "resolved": "2012-12-11T12:13:57.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "I ran into the same issue and tried to investigate it: There may be dependencies between security groups (e.g. allow traffic to/from another group), so these dependencies must be removed before the groups can be deleted. The current script tries to do this, but it needs an additional for-loop: the current script removes an individual group's rules then deletes that group, but it should first remove all rules from all groups then delete all groups. I tried modifying the script to do this (https://gist.github.com/4187604), but it ran into the same error. When I ran the destroy command a second time, it successfully deleted the groups, so there may be a race condition there.", "created": "2012-12-01T23:32:39.646+0000"}, {"author": "Peter Sankauskas", "body": "I think that Gist will work, but perhaps there needs to be a delay between the steps. I get the feeling that behind the scenes the AWS backend is using eventual consistency, which is why I didn't experience this issues but others are.", "created": "2012-12-04T11:45:15.429+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Have you tried that, Peter? Would be nice to see this fixed.", "created": "2012-12-10T16:20:43.753+0000"}, {"author": "Peter Sankauskas", "body": "Here you go: https://github.com/mesos/spark/pull/323 I tried lowering the sleep delay in between deleting the rules and deleting the groups, but anything below 30 seconds caused issues. 30 seconds seems to be as low as we can consistently go.", "created": "2012-12-10T17:46:49.021+0000"}, {"author": "Peter Sankauskas", "body": "It turns out this is far more error prone than I first though. There are multiple dependencies in play: - dependencies between rules in security groups - instances may not have been terminated completely when trying to delete a security group - AWS back-end eventual consistency - group.revoke() returns True even when it fails The unfortunate result is the code is somewhat messy. This is as clean as I can make - there are 3 retries to delete the all groups, and exceptions are caught. By default, groups will not be deleted, so most people won't experience this. You can delete groups when destroying a cluster by adding `--delete-groups`", "created": "2012-12-11T10:54:49.681+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Alright, I've merged your commit. Thanks for looking into this and fixing it!", "created": "2012-12-11T12:02:37.795+0000"}], "num_comments": 6, "text": "Issue: SPARK-626\nSummary: deleting security groups gives me a 400 error\nDescription: Filing on behalf of Shivaram: Deleting security group tinytasks-test-zoo ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 637, in <module> main() File \"./spark_ec2.py\", line 571, in main conn.delete_security_group(group.name) File \"/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2039, in delete_security_group File \"/home/shivaram/projects/tinytasks/tiny-tasks-spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.InUse</Code><Message>Group 905882038624:tinytasks-test-zoo is used by groups: 905882038624:tinytasks-test-master 905882038624:tinytasks-test-slaves</Message></Error></Errors><RequestID>5067d547-e88c-45d0-8ef0-15db51a444c0</RequestID></Response>\n\nComments (6):\n1. Josh Rosen: I ran into the same issue and tried to investigate it: There may be dependencies between security groups (e.g. allow traffic to/from another group), so these dependencies must be removed before the groups can be deleted. The current script tries to do this, but it needs an additional for-loop: the current script removes an individual group's rules then deletes that group, but it should first remove all rules from all groups then delete all groups. I tried modifying the script to do this (https://gist.github.com/4187604), but it ran into the same error. When I ran the destroy command a second time, it successfully deleted the groups, so there may be a race condition there.\n2. Peter Sankauskas: I think that Gist will work, but perhaps there needs to be a delay between the steps. I get the feeling that behind the scenes the AWS backend is using eventual consistency, which is why I didn't experience this issues but others are.\n3. Matei Alexandru Zaharia: Have you tried that, Peter? Would be nice to see this fixed.\n4. Peter Sankauskas: Here you go: https://github.com/mesos/spark/pull/323 I tried lowering the sleep delay in between deleting the rules and deleting the groups, but anything below 30 seconds caused issues. 30 seconds seems to be as low as we can consistently go.\n5. Peter Sankauskas: It turns out this is far more error prone than I first though. There are multiple dependencies in play: - dependencies between rules in security groups - instances may not have been terminated completely when trying to delete a security group - AWS back-end eventual consistency - group.revoke() returns True even when it fails The unfortunate result is the code is somewhat messy. This is as clean as I can make - there are 3 retries to delete the all groups, and exceptions are caught. By default, groups will not be deleted, so most people won't experience this. You can delete groups when destroying a cluster by adding `--delete-groups`\n6. Matei Alexandru Zaharia: Alright, I've merged your commit. Thanks for looking into this and fixing it!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.013657"}}
{"id": "16967286f4041ce609d53b7fccadfb19", "issue_key": "SPARK-627", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Unimplemented configuration options in spark-daemon[s].sh", "description": "The spark-daemon.sh and spark-daemons.sh scripts in the Spark bin directory have command-line arguments for features that aren't implemented, such as the ability to pass the configuration file and slave files as command-line arguments:  Usage: spark-daemons.sh [--config confdir] [--hosts hostlistfile] [start|stop] command args...  We should either implement these features or remove the options.", "reporter": "Josh Rosen", "assignee": "Karthik G Tunga", "created": "2012-12-01T16:50:00.000+0000", "updated": "2013-10-19T18:06:24.000+0000", "resolved": "2013-10-19T18:06:24.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Aaron Davidson", "body": "Assigned by request", "created": "2013-10-14T09:37:51.159+0000"}, {"author": "Karthik G Tunga", "body": "I have a couple of questions 1) In case the --config argument is not a valid directory should the execution abort or should it continue taking the default path ? 2) Can I go ahead and remove the --hosts argument ? Do we have a use-case where it would be required ? It can still be achieved by creating a separate conf directory and passing it as an argument.", "created": "2013-10-14T23:52:35.488+0000"}, {"author": "Karthik G Tunga", "body": "The changes are checked in.", "created": "2013-10-19T17:51:11.721+0000"}, {"author": "Aaron Davidson", "body": "https://github.com/apache/incubator-spark/pull/69", "created": "2013-10-19T18:06:24.118+0000"}], "num_comments": 4, "text": "Issue: SPARK-627\nSummary: Unimplemented configuration options in spark-daemon[s].sh\nDescription: The spark-daemon.sh and spark-daemons.sh scripts in the Spark bin directory have command-line arguments for features that aren't implemented, such as the ability to pass the configuration file and slave files as command-line arguments:  Usage: spark-daemons.sh [--config confdir] [--hosts hostlistfile] [start|stop] command args...  We should either implement these features or remove the options.\n\nComments (4):\n1. Aaron Davidson: Assigned by request\n2. Karthik G Tunga: I have a couple of questions 1) In case the --config argument is not a valid directory should the execution abort or should it continue taking the default path ? 2) Can I go ahead and remove the --hosts argument ? Do we have a use-case where it would be required ? It can still be achieved by creating a separate conf directory and passing it as an argument.\n3. Karthik G Tunga: The changes are checked in.\n4. Aaron Davidson: https://github.com/apache/incubator-spark/pull/69", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.014562"}}
{"id": "fd245ca3993749b5444fb41527342c4e", "issue_key": "SPARK-628", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Make deletion of EC2 security groups optional", "description": "The [recent change|https://github.com/mesos/spark/pull/306] to automatically delete EC2 security groups breaks support for custom security groups. For example, I use launch clusters using custom security groups that open ports used by the YourKit remote debugger. I think that deletion of security groups should be optional and disabled by default to avoid surprising users with unexpected behavior or breaking existing deployments when users upgrade.", "reporter": "Josh Rosen", "assignee": "Peter Sankauskas", "created": "2012-12-01T23:07:46.000+0000", "updated": "2012-12-12T13:13:01.000+0000", "resolved": "2012-12-12T13:13:01.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/323", "created": "2012-12-12T13:13:01.458+0000"}], "num_comments": 1, "text": "Issue: SPARK-628\nSummary: Make deletion of EC2 security groups optional\nDescription: The [recent change|https://github.com/mesos/spark/pull/306] to automatically delete EC2 security groups breaks support for custom security groups. For example, I use launch clusters using custom security groups that open ports used by the YourKit remote debugger. I think that deletion of security groups should be optional and disabled by default to avoid surprising users with unexpected behavior or breaking existing deployments when users upgrade.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/323", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.014562"}}
{"id": "a14a9e759ac73df611e1123bedf7e9a0", "issue_key": "SPARK-629", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Standalone job details page has strange value for number of cores", "description": "When I view the job details page of a job running on a standalone cluster, I see the following strange output:  Cores: 2147483647 (400 Granted )  I'm not sure where 2147483647 is coming from (it's Integer.MAX_VALUE). Looking at the code for this job details page, this is generated by the following:  <li><strong>Cores:</strong> @job.desc.cores (@job.coresGranted Granted @if(job.desc.cores == Integer.MAX_VALUE) { } else { , @job.coresLeft } ) </li>  I'm not sure what this is supposed to do; is the idea to display something like \"Cores: totalCores (x granted, y pending)\"? Does Integer.MAX_VALUE have any special significance when used as the number of cores in a JobDescription?", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-12-04T14:50:23.000+0000", "updated": "2013-05-04T22:52:32.000+0000", "resolved": "2013-05-04T22:52:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "This is still a problem in 0.7.", "created": "2013-04-05T19:54:41.559+0000"}, {"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/597", "created": "2013-05-04T22:52:32.739+0000"}], "num_comments": 2, "text": "Issue: SPARK-629\nSummary: Standalone job details page has strange value for number of cores\nDescription: When I view the job details page of a job running on a standalone cluster, I see the following strange output:  Cores: 2147483647 (400 Granted )  I'm not sure where 2147483647 is coming from (it's Integer.MAX_VALUE). Looking at the code for this job details page, this is generated by the following:  <li><strong>Cores:</strong> @job.desc.cores (@job.coresGranted Granted @if(job.desc.cores == Integer.MAX_VALUE) { } else { , @job.coresLeft } ) </li>  I'm not sure what this is supposed to do; is the idea to display something like \"Cores: totalCores (x granted, y pending)\"? Does Integer.MAX_VALUE have any special significance when used as the number of cores in a JobDescription?\n\nComments (2):\n1. Josh Rosen: This is still a problem in 0.7.\n2. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/597", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.014562"}}
{"id": "a9f5bf36f1aa1fbcf46533aeb03e5cb6", "issue_key": "SPARK-630", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Master web UI shows some finished/killed executors as running", "description": "When I view a finished job's Job Details page on the standalone master, it shows all executors as RUNNING. However, when I view any of the workers' pages, the same executor appears under the \"Finished Executors\" list.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-12-04T18:15:10.000+0000", "updated": "2013-05-04T22:52:12.000+0000", "resolved": "2013-05-04T22:52:12.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/597", "created": "2013-05-04T22:52:12.921+0000"}], "num_comments": 1, "text": "Issue: SPARK-630\nSummary: Master web UI shows some finished/killed executors as running\nDescription: When I view a finished job's Job Details page on the standalone master, it shows all executors as RUNNING. However, when I view any of the workers' pages, the same executor appears under the \"Finished Executors\" list.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/597", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.015598"}}
{"id": "be33104df39f8c0625b09e2b52af066c", "issue_key": "SPARK-631", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "SPARK_LOCAL_IP environment variable should also affect spark.master.host", "description": "So that we can have a single variable for configuring the IP address that Spark uses.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2012-12-06T18:10:09.000+0000", "updated": "2012-12-08T01:11:30.000+0000", "resolved": "2012-12-08T01:11:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "This was actually happening correctly by default. I also updated it so that Spark prefers to bind to a non-loopback address in case InetAddress.getLocalHost returns a loopback one.", "created": "2012-12-08T01:11:16.073+0000"}], "num_comments": 1, "text": "Issue: SPARK-631\nSummary: SPARK_LOCAL_IP environment variable should also affect spark.master.host\nDescription: So that we can have a single variable for configuring the IP address that Spark uses.\n\nComments (1):\n1. Matei Alexandru Zaharia: This was actually happening correctly by default. I also updated it so that Spark prefers to bind to a non-loopback address in case InetAddress.getLocalHost returns a loopback one.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.015598"}}
{"id": "b6593127a03b3a56cd59e9563d4647a6", "issue_key": "SPARK-632", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Akka system names need to be normalized (since they are case-sensitive)", "description": "The \"system\" name of the Akka full path is case-sensitive (see http://akka.io/faq/#what_is_the_name_of_a_remote_actor). Since DNS names are case-insensitive and we're using them in the \"system\" name, we need to normalize them (e.g. make them all lowercase). Otherwise, users will find the \"workers\" will not be able to connect with the \"master\" even though the URI appears to be correct. For example, Berkeley DNS occasionally uses names e.g. foo.Berkley.EDU. If I used foo.berkeley.edu as the master adddress, the workers would write to their logs that they are connecting to foo.berkeley.edu but failed to. They never show up in the master UI. If use the foo.Berkeley.EDU address, everything works as it should.", "reporter": "Matt Massie", "assignee": null, "created": "2012-12-07T11:44:42.000+0000", "updated": "2014-11-11T09:13:29.000+0000", "resolved": "2014-11-11T09:13:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Andrew Ash", "body": "// link moved to http://doc.akka.io/docs/akka/current/additional/faq.html#what-is-the-name-of-a-remote-actor I believe having the hostname change case will still break Spark. But after a search of the dev and user mailing lists over the past year I haven't seen any other users with this issue. A potential fix could be to call .toLower on the hostname in the Akka string across the cluster, but it's a little dirty to make this assumption everywhere. Technically [hostnames ARE case insensitive|http://serverfault.com/questions/261341/is-the-hostname-case-sensitive] so Spark's behavior is wrong, but the issue is in the underlying Akka library. This is the same underlying behavior where Akka requires that hostnames exactly match as well -- you can't use an IP address to refer to a Akka listening on a hostname -- SPARK-625. Until Akka handles differently-cased hostnames I think can only be done with an ugly workaround. Possibly relevant Akka issues: - https://github.com/akka/akka/issues/15990 - https://github.com/akka/akka/issues/15007 My preference would be to close this as \"Won't Fix\" until it's raised again as a problem from the community. cc [~rxin]", "created": "2014-11-11T08:43:14.796+0000"}, {"author": "Reynold Xin", "body": "Sounds good. In the future we might roll our own RPC rather than using Actor for RPC. I think the current RPC library built for the shuffle service is already ok with case insensitive hostnames.", "created": "2014-11-11T09:13:23.188+0000"}], "num_comments": 2, "text": "Issue: SPARK-632\nSummary: Akka system names need to be normalized (since they are case-sensitive)\nDescription: The \"system\" name of the Akka full path is case-sensitive (see http://akka.io/faq/#what_is_the_name_of_a_remote_actor). Since DNS names are case-insensitive and we're using them in the \"system\" name, we need to normalize them (e.g. make them all lowercase). Otherwise, users will find the \"workers\" will not be able to connect with the \"master\" even though the URI appears to be correct. For example, Berkeley DNS occasionally uses names e.g. foo.Berkley.EDU. If I used foo.berkeley.edu as the master adddress, the workers would write to their logs that they are connecting to foo.berkeley.edu but failed to. They never show up in the master UI. If use the foo.Berkeley.EDU address, everything works as it should.\n\nComments (2):\n1. Andrew Ash: // link moved to http://doc.akka.io/docs/akka/current/additional/faq.html#what-is-the-name-of-a-remote-actor I believe having the hostname change case will still break Spark. But after a search of the dev and user mailing lists over the past year I haven't seen any other users with this issue. A potential fix could be to call .toLower on the hostname in the Akka string across the cluster, but it's a little dirty to make this assumption everywhere. Technically [hostnames ARE case insensitive|http://serverfault.com/questions/261341/is-the-hostname-case-sensitive] so Spark's behavior is wrong, but the issue is in the underlying Akka library. This is the same underlying behavior where Akka requires that hostnames exactly match as well -- you can't use an IP address to refer to a Akka listening on a hostname -- SPARK-625. Until Akka handles differently-cased hostnames I think can only be done with an ugly workaround. Possibly relevant Akka issues: - https://github.com/akka/akka/issues/15990 - https://github.com/akka/akka/issues/15007 My preference would be to close this as \"Won't Fix\" until it's raised again as a problem from the community. cc [~rxin]\n2. Reynold Xin: Sounds good. In the future we might roll our own RPC rather than using Actor for RPC. I think the current RPC library built for the shuffle service is already ok with case insensitive hostnames.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.015598"}}
{"id": "4e981b5144978fb9ca48db2cc3a1bc25", "issue_key": "SPARK-633", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support dropping blocks and RDDs from block manager", "description": "", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2012-12-10T13:56:45.000+0000", "updated": "2013-01-20T12:52:22.000+0000", "resolved": "2013-01-20T12:52:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "First pull request for this ticket https://github.com/mesos/spark/pull/327", "created": "2012-12-13T00:25:16.233+0000"}, {"author": "Josh Rosen", "body": "Resolving since the pull request was merged.", "created": "2013-01-20T12:52:22.926+0000"}], "num_comments": 2, "text": "Issue: SPARK-633\nSummary: Support dropping blocks and RDDs from block manager\n\nComments (2):\n1. Reynold Xin: First pull request for this ticket https://github.com/mesos/spark/pull/327\n2. Josh Rosen: Resolving since the pull request was merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.015598"}}
{"id": "a9c613e4f73ad1af3ee516884e578334", "issue_key": "SPARK-634", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Track and display a read count for each block replica in BlockManager", "description": "", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2012-12-11T11:38:55.000+0000", "updated": "2020-05-17T18:20:56.000+0000", "resolved": "2016-01-18T10:22:46.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Reynold Xin", "body": "This should be part of the exposed metrics.", "created": "2013-06-13T22:57:36.718+0000"}, {"author": "Patrick McFadin", "body": "Hey [~rxin] - what do you mean by number of uses? Did you mean the number of machines it's on? That seems redundant with the list of machines which we now display.", "created": "2013-07-12T23:20:58.232+0000"}, {"author": "Reynold Xin", "body": "Number of times a block is used.", "created": "2013-07-13T00:01:06.723+0000"}, {"author": "Sean R. Owen", "body": "Closing just because of extreme age", "created": "2016-01-18T10:22:46.189+0000"}], "num_comments": 4, "text": "Issue: SPARK-634\nSummary: Track and display a read count for each block replica in BlockManager\n\nComments (4):\n1. Reynold Xin: This should be part of the exposed metrics.\n2. Patrick McFadin: Hey [~rxin] - what do you mean by number of uses? Did you mean the number of machines it's on? That seems redundant with the list of machines which we now display.\n3. Reynold Xin: Number of times a block is used.\n4. Sean R. Owen: Closing just because of extreme age", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.015598"}}
{"id": "4c9193a8ece7f169050d84f9ab52e70b", "issue_key": "SPARK-635", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Pass a TaskContext object to compute() interface", "description": "TaskContext should allow compute() to register a callback, executed when the task finishes executing. The use case is for limit on s3. The HadoopRDD interface should register a call back to close the stream when the task ends.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2012-12-13T00:13:28.000+0000", "updated": "2012-12-30T14:54:56.000+0000", "resolved": "2012-12-30T14:54:56.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "Actually, TaskContext exists already on the master side. Matei - any good idea for naming? This will be on the slave side.", "created": "2012-12-13T00:25:46.755+0000"}, {"author": "Reynold Xin", "body": "Ignore my previous comment - I will have a pull request soon.", "created": "2012-12-13T14:46:39.178+0000"}, {"author": "Reynold Xin", "body": "pull request: https://github.com/mesos/spark/pull/328", "created": "2012-12-13T16:15:32.398+0000"}], "num_comments": 3, "text": "Issue: SPARK-635\nSummary: Pass a TaskContext object to compute() interface\nDescription: TaskContext should allow compute() to register a callback, executed when the task finishes executing. The use case is for limit on s3. The HadoopRDD interface should register a call back to close the stream when the task ends.\n\nComments (3):\n1. Reynold Xin: Actually, TaskContext exists already on the master side. Matei - any good idea for naming? This will be on the slave side.\n2. Reynold Xin: Ignore my previous comment - I will have a pull request soon.\n3. Reynold Xin: pull request: https://github.com/mesos/spark/pull/328", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.016563"}}
{"id": "4d4f8715c34eaf0f9ba3ef47514c67a8", "issue_key": "SPARK-636", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add mechanism to run system management/configuration tasks on all workers", "description": "It would be useful to have a mechanism to run a task on all workers in order to perform system management tasks, such as purging caches or changing system properties. This is useful for automated experiments and benchmarking; I don't envision this being used for heavy computation. Right now, I can mimic this with something like  sc.parallelize(0 until numMachines, numMachines).foreach { }  but this does not guarantee that every worker runs a task and requires my user code to know the number of workers. One sample use case is setup and teardown for benchmark tests. For example, I might want to drop cached RDDs, purge shuffle data, and call {{System.gc()}} between test runs. It makes sense to incorporate some of this functionality, such as dropping cached RDDs, into Spark itself, but it might be helpful to have a general mechanism for running ad-hoc tasks like {{System.gc()}}.", "reporter": "Josh Rosen", "assignee": null, "created": "2012-12-13T09:45:59.000+0000", "updated": "2019-05-21T05:37:14.000+0000", "resolved": "2019-05-21T05:37:14.000+0000", "labels": ["bulk-closed"], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "For the streaming branch we had to add a stricter version of sc.makeRDD (parallelize equivalent) that takes placement constraints for each partition. This could be used for a poor-mans version of what you are looking for, or we could implement in a way that pushes down to the scheduler and does not require explicit knowledge of the nodes in the cluster.", "created": "2012-12-17T22:14:43.913+0000"}, {"author": "Holden Karau", "body": "Does broadcasting get us close enough to handling this or is this something we are still considering for the API?", "created": "2016-10-07T21:39:05.768+0000"}, {"author": "Luis Ramos", "body": "I feel like the broadcasting mechanism doesn't get me \"close\" enough to solve my issue (initialization of a logging system). That's partly because my initialization would be deferred (meaning a loss of useful logs), and also it could enable us to have some 'init' code that is guaranteed to only be evaluated once as opposed to implementing that 'guarantee' yourself, which can currently lead to bad practices. Edit: For some context, I'm approaching this issue from SPARK-650", "created": "2016-10-14T11:06:30.699+0000"}, {"author": "Michael Schmeißer", "body": "I agree, that's why I also feel that these issues are no duplicates.", "created": "2016-10-15T15:18:02.839+0000"}, {"author": "Holden Karau", "body": "If you have a logging system you want to initialize wouldn't using an object with lazy initialization on call be sufficient?", "created": "2016-11-25T14:20:49.361+0000"}], "num_comments": 5, "text": "Issue: SPARK-636\nSummary: Add mechanism to run system management/configuration tasks on all workers\nDescription: It would be useful to have a mechanism to run a task on all workers in order to perform system management tasks, such as purging caches or changing system properties. This is useful for automated experiments and benchmarking; I don't envision this being used for heavy computation. Right now, I can mimic this with something like  sc.parallelize(0 until numMachines, numMachines).foreach { }  but this does not guarantee that every worker runs a task and requires my user code to know the number of workers. One sample use case is setup and teardown for benchmark tests. For example, I might want to drop cached RDDs, purge shuffle data, and call {{System.gc()}} between test runs. It makes sense to incorporate some of this functionality, such as dropping cached RDDs, into Spark itself, but it might be helpful to have a general mechanism for running ad-hoc tasks like {{System.gc()}}.\n\nComments (5):\n1. Patrick McFadin: For the streaming branch we had to add a stricter version of sc.makeRDD (parallelize equivalent) that takes placement constraints for each partition. This could be used for a poor-mans version of what you are looking for, or we could implement in a way that pushes down to the scheduler and does not require explicit knowledge of the nodes in the cluster.\n2. Holden Karau: Does broadcasting get us close enough to handling this or is this something we are still considering for the API?\n3. Luis Ramos: I feel like the broadcasting mechanism doesn't get me \"close\" enough to solve my issue (initialization of a logging system). That's partly because my initialization would be deferred (meaning a loss of useful logs), and also it could enable us to have some 'init' code that is guaranteed to only be evaluated once as opposed to implementing that 'guarantee' yourself, which can currently lead to bad practices. Edit: For some context, I'm approaching this issue from SPARK-650\n4. Michael Schmeißer: I agree, that's why I also feel that these issues are no duplicates.\n5. Holden Karau: If you have a logging system you want to initialize wouldn't using an object with lazy initialization on call be sufficient?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.016563"}}
{"id": "2956fe0676e14a4d3a8a22be86de3e61", "issue_key": "SPARK-637", "issue_type": "New Feature", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Create troubleshooting checklist", "description": "We should provide a checklist for troubleshooting common Spark problems. For example, it could include steps like \"check that the Spark code was copied to all nodes\" and \"check that the workers successfully connect to the master.\"", "reporter": "Josh Rosen", "assignee": null, "created": "2012-12-13T11:55:59.000+0000", "updated": "2014-09-21T14:34:21.000+0000", "resolved": "2014-09-21T14:34:21.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Sean R. Owen", "body": "Last old-issue update for the day: this can be subsumed by https://issues.apache.org/jira/browse/SPARK-719 ?", "created": "2014-09-11T09:03:34.001+0000"}, {"author": "Matthew Farrellee", "body": "this is a good idea, and it will take a significant amount of effort. it looks like nothing has happened for almost 2 years. i'm going to close this, but feel free to re-open and push forward with it.", "created": "2014-09-21T14:34:11.491+0000"}], "num_comments": 2, "text": "Issue: SPARK-637\nSummary: Create troubleshooting checklist\nDescription: We should provide a checklist for troubleshooting common Spark problems. For example, it could include steps like \"check that the Spark code was copied to all nodes\" and \"check that the workers successfully connect to the master.\"\n\nComments (2):\n1. Sean R. Owen: Last old-issue update for the day: this can be subsumed by https://issues.apache.org/jira/browse/SPARK-719 ?\n2. Matthew Farrellee: this is a good idea, and it will take a significant amount of effort. it looks like nothing has happened for almost 2 years. i'm going to close this, but feel free to re-open and push forward with it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.016563"}}
{"id": "9e020d03d2418030456bd51aaf43c260", "issue_key": "SPARK-638", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting", "description": "spark-ec2 has a --cluster-type option to launch standalone clusters, but this is broken because SPARK_MASTER_IP is set in the start-slaves.sh script but not in start-master.sh. This causes the workers to connect to the master on the wrong address, which fails.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-12-13T17:04:22.000+0000", "updated": "2012-12-13T18:08:16.000+0000", "resolved": "2012-12-13T18:08:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Pull request: https://github.com/mesos/spark/pull/330", "created": "2012-12-13T17:32:36.894+0000"}], "num_comments": 1, "text": "Issue: SPARK-638\nSummary: Standalone --cluster-type option broken in spark-ec2 due to SPARK_MASTER_IP setting\nDescription: spark-ec2 has a --cluster-type option to launch standalone clusters, but this is broken because SPARK_MASTER_IP is set in the start-slaves.sh script but not in start-master.sh. This causes the workers to connect to the master on the wrong address, which fails.\n\nComments (1):\n1. Josh Rosen: Pull request: https://github.com/mesos/spark/pull/330", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.016563"}}
{"id": "3de17e759f4d729285e9708cb6c1afc3", "issue_key": "SPARK-639", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Standalone cluster should report executor exit codes more nicely to clients", "description": "Right now they are just part of a string message, requiring string parsing to make sense of them.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2012-12-13T20:15:44.000+0000", "updated": "2014-11-11T08:58:00.000+0000", "resolved": "2014-11-11T08:58:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Andrew Ash", "body": "It looks reporting executor exit statuses to the user was done in commit fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 by [~woggle] on 12 Dec 2012 (one day before this ticket was created). When Spark submit was added I think it may have dropped these exit statuses, but they were re-added in SPARK-3759. I think this can be closed with a fix version of 0.7.0  aash@aash-mbp ~/git/spark$ git log v0.6.2 | grep fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 aash@aash-mbp ~/git/spark$ git log v0.7.0 | grep fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 commit fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 aash@aash-mbp ~/git/spark$ git log v0.6.2 | grep 'Normalize executor exit statuses and report them to the user.' aash@aash-mbp ~/git/spark$ git log v0.7.0 | grep 'Normalize executor exit statuses and report them to the user.' Normalize executor exit statuses and report them to the user. aash@aash-mbp ~/git/spark$ git log | grep 'Normalize executor exit statuses and report them to the user.' Normalize executor exit statuses and report them to the user. aash@aash-mbp ~/git/spark$  cc [~rxin]", "created": "2014-11-11T08:56:09.397+0000"}, {"author": "Reynold Xin", "body": "Fixed according to [~aash] :)", "created": "2014-11-11T08:58:00.131+0000"}], "num_comments": 2, "text": "Issue: SPARK-639\nSummary: Standalone cluster should report executor exit codes more nicely to clients\nDescription: Right now they are just part of a string message, requiring string parsing to make sense of them.\n\nComments (2):\n1. Andrew Ash: It looks reporting executor exit statuses to the user was done in commit fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 by [~woggle] on 12 Dec 2012 (one day before this ticket was created). When Spark submit was added I think it may have dropped these exit statuses, but they were re-added in SPARK-3759. I think this can be closed with a fix version of 0.7.0  aash@aash-mbp ~/git/spark$ git log v0.6.2 | grep fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 aash@aash-mbp ~/git/spark$ git log v0.7.0 | grep fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 commit fa9df4a45daf5fd8b19df20c1fb7466bde3b2054 aash@aash-mbp ~/git/spark$ git log v0.6.2 | grep 'Normalize executor exit statuses and report them to the user.' aash@aash-mbp ~/git/spark$ git log v0.7.0 | grep 'Normalize executor exit statuses and report them to the user.' Normalize executor exit statuses and report them to the user. aash@aash-mbp ~/git/spark$ git log | grep 'Normalize executor exit statuses and report them to the user.' Normalize executor exit statuses and report them to the user. aash@aash-mbp ~/git/spark$  cc [~rxin]\n2. Reynold Xin: Fixed according to [~aash] :)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.016563"}}
{"id": "05e8f8bda5d2873d3c16aad79a0008fb", "issue_key": "SPARK-640", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Update Hadoop 1 version to 1.1.0 (especially on AMIs)", "description": "Hadoop 1.1.0 has a fix to the notorious \"trailing slash for directory objects in S3\" issue: https://issues.apache.org/jira/browse/HADOOP-5836, so would be good to support on the AMIs.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2012-12-13T21:03:15.000+0000", "updated": "2014-09-04T19:54:50.000+0000", "resolved": "2014-09-04T08:53:10.000+0000", "labels": [], "components": [], "comments": [{"author": "Sean R. Owen", "body": "This looks stale right? Hadoop 1 version has been at 1.2.1 for some time.", "created": "2014-09-04T08:53:10.148+0000"}, {"author": "Matei Alexandru Zaharia", "body": "[~pwendell] what is our Hadoop 1 version on AMIs now?", "created": "2014-09-04T19:54:50.310+0000"}], "num_comments": 2, "text": "Issue: SPARK-640\nSummary: Update Hadoop 1 version to 1.1.0 (especially on AMIs)\nDescription: Hadoop 1.1.0 has a fix to the notorious \"trailing slash for directory objects in S3\" issue: https://issues.apache.org/jira/browse/HADOOP-5836, so would be good to support on the AMIs.\n\nComments (2):\n1. Sean R. Owen: This looks stale right? Hadoop 1 version has been at 1.2.1 for some time.\n2. Matei Alexandru Zaharia: [~pwendell] what is our Hadoop 1 version on AMIs now?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.017647"}}
{"id": "a897f595e622c549b5012567f00b2d1b", "issue_key": "SPARK-641", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "spark-ec2 standalone launch should create ~/mesos-ec2/slaves", "description": "When launching a standalone cluster using the --cluster-type option, the ~/mesos-ec2/slaves file is not populated (this is normally done by the ~/mesos-ec2/setup script). We should fix this, since users may still wish to use scripts like copy-dir to copy files.", "reporter": "Josh Rosen", "assignee": "Shivaram Venkataraman", "created": "2012-12-14T10:05:13.000+0000", "updated": "2013-04-05T19:45:07.000+0000", "resolved": "2013-04-05T19:45:07.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I actually would prefer not to support a standalone mode EC2 cluster, or else to switch the default cluster to be the standalone one. The reason is that it's annoying to maintain two types of AMIs and two ways of running on EC2, and it confuses users (they will wonder why go for one type over the other). So maybe we should just switch the AMIs to use the standalone mode at some point.", "created": "2012-12-14T18:26:41.851+0000"}, {"author": "Josh Rosen", "body": "It might be useful to support this feature for our own testing purposes. It's possible to support both cluster modes using a single AMI by running a different EC2-side configuration script from spark-ec2 depending on the cluster mode. After launching the cluster, spark-ec2 could just run something similar to ~/mesos-ec2/setup, but for standalone mode. The two scripts could share most of their code. This would block on SPARK-521.", "created": "2012-12-14T18:51:39.336+0000"}, {"author": "Josh Rosen", "body": "The EC2 scripts in 0.7 properly support both standalone and Mesos clusters, so I'm marking this as fixed.", "created": "2013-04-05T19:45:07.407+0000"}], "num_comments": 3, "text": "Issue: SPARK-641\nSummary: spark-ec2 standalone launch should create ~/mesos-ec2/slaves\nDescription: When launching a standalone cluster using the --cluster-type option, the ~/mesos-ec2/slaves file is not populated (this is normally done by the ~/mesos-ec2/setup script). We should fix this, since users may still wish to use scripts like copy-dir to copy files.\n\nComments (3):\n1. Matei Alexandru Zaharia: I actually would prefer not to support a standalone mode EC2 cluster, or else to switch the default cluster to be the standalone one. The reason is that it's annoying to maintain two types of AMIs and two ways of running on EC2, and it confuses users (they will wonder why go for one type over the other). So maybe we should just switch the AMIs to use the standalone mode at some point.\n2. Josh Rosen: It might be useful to support this feature for our own testing purposes. It's possible to support both cluster modes using a single AMI by running a different EC2-side configuration script from spark-ec2 depending on the cluster mode. After launching the cluster, spark-ec2 could just run something similar to ~/mesos-ec2/setup, but for standalone mode. The two scripts could share most of their code. This would block on SPARK-521.\n3. Josh Rosen: The EC2 scripts in 0.7 properly support both standalone and Mesos clusters, so I'm marking this as fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.017647"}}
{"id": "5b86ba59f8ea4deb0b6c48e0a35d222e", "issue_key": "SPARK-642", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS", "description": "spark-ec2's standalone cluster launch script does not configure SPARK_MEM and SPARK_JAVA_OPTS. This is done automatically when running under Mesos mode by using the scripts included in the AMI. We should either have feature parity between the --cluster-type modes or remove the feature if it's too difficult to support both modes.", "reporter": "Josh Rosen", "assignee": "Shivaram Venkataraman", "created": "2012-12-14T10:47:56.000+0000", "updated": "2013-04-05T19:48:04.000+0000", "resolved": "2013-04-05T19:48:04.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Josh Rosen", "body": "This was fixed in 0.7, which fixed standalone mode support in spark-ec2.", "created": "2013-04-05T19:48:04.850+0000"}], "num_comments": 1, "text": "Issue: SPARK-642\nSummary: spark-ec2 standalone launch should set SPARK_MEM and SPARK_JAVA_OPTS\nDescription: spark-ec2's standalone cluster launch script does not configure SPARK_MEM and SPARK_JAVA_OPTS. This is done automatically when running under Mesos mode by using the scripts included in the AMI. We should either have feature parity between the --cluster-type modes or remove the feature if it's too difficult to support both modes.\n\nComments (1):\n1. Josh Rosen: This was fixed in 0.7, which fixed standalone mode support in spark-ec2.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.017647"}}
{"id": "8ac00f9428d3b87fced3042e2d3e462b", "issue_key": "SPARK-643", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Standalone master crashes during actor restart", "description": "The standalone master will crash if it restarts due to an exception:  12/12/15 03:10:47 ERROR master.Master: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times. spark.SparkException: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times. at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:103) at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:62) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.deploy.master.Master.apply(Master.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 12/12/15 03:10:47 INFO master.Master: Starting Spark master at spark://ip-10-226-87-193:7077 12/12/15 03:10:47 INFO io.IoWorker: IoWorker thread 'spray-io-worker-1' started 12/12/15 03:10:47 ERROR master.Master: Failed to create web UI akka.actor.InvalidActorNameException:actor name HttpServer is not unique! [05aed000-4665-11e2-b361-12313d316833] at akka.actor.ActorCell.actorOf(ActorCell.scala:392) at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.liftedTree1$1(ActorRefProvider.scala:394) at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:394) at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:392) at akka.actor.Actor$class.apply(Actor.scala:318) at akka.actor.LocalActorRefProvider$Guardian.apply(ActorRefProvider.scala:388) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)  When the Master actor restarts, Akka calls the {{postRestart}} hook. [By default|http://doc.akka.io/docs/akka/snapshot/general/supervision.html#supervision-restart], this calls {{preStart}}. The standalone master's {{preStart}} method tries to start the webUI but crashes because it is already running. I ran into this after a job failed more than 11 times, which causes the Master to throw a SparkException from its {{receive}} method. The solution is to implement a custom {{postRestart}} hook.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-12-14T19:19:26.000+0000", "updated": "2014-11-06T17:33:50.000+0000", "resolved": "2014-11-06T17:33:50.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "I'd like to add unit tests for executor failures, but I can't seem to find a way to reproduce them in tests. Writing a UDF that throws an exception just results in task failures; I want to completely kill the executor processes. I tried adding a call to System.exit() in a UDF, hoping that it would only be executed in the Executor's JVM, but this caused test runner to exit. I was using local-cluster mode, which looks like it spawns separate JVMs for Executors. Any ideas?", "created": "2012-12-19T11:59:21.072+0000"}, {"author": "Matei Alexandru Zaharia", "body": "You could add extra testing in the code to allow Executors to crash. Or, you can use the \"local-cluster\" mode, where System.exit will indeed crash an executor without killing the whole test runner.", "created": "2012-12-20T09:12:24.410+0000"}, {"author": "Matei Alexandru Zaharia", "body": "You should add extra testing in the code to allow Executors to crash. Or, you can use the \"local-cluster\" mode, where System.exit will indeed crash an executor without killing the whole test runner. Matei", "created": "2012-12-20T09:13:36.589+0000"}, {"author": "Josh Rosen", "body": "Restarted Akka actors have fresh states (see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion and http://doc.akka.io/docs/akka/snapshot/general/supervision.html#What_Restarting_Means), so allowing actors to restart might lead to unexpected behavior. I propose that we add {{postRestart}} methods so that actor restarts always lead to crashes:  override def postRestart(reason: Throwable) { logError(\"Spark worker actor failed: \" + reason) // Allowing the actor to restart will cause problems, because the new actor will have a fresh // state; see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion // We could try copying the state, but it's probably safer to just exit: logError(\"Exiting because we do not restart failed worker actors\") System.exit(1) }  This should go in at least the standalone Worker and Master actors (where I've observed problems related to actor restarts), but we might want to add it elsewhere.", "created": "2012-12-20T23:29:09.690+0000"}], "num_comments": 4, "text": "Issue: SPARK-643\nSummary: Standalone master crashes during actor restart\nDescription: The standalone master will crash if it restarts due to an exception:  12/12/15 03:10:47 ERROR master.Master: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times. spark.SparkException: Job SkewBenchmark wth ID job-20121215031047-0000 failed 11 times. at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:103) at spark.deploy.master.Master$$anonfun$receive$1.apply(Master.scala:62) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.deploy.master.Master.apply(Master.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 12/12/15 03:10:47 INFO master.Master: Starting Spark master at spark://ip-10-226-87-193:7077 12/12/15 03:10:47 INFO io.IoWorker: IoWorker thread 'spray-io-worker-1' started 12/12/15 03:10:47 ERROR master.Master: Failed to create web UI akka.actor.InvalidActorNameException:actor name HttpServer is not unique! [05aed000-4665-11e2-b361-12313d316833] at akka.actor.ActorCell.actorOf(ActorCell.scala:392) at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.liftedTree1$1(ActorRefProvider.scala:394) at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:394) at akka.actor.LocalActorRefProvider$Guardian$$anonfun$receive$1.apply(ActorRefProvider.scala:392) at akka.actor.Actor$class.apply(Actor.scala:318) at akka.actor.LocalActorRefProvider$Guardian.apply(ActorRefProvider.scala:388) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)  When the Master actor restarts, Akka calls the {{postRestart}} hook. [By default|http://doc.akka.io/docs/akka/snapshot/general/supervision.html#supervision-restart], this calls {{preStart}}. The standalone master's {{preStart}} method tries to start the webUI but crashes because it is already running. I ran into this after a job failed more than 11 times, which causes the Master to throw a SparkException from its {{receive}} method. The solution is to implement a custom {{postRestart}} hook.\n\nComments (4):\n1. Josh Rosen: I'd like to add unit tests for executor failures, but I can't seem to find a way to reproduce them in tests. Writing a UDF that throws an exception just results in task failures; I want to completely kill the executor processes. I tried adding a call to System.exit() in a UDF, hoping that it would only be executed in the Executor's JVM, but this caused test runner to exit. I was using local-cluster mode, which looks like it spawns separate JVMs for Executors. Any ideas?\n2. Matei Alexandru Zaharia: You could add extra testing in the code to allow Executors to crash. Or, you can use the \"local-cluster\" mode, where System.exit will indeed crash an executor without killing the whole test runner.\n3. Matei Alexandru Zaharia: You should add extra testing in the code to allow Executors to crash. Or, you can use the \"local-cluster\" mode, where System.exit will indeed crash an executor without killing the whole test runner. Matei\n4. Josh Rosen: Restarted Akka actors have fresh states (see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion and http://doc.akka.io/docs/akka/snapshot/general/supervision.html#What_Restarting_Means), so allowing actors to restart might lead to unexpected behavior. I propose that we add {{postRestart}} methods so that actor restarts always lead to crashes:  override def postRestart(reason: Throwable) { logError(\"Spark worker actor failed: \" + reason) // Allowing the actor to restart will cause problems, because the new actor will have a fresh // state; see https://groups.google.com/d/topic/akka-user/HN5zEsMd_PA/discussion // We could try copying the state, but it's probably safer to just exit: logError(\"Exiting because we do not restart failed worker actors\") System.exit(1) }  This should go in at least the standalone Worker and Master actors (where I've observed problems related to actor restarts), but we might want to add it elsewhere.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.017647"}}
{"id": "412178b12374575b0c0720ed91b3842b", "issue_key": "SPARK-644", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Jobs canceled due to repeated executor failures may hang", "description": "In order to prevent an infinite loop, the standalone master aborts jobs that experience more than 10 executor failures (see https://github.com/mesos/spark/pull/210). Currently, the master crashes when aborting jobs (this is the issue that uncovered SPARK-643). If we fix the crash, which involves removing a {{throw}} from the actor's {{receive}} method, then these failures can lead to a hang because they cause the job to be removed from the master's scheduler, but the upstream scheduler components aren't notified of the failure and will wait for the job to finish. I've considered fixing this by adding additional callbacks to propagate the failure to the higher-level schedulers. It might be cleaner to move the decision to abort the job into the higher-level layers of the scheduler, sending an {{AbortJob(jobId)}} method to the Master. The Client is already notified of executor state changes, so it may be able to make the decision to abort (or defer that decision to a higher layer).", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2012-12-20T23:55:35.000+0000", "updated": "2014-11-06T17:33:56.000+0000", "resolved": "2014-11-06T17:33:56.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-644\nSummary: Jobs canceled due to repeated executor failures may hang\nDescription: In order to prevent an infinite loop, the standalone master aborts jobs that experience more than 10 executor failures (see https://github.com/mesos/spark/pull/210). Currently, the master crashes when aborting jobs (this is the issue that uncovered SPARK-643). If we fix the crash, which involves removing a {{throw}} from the actor's {{receive}} method, then these failures can lead to a hang because they cause the job to be removed from the master's scheduler, but the upstream scheduler components aren't notified of the failure and will wait for the job to finish. I've considered fixing this by adding additional callbacks to propagate the failure to the higher-level schedulers. It might be cleaner to move the decision to abort the job into the higher-level layers of the scheduler, sending an {{AbortJob(jobId)}} method to the Master. The Client is already notified of executor state changes, so it may be able to make the decision to abort (or defer that decision to a higher layer).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.018562"}}
{"id": "e56cc40f894cddaabacb3a9d02b8ef62", "issue_key": "SPARK-645", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Calling distinct() without parentheses fails", "description": "While distinct now supports a number of splits parameter and has a default value for that parameter, it only supports default calls of a form similar to rdd.distinct().persist and not rdd.distinct.persist. The without-parentheses form should be allowed -- as it is for persist().", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "created": "2012-12-24T01:47:31.000+0000", "updated": "2012-12-24T08:05:46.000+0000", "resolved": "2012-12-24T08:05:46.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Mark Hamstra", "body": "Pull request submitted.", "created": "2012-12-24T02:44:11.338+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Committed it. Thanks Mark!", "created": "2012-12-24T08:05:46.583+0000"}], "num_comments": 2, "text": "Issue: SPARK-645\nSummary: Calling distinct() without parentheses fails\nDescription: While distinct now supports a number of splits parameter and has a default value for that parameter, it only supports default calls of a form similar to rdd.distinct().persist and not rdd.distinct.persist. The without-parentheses form should be allowed -- as it is for persist().\n\nComments (2):\n1. Mark Hamstra: Pull request submitted.\n2. Matei Alexandru Zaharia: Committed it. Thanks Mark!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.018562"}}
{"id": "63dfa4fd8588741799baba5feedd678a", "issue_key": "SPARK-646", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Floating point overflow/underflow in LR examples", "description": "The SparkLR examples call scala.math.exp() with very large or small exponents, causing its result to be rounded to 0 or Infinity. Is this a bug? I discovered this while porting the LR example to Python, because Python's math.exp() function rounds very small results to 0 but raises OverflowError for large results. In Scala:  scala> import math.exp import math.exp scala> math.exp(10000) res4: Double = Infinity scala> math.exp(-10000) res5: Double = 0.0  Python:  from math import exp exp(10000) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> OverflowError: math range error exp(-10000) 0.0  I added a call to println(\"\" + (-p.y * (w dot p.x))) in the map UDF in SparkLR and SparkHdfsLR to log the exponents, and in both cases I saw small exponents like  -4.967736504527945 -1.0153344192159428 0.4639647012587064  in the first round and huge exponents like  -3731.0565020800145 469.3852842964799 -2838.8348220771445  in all later rounds. The examples calculate the gradients using  (1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y * p.x.  The exponent (w dot p.x) grows rapidly because the magnitudes of w's components grow rapidly. I'm not familiar enough with logistic regression to know whether this is common or how to fix this. This could be a problem because a model whose weights have large magnitudes would always make predictions with extremely high confidence (e.g. p(y = 1 | x) is always 0 or 1, due to rounding).", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2012-12-29T17:59:52.000+0000", "updated": "2013-08-06T23:24:46.000+0000", "resolved": "2013-08-06T23:24:46.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Josh Rosen", "body": "This is still a problem, particularly in PySpark: https://groups.google.com/d/msg/spark-users/ngHhosQN2MY/WZmjHzCtfU0J", "created": "2013-04-28T16:26:45.718+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This is no longer a problem in the new numpy-based LR.", "created": "2013-08-06T23:24:46.319+0000"}], "num_comments": 2, "text": "Issue: SPARK-646\nSummary: Floating point overflow/underflow in LR examples\nDescription: The SparkLR examples call scala.math.exp() with very large or small exponents, causing its result to be rounded to 0 or Infinity. Is this a bug? I discovered this while porting the LR example to Python, because Python's math.exp() function rounds very small results to 0 but raises OverflowError for large results. In Scala:  scala> import math.exp import math.exp scala> math.exp(10000) res4: Double = Infinity scala> math.exp(-10000) res5: Double = 0.0  Python:  from math import exp exp(10000) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> OverflowError: math range error exp(-10000) 0.0  I added a call to println(\"\" + (-p.y * (w dot p.x))) in the map UDF in SparkLR and SparkHdfsLR to log the exponents, and in both cases I saw small exponents like  -4.967736504527945 -1.0153344192159428 0.4639647012587064  in the first round and huge exponents like  -3731.0565020800145 469.3852842964799 -2838.8348220771445  in all later rounds. The examples calculate the gradients using  (1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y * p.x.  The exponent (w dot p.x) grows rapidly because the magnitudes of w's components grow rapidly. I'm not familiar enough with logistic regression to know whether this is common or how to fix this. This could be a problem because a model whose weights have large magnitudes would always make predictions with extremely high confidence (e.g. p(y = 1 | x) is always 0 or 1, due to rounding).\n\nComments (2):\n1. Josh Rosen: This is still a problem, particularly in PySpark: https://groups.google.com/d/msg/spark-users/ngHhosQN2MY/WZmjHzCtfU0J\n2. Matei Alexandru Zaharia: This is no longer a problem in the new numpy-based LR.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.018562"}}
{"id": "59e4024ba674be685883db9fb87f97f6", "issue_key": "SPARK-647", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ConnectionManager.sendMessage may create too many unnecessary connections", "description": "In ConnectionManager, sendMessage creates a new SendingConnection when connection host key is not found in connectionsById. But there might be too many unnecessary connections created before connectionsById is updated in the connection-manager-thread.run. In out test ConnectionMangerTest fails on \"connection reset by peer\" or \"timeout\" when there're too many message sending threads. We should make connectionRequests a map to track the connections by key so that connections can be reused.", "reporter": "Shane Huang", "assignee": "Shane Huang", "created": "2013-01-07T22:59:19.000+0000", "updated": "2013-01-10T17:54:04.000+0000", "resolved": "2013-01-10T17:54:04.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Shane Huang", "body": "Submitted a pull request for this issue @ https://github.com/mesos/spark/pull/356", "created": "2013-01-08T00:01:47.160+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Shane fixed this in https://github.com/mesos/spark/pull/356. Will also merge it to 0.6 branch.", "created": "2013-01-10T17:54:04.542+0000"}], "num_comments": 2, "text": "Issue: SPARK-647\nSummary: ConnectionManager.sendMessage may create too many unnecessary connections\nDescription: In ConnectionManager, sendMessage creates a new SendingConnection when connection host key is not found in connectionsById. But there might be too many unnecessary connections created before connectionsById is updated in the connection-manager-thread.run. In out test ConnectionMangerTest fails on \"connection reset by peer\" or \"timeout\" when there're too many message sending threads. We should make connectionRequests a map to track the connections by key so that connections can be reused.\n\nComments (2):\n1. Shane Huang: Submitted a pull request for this issue @ https://github.com/mesos/spark/pull/356\n2. Matei Alexandru Zaharia: Shane fixed this in https://github.com/mesos/spark/pull/356. Will also merge it to 0.6 branch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.019566"}}
{"id": "7a197f41579257082bb9bd08d4e6393c", "issue_key": "SPARK-648", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "takeSample() with repetitions should be able to return more items than an RDD contains", "description": "If I use takeSample() _with repetition_ and attempt to take more items than the RDD contains, I may receive fewer than {{num}} items:  scala> sc.parallelize(0 to 1).takeSample(true, 10, 42) res17: Array[Int] = Array(1, 0) scala> sc.parallelize(0 to 4).takeSample(true, 10, 42) res33: Array[Int] = Array(3, 2, 0, 3, 0)  If we fix this, we should add more tests for sample() and takeSample(), since right now they're only called in one test in the JavaAPISuite.", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-01-08T15:27:55.000+0000", "updated": "2013-08-07T10:45:42.000+0000", "resolved": "2013-08-06T23:17:10.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "When / where was this fixed? Is there a unit test for the fix?", "created": "2013-08-07T10:45:42.887+0000"}], "num_comments": 1, "text": "Issue: SPARK-648\nSummary: takeSample() with repetitions should be able to return more items than an RDD contains\nDescription: If I use takeSample() _with repetition_ and attempt to take more items than the RDD contains, I may receive fewer than {{num}} items:  scala> sc.parallelize(0 to 1).takeSample(true, 10, 42) res17: Array[Int] = Array(1, 0) scala> sc.parallelize(0 to 4).takeSample(true, 10, 42) res33: Array[Int] = Array(3, 2, 0, 3, 0)  If we fix this, we should add more tests for sample() and takeSample(), since right now they're only called in one test in the JavaAPISuite.\n\nComments (1):\n1. Josh Rosen: When / where was this fixed? Is there a unit test for the fix?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.019566"}}
{"id": "3edf127d1f980b9c97e56465ee9bcf37", "issue_key": "SPARK-649", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Windows support for PySpark", "description": "Provide Windows support for PySpark: - Add a {{pyspark.cmd}} script. - Maybe use sockets instead of pipes for communication between Java and the Python worker processes (does Spark's regular {{RDD.pipe()}} method work under Windows?).", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-01-10T22:45:49.000+0000", "updated": "2013-09-01T22:27:34.000+0000", "resolved": "2013-09-01T22:27:34.000+0000", "labels": ["Starter"], "components": ["PySpark", "Windows"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I'm pretty sure pipe() works on Windows, though I haven't tried it. There is still a notion of standard in and out on Windows.", "created": "2013-01-13T14:33:42.273+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Fixed here: https://github.com/mesos/spark/pull/885.", "created": "2013-09-01T22:27:34.339+0000"}], "num_comments": 2, "text": "Issue: SPARK-649\nSummary: Windows support for PySpark\nDescription: Provide Windows support for PySpark: - Add a {{pyspark.cmd}} script. - Maybe use sockets instead of pipes for communication between Java and the Python worker processes (does Spark's regular {{RDD.pipe()}} method work under Windows?).\n\nComments (2):\n1. Matei Alexandru Zaharia: I'm pretty sure pipe() works on Windows, though I haven't tried it. There is still a notion of standard in and out on Windows.\n2. Matei Alexandru Zaharia: Fixed here: https://github.com/mesos/spark/pull/885.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.019566"}}
{"id": "7095e9332598dac465cdfb7e3f2f398b", "issue_key": "SPARK-650", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add a \"setup hook\" API for running initialization code on each executor", "description": "Would be useful to configure things like reporting libraries", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-01-11T16:46:50.000+0000", "updated": "2021-05-25T01:42:39.000+0000", "resolved": "2021-05-25T01:42:39.000+0000", "labels": ["bulk-closed"], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "As mentioned in SPARK-572 static classes' initialization methods are being \"abused\" to perform this functionality. [~matei] do you still feel that a per-executor initialization function is a hook that Spark should expose in its public API?", "created": "2014-11-11T08:58:40.267+0000"}, {"author": "Lars Francke", "body": "Not [~matei] but I think this would be a good idea to have. Abusing another undocumented concept doesn't seem like a nice way to treat a useful and common use-case.", "created": "2015-07-29T11:53:37.249+0000"}, {"author": "Michael Schmeißer", "body": "I would need this feature as well to perform some initialization of the logging system (which reads its configuration from an external source rather than just a file).", "created": "2015-09-28T09:12:37.018+0000"}, {"author": "Luis Ramos", "body": "I have similar requirements to Michael's – this would be a very useful feature to have.", "created": "2016-10-07T11:32:49.326+0000"}, {"author": "Holden Karau", "body": "I think this is a duplicate of SPARK-636 yes?", "created": "2016-10-07T21:39:53.046+0000"}, {"author": "Michael Schmeißer", "body": "To me, the two seem related, but not exact duplicates. SPARK-636 seems to aim for a more generic mechanism.", "created": "2016-10-10T10:47:50.836+0000"}, {"author": "Holden Karau", "body": "Would people feel ok if we marked this as a duplicate of 636 since it does seem like this a subset of 636.", "created": "2016-10-13T22:12:13.499+0000"}, {"author": "Michael Schmeißer", "body": "I disagree that those issues are duplicates. Spark-636 looks for a generic way to execute code on the Executors, but not for a reliable and easy mechanism to execute code during Executor initialization.", "created": "2016-10-15T15:16:21.322+0000"}, {"author": "Sean R. Owen", "body": "In practice, these should probably all be WontFix as it hasn't mattered enough to implement in almost 4 years. It really doesn't matter.", "created": "2016-10-15T15:55:05.853+0000"}, {"author": "Michael Schmeißer", "body": "Then somebody should please explain to me, how this doesn't matter or rather how certain use-cases are supposed to be solved. We need to initialize each JVM and connect it to our logging system, set correlation IDs, initialize contexts and so on. I guess that most users just have implemented work-arounds as we did, but in an enterprise environment, this is really not the preferable long-term solution to me. Plus, I think that it would really not be hard to implement this feature for someone who has knowledge about the Spark executor setup.", "created": "2016-10-15T16:11:37.316+0000"}, {"author": "Sean R. Owen", "body": "Sorry, I mean the _status_ doesn't matter. Most issues this old are obsolete or de facto won't-fix. Resolving it or not doesn't matter. I would even say this is 'not a problem', because a simple singleton provides once-per-executor execution of whatever you like. It's more complex to make a custom mechanism that makes you route this via Spark. That's probably way this hasn't proved necessary.", "created": "2016-10-15T16:24:53.867+0000"}, {"author": "Olivier Armand", "body": "Sean, a singleton is not the best option in our case. The Spark Streaming executors are writing to HBase, we need to initialize the HBase connection. The singleton seems (or seemed when we tested it for our customer a few months after this issue was raised) to be created when the first RDD is processed by the executor, and not when the driver starts. This imposes very high processing time for the first events.", "created": "2016-10-15T17:45:51.979+0000"}, {"author": "Sean R. Owen", "body": "If you need init to happen ASAP when the driver starts, isn't any similar mechanism going to be about the same in this regard? This cost is paid just once, and I don't think in general startup is very low latency for any Spark app.", "created": "2016-10-15T18:24:00.300+0000"}, {"author": "Lars Francke", "body": "I also have to disagree with this being a duplicate or obsolete. [~oarmand] and [~Skamandros] already mentioned reasons regarding the duplication. About it being obsolete: I have seen multiple clients facing this problem, finding this issue and hoping it'd get fixed some day. I would hesitate a guess and say that most _users_ of Spark have no JIRA account here and do not register or log in just to vote for this issue. That said: This issue is (with six votes) in the top 150 out of almost 17k total issues in the Spark project. As it happens this is a non-trivial thing to implement in Spark (as far as I can tell from my limited knowledge of the inner workings) so it's pretty hard for a \"drive by\" contributor to help here. You had the discussion about community perception on the mailing list (re: Spark Improvement Proposals) and this issue happens to be one of those that at least I see popping up every once in a while in discussions with clients. I would love to see this issue staying open as a feature request and have some hope that someone someday will implement it.", "created": "2016-10-15T22:54:34.790+0000"}, {"author": "Sean R. Owen", "body": "As you wish, but, I disagree with this type of reasoning about JIRAs. I dont think anyone has addressed why a singleton isn't the answer. I can think of corner cases, but, that's why I suspect it isn't something that has needed implementing.", "created": "2016-10-15T22:58:55.571+0000"}, {"author": "Lars Francke", "body": "I can only come up with three reasons at the moment. I hope they all make sense. 1) Singletons/Static Initialisers run once per Class Loader where this class is being loaded/used. I haven't actually seen this being a problem (and it might actually be desired behaviour in this case) but making the init step explicit would prevent this from ever becoming one. 2) I'd like to fail fast for some things and not upon first access (which might be behind a conditional somewhere) 3) It is hard enough to reason about where some piece of code is running as it is (Driver or Task/Executor), adding Singletons to the mix makes it even more confusing. Thank you for reopening!", "created": "2016-10-15T23:21:55.768+0000"}, {"author": "Sean R. Owen", "body": "Reopening doesn't do anything by itself, or cause anyone to consider this. If this just sits for another year, it will have been a tiny part of a larger problem. I would ask those asking to keep this open to advance the discussion, or else I think you'd agree it eventually should be closed. (Here, I'm really speaking about hundreds of issues like this here, not so much this one.) Part of the problem is that I don't think the details of this feature request were ever elaborated. I think that if you dig into what it would mean, you'd find that a) it's kind of tricky to define and then implement all the right semantics, and b) almost any use case along these lines in my experience is resolved as I suggest, with a simple per-JVM initialization. If the response lately here is, well, we're not quite sure how that works, then we need to get to the bottom of that, not just insisting an issue stay open. To your points: - The executor is going to load user code into one classloader, so we do have that an executor = JVM = classloader. - You can fail things as fast as you like by invoking this init as soon as like in your app. - It's clear where things execute, or else, we must assume app developers understand this or else all bets are off. The driver program executes things in the driver unless they're part of a distributed map() etc operation, which clearly execute on the executor. These IMHO aren't reasons to design a new, different, bespoke mechanism. That has a cost too, if you're positing that it's hard to understand when things run where. The one catch I see is that, by design, we don't control which tasks run on what executors. We can't guarantee init code runs on all executors this way. But, is it meaningful to initialize an executor that never sees an app's tasks? it can't be. Lazy init is a good thing and compatible with the Spark model. If startup time is an issue (and I'm still not clear on the latency problem mentioned above), then it gets a little more complicated, but, that's also a little more niche: just run a dummy mapPartitions at the outset on the same data that the first job would touch, even asynchronously if you like with other driver activities. No need to wait; it just gives the init a head-start on the executors that will need it straight away. That's just my opinion of course, but I think those are the questions that would need to be answered to argue something happens here.", "created": "2016-10-16T09:01:32.377+0000"}, {"author": "Olivier Armand", "body": "> \"just run a dummy mapPartitions at the outset on the same data that the first job would touch\" But this wouldn't work for Spark Streaming? (our case).", "created": "2016-10-16T10:06:48.372+0000"}, {"author": "Sean R. Owen", "body": "It would work in this case to immediately schedule initialization on the executors because it sounds like data arrives immediately in your case. The part I am missing is how it can occur faster than this with another mechanism.", "created": "2016-10-16T10:13:01.149+0000"}, {"author": "Olivier Armand", "body": "Data doesn't arrives necessarily immediately, but we need to ensure that when it arrives, lazy initialization doesn't introduce latency.", "created": "2016-10-16T10:24:00.187+0000"}, {"author": "Sean R. Owen", "body": "Yeah that's a decent use case, because latency is an issue (streaming) and you potentially have time to set up before latency matters. You can still use this approach because empty RDDs arrive if no data has, and empty RDDs can still be repartitioned. Here's a way to, once, if the first RDD has no data, do something once per partition, which ought to amount to at least once per executor:  var first = true lines.foreachRDD { rdd => if (first) { if (rdd.isEmpty) { rdd.repartition(sc.defaultParallelism).foreachPartition(_ => Thing.initOnce()) } first = false } }  \"ought\", because, there isn't actually a guarantee that it will put the empty partitions on different executors. In practice, it seems to, when I just tried it. That's a partial solution, but it's an optimization anyway, and maybe it helps you right now. I am still not sure it means this needs a whole mechanism, if this is the only type of use case. Maybe there are others.", "created": "2016-10-16T12:51:14.478+0000"}, {"author": "Michael Schmeißer", "body": "Ok, let me explain the specific problems that we have encountered, which might help to understand the issue and possible solutions: We need to run some code on the executors before anything gets processed, e.g. initialization of the log system or context setup. To do this, we need information that is present on the driver, but not on the executors. Our current solution is to provide a base class for Spark function implementations which contains the information from the driver and initializes everything in its readObject method. Since multiple narrow-dependent functions may be executed on the same executor JVM subsequently, this class needs to make sure that initialization doesn't run multiple times. Sure, that's not hard to do, but if you mix setup and cleanup logic for functions, partitions and/or the JVM itself, it can get quite confusing without explicit hooks. So, our solution basically works, but with that approach, you can't use lambdas for Spark functions, which is quite inconvenient, especially for simple map operations. Even worse, if you use a lambda or otherwise forget to extend the required base class, the initialization doesn't occur and very weird exceptions follow, depending on which resource your function tries to access during its execution. Or if you have very bad luck, no exception will occur, but the log messages will get logged to an incorrect destination. It's very hard to prevent such cases without an explicit initialization mechanism and in a team with several developers, you can't expect everyone to know what is going on there.", "created": "2016-10-16T14:25:44.837+0000"}, {"author": "Sean R. Owen", "body": "This is still easy to do with mapPartitions, which can call {{initWithTheseParamsIfNotAlreadyInitialized(...)}} once per partition, which should guarantee it happens once per JVM before anything else proceeds. I don't think you need to bury it in serialization logic. I can see there are hard ways to implement this, but I believe an easy way is still readily available within the existing API mechanisms.", "created": "2016-10-16T14:30:56.486+0000"}, {"author": "Michael Schmeißer", "body": "But I'll need to have an RDD to do this, I can't just do it during the SparkContext setup - right now, we have multiple sources of RDDs and every developer would still need to know that they have to run this code after creating an RDD, won't they? Or is there some way to use a \"pseudo-RDD\" right after creation of the SparkContext to execute the init code on the executors?", "created": "2016-10-16T14:37:24.368+0000"}, {"author": "Sean R. Owen", "body": "But, why do you need to do it before you have an RDD? You can easily make this a library function. Or, just some static init that happens on demand whenever a certain class is loaded. The nice thing about that is that it's transparent, just like with any singleton / static init in the JVM. If you really want, you can make an empty RDD and repartition it and use that as a dummy, but it only serves to do some initialization early that would happen transparently anyway.", "created": "2016-10-16T15:29:46.061+0000"}, {"author": "Michael Schmeißer", "body": "What if I have a Hadoop InputFormat? Then, certain things happen before the first RDD exists, don't they? I'll give the solution with the empty RDD a shot next week, this sounds a little bit better than what we have right now, but it still relies on certain internals of Spark which are most likely undocumented and might change in future? I've had the feeling that Spark basically has a functional approach with the RDDs and executing anything on an empty RDD could be optimized to just do nothing?", "created": "2016-10-16T20:35:57.177+0000"}, {"author": "Sean R. Owen", "body": "BTW I am not suggesting an \"empty RDD\" for your case. That was specific to the streaming scenario. For this, again, why not just access some initialization method during class init of some class that is referenced wherever you want, including a custom InputFormat? This can be made to happen once per JVM (class loader), from any code, at class init time before anything else can happen. It's just a standard Java mechanism. If you mean it requires some configuration not available at class-loading time you can still make such an init take place wherever, as soon as, such configuration is available. Even in an InputFormat. Although I can imagine corner cases where this becomes hard, I think it's over-thinking this to imagine a whole new lifecycle method to accomplish what basic JVM mechanisms allow.", "created": "2016-10-17T10:00:27.129+0000"}, {"author": "Michael Schmeißer", "body": "I agree that static initialization would solve the problem for cases where everything is known or can be loaded at class-loading time, e.g. from property files in the artifact itself. For situations like RecordReaders, it might also work, because they have an initialize method where they get contextual information that could have been enriched with the required values from the driver. However, we also have other cases, where information from the driver is needed. Imagine the following case: We have a temporary directory in HDFS which is determined by the Oozie workflow instance ID. The driver knows this information, because it is provided by Oozie via main method arguments. The executor needs this information as well, e.g. to load some data that is required to initialize a static context. Then, the question arises: How does the information get to the executor? Either with the function instance which would mean that the developer of the function needs to know that he has to call an initialization method in every function or at least in every first function on an RDD (which he probably doesn't know, because he received the RDD from a different part of the application). Or with an explicit mechanism which is executed before the developer functions run on any executor. Which would lead me again to the \"empty RDD\" workaround.", "created": "2016-10-17T11:35:25.594+0000"}, {"author": "Sean R. Owen", "body": "Yep, if you must pass some configuration, it generally can't happen magically at class-loading time. You can provide a \"initIfNeeded(conf)\" method that must be explicitly called in key places, but, that's simple and canonical Java practice. In your example, there's no need to do anything. Just use the info in the function the executor runs. It's passed in the closure. This is entirely normal Spark.", "created": "2016-10-17T11:39:41.822+0000"}, {"author": "Robert Neumann", "body": "I am supporting Olivier Armand. We need a way in our Streaming job to setup an HBase connection per executor (and not per partition). A Singleton is not something we are looking at for this purpose.", "created": "2016-12-02T09:36:24.216+0000"}, {"author": "Sean R. Owen", "body": "Why would a singleton not work? This is really the essential question in this thread.", "created": "2016-12-02T10:25:02.800+0000"}, {"author": "Robert Neumann", "body": "Sean, I agree this is the essential question in this thread. If we get this sorted out, then we are good and can achieve consensus on what to do with this ticket. A singleton \"works\" indeed. However, from a software engineering point of view it is not nice. There exists a class of Spark Streaming jobs that requires \"setup -> do -> cleanup\" semantics. The framework (in this case Spark Streaming) should explicitly support these semantics through appropriate API hooks. A singleton instead would hide these semantics and you would need to implement some laxy code to check whether an HBase connection was already setup or not; the singelton would need to do this for every write operation to HBase. I do not think that application logic (the Singleton within the Spark Streaming job) is the right place to wire in the \"setup -> do -> cleanup\" pattern. It is a generic pattern and there exists a class of Spark Streaming jobs (not only one specific Streaming job) that are based on this pattern.", "created": "2016-12-02T10:44:06.423+0000"}, {"author": "Herman van Hövell", "body": "[~lars_francke][~Skamandros][~rneumann] If you think that this is an important feature, then write a design doc and open a PR.", "created": "2016-12-02T11:05:49.013+0000"}, {"author": "Robert Neumann", "body": "OK. Will do.", "created": "2016-12-02T12:08:44.280+0000"}, {"author": "Michael Schmeißer", "body": "A singleton is not really feasible if additional information is required which is known (or determined) by the driver and thus needs to be sent to the executors for the initialization to happen. In this case, the options are 1) use some side-channel that is \"magically\" inferred by the executor, 2) use an empty RDD, repartition it to the number of executors and run mapPartitions on it, 3) piggy-back the JavaSerializer to run the initialization before any function is called or 4) require every function which may need the resource to initialize it on its own. Each of these options has significant drawbacks in my opinion. While 4 sounds good for most cases, it has some cons which I've described earlier (my comment from Oct 16) and make it unfeasible for our use-case. Option 1 might be possible, but the data flow wouldn't be all that obvious. Right now, we go with a mix of option 2 and 3 (try to determine the number of executors and if you can't, hijack the serializer), but really, this is hacked and might break in future releases of Spark.", "created": "2016-12-05T12:39:37.188+0000"}, {"author": "Michael Schmeißer", "body": "Thanks [~robert.neumann]! I am ready to help, if I can.", "created": "2016-12-05T12:41:42.168+0000"}, {"author": "Sean R. Owen", "body": "Why? info X can be included in the closure, and the executor can call \"single.getInstance(X)\" to pass this info. Init happens only once in any event.", "created": "2016-12-05T19:42:10.372+0000"}, {"author": "Michael Schmeißer", "body": "Sure it can be included in the closure and this was also our first solution to the problem. But if the application has many layers and you need the resource which requires info X to initialize often, it soon gets very inconvenient because you have to pass X around a lot and pollute your APIs. Thus, our next solution was to create a base function class which takes X in its constructor and makes sure that the resource is initialized on the executor side if it wasn't before. The drawback of this solution is that the function developer can forget to extend the function base class and then he may or may not be able to access the resource depending on whether a function has run before on the executor which performed the initialization. This is really error-prone (actually led to errors) and even if done correctly, prevents lambdas from beeing used for functions. As a result, we now use the \"empty RDD\" approach or piggy-back the Spark JavaSerializer. Both works fine and initializes the executor-side resource properly on all executors. So, from a function developer's point-of-view that's nice, but overall, the solution relies on Spark internals to work which is why I would rather have an explicit mechanism to perform such an initialization.", "created": "2016-12-06T13:53:26.651+0000"}, {"author": "Herman van Hövell", "body": "If you only try to propagate information, then you can use SparkContext.localProperties and the TaskContext on the executor side. They provide the machinery to do this.", "created": "2016-12-06T15:13:36.417+0000"}, {"author": "Herman van Hövell", "body": "A creatively applied broadcast variable might also do the trick BTW.", "created": "2016-12-06T15:16:23.015+0000"}, {"author": "Michael Schmeißer", "body": "No, it's not just about propagating information - some code actually needs to be run. We have some static utilities which need to be initialized, but they don't know anything about Spark but are rather provided by external libraries. Thus, we need to actually trigger the initialization on all executors. The only other way that I see is to wrap all access to those external utilities with something on our side that is Spark-aware and initializes them if needed. But I think compared to this, our current solution is better.", "created": "2016-12-08T09:13:35.344+0000"}, {"author": "Ryan Williams", "body": "Both suggested workarounds here are lacking or broken / actively harmful, afaict, and the use case is real and valid. The ADAM project struggled for >2 years with this problem: - [a 3rd-party {{OutputFormat}} required this field to be set|https://github.com/HadoopGenomics/Hadoop-BAM/blob/eb688fb90c60e8c956f9d1e4793fea01e3164056/src/main/java/org/seqdoop/hadoop_bam/KeyIgnoringAnySAMOutputFormat.java#L93] - the value of the field is computed on the driver, and needs to somehow be sent to and set in each executor JVM. h3. {{mapPartitions}} hack [Some attempts to set the field via a dummy {{mapPartitions}} job|https://github.com/hammerlab/adam/blob/b87bfb72c7411b5ea088b12334aa1b548102eb4b/adam-core/src/main/scala/org/bdgenomics/adam/rdd/read/AlignmentRecordRDDFunctions.scala#L134-L146] actually added [pernicious, non-deterministic bugs|https://github.com/bigdatagenomics/adam/issues/676#issuecomment-219347677]. In general Spark seems to provide no guarantees that ≥1 tasks will get scheduled on each executor in such a situation: - in the above, node locality resulted in some executors being missed - dynamic-allocation also offers chances for executors to come online later and never be initialized h3. object/singleton initialization How can one use singleton initialization to pass an object from the driver to each executor? Maybe I've missed this in the discussion above. In the end, ADAM decided to write the object to a file and route that file's path to the {{OutputFormat}} via a hadoop configuration value, which is pretty inelegant. h4. Another use case I have another need for this atm where regular lazy-object-initialization is also insufficient: [due to a rough-edge in Scala programs' classloader configuration, {{FileSystemProvider}}'s in user JARs are not loaded properly|https://github.com/scala/bug/issues/10247]. [A workaround discussed in the 1st post on that issue fixes the problem|https://github.com/hammerlab/spark-commands/blob/1.0.3/src/main/scala/org/hammerlab/commands/FileSystems.scala#L8-L20], but needs to be run before {{FileSystemProvider.installedProviders}} is first called on the JVM, which can be triggered by numerous {{java.nio.file}} operations. I don't see a clear way to work in code in that will always lazily call my {{FileSystems.load}} function on each executor, let alone ensure that it happens before any code in the JAR calls e.g. {{Paths.get}}.", "created": "2017-04-10T18:33:06.152+0000"}, {"author": "Mathieu Boespflug", "body": "[~Skamandros] how did you manage to hook `JavaSerializer`? I tried doing so myself, by defining a new subclass, but then I need to make sure that new class is installed on all executors. Meaning I have to copy a .jar on all my nodes manually. For some reason Spark won't try looking for the serializer inside my application JAR.", "created": "2017-04-13T13:00:31.116+0000"}, {"author": "Ritesh Tijoriwala", "body": "[~Skamandros] - I would also like to know about hooking 'JavaSerializer'. I have a similar use case where I need to initialize set of objects/resources on each executor. I would also like to know if anybody has a way to hook into some \"clean up\" on each executor when 1) the executor shutdown 2) when a batch finishes and before next batch starts", "created": "2017-04-14T19:14:22.873+0000"}, {"author": "Michael Schmeißer", "body": "In a nutshell, we have our own class \"MySerializer\" which is derived from {{org.apache.spark.serializer.JavaSerializer}} and performs our custom initialization in {{MySerializer#newInstance}} before calling the super method {{org.apache.spark.serializer.JavaSerializer#newInstance}}. Then, when building the SparkConf for initialization of the SparkContext, we add {{pSparkConf.set(\"spark.closure.serializer\", MySerializer.class.getCanonicalName());}}. We package this with our application JAR and it works. So I think you have to look at your classpath configuration [~mboes]. In our case, the JAR which contains the closure serializer is listed in the following properties (we use Spark 1.5.0 on YARN in cluster mode): * driver.extraClassPath * executor.extraClassPath * yarn.secondary.jars * spark.yarn.secondary.jars * spark.driver.extraClassPath * spark.executor.extraClassPath If I recall it correctly, the variants without the \"spark.\" prefix are produced by us because we prefix all of our properties with \"spark.\" to transfer them via Oozie and unmask them again later, so you should only need the properties with the \"spark.\" prefix. Regarding the questions of [~riteshtijoriwala]: 1) Please see the related issue SPARK-1107. 2) You can add a TaskCompletionListener with {{org.apache.spark.TaskContext#addTaskCompletionListener(org.apache.spark.util.TaskCompletionListener)}}. To get the current TaskContext on the executor, just use {{org.apache.spark.TaskContext#get}}. We have some functionality to log the progress of a function in fixed intervals (e.g. every 1,000 records). To do this, you can use mapPartitions with a custom iterator.", "created": "2017-04-20T15:28:37.716+0000"}, {"author": "Ritesh Tijoriwala", "body": "[~Skamandros] - Any similar tricks for spark 2.0.0? I see the config option to set the closure serializer has been removed - https://issues.apache.org/jira/browse/SPARK-12414. Currently we do \"set of different things\" to ensure our classes are loaded/instantiated before spark starts execution of its stages. It would be nice to consolidate this in one place/hook.", "created": "2017-06-19T10:17:03.616+0000"}, {"author": "Michael Schmeißer", "body": "[~riteshtijoriwala] - Sorry, but I am not familiar with Spark 2.0.0 yet. But what I can say is that we have raised a Cloudera support case to address this issue so maybe we can expect some help from this side.", "created": "2017-06-19T15:05:35.804+0000"}, {"author": "Louis Bergelson", "body": "I can't understand how people are dismissing this as not an issue. There are many cases where you need to initialize something on an executor, and many of them need input from the driver. All of the given workarounds are terrible hacks and at best force bad design, and at worst introduce confusing and non-deterministic bugs. Any time that the recommended solution to a common problem that many people are having is to abuse the Serializer in order to trick it into executing non-serialization code it seems obvious that there's a missing capability in the system. The fact that executors can come on and offline at any time during the run makes it especially essential that we have a robust way of initializing them. I just really don't understand the opposition to adding an initialization hook, it would solve so many problems in a clean way and doesn't seem like it would be particularly problematic on its own.", "created": "2017-08-01T16:13:11.947+0000"}, {"author": "Sean R. Owen", "body": "I still don't see an argument against my primary suggestion: the singleton. The last comment on it just said, oh, how do you do it? it's quite possible. Nothing to do with the serializer.", "created": "2017-08-01T17:12:47.224+0000"}, {"author": "Michael Schmeißer", "body": "Please see my comment from 05/Dec/16 12:39 and the following discussion - we are kind of going in circles here. I tried to explain the (real) problems we were facing as good as I can and which solution we applied to them and why other solutions have been dismissed. The fact is: There are numerous people here who seem to have the same issues and are glad to apply the workaround because \"using the singleton\" doesn't seem to provide a solution to them either. Probably we all don't understand how to do this but then again there seems to be something missing - at least documentation, doesn't it? What I can tell you in addition is that we have concerned experienced developers with the topic who have used quite a few singletons.", "created": "2017-08-01T23:54:05.233+0000"}, {"author": "Sean R. Owen", "body": "Are you looking for an example of how it works? something like this, for what I assume is the common case of something like initializing a connection to an external resource:  val config = ... df.mapPartitions { it => MyResource.initIfNeeded(config) it.map(...) } ... object MyResource { private var initted = false def initIfNeeded(config: Config): Unit = this.synchronized { if (!initted) { initializeResource(config) initted = true } }  If config is big, or tricky to pass around, that too can be read directly from a location, or wrapped up in some object in your code. It can actually be:  df.mapPartitions { it => MyResource.initIfNeeded() it.map(...) } ... object MyResource { private var initted = false def initIfNeeded(): Unit = this.synchronized { if (!initted) { val config = getConf() initializeResource(config) initted = true } }  You get the idea. This is not a special technique, not even really singletons. Just making a method that executes the first time it's called and then does nothing after. If you don't like having to call initResource -- call that in whatever code produces the resource connection or whatever. We can imagine objections and answers like this all day I'm sure. I think it covers all use cases I can imagine that a setup hook does, so the question is just is it easy enough? You're saying it's unusably hard, and proposing some hack on the serializer that sounds much more error-prone. I just cannot agree with this. This is much simpler than other solutions people are arguing against here, which I also think are too complex. Was it just a misunderstanding of the proposal? [~louisb@broadinstitute.org] have you considered the implications of the semantics of a setup hook? for example, if setup fails on an executor, can you schedule a task that needed it? how do you track that? Here, the semantics are obvious.", "created": "2017-08-02T08:15:47.961+0000"}, {"author": "Louis Bergelson", "body": "[~srowen] Thanks for the reply and the example. Unfortunately, I still believe that the singleton approach doesn't work well for our use case. We don't have a single resource which needs initialization and can always be wrapped in a singleton. We have a sprawl of legacy dependencies that need to be initialized in certain ways before use, and then can be called into from literally hundreds of entry points. One of the things that needs initializing is the set of FileSystemProviders that [~rdub] mentioned above. This has to be done before potentially any file access in our dependencies. It's implausible to wrap all of our library code into singleton objects and it's difficult to always call initResources() before every library call. It requires a lot of discipline on the part of the developers. Since we develop a framework for biologists to use to write tools, any thing that has to be enforced by convention isn't ideal and is likely to cause problems. People will forget to start their work by calling initResources() or worse, they'll remember to call initResources(), but only at the start of the first stage. Then they'll run into issues when executors die and are replaced during a later stage and the initialization doesn't run on the new executor. For something that could be cleanly wrapped in a singleton I agree that the semantics are obvious, but for the case where you're calling init() before running your code, the semantics are confusing and error prone. I'm sure there are complications from introducing a setup hook, but the one you mention seems simple enough to me. If a setup fails, that executor is killed and can't schedule tasks. There would probably have to be a mechanism for timing out after a certain number of failed executor starts, but I suspect that that exists already in some fashion for other sorts of failures.", "created": "2017-08-07T19:14:01.570+0000"}, {"author": "Sean R. Owen", "body": "I can also imagine cases involving legacy code that make this approach hard to implement. Still, it's possible with enough 'discipline', but this is true of wrangling any legacy code. I don't think the question of semantics is fully appreciated here. Is killing the app's other tasks on the same executor reasonable behavior? how many failures are allowed by default by this new mechanism? what do you do if init never returns? for how long? Are you willing to reschedule the task on another executor? how does it interact with locality? I know, any change raises questions, but this one raises a lot. It's a conceptual change in Spark and I'm just sure it's not going to happen 3 years in. Tasks have never had special status or lifecycle w.r.t. executors and that's a positive thing, really.", "created": "2017-08-07T19:29:18.407+0000"}, {"author": "yiming.xu", "body": "I need a hook too. Some case, We need init something like spring initbean :(", "created": "2017-09-08T06:54:47.410+0000"}, {"author": "quang nguyen", "body": "Hi, We had an application run on spark cluster to a secured hdfs(kerberos) Because spark had not supported for kerberos yet, it will be convenient for us that spark supports a setup hook to login as an user on each executor. Can u figure out another solution for us? (run on yarn mode isn't an option)", "created": "2017-11-06T07:21:11.959+0000"}, {"author": "Sina Madani", "body": "I too have this problem. It seems that Apache Flink solves this quite nicely by having \"RichFunction\" variants for operations like map, filter, reduce etc. A RichFunction, such as RichMapFunction, provides open(Configuration parameters) and close() methods which can be used to run setup and teardown code once per worker and also initialise the worker from primitive key-value pairs.", "created": "2018-06-13T12:21:04.329+0000"}, {"author": "Avi minsky", "body": "We encountered an issue with the combination of lazy static loading and speculation. Because speculation kills tasks it might kill while loading lazy static classes which make them unusuable and later all application might fail for noclassdeferror", "created": "2018-06-19T18:21:02.144+0000"}, {"author": "Imran Rashid", "body": "Folks may be interested in SPARK-24918. perhaps one should be closed a duplicate of the other, but for now there is some discussion on both, so I'll leave them open for the time being", "created": "2018-08-13T20:11:42.653+0000"}], "num_comments": 58, "text": "Issue: SPARK-650\nSummary: Add a \"setup hook\" API for running initialization code on each executor\nDescription: Would be useful to configure things like reporting libraries\n\nComments (58):\n1. Andrew Ash: As mentioned in SPARK-572 static classes' initialization methods are being \"abused\" to perform this functionality. [~matei] do you still feel that a per-executor initialization function is a hook that Spark should expose in its public API?\n2. Lars Francke: Not [~matei] but I think this would be a good idea to have. Abusing another undocumented concept doesn't seem like a nice way to treat a useful and common use-case.\n3. Michael Schmeißer: I would need this feature as well to perform some initialization of the logging system (which reads its configuration from an external source rather than just a file).\n4. Luis Ramos: I have similar requirements to Michael's – this would be a very useful feature to have.\n5. Holden Karau: I think this is a duplicate of SPARK-636 yes?\n6. Michael Schmeißer: To me, the two seem related, but not exact duplicates. SPARK-636 seems to aim for a more generic mechanism.\n7. Holden Karau: Would people feel ok if we marked this as a duplicate of 636 since it does seem like this a subset of 636.\n8. Michael Schmeißer: I disagree that those issues are duplicates. Spark-636 looks for a generic way to execute code on the Executors, but not for a reliable and easy mechanism to execute code during Executor initialization.\n9. Sean R. Owen: In practice, these should probably all be WontFix as it hasn't mattered enough to implement in almost 4 years. It really doesn't matter.\n10. Michael Schmeißer: Then somebody should please explain to me, how this doesn't matter or rather how certain use-cases are supposed to be solved. We need to initialize each JVM and connect it to our logging system, set correlation IDs, initialize contexts and so on. I guess that most users just have implemented work-arounds as we did, but in an enterprise environment, this is really not the preferable long-term solution to me. Plus, I think that it would really not be hard to implement this feature for someone who has knowledge about the Spark executor setup.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.021561"}}
{"id": "c336e6fef14f78bbeef3c3d60eba30ac", "issue_key": "SPARK-651", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Port sample()/takeSample() to PySpark", "description": "Due to batching, PySpark can't directly call Spark's sample() and takeSample() methods. Now that PySpark has mapPartitionsWithSplit(), we need to port the sample() / takeSample() code to Python. We should probably resolve SPARK-648 before porting the code. Sampling with repetition is implemented by sampling from a Poisson distribution. NumPy has a fast implementation of this, but I don't want to rely on the NumPy library because we can't easily package it with PySpark (the compiled C extensions are platform / architecture specific). We might be able to write code to use NumPy as a performance optimization if it's installed, falling back on a pure-Python implementation otherwise. To ensure deterministic behavior, it's important that the NumPy and pure-Python implementations return the same results.", "reporter": "Josh Rosen", "assignee": "Andre Schumacher", "created": "2013-01-13T00:05:52.000+0000", "updated": "2013-09-01T15:15:24.000+0000", "resolved": "2013-09-01T15:15:24.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Andre Schumacher", "body": "This issue is now addressed in this pull request: https://github.com/mesos/spark/pull/861", "created": "2013-08-26T12:50:37.720+0000"}], "num_comments": 1, "text": "Issue: SPARK-651\nSummary: Port sample()/takeSample() to PySpark\nDescription: Due to batching, PySpark can't directly call Spark's sample() and takeSample() methods. Now that PySpark has mapPartitionsWithSplit(), we need to port the sample() / takeSample() code to Python. We should probably resolve SPARK-648 before porting the code. Sampling with repetition is implemented by sampling from a Poisson distribution. NumPy has a fast implementation of this, but I don't want to rely on the NumPy library because we can't easily package it with PySpark (the compiled C extensions are platform / architecture specific). We might be able to write code to use NumPy as a performance optimization if it's installed, falling back on a pure-Python implementation otherwise. To ensure deterministic behavior, it's important that the NumPy and pure-Python implementations return the same results.\n\nComments (1):\n1. Andre Schumacher: This issue is now addressed in this pull request: https://github.com/mesos/spark/pull/861", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.021561"}}
{"id": "2d9becb948c5d5ceb8fa1f96ffda6d1a", "issue_key": "SPARK-652", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Propagate exceptions from PySpark workers to the driver", "description": "PySpark should propagate exceptions thrown by Python workers so that they are visible to the Python driver. This would greatly simplify debugging.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-01-13T00:08:08.000+0000", "updated": "2013-01-31T22:00:43.000+0000", "resolved": "2013-01-31T22:00:43.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Duplicate of SPARK-673", "created": "2013-01-31T22:00:43.333+0000"}], "num_comments": 1, "text": "Issue: SPARK-652\nSummary: Propagate exceptions from PySpark workers to the driver\nDescription: PySpark should propagate exceptions thrown by Python workers so that they are visible to the Python driver. This would greatly simplify debugging.\n\nComments (1):\n1. Josh Rosen: Duplicate of SPARK-673", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.022560"}}
{"id": "d83f0b44ac2113e53e1f42d1f543aa12", "issue_key": "SPARK-653", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add accumulators to PySpark", "description": "Doesn't seem like it would be *that* bad to implement.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-01-13T14:32:36.000+0000", "updated": "2013-01-20T11:01:34.000+0000", "resolved": "2013-01-20T11:01:28.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Added in https://github.com/mesos/spark/pull/387", "created": "2013-01-20T11:01:28.758+0000"}], "num_comments": 1, "text": "Issue: SPARK-653\nSummary: Add accumulators to PySpark\nDescription: Doesn't seem like it would be *that* bad to implement.\n\nComments (1):\n1. Josh Rosen: Added in https://github.com/mesos/spark/pull/387", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.022560"}}
{"id": "9f39bd6e1506a6ff683b97d800fb38d5", "issue_key": "SPARK-654", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Use ID of hash function when comparing Python partitioner objects in equals()", "description": "Right now we might compare two Python partitioners as equal even if they have different hash functions.", "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "created": "2013-01-13T20:24:51.000+0000", "updated": "2013-01-20T17:09:19.000+0000", "resolved": "2013-01-20T17:09:19.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "We should fix this, although it isn't currently a correctness problem because joins / groups / cogroups are not implemented in terms of the Java / Scala implementations, so PythonPartitioners are never compared for equality. This reminds me that I should implement a co-partitioning-aware joins in PySpark, for which I'll open a different issue.", "created": "2013-01-14T15:20:29.217+0000"}], "num_comments": 1, "text": "Issue: SPARK-654\nSummary: Use ID of hash function when comparing Python partitioner objects in equals()\nDescription: Right now we might compare two Python partitioners as equal even if they have different hash functions.\n\nComments (1):\n1. Josh Rosen: We should fix this, although it isn't currently a correctness problem because joins / groups / cogroups are not implemented in terms of the Java / Scala implementations, so PythonPartitioners are never compared for equality. This reminds me that I should implement a co-partitioning-aware joins in PySpark, for which I'll open a different issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.022560"}}
{"id": "26c025d679e109161877dea460d4d7eb", "issue_key": "SPARK-655", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Implement co-partitioning aware joins in PySpark", "description": "Implement co-partitioning aware joins in PySpark", "reporter": "Josh Rosen", "assignee": null, "created": "2013-01-14T15:22:08.000+0000", "updated": "2015-02-17T00:09:36.000+0000", "resolved": "2015-02-17T00:09:21.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-655\nSummary: Implement co-partitioning aware joins in PySpark\nDescription: Implement co-partitioning aware joins in PySpark", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.022560"}}
{"id": "1bfd1aaf8e787f32393376339c194c52", "issue_key": "SPARK-656", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Let Amazon choose our EC2 clusters' availability zone if the user does not specify one", "description": "Amazon will automatically assign the zone with the most free resources if you don't specify one in your request.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-01-15T18:26:57.000+0000", "updated": "2014-11-06T07:00:34.000+0000", "resolved": "2014-11-06T07:00:34.000+0000", "labels": [], "components": ["EC2"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-656\nSummary: Let Amazon choose our EC2 clusters' availability zone if the user does not specify one\nDescription: Amazon will automatically assign the zone with the most free resources if you don't specify one in your request.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.022560"}}
{"id": "d6b2b57236e3bc23100a8b5cfe7de0bb", "issue_key": "SPARK-657", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Don't use multiple loopback IP addresses in unit tests", "description": "Right now we require users to manually alias their interface as 127.100.0.1, 2, etc, at least on some systems (Mac OS X).", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-01-16T11:07:17.000+0000", "updated": "2013-01-27T23:18:21.000+0000", "resolved": "2013-01-27T23:18:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Mikhail Bautin", "body": "By the way, the work-around for this is sudo ifconfig lo0 add 127.100.0.1 sudo ifconfig lo0 add 127.100.0.2 sudo ifconfig lo0 add 127.100.0.3 etc.", "created": "2013-01-16T14:16:32.510+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Closed by commit https://github.com/mesos/spark/commit/44b4a0f88fcb31727347b755ae8ec14d69571b52", "created": "2013-01-27T23:18:05.926+0000"}], "num_comments": 2, "text": "Issue: SPARK-657\nSummary: Don't use multiple loopback IP addresses in unit tests\nDescription: Right now we require users to manually alias their interface as 127.100.0.1, 2, etc, at least on some systems (Mac OS X).\n\nComments (2):\n1. Mikhail Bautin: By the way, the work-around for this is sudo ifconfig lo0 add 127.100.0.1 sudo ifconfig lo0 add 127.100.0.2 sudo ifconfig lo0 add 127.100.0.3 etc.\n2. Matei Alexandru Zaharia: Closed by commit https://github.com/mesos/spark/commit/44b4a0f88fcb31727347b755ae8ec14d69571b52", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.023563"}}
{"id": "8985a21c06edf8e66c179d86ca90d38a", "issue_key": "SPARK-658", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Make Spark execution time logging more obvious and easier to read", "description": "I believe we log some execution time, but they are hard to read. Making those execution time (by stages) easier to read would be a great start to performance analysis.", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-01-16T11:40:54.000+0000", "updated": "2013-01-29T21:30:32.000+0000", "resolved": "2013-01-29T21:11:21.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-658\nSummary: Make Spark execution time logging more obvious and easier to read\nDescription: I believe we log some execution time, but they are hard to read. Making those execution time (by stages) easier to read would be a great start to performance analysis.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.023563"}}
{"id": "f30e68645b0727895d92c82b1d0be244", "issue_key": "SPARK-659", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "The master web interface is broken for Scala 2.10", "description": "To reproduce: Build branch scala-2.10. Edit the \"run\" script to use Scala 2.10 and comment out the REPL stuff. Launch with \"./run spark.deploy.master.Master\". Visit localhost:8080 in a browser. This generates a MatchError: None at MasterWebUI.scala:28 Thanks! Also, it would be nice if you used a Github issue tracker.", "reporter": "Eric Christiansen", "assignee": null, "created": "2013-01-18T16:27:38.000+0000", "updated": "2013-05-14T15:30:16.000+0000", "resolved": "2013-05-14T15:30:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Eric Christiansen", "body": "This is fixed in the current 2.10 branch. Hrm, I don't think I have permissions to close this.", "created": "2013-03-21T20:53:40.739+0000"}, {"author": "Josh Rosen", "body": "Closing this issue, since [~emchristiansen]'s comment says that it's fixed. If this is still broken, please re-open this issue.", "created": "2013-05-14T15:30:16.472+0000"}], "num_comments": 2, "text": "Issue: SPARK-659\nSummary: The master web interface is broken for Scala 2.10\nDescription: To reproduce: Build branch scala-2.10. Edit the \"run\" script to use Scala 2.10 and comment out the REPL stuff. Launch with \"./run spark.deploy.master.Master\". Visit localhost:8080 in a browser. This generates a MatchError: None at MasterWebUI.scala:28 Thanks! Also, it would be nice if you used a Github issue tracker.\n\nComments (2):\n1. Eric Christiansen: This is fixed in the current 2.10 branch. Hrm, I don't think I have permissions to close this.\n2. Josh Rosen: Closing this issue, since [~emchristiansen]'s comment says that it's fixed. If this is still broken, please re-open this issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.023563"}}
{"id": "b4b1b0e3d6ea9f5f59679e3fc523d19e", "issue_key": "SPARK-660", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add StorageLevel support in Python", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Davidson", "created": "2013-01-19T22:53:13.000+0000", "updated": "2013-09-10T10:59:01.000+0000", "resolved": "2013-09-10T10:59:01.000+0000", "labels": ["Starter"], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-660\nSummary: Add StorageLevel support in Python", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.023563"}}
{"id": "53d3fc6dd699612a968d3e3936589637", "issue_key": "SPARK-661", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Java unit tests don't seem to run with Maven", "description": "I don't see them in the test output. Definitely important to fix this, though they do run with SBT or if you launch them manually.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-01-20T23:44:52.000+0000", "updated": "2014-07-26T20:04:52.000+0000", "resolved": "2014-07-26T20:04:52.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "The Java API tests are written using JUnit. In sbt, I had to add a plugin so that scalatest picked up those tests:  \"com.novocode\" % \"junit-interface\" % \"0.8\" % \"test\",  Maybe we have to do something similar with Maven.", "created": "2013-01-21T10:35:41.751+0000"}, {"author": "Matt Massie", "body": "Is this still an issue? When I just ran,  $ mvn -Phadoop1 clean test  all the test were executed as expected. Maven finds tests by looking for any source that starts or ends with Test, e.g. MyTest.scala, TestMine.scala. You can also just drop the tests in the Maven test directory for a (sub-)project. For example, a typical project layout is.  project/src project/src/main/java project/src/main/resources project/src/main/scala project/src/test/java project/src/test/resources project/src/test/scala  Ideally, we would have a test class for each source file, e.g. Foo.java FooTest.java, Bar.scala, BarTest.scala. The output of all tests is in the 'surefire-reports' directory, e.g.  $ find . -name \"surefire-reports\" ./core/target/surefire-reports ./bagel/target/surefire-reports ./streaming/target/surefire-reports  Note: these reports are what Jenkins will read to present information about the state of the build.", "created": "2013-01-23T10:23:38.373+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Did you see JavaAPISuite run? There are two test suites called this (one in spark and one in spark.streaming) that are written in Java, as JUnit tests. I don't think the ScalaTest plugin runs them.", "created": "2013-01-23T10:35:01.346+0000"}], "num_comments": 3, "text": "Issue: SPARK-661\nSummary: Java unit tests don't seem to run with Maven\nDescription: I don't see them in the test output. Definitely important to fix this, though they do run with SBT or if you launch them manually.\n\nComments (3):\n1. Josh Rosen: The Java API tests are written using JUnit. In sbt, I had to add a plugin so that scalatest picked up those tests:  \"com.novocode\" % \"junit-interface\" % \"0.8\" % \"test\",  Maybe we have to do something similar with Maven.\n2. Matt Massie: Is this still an issue? When I just ran,  $ mvn -Phadoop1 clean test  all the test were executed as expected. Maven finds tests by looking for any source that starts or ends with Test, e.g. MyTest.scala, TestMine.scala. You can also just drop the tests in the Maven test directory for a (sub-)project. For example, a typical project layout is.  project/src project/src/main/java project/src/main/resources project/src/main/scala project/src/test/java project/src/test/resources project/src/test/scala  Ideally, we would have a test class for each source file, e.g. Foo.java FooTest.java, Bar.scala, BarTest.scala. The output of all tests is in the 'surefire-reports' directory, e.g.  $ find . -name \"surefire-reports\" ./core/target/surefire-reports ./bagel/target/surefire-reports ./streaming/target/surefire-reports  Note: these reports are what Jenkins will read to present information about the state of the build.\n3. Matei Alexandru Zaharia: Did you see JavaAPISuite run? There are two test suites called this (one in spark and one in spark.streaming) that are written in Java, as JUnit tests. I don't think the ScalaTest plugin runs them.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.024561"}}
{"id": "fac759eea0d0f9006b47fc0a40c38cda", "issue_key": "SPARK-662", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Executor should only download files & jars once", "description": "Executor.updateDependencies is called by each task, without any synchronization. This means that all threads try to simultaneously download jars & files from the client host, which make the fetch much slower if there are many threads per node.", "reporter": "Imran Rashid", "assignee": "Josh Rosen", "created": "2013-01-21T15:34:29.000+0000", "updated": "2014-07-28T05:25:47.000+0000", "resolved": "2013-05-19T13:20:27.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Looks like I fixed this in LocalScheduler.updateDependencies (https://github.com/JoshRosen/spark/commit/bd237d4a9d7f08eb143b2a2b8636a6a8453225ea) but not for Executor. I'm actually working on some fixes for addFile() / addJar(), so I'll add synchronization.", "created": "2013-01-21T15:43:38.776+0000"}, {"author": "Josh Rosen", "body": "This should be fixed in master, but we should backport the fix to branch-0.6.", "created": "2013-01-29T08:25:20.734+0000"}, {"author": "Josh Rosen", "body": "I was planning to backport this to branch-0.6, but I never got around to it. Do we still want to backport performance improvements like this, or are we only backporting bugfixes? Can I mark this as resolved?", "created": "2013-04-26T09:36:26.971+0000"}, {"author": "Josh Rosen", "body": "Marking as resolved, since it looks like we're no longer backporting bugfixes to branch-0.6 (?).", "created": "2013-05-19T13:20:27.793+0000"}], "num_comments": 4, "text": "Issue: SPARK-662\nSummary: Executor should only download files & jars once\nDescription: Executor.updateDependencies is called by each task, without any synchronization. This means that all threads try to simultaneously download jars & files from the client host, which make the fetch much slower if there are many threads per node.\n\nComments (4):\n1. Josh Rosen: Looks like I fixed this in LocalScheduler.updateDependencies (https://github.com/JoshRosen/spark/commit/bd237d4a9d7f08eb143b2a2b8636a6a8453225ea) but not for Executor. I'm actually working on some fixes for addFile() / addJar(), so I'll add synchronization.\n2. Josh Rosen: This should be fixed in master, but we should backport the fix to branch-0.6.\n3. Josh Rosen: I was planning to backport this to branch-0.6, but I never got around to it. Do we still want to backport performance improvements like this, or are we only backporting bugfixes? Can I mark this as resolved?\n4. Josh Rosen: Marking as resolved, since it looks like we're no longer backporting bugfixes to branch-0.6 (?).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.024561"}}
{"id": "a2da15b3f2a38af512e4847d5a57cf95", "issue_key": "SPARK-663", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Implement Fair scheduler within ClusterScheduler", "description": "Each SparkContext currently has a simple FIFO scheduler for task assignment. Applications which use long-running jobs (as in Shark) on the cluster submit tasks which are processed in a strict FIFO fashion. In the Shark example, the FIFO scheduler causes Spark to process one query at a time, which heavily impacts short running queries that are submitted after the long running tasks. In other words, a large query will block completion of a smaller query until the big query has completed.", "reporter": "Bradley Freeman", "assignee": "xiajunluan", "created": "2013-01-23T14:41:16.000+0000", "updated": "2014-03-30T23:32:00.000+0000", "resolved": "2013-08-06T22:59:51.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Reynold Xin", "body": "Moving some notes from my email to Harold. Quoting myself: If the purpose is to prevent large queries from blocking small queries - how about implement the fair sharing policy in TaskScheduler level so we can avoid changing any of the APIs? It is not entirely \"fair\" because a query with multiple stages will gain more shares - but I think it would work for most purposes?", "created": "2013-02-28T10:50:52.525+0000"}, {"author": "Reynold Xin", "body": "From Patrick: To be even more concrete, if you look at the following method in ClusterScheduler: def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] This sorts the activeTaskSetQueue by priority. Instead, it could sort it based on a FairShare calculation. This wouldn't be able to add weights/pools to certain users without adding tracing data up and down the stack, but it would prevent head-of-line blocking.", "created": "2013-02-28T10:51:23.421+0000"}, {"author": "Reynold Xin", "body": "From Patrick: Sorry for the delay - we had some internal discussion about this. To enable what you want (having tracking for each user and combined user pools with shares) we need to add some instrumentation throughout the stack. I spoke with Matei about this a bit, he said the best way to do this is to have a ThreadLocal variable that tracks meta-data about current user. My sense is that this would involve having some functions in SparkContext that lets you set information about the current running user and then this gets passed down do the TaskScheduler so it can do things like fair sharing. The you could provide various policies within the TaskScheduler implementation (e.g. ClusterScheduler) and one of them could be fair sharing.", "created": "2013-02-28T10:52:23.489+0000"}, {"author": "Harold Lim", "body": "Just to give even more details. If we want something similar to the Hadoop Fair Scheduler that assigns jobs to one of the pools, where each pool has their own share or weights, then we need to modify the Spark Stack to be able to propagate this information down to the ClusterScheduler. If we look at the current implementation (TaskSet, etc), there is no way to attach this information and for the scheduler to know this information. As the previous comment from Reynold suggests, we can use ThreadLocal variable. In scala, we can use DynamicVariable. My design thoughts are as follows: We have to modify SparkContext, DagScheduler, DagSchedulerEvent, Stage, and TaskSet. In the SparkContext class: 1. we can have a DynamicVariable of type Properties (like Java properties) or something similar to that (like Configuration in Hadoop). This way, we can use this to add meta-data for future features too (e.g., if we need more meta information for some new future scheduler, we can use this). For example, private val localProperties = new DynamicVariable[Properties](null) 2. We can expose a function that allows users (threads) to set values to this variable (e.g., def function addLocalProperties(String key, String value)). 3. We modify the runJob and runApproximateJob method, so that when it calls the dagScheduler.runJob or dagScheduler.runApproximateJob, it includes the value of the DynamicVariable as another parameter. Note that with this, there will be no changes needed for users of Spark. In the DagSchedulerEvent class: 1. We modify the JobSubmitted case class to have Properties member val In the Stage class: 1. We add a Properties member val In the TaskSet class 1. We add a Properties member val In the DagScheduler class: 1. We modify runJob and runApproximateJob methods to have a new parameter of type Properties, with default value so that it doesn't break any other code (if exists) up the spark stack that didn't pass a properties when calling this method. 2. We make the necessary changes so that when a Job, stage, or TaskSet is created, the corresponding properties (from 1) are included. I don't think we can also use a thread local variable in DagScheduler class, like the ones we propose in the SparkContext because jobs submitted (runJob) are simply put in an event queue. There is a separate long running thread that loops and reads from the event queue. This is why I think when the spark context submits a job to the DagScheduler (runJob), we just attach the properties to the job created and correspondingly use this attached properties and attach to the stages/TaskSets when they are created. The above changes are just for passing information down the TaskScheduler. For the Fair scheduler, we can extend the ClusterScheduler, with the following changes: 1. Instead of just having a single ActiveTaskSetsQueue, we want a queue of pools, where each pool contains an ActiveTaskSetsQueue. We can create a separate class that represents this pool. 2. In the constructor (like in the Hadoop fair scheduler), we can also read from an allocation xml to initialize the pools 3. We override the submitTasks method. We first check the properties of the TaskSet to determine which pool it belongs to. Like in ClusterScheduler, we create a TaskSetManager for each TaskSet, but we add it to the corresponding pool's ActiveTaskSetQueue. 4. We override the taskSetFinished method, to also remove TaskSetManager from the corresponding pool. 5. We override the other methods, that use ActiveTaskSetsQueue, to use pool.ActiveTaskSetsQueue 6. We create a comparator function that compares pools used for sorting the pools for fair sharing. I think the comparator function that compares pools need to know the number of tasks currently running for each pool. We can make the necessary changes in the code to easily extract this information (e.g., maybe in the pool class we can keep track of this such that when a new task is launched it gets incremented and when a task ended, it can get decremented) 7. We override resourceOffers. For each offer, we sort the pools using the comparator function in 6. We then start from the head of the sorted pool to find a task to assign, until we find a pool that can use the offer. This also handles delay scheduling, so if the current pool's managers didn't take the offer, we can simply go to the next pool.", "created": "2013-02-28T11:49:45.667+0000"}, {"author": "xiajunluan", "body": "Take sharkserver for example, need we change the sharkserver codes(e.g. call addLocalProperties exposed by SparkContext) to support fair scheduler in spark cluster scheduler? if so, could sharkserver get the user info that submits queries to sharkserver?", "created": "2013-02-28T19:32:37.824+0000"}, {"author": "Harold Lim", "body": "I haven't really looked at the Shark code, but I think there will not be any significant change. What I was thinking is users of Shark can probably set this information in the hive conf. The sharkserver can then parse the \"pool\" information and set it in the thread that process the client requests. One issue I think is that the SharkServer uses a Threadpool to handle client requests. So, we have to be careful in using the DynamicVariable across shared threads. We can probably expose a function in SparkContext to clear/unset the DynamicVariable and the SharkServer can use it whenever a new request comes in.", "created": "2013-03-01T08:52:19.038+0000"}], "num_comments": 6, "text": "Issue: SPARK-663\nSummary: Implement Fair scheduler within ClusterScheduler\nDescription: Each SparkContext currently has a simple FIFO scheduler for task assignment. Applications which use long-running jobs (as in Shark) on the cluster submit tasks which are processed in a strict FIFO fashion. In the Shark example, the FIFO scheduler causes Spark to process one query at a time, which heavily impacts short running queries that are submitted after the long running tasks. In other words, a large query will block completion of a smaller query until the big query has completed.\n\nComments (6):\n1. Reynold Xin: Moving some notes from my email to Harold. Quoting myself: If the purpose is to prevent large queries from blocking small queries - how about implement the fair sharing policy in TaskScheduler level so we can avoid changing any of the APIs? It is not entirely \"fair\" because a query with multiple stages will gain more shares - but I think it would work for most purposes?\n2. Reynold Xin: From Patrick: To be even more concrete, if you look at the following method in ClusterScheduler: def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] This sorts the activeTaskSetQueue by priority. Instead, it could sort it based on a FairShare calculation. This wouldn't be able to add weights/pools to certain users without adding tracing data up and down the stack, but it would prevent head-of-line blocking.\n3. Reynold Xin: From Patrick: Sorry for the delay - we had some internal discussion about this. To enable what you want (having tracking for each user and combined user pools with shares) we need to add some instrumentation throughout the stack. I spoke with Matei about this a bit, he said the best way to do this is to have a ThreadLocal variable that tracks meta-data about current user. My sense is that this would involve having some functions in SparkContext that lets you set information about the current running user and then this gets passed down do the TaskScheduler so it can do things like fair sharing. The you could provide various policies within the TaskScheduler implementation (e.g. ClusterScheduler) and one of them could be fair sharing.\n4. Harold Lim: Just to give even more details. If we want something similar to the Hadoop Fair Scheduler that assigns jobs to one of the pools, where each pool has their own share or weights, then we need to modify the Spark Stack to be able to propagate this information down to the ClusterScheduler. If we look at the current implementation (TaskSet, etc), there is no way to attach this information and for the scheduler to know this information. As the previous comment from Reynold suggests, we can use ThreadLocal variable. In scala, we can use DynamicVariable. My design thoughts are as follows: We have to modify SparkContext, DagScheduler, DagSchedulerEvent, Stage, and TaskSet. In the SparkContext class: 1. we can have a DynamicVariable of type Properties (like Java properties) or something similar to that (like Configuration in Hadoop). This way, we can use this to add meta-data for future features too (e.g., if we need more meta information for some new future scheduler, we can use this). For example, private val localProperties = new DynamicVariable[Properties](null) 2. We can expose a function that allows users (threads) to set values to this variable (e.g., def function addLocalProperties(String key, String value)). 3. We modify the runJob and runApproximateJob method, so that when it calls the dagScheduler.runJob or dagScheduler.runApproximateJob, it includes the value of the DynamicVariable as another parameter. Note that with this, there will be no changes needed for users of Spark. In the DagSchedulerEvent class: 1. We modify the JobSubmitted case class to have Properties member val In the Stage class: 1. We add a Properties member val In the TaskSet class 1. We add a Properties member val In the DagScheduler class: 1. We modify runJob and runApproximateJob methods to have a new parameter of type Properties, with default value so that it doesn't break any other code (if exists) up the spark stack that didn't pass a properties when calling this method. 2. We make the necessary changes so that when a Job, stage, or TaskSet is created, the corresponding properties (from 1) are included. I don't think we can also use a thread local variable in DagScheduler class, like the ones we propose in the SparkContext because jobs submitted (runJob) are simply put in an event queue. There is a separate long running thread that loops and reads from the event queue. This is why I think when the spark context submits a job to the DagScheduler (runJob), we just attach the properties to the job created and correspondingly use this attached properties and attach to the stages/TaskSets when they are created. The above changes are just for passing information down the TaskScheduler. For the Fair scheduler, we can extend the ClusterScheduler, with the following changes: 1. Instead of just having a single ActiveTaskSetsQueue, we want a queue of pools, where each pool contains an ActiveTaskSetsQueue. We can create a separate class that represents this pool. 2. In the constructor (like in the Hadoop fair scheduler), we can also read from an allocation xml to initialize the pools 3. We override the submitTasks method. We first check the properties of the TaskSet to determine which pool it belongs to. Like in ClusterScheduler, we create a TaskSetManager for each TaskSet, but we add it to the corresponding pool's ActiveTaskSetQueue. 4. We override the taskSetFinished method, to also remove TaskSetManager from the corresponding pool. 5. We override the other methods, that use ActiveTaskSetsQueue, to use pool.ActiveTaskSetsQueue 6. We create a comparator function that compares pools used for sorting the pools for fair sharing. I think the comparator function that compares pools need to know the number of tasks currently running for each pool. We can make the necessary changes in the code to easily extract this information (e.g., maybe in the pool class we can keep track of this such that when a new task is launched it gets incremented and when a task ended, it can get decremented) 7. We override resourceOffers. For each offer, we sort the pools using the comparator function in 6. We then start from the head of the sorted pool to find a task to assign, until we find a pool that can use the offer. This also handles delay scheduling, so if the current pool's managers didn't take the offer, we can simply go to the next pool.\n5. xiajunluan: Take sharkserver for example, need we change the sharkserver codes(e.g. call addLocalProperties exposed by SparkContext) to support fair scheduler in spark cluster scheduler? if so, could sharkserver get the user info that submits queries to sharkserver?\n6. Harold Lim: I haven't really looked at the Shark code, but I think there will not be any significant change. What I was thinking is users of Shark can probably set this information in the hive conf. The sharkserver can then parse the \"pool\" information and set it in the thread that process the client requests. One issue I think is that the SharkServer uses a Threadpool to handle client requests. So, we have to be careful in using the DynamicVariable across shared threads. We can probably expose a function in SparkContext to clear/unset the DynamicVariable and the SharkServer can use it whenever a new request comes in.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.024561"}}
{"id": "4c3f358fb37f6d4fe8e7d2d087454181", "issue_key": "SPARK-664", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Accumulator updates should get locally merged before sent to the driver", "description": "Whenever a task finishes, the accumulator updates from that task are immediately sent back to the driver. When the accumulator updates are big, this is inefficient because (a) a lot more data has to be sent to the driver and (b) the driver has to do all the work of merging the updates together. Probably doesn't matter for small accumulators / low number of tasks, but if both are big, this could be a big bottleneck.", "reporter": "Imran Rashid", "assignee": null, "created": "2013-01-23T21:12:25.000+0000", "updated": "2015-05-06T16:45:58.000+0000", "resolved": "2015-05-06T16:45:58.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Imran Rashid", "body": "this also applies to reduce() as well -- all the merging is done on the driver, but the executors can do most of the merging", "created": "2013-04-24T11:46:12.629+0000"}, {"author": "Andrew Ash", "body": "[~irashid] it sounds like your proposal is to batch accumulator updates between tasks on the executor before sending them back to the driver? I agree this would reduce the amount of network traffic, but the batching would come at a cost of higher latency between task completion and accumulator update landing in the accumulator in the driver. With the completion of SPARK-2380 these accumulators are now shown in the UI, so increasing latency would have an effect on end users. If network bandwidth and UI update latency are fundamentally at odds, maybe this is a case for a user option to choose to optimize for network or UI, something like {{spark.accumulators.mergeUpdatesOnExecutor}} defaulted to false. cc [~pwendell] for thoughts", "created": "2014-11-14T09:25:22.603+0000"}, {"author": "Imran Rashid", "body": "Hi [~aash] , thanks for taking another look at this -- sorry I have been aloof for a little while. I didn't know about SPARK-2380 , obviously this was created long before that. Honestly, I'm not a big fan of SPARK-2380, it seems to really limit what we can do with accumulators. We could really use them to expose a completely different model of computation. Let me give an example use case. Accumulators are in principle general enough that they let you compute lots of different things in one pass. Eg., by using accumulators, you could: * create a bloom filter of records that meet some criteria * assign records to different buckets, and count how many are in each bucket, even up to 100K buckets (eg., by having accumulator of {{Array<Long>}}) * use hyperloglog to count how many distinct ids you have * filter down to only those records with some parsing error, for a closer look (just by using plain old {{rdd.filter()}} You could do all that in one pass, if the first 3 were done w/ accumulators. When I started using spark, I actually wrote a bunch of code to do exactly that kind of thing. But it performed really poorly -- after some profiling & investigating how accumulators work, I saw why. Those big accumulators I was creating just put a lot of work on the driver. Accumulators provide the right API to do that kind of thing, but the implementation would have to change. I definitely agree that if the results get merged on the executor before getting sent to the executor, it increases the latency of the *per-task* results, but does that matter? I would prefer that we have something that supports the more general computation model, and the important thing is only the latency of the *overall* result. It feels like we're moving to accumulators being treated just like counters (but with an awkward api).", "created": "2014-11-16T19:55:36.110+0000"}, {"author": "Reynold Xin", "body": "I'm going to close this due to inactivity. Also unclear because based on PRs submitted in the past we are going towards the direction of lower latency rather than higher throughput.", "created": "2015-05-06T16:45:52.527+0000"}], "num_comments": 4, "text": "Issue: SPARK-664\nSummary: Accumulator updates should get locally merged before sent to the driver\nDescription: Whenever a task finishes, the accumulator updates from that task are immediately sent back to the driver. When the accumulator updates are big, this is inefficient because (a) a lot more data has to be sent to the driver and (b) the driver has to do all the work of merging the updates together. Probably doesn't matter for small accumulators / low number of tasks, but if both are big, this could be a big bottleneck.\n\nComments (4):\n1. Imran Rashid: this also applies to reduce() as well -- all the merging is done on the driver, but the executors can do most of the merging\n2. Andrew Ash: [~irashid] it sounds like your proposal is to batch accumulator updates between tasks on the executor before sending them back to the driver? I agree this would reduce the amount of network traffic, but the batching would come at a cost of higher latency between task completion and accumulator update landing in the accumulator in the driver. With the completion of SPARK-2380 these accumulators are now shown in the UI, so increasing latency would have an effect on end users. If network bandwidth and UI update latency are fundamentally at odds, maybe this is a case for a user option to choose to optimize for network or UI, something like {{spark.accumulators.mergeUpdatesOnExecutor}} defaulted to false. cc [~pwendell] for thoughts\n3. Imran Rashid: Hi [~aash] , thanks for taking another look at this -- sorry I have been aloof for a little while. I didn't know about SPARK-2380 , obviously this was created long before that. Honestly, I'm not a big fan of SPARK-2380, it seems to really limit what we can do with accumulators. We could really use them to expose a completely different model of computation. Let me give an example use case. Accumulators are in principle general enough that they let you compute lots of different things in one pass. Eg., by using accumulators, you could: * create a bloom filter of records that meet some criteria * assign records to different buckets, and count how many are in each bucket, even up to 100K buckets (eg., by having accumulator of {{Array<Long>}}) * use hyperloglog to count how many distinct ids you have * filter down to only those records with some parsing error, for a closer look (just by using plain old {{rdd.filter()}} You could do all that in one pass, if the first 3 were done w/ accumulators. When I started using spark, I actually wrote a bunch of code to do exactly that kind of thing. But it performed really poorly -- after some profiling & investigating how accumulators work, I saw why. Those big accumulators I was creating just put a lot of work on the driver. Accumulators provide the right API to do that kind of thing, but the implementation would have to change. I definitely agree that if the results get merged on the executor before getting sent to the executor, it increases the latency of the *per-task* results, but does that matter? I would prefer that we have something that supports the more general computation model, and the important thing is only the latency of the *overall* result. It feels like we're moving to accumulators being treated just like counters (but with an awkward api).\n4. Reynold Xin: I'm going to close this due to inactivity. Also unclear because based on PRs submitted in the past we are going towards the direction of lower latency rather than higher throughput.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.025583"}}
{"id": "b6995f803806c20aa975922d70e84316", "issue_key": "SPARK-665", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Create RPM packages for Spark", "description": "This could be doable with the JRPM Maven plugin, similar to how we make Debian packages now, but I haven't looked into it. The plugin is described at http://jrpm.sourceforge.net.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-01-24T19:00:29.000+0000", "updated": "2015-02-11T08:46:43.000+0000", "resolved": "2015-02-11T08:46:43.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Josh Rosen", "body": "Providing RPM packages will make it easier to install Spark via Puppet (see https://github.com/deric/puppet-spark for example).", "created": "2013-05-22T16:32:01.075+0000"}, {"author": "Rohit Rai", "body": "Probably we can give sbt-native-packager a shot. https://github.com/sbt/sbt-native-packager Will try it out when i get a chance.", "created": "2013-06-02T12:29:04.653+0000"}, {"author": "Shengzhe Yao", "body": "I would highly suggest to use \"fpm\" (https://github.com/jordansissel/fpm) for this task. Here is what we do in RocketFuel (http://rocketfuel.com), we create a directory hierarchy and put into all necessary jars, then call fpm to do the hard work. Please note, configurations are not part of rpm in our case since we always use puppet to manage them. ==========Rpm Build Script with fpm =============== SCALA_VERSION=2.9.3 SPARK_VERSION=0.8.1 SPARK_DEPLOY_PATH=/usr/share/spark rm -rf build mkdir build cd build mkdir -p .$SPARK_DEPLOY_PATH .$SPARK_DEPLOY_PATH/bin cp ../bin/*.sh .$SPARK_DEPLOY_PATH/bin cp ../spark-class .$SPARK_DEPLOY_PATH/spark cp ../spark-shell ../spark-executor ../run-example .$SPARK_DEPLOY_PATH cp ../assembly/target/scala-$SCALA_VERSION/spark*.jar .$SPARK_DEPLOY_PATH/spark-$SPARK_VERSION.jar cp ../examples/target/scala-$SCALA_VERSION/spark*.jar .$SPARK_DEPLOY_PATH/examples-$SPARK_VERSION.jar cp ../tools/target/spark-tools*\\[0-9Tg\\].jar .$SPARK_DEPLOY_PATH/tools-$SPARK_VERSION.jar fpm -s dir -t rpm -n \"rfi-spark\" -v \"$SPARK_VERSION.$BUILD_NUMBER\" -a \"all\" --prefix \"/\" --url \"http://spark.incubator.apache.org\" --maintainer \"MAINTAINER_EMAIL\" --description \"An open source cluster computing system that aims to make data analytics fast\" --license \"Apache Software Foundation (ASF)\" --vendor \"Apache Software Foundation (ASF)\" --category \"grid-thirdparty\" --epoch 1 --verbose .", "created": "2014-01-22T22:59:06.609+0000"}, {"author": "Andrew Ash", "body": "This role of creating RPM packages seems to have been taken up by the various Hadoop distributors for their distributions (Cloudera, MapR, HortonWorks, etc). Does the Apache Spark team still intend to create RPMs for Spark? An obvious subsequent request would be to also release in the DEB format. I don't think this is a route we want to go down now but wanted to hear others' thoughts.", "created": "2014-11-14T10:25:53.660+0000"}, {"author": "Sean R. Owen", "body": "There are already Debian packages. I agree that you can find distros and RPMs easily these days. My feeling is that the build is very complex at this point and the real cost of attempting to maintain another packaging probably isn't justified.", "created": "2014-11-14T10:31:44.440+0000"}, {"author": "Andrew Ash", "body": "Sean are you suggesting dropping the .deb packages that Apache Spark releases as a simplification effort? It feels inequitable to support one packaging format (deb) but not the other (rpm).", "created": "2014-11-14T10:40:19.789+0000"}, {"author": "Sean R. Owen", "body": "Not suggesting that, no. I suppose it does depend on demand. Apparently there was enough of a need for a .deb package that it was contributed and maintained, but perhaps not for .rpm, because there are other sources? If there were demand it could be worth the cost.", "created": "2014-11-14T10:45:21.197+0000"}, {"author": "Christian Tzolov", "body": "I've looked at the JRPM maven plugin but unlike the jdeb one, JRPM depends on native rpm libraries (e.g. it is platform dependent) My somewhat pragmatic approach to build Spark RPM is to reuse the existing deb package and convert it into rpm using the 'alien' tool. I've wrapped the spark-rpm build pipeline into a parameterizable docker container: https://registry.hub.docker.com/u/tzolov/apache-spark-build-pipeline", "created": "2015-01-26T13:08:48.260+0000"}, {"author": "Sean R. Owen", "body": "Given SPARK-5727, I suggest this will also be WontFix, in favor of delegating this to projects like Bigtop.", "created": "2015-02-11T08:46:43.477+0000"}], "num_comments": 9, "text": "Issue: SPARK-665\nSummary: Create RPM packages for Spark\nDescription: This could be doable with the JRPM Maven plugin, similar to how we make Debian packages now, but I haven't looked into it. The plugin is described at http://jrpm.sourceforge.net.\n\nComments (9):\n1. Josh Rosen: Providing RPM packages will make it easier to install Spark via Puppet (see https://github.com/deric/puppet-spark for example).\n2. Rohit Rai: Probably we can give sbt-native-packager a shot. https://github.com/sbt/sbt-native-packager Will try it out when i get a chance.\n3. Shengzhe Yao: I would highly suggest to use \"fpm\" (https://github.com/jordansissel/fpm) for this task. Here is what we do in RocketFuel (http://rocketfuel.com), we create a directory hierarchy and put into all necessary jars, then call fpm to do the hard work. Please note, configurations are not part of rpm in our case since we always use puppet to manage them. ==========Rpm Build Script with fpm =============== SCALA_VERSION=2.9.3 SPARK_VERSION=0.8.1 SPARK_DEPLOY_PATH=/usr/share/spark rm -rf build mkdir build cd build mkdir -p .$SPARK_DEPLOY_PATH .$SPARK_DEPLOY_PATH/bin cp ../bin/*.sh .$SPARK_DEPLOY_PATH/bin cp ../spark-class .$SPARK_DEPLOY_PATH/spark cp ../spark-shell ../spark-executor ../run-example .$SPARK_DEPLOY_PATH cp ../assembly/target/scala-$SCALA_VERSION/spark*.jar .$SPARK_DEPLOY_PATH/spark-$SPARK_VERSION.jar cp ../examples/target/scala-$SCALA_VERSION/spark*.jar .$SPARK_DEPLOY_PATH/examples-$SPARK_VERSION.jar cp ../tools/target/spark-tools*\\[0-9Tg\\].jar .$SPARK_DEPLOY_PATH/tools-$SPARK_VERSION.jar fpm -s dir -t rpm -n \"rfi-spark\" -v \"$SPARK_VERSION.$BUILD_NUMBER\" -a \"all\" --prefix \"/\" --url \"http://spark.incubator.apache.org\" --maintainer \"MAINTAINER_EMAIL\" --description \"An open source cluster computing system that aims to make data analytics fast\" --license \"Apache Software Foundation (ASF)\" --vendor \"Apache Software Foundation (ASF)\" --category \"grid-thirdparty\" --epoch 1 --verbose .\n4. Andrew Ash: This role of creating RPM packages seems to have been taken up by the various Hadoop distributors for their distributions (Cloudera, MapR, HortonWorks, etc). Does the Apache Spark team still intend to create RPMs for Spark? An obvious subsequent request would be to also release in the DEB format. I don't think this is a route we want to go down now but wanted to hear others' thoughts.\n5. Sean R. Owen: There are already Debian packages. I agree that you can find distros and RPMs easily these days. My feeling is that the build is very complex at this point and the real cost of attempting to maintain another packaging probably isn't justified.\n6. Andrew Ash: Sean are you suggesting dropping the .deb packages that Apache Spark releases as a simplification effort? It feels inequitable to support one packaging format (deb) but not the other (rpm).\n7. Sean R. Owen: Not suggesting that, no. I suppose it does depend on demand. Apparently there was enough of a need for a .deb package that it was contributed and maintained, but perhaps not for .rpm, because there are other sources? If there were demand it could be worth the cost.\n8. Christian Tzolov: I've looked at the JRPM maven plugin but unlike the jdeb one, JRPM depends on native rpm libraries (e.g. it is platform dependent) My somewhat pragmatic approach to build Spark RPM is to reuse the existing deb package and convert it into rpm using the 'alien' tool. I've wrapped the spark-rpm build pipeline into a parameterizable docker container: https://registry.hub.docker.com/u/tzolov/apache-spark-build-pipeline\n9. Sean R. Owen: Given SPARK-5727, I suggest this will also be WontFix, in favor of delegating this to projects like Bigtop.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.025583"}}
{"id": "9df5196b45df96eaa7a4ad1467055096", "issue_key": "SPARK-666", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Make Spark's master debug level logging consumable", "description": "Right now if you turn debug level logging on, the console gets flooded with debug messages that basically make it impossible to use the debug level.", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-01-24T19:03:16.000+0000", "updated": "2013-01-29T21:31:10.000+0000", "resolved": "2013-01-29T21:11:34.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-666\nSummary: Make Spark's master debug level logging consumable\nDescription: Right now if you turn debug level logging on, the console gets flooded with debug messages that basically make it impossible to use the debug level.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.025583"}}
{"id": "1281f58ced677395762cda80978162a7", "issue_key": "SPARK-667", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "IntelliJ may insert stubs for inherited methods in Java Function classes", "description": "When implementing Java API Function classes (e.g. PairFlatMapFunction) in IntelliJ, it looks like IntelliJ may auto-complete empty stubs for compose() and andThen() (here's an example: https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion). Default implementations of these are inherited from AbstractFunction1, so this shouldn't happen. Haven't investigated too much, but it looks like this may only happen when using the Java API in a pure-Java project that depends on the Spark JAR; this doesn't seem to happen in my Spark IntelliJ workspace, but that may be because it's a mixed Scala + Java project. This is a minor annoyance; maybe we can fix this by explicitly implementing compose() and andThen() in the public Function classes (just calling super in each implementation).", "reporter": "Josh Rosen", "assignee": null, "created": "2013-01-26T13:01:07.000+0000", "updated": "2013-10-22T14:17:43.000+0000", "resolved": "2013-10-22T14:17:43.000+0000", "labels": ["Starter"], "components": ["Java API"], "comments": [{"author": "Josh Rosen", "body": "Since I haven't been able to reproduce this, I'm going to close this issue. I'll chalk the old behavior up to an IntelliJ bug.", "created": "2013-10-22T14:17:43.600+0000"}], "num_comments": 1, "text": "Issue: SPARK-667\nSummary: IntelliJ may insert stubs for inherited methods in Java Function classes\nDescription: When implementing Java API Function classes (e.g. PairFlatMapFunction) in IntelliJ, it looks like IntelliJ may auto-complete empty stubs for compose() and andThen() (here's an example: https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion). Default implementations of these are inherited from AbstractFunction1, so this shouldn't happen. Haven't investigated too much, but it looks like this may only happen when using the Java API in a pure-Java project that depends on the Spark JAR; this doesn't seem to happen in my Spark IntelliJ workspace, but that may be because it's a mixed Scala + Java project. This is a minor annoyance; maybe we can fix this by explicitly implementing compose() and andThen() in the public Function classes (just calling super in each implementation).\n\nComments (1):\n1. Josh Rosen: Since I haven't been able to reproduce this, I'm going to close this issue. I'll chalk the old behavior up to an IntelliJ bug.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.026564"}}
{"id": "c3de54c37800cbf6f341973f06c9b607", "issue_key": "SPARK-668", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors", "description": "As described in https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion, calls to JavaRDdLike.flatMap(PairFlatMapFunction) may be falsely rejected by the compiler with \"cannot find symbol; method: flatMap\" errors. Here's a complete standalone example that reproduces the problem: https://gist.github.com/4640356 I tried implementing a similar example in pure-Java (no Spark code) and was able to get the proper typechecking, so I suspect that this might be a Scala compiler bug.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-01-26T14:40:56.000+0000", "updated": "2014-11-18T20:12:56.000+0000", "resolved": "2013-01-26T16:34:09.000+0000", "labels": [], "components": ["Java API"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/417", "created": "2013-01-26T16:34:09.631+0000"}, {"author": "Josh Rosen", "body": "For reference, I think that this was caused by https://issues.scala-lang.org/browse/SI-6057", "created": "2014-11-18T20:12:56.799+0000"}], "num_comments": 2, "text": "Issue: SPARK-668\nSummary: JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors\nDescription: As described in https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion, calls to JavaRDdLike.flatMap(PairFlatMapFunction) may be falsely rejected by the compiler with \"cannot find symbol; method: flatMap\" errors. Here's a complete standalone example that reproduces the problem: https://gist.github.com/4640356 I tried implementing a similar example in pure-Java (no Spark code) and was able to get the proper typechecking, so I suspect that this might be a Scala compiler bug.\n\nComments (2):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/417\n2. Josh Rosen: For reference, I think that this was caused by https://issues.scala-lang.org/browse/SI-6057", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.026564"}}
{"id": "05e92f285b1e3aaa38c05f4eddd7d62a", "issue_key": "SPARK-669", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Send back task results through BlockManager instead of Akka messages", "description": "A common problem for users is that their task results are multiple MB in size, and Akka cannot send messages larger than its frameSize. It would be better to avoid this altogether by sending them through the BlockManager. The driver would then delete the result from the remote node after it fetched it.", "reporter": "Matei Alexandru Zaharia", "assignee": "Kay Ousterhout", "created": "2013-01-27T22:49:41.000+0000", "updated": "2013-10-04T15:11:54.000+0000", "resolved": "2013-10-04T15:11:54.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Looks like this has affected some users: https://groups.google.com/d/msg/spark-users/WG87fG8rrKY/6VpJ1AFXS-cJ", "created": "2013-05-10T22:18:20.331+0000"}, {"author": "Reynold Xin", "body": "For small task results, it is better to piggyback the akka task status message. For large ones, better using our own layer.", "created": "2013-05-10T22:46:23.221+0000"}, {"author": "Patrick McFadin", "body": "Josh has a mock-up of this in a branch (borrowed from comment in SPARK-747). https://github.com/mesos/spark/pull/610 Also see his comment here: https://github.com/mesos/spark/pull/610#issuecomment-18021295", "created": "2013-07-30T14:35:26.383+0000"}, {"author": "Kay Ousterhout", "body": "Fixed by https://github.com/apache/incubator-spark/pull/10", "created": "2013-10-04T15:11:54.327+0000"}], "num_comments": 4, "text": "Issue: SPARK-669\nSummary: Send back task results through BlockManager instead of Akka messages\nDescription: A common problem for users is that their task results are multiple MB in size, and Akka cannot send messages larger than its frameSize. It would be better to avoid this altogether by sending them through the BlockManager. The driver would then delete the result from the remote node after it fetched it.\n\nComments (4):\n1. Josh Rosen: Looks like this has affected some users: https://groups.google.com/d/msg/spark-users/WG87fG8rrKY/6VpJ1AFXS-cJ\n2. Reynold Xin: For small task results, it is better to piggyback the akka task status message. For large ones, better using our own layer.\n3. Patrick McFadin: Josh has a mock-up of this in a branch (borrowed from comment in SPARK-747). https://github.com/mesos/spark/pull/610 Also see his comment here: https://github.com/mesos/spark/pull/610#issuecomment-18021295\n4. Kay Ousterhout: Fixed by https://github.com/apache/incubator-spark/pull/10", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.026564"}}
{"id": "9e8dc292ece7e808efccff39ba115621", "issue_key": "SPARK-670", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "spark-ec2 should warn if you use the \"start\" command without passing a SSH key file", "description": "Right now it doesn't, so you get an error saying \"Identity file None not accessible\" from SSH if you don't pass the -i option. The \"launch\" command should likewise check for this.", "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "created": "2013-01-30T11:07:14.000+0000", "updated": "2013-05-08T23:18:21.000+0000", "resolved": "2013-05-08T23:18:21.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/599", "created": "2013-05-08T23:18:21.190+0000"}], "num_comments": 1, "text": "Issue: SPARK-670\nSummary: spark-ec2 should warn if you use the \"start\" command without passing a SSH key file\nDescription: Right now it doesn't, so you get an error saying \"Identity file None not accessible\" from SSH if you don't pass the -i option. The \"launch\" command should likewise check for this.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/599", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.026564"}}
{"id": "438281436d7832d6720c7b111aa9cde4", "issue_key": "SPARK-671", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark runs out of memory on fork/exec (affects both pipes and python)", "description": "Because the JVM uses fork/exec to launch child processes, any child process initially has the memory footprint of its parent. In the case of a large Spark JVM that spawns many child processes (for Pipe or Python support), this quickly leads to kernel memory exhaustion. This problem is discussed here: https://gist.github.com/1970815 It results in errors like this:  13/01/31 20:18:48 INFO cluster.TaskSetManager: Loss was due to java.io.IOException: Cannot run program \"cat\": java.io.IOException: error=12, Cannot allocate memory at java.lang.ProcessBuilder.start(ProcessBuilder.java:475) at spark.rdd.PipedRDD.compute(PipedRDD.scala:38) at spark.RDD.computeOrReadCheckpoint(RDD.scala:203) at spark.RDD.iterator(RDD.scala:192) at spark.scheduler.ResultTask.run(ResultTask.scala:76)  I was able to workaround by allowing for memory over-commitment by the kernel on all slaves,  echo 1 > /proc/sys/vm/overcommit_memory  but we should try to include a more robust solution, such as the one here: https://github.com/axiak/java_posix_spawn", "reporter": "Patrick McFadin", "assignee": "Jey Kottalam", "created": "2013-01-31T12:52:26.000+0000", "updated": "2015-11-02T12:11:18.000+0000", "resolved": "2013-06-28T20:30:59.000+0000", "labels": [], "components": ["PySpark", "Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "You might want to make sure this still happens now that you fixed the bug which launches JVM's for every task.", "created": "2013-02-01T12:39:47.207+0000"}, {"author": "Patrick McFadin", "body": "Actually, since we saw this even with rdd.pipe(), probably it's still an issue.", "created": "2013-02-01T12:40:31.918+0000"}, {"author": "Josh Rosen", "body": "It seems like this is a JVM problem that only affects some platforms. It sounds like Jenkins and Hadoop don't work around this, so maybe a fix is out of scope for us. I propose that we add some configuration documentation on how to work around this issue (e.g. through overcommit_memory or adding extra virtual memory), then resolve this issue as \"Won't Fix.\"", "created": "2013-02-01T14:52:10.681+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Don't put a fix version on this since it's not yet fixed. You're supposed to only assign that once you fix it.", "created": "2013-02-14T23:32:33.121+0000"}, {"author": "Matei Alexandru Zaharia", "body": "And yes I agree that we probably don't want to work around this if Hadoop and Jenkins don't. We might just leave it as an open issue and document it in various places.", "created": "2013-02-14T23:33:11.722+0000"}, {"author": "Patrick McFadin", "body": "Matei - that's not how Fix versions are used in JIRA. Fix versions are for the version of the intended fix, regardless of whether the issue is completed, it says so clearly in the JIRA docs: https://confluence.atlassian.com/display/JIRA/What+is+an+Issue This is necessary for using \"Roadmap\" features of JIRA - to answer questions like: \"How many outstanding issues are there for 0.7\". That way people will actually know what remains before a release comes out and can track projects. The other projects (STREAMING, SHARK) also use fix versions like this pervasively.", "created": "2013-02-15T10:25:48.890+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Ah, I was actually going by what I saw happen in Hadoop. In this case though, let's not assign fix versions unless we actually make a roadmap where we agree we'll do this for a particular version. (At least for Spark.)", "created": "2013-02-15T21:04:17.393+0000"}, {"author": "Josh Rosen", "body": "This was fixed by Jey in https://github.com/mesos/spark/pull/563, which uses a separate process to fork PySpark's {{python}} processes.", "created": "2013-06-28T20:30:59.041+0000"}, {"author": "Josh Rosen", "body": "AFAIK, this has only been fixed for PySpark, not for general pipe() calls in Spark. If we still need to fix that, please re-open this issue (or open a new linked issue).", "created": "2013-06-28T20:32:29.869+0000"}, {"author": "hotdog", "body": "is it still a problem in latest version? I 'm using pipe() operation, and found that if I use pipe() before any shuffle task, the memory always grows very high. maybe the memory usage of sub process is not a constant volume ? is it affected by the memory of the parent process?", "created": "2015-11-02T12:11:18.180+0000"}], "num_comments": 10, "text": "Issue: SPARK-671\nSummary: Spark runs out of memory on fork/exec (affects both pipes and python)\nDescription: Because the JVM uses fork/exec to launch child processes, any child process initially has the memory footprint of its parent. In the case of a large Spark JVM that spawns many child processes (for Pipe or Python support), this quickly leads to kernel memory exhaustion. This problem is discussed here: https://gist.github.com/1970815 It results in errors like this:  13/01/31 20:18:48 INFO cluster.TaskSetManager: Loss was due to java.io.IOException: Cannot run program \"cat\": java.io.IOException: error=12, Cannot allocate memory at java.lang.ProcessBuilder.start(ProcessBuilder.java:475) at spark.rdd.PipedRDD.compute(PipedRDD.scala:38) at spark.RDD.computeOrReadCheckpoint(RDD.scala:203) at spark.RDD.iterator(RDD.scala:192) at spark.scheduler.ResultTask.run(ResultTask.scala:76)  I was able to workaround by allowing for memory over-commitment by the kernel on all slaves,  echo 1 > /proc/sys/vm/overcommit_memory  but we should try to include a more robust solution, such as the one here: https://github.com/axiak/java_posix_spawn\n\nComments (10):\n1. Patrick McFadin: You might want to make sure this still happens now that you fixed the bug which launches JVM's for every task.\n2. Patrick McFadin: Actually, since we saw this even with rdd.pipe(), probably it's still an issue.\n3. Josh Rosen: It seems like this is a JVM problem that only affects some platforms. It sounds like Jenkins and Hadoop don't work around this, so maybe a fix is out of scope for us. I propose that we add some configuration documentation on how to work around this issue (e.g. through overcommit_memory or adding extra virtual memory), then resolve this issue as \"Won't Fix.\"\n4. Matei Alexandru Zaharia: Don't put a fix version on this since it's not yet fixed. You're supposed to only assign that once you fix it.\n5. Matei Alexandru Zaharia: And yes I agree that we probably don't want to work around this if Hadoop and Jenkins don't. We might just leave it as an open issue and document it in various places.\n6. Patrick McFadin: Matei - that's not how Fix versions are used in JIRA. Fix versions are for the version of the intended fix, regardless of whether the issue is completed, it says so clearly in the JIRA docs: https://confluence.atlassian.com/display/JIRA/What+is+an+Issue This is necessary for using \"Roadmap\" features of JIRA - to answer questions like: \"How many outstanding issues are there for 0.7\". That way people will actually know what remains before a release comes out and can track projects. The other projects (STREAMING, SHARK) also use fix versions like this pervasively.\n7. Matei Alexandru Zaharia: Ah, I was actually going by what I saw happen in Hadoop. In this case though, let's not assign fix versions unless we actually make a roadmap where we agree we'll do this for a particular version. (At least for Spark.)\n8. Josh Rosen: This was fixed by Jey in https://github.com/mesos/spark/pull/563, which uses a separate process to fork PySpark's {{python}} processes.\n9. Josh Rosen: AFAIK, this has only been fixed for PySpark, not for general pipe() calls in Spark. If we still need to fix that, please re-open this issue (or open a new linked issue).\n10. hotdog: is it still a problem in latest version? I 'm using pipe() operation, and found that if I use pipe() before any shuffle task, the memory always grows very high. maybe the memory usage of sub process is not a constant volume ? is it affected by the memory of the parent process?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.026564"}}
{"id": "bde379742ad57929bd28f74c4d0f6b55", "issue_key": "SPARK-672", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Executor gets stuck in a \"zombie\" state after running out of memory", "description": "As a result of running a workload, an executor ran out of memory, but the executor process stayed up. Also (not sure this is related) the standalone worker process stayed up but disappeared from the master web UI.", "reporter": "Mikhail Bautin", "assignee": null, "created": "2013-01-31T12:54:32.000+0000", "updated": "2015-02-08T13:04:48.000+0000", "resolved": "2015-02-08T13:04:47.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "[~mbautin] when an executor JVM is under extremely heavy GC load, it will often lock up, not even responding to normal kill commands and requiring a kill -9 to shut down. I agree that Spark could behave better in these situations. What is your preference for handling these issues -- maybe give the executor a timeout and kill -9 it after a certain period of time of non-responsiveness?", "created": "2014-11-14T08:58:10.240+0000"}, {"author": "Sean R. Owen", "body": "The right-er answer is to fail for lack of memory faster, per SPARK-1989.", "created": "2015-02-08T13:04:48.027+0000"}], "num_comments": 2, "text": "Issue: SPARK-672\nSummary: Executor gets stuck in a \"zombie\" state after running out of memory\nDescription: As a result of running a workload, an executor ran out of memory, but the executor process stayed up. Also (not sure this is related) the standalone worker process stayed up but disappeared from the master web UI.\n\nComments (2):\n1. Andrew Ash: [~mbautin] when an executor JVM is under extremely heavy GC load, it will often lock up, not even responding to normal kill commands and requiring a kill -9 to shut down. I agree that Spark could behave better in these situations. What is your preference for handling these issues -- maybe give the executor a timeout and kill -9 it after a certain period of time of non-responsiveness?\n2. Sean R. Owen: The right-er answer is to fail for lack of memory faster, per SPARK-1989.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.027595"}}
{"id": "0cd152ea0c000ad894534269380938eb", "issue_key": "SPARK-673", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark should capture and re-throw Python exceptions", "description": "Right now if there is an exception inside of a PySpark worker, it causes the worker process to exit prematurely, triggering an EOF exception at the JVM worker. This means you have to go dig through worker logs to find the exception trace. It would be more helpful if the Python worker instead caught the exception and passed the string representation of the exception to the JVM worker, which could then wrap it in a Java exception. e.g.  throw new PythonException(exnString).  This would make it much easier to debug python tasks, since that string would show up at the driver.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-01-31T14:18:48.000+0000", "updated": "2013-02-01T00:34:37.000+0000", "resolved": "2013-02-01T00:34:37.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Added in https://github.com/mesos/spark/pull/434", "created": "2013-02-01T00:34:37.716+0000"}], "num_comments": 1, "text": "Issue: SPARK-673\nSummary: PySpark should capture and re-throw Python exceptions\nDescription: Right now if there is an exception inside of a PySpark worker, it causes the worker process to exit prematurely, triggering an EOF exception at the JVM worker. This means you have to go dig through worker logs to find the exception trace. It would be more helpful if the Python worker instead caught the exception and passed the string representation of the exception to the JVM worker, which could then wrap it in a Java exception. e.g.  throw new PythonException(exnString).  This would make it much easier to debug python tasks, since that string would show up at the driver.\n\nComments (1):\n1. Josh Rosen: Added in https://github.com/mesos/spark/pull/434", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.027595"}}
{"id": "e946d9a8e0e383e9d5e797475ecc3072", "issue_key": "SPARK-674", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Gateway JVM's should not be launched on slave", "description": "The slaves seem to launch a new JVM for each task (to run the Gateway Java program). This is a bug since that program is only needed on the driver. It causes increased latency for tasks - due to JVM launch - and also memory pressure, since the gateway asks for SPARK_MEM memory on launch.", "reporter": "Patrick McFadin", "assignee": "Josh Rosen", "created": "2013-01-31T14:22:52.000+0000", "updated": "2013-02-01T11:45:24.000+0000", "resolved": "2013-02-01T11:45:24.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/438", "created": "2013-02-01T11:45:24.546+0000"}], "num_comments": 1, "text": "Issue: SPARK-674\nSummary: Gateway JVM's should not be launched on slave\nDescription: The slaves seem to launch a new JVM for each task (to run the Gateway Java program). This is a bug since that program is only needed on the driver. It causes increased latency for tasks - due to JVM launch - and also memory pressure, since the gateway asks for SPARK_MEM memory on launch.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/438", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028562"}}
{"id": "e169ffde0d2d0b0043b10d451637cd12", "issue_key": "SPARK-675", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Gateway JVM should ask for less than SPARK_MEM memory", "description": "This is not so big of a deal assuming that we fix SPARK-674, but it would be nice if the gateway JVM asked for less than SPARK_MEM amount of memory. This might require decoupling the class-path component of \"run.sh\" so it can be used independently.", "reporter": "Patrick McFadin", "assignee": "Josh Rosen", "created": "2013-01-31T14:24:21.000+0000", "updated": "2014-09-08T16:45:10.000+0000", "resolved": "2014-09-08T16:45:10.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "This might be possible to do now that we have `compute-classpath.sh` in Spark 0.8. https://groups.google.com/d/msg/spark-users/yPYEGk2g9Ug/ox1IHHCk2CMJ suggests that maybe we should have a separate setting for controlling the gateway's memory usage, e.g. PYSPARK_GATEWAY_MEM or something like that.", "created": "2013-07-28T21:33:19.392+0000"}, {"author": "Matthew Farrellee", "body": "[~joshrosen] it looks like SPARK-674 was resolved, do you think this is still an issue or can it be closed?", "created": "2014-09-07T14:44:44.736+0000"}, {"author": "Josh Rosen", "body": "Thanks for the reminder. I'm going to close this since it only affected a very old version of Spark and most code related to this has been significantly changed (for example, I don't think we have SPARK_MEM anymore, and I think the gateway JVM's heap size is controlled through spark-submit settings (see PYSPARK_SUBMIT_ARGS)).", "created": "2014-09-08T16:45:10.964+0000"}], "num_comments": 3, "text": "Issue: SPARK-675\nSummary: Gateway JVM should ask for less than SPARK_MEM memory\nDescription: This is not so big of a deal assuming that we fix SPARK-674, but it would be nice if the gateway JVM asked for less than SPARK_MEM amount of memory. This might require decoupling the class-path component of \"run.sh\" so it can be used independently.\n\nComments (3):\n1. Josh Rosen: This might be possible to do now that we have `compute-classpath.sh` in Spark 0.8. https://groups.google.com/d/msg/spark-users/yPYEGk2g9Ug/ox1IHHCk2CMJ suggests that maybe we should have a separate setting for controlling the gateway's memory usage, e.g. PYSPARK_GATEWAY_MEM or something like that.\n2. Matthew Farrellee: [~joshrosen] it looks like SPARK-674 was resolved, do you think this is still an issue or can it be closed?\n3. Josh Rosen: Thanks for the reminder. I'm going to close this since it only affected a very old version of Spark and most code related to this has been significantly changed (for example, I don't think we have SPARK_MEM anymore, and I think the gateway JVM's heap size is controlled through spark-submit settings (see PYSPARK_SUBMIT_ARGS)).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "064c189d8952adac94037b9082db3c18", "issue_key": "SPARK-676", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Abbreviation in SPARK_MEM but not in SPARK_WORKER_MEMORY", "description": "{{SPARK_MEM}} and {{SPARK_WORKER_MEMORY}} are inconsistent in how they abbreviate \"memory\". This is a potential source of typos.", "reporter": "Josh Rosen", "assignee": "Mark Grover", "created": "2013-02-02T16:33:27.000+0000", "updated": "2013-10-20T14:26:21.000+0000", "resolved": "2013-10-20T14:26:21.000+0000", "labels": [], "components": [], "comments": [{"author": "SeanM", "body": "Is there a nomenclature preference? Should we stick with MEM and change SPARK_WORKER_MEMORY -> SPARK_WORKER_MEM then?", "created": "2013-08-15T20:16:27.004+0000"}, {"author": "Hilfi Madari Alkaff", "body": "Is this still open (And is the above comment the desired solution)?", "created": "2013-10-05T15:35:08.777+0000"}, {"author": "Mark Grover", "body": "I was working on this last weekend but forgot to assign it to myself, sorry! Anyways, I have created a pull request for doing this in a backwards compatible way: https://github.com/apache/incubator-spark/pull/48", "created": "2013-10-09T12:49:00.532+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I've created SPARK-929 to track deprecating SPARK_MEM", "created": "2013-10-09T19:35:05.897+0000"}, {"author": "Mark Grover", "body": "Thanks! This JIRA, in my opinion, should be marked as Won't Fix (see pull request for more details). Perhaps, I need to be given some JIRA karma because I can't figure out a way to resolve it.", "created": "2013-10-09T19:59:56.273+0000"}], "num_comments": 5, "text": "Issue: SPARK-676\nSummary: Abbreviation in SPARK_MEM but not in SPARK_WORKER_MEMORY\nDescription: {{SPARK_MEM}} and {{SPARK_WORKER_MEMORY}} are inconsistent in how they abbreviate \"memory\". This is a potential source of typos.\n\nComments (5):\n1. SeanM: Is there a nomenclature preference? Should we stick with MEM and change SPARK_WORKER_MEMORY -> SPARK_WORKER_MEM then?\n2. Hilfi Madari Alkaff: Is this still open (And is the above comment the desired solution)?\n3. Mark Grover: I was working on this last weekend but forgot to assign it to myself, sorry! Anyways, I have created a pull request for doing this in a backwards compatible way: https://github.com/apache/incubator-spark/pull/48\n4. Matei Alexandru Zaharia: I've created SPARK-929 to track deprecating SPARK_MEM\n5. Mark Grover: Thanks! This JIRA, in my opinion, should be marked as Won't Fix (see pull request for more details). Perhaps, I need to be given some JIRA karma because I can't figure out a way to resolve it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "9bf7771f980a6c5d60881bd95bafd178", "issue_key": "SPARK-677", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark should not collect results through local filesystem", "description": "Py4J is slow when transferring large arrays, so PySpark currently dumps data to the disk and reads it back in order to collect() RDDs. On large enough datasets, this data will spill from the buffer cache and write to the physical disk, resulting in terrible performance. Instead, we should stream the data from Java to Python over a local socket or a FIFO.", "reporter": "Josh Rosen", "assignee": "Davies Liu", "created": "2013-02-02T17:28:30.000+0000", "updated": "2015-05-22T20:40:07.000+0000", "resolved": "2015-05-22T20:40:07.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Peter Aberline", "body": "Perhaps a ZeroMQ IPC socket between python and JVM might be might be useful here? This is at the cost of adding a dependency to ZeroMQ of course, though.", "created": "2013-07-09T11:26:38.529+0000"}, {"author": "Matei Alexandru Zaharia", "body": "A local socket is probably fine, as long as we turn it into a file object by getting its file descriptor, as we did in the communication from workers to the Java process. The built-in socket.makefile in Python results in a very slow file object.", "created": "2013-08-06T23:23:47.584+0000"}, {"author": "Matthew Farrellee", "body": "this can also be used to address the fragile nature of py4j connection construction. the parent can create the fifo.", "created": "2014-07-18T15:46:40.876+0000"}, {"author": "Matei Alexandru Zaharia", "body": "[~joshrosen] is this fixed now?", "created": "2014-11-06T17:34:44.030+0000"}, {"author": "Josh Rosen", "body": "No, it's still an issue in 1.2.0:  def collect(self): \"\"\" Return a list that contains all of the elements in this RDD. \"\"\" with SCCallSiteSync(self.context) as css: bytesInJava = self._jrdd.collect().iterator() return list(self._collect_iterator_through_file(bytesInJava)) def _collect_iterator_through_file(self, iterator): # Transferring lots of data through Py4J can be slow because # socket.readline() is inefficient. Instead, we'll dump the data to a # file and read it back. tempFile = NamedTemporaryFile(delete=False, dir=self.ctx._temp_dir) tempFile.close() self.ctx._writeToFile(iterator, tempFile.name) # Read the data into Python and deserialize it: with open(tempFile.name, 'rb') as tempFile: for item in self._jrdd_deserializer.load_stream(tempFile): yield item os.unlink(tempFile.name)", "created": "2014-11-06T19:12:33.072+0000"}, {"author": "Apache Spark", "body": "User 'davies' has created a pull request for this issue: https://github.com/apache/spark/pull/4923", "created": "2015-03-09T21:27:46.582+0000"}, {"author": "Josh Rosen", "body": "This was fixed for 1.3.1, 1.2.2, and 1.4.0. I don't think that we'l do a 1.1.x backport, so I'm going to mark this as resolved.", "created": "2015-05-22T20:40:07.620+0000"}], "num_comments": 7, "text": "Issue: SPARK-677\nSummary: PySpark should not collect results through local filesystem\nDescription: Py4J is slow when transferring large arrays, so PySpark currently dumps data to the disk and reads it back in order to collect() RDDs. On large enough datasets, this data will spill from the buffer cache and write to the physical disk, resulting in terrible performance. Instead, we should stream the data from Java to Python over a local socket or a FIFO.\n\nComments (7):\n1. Peter Aberline: Perhaps a ZeroMQ IPC socket between python and JVM might be might be useful here? This is at the cost of adding a dependency to ZeroMQ of course, though.\n2. Matei Alexandru Zaharia: A local socket is probably fine, as long as we turn it into a file object by getting its file descriptor, as we did in the communication from workers to the Java process. The built-in socket.makefile in Python results in a very slow file object.\n3. Matthew Farrellee: this can also be used to address the fragile nature of py4j connection construction. the parent can create the fifo.\n4. Matei Alexandru Zaharia: [~joshrosen] is this fixed now?\n5. Josh Rosen: No, it's still an issue in 1.2.0:  def collect(self): \"\"\" Return a list that contains all of the elements in this RDD. \"\"\" with SCCallSiteSync(self.context) as css: bytesInJava = self._jrdd.collect().iterator() return list(self._collect_iterator_through_file(bytesInJava)) def _collect_iterator_through_file(self, iterator): # Transferring lots of data through Py4J can be slow because # socket.readline() is inefficient. Instead, we'll dump the data to a # file and read it back. tempFile = NamedTemporaryFile(delete=False, dir=self.ctx._temp_dir) tempFile.close() self.ctx._writeToFile(iterator, tempFile.name) # Read the data into Python and deserialize it: with open(tempFile.name, 'rb') as tempFile: for item in self._jrdd_deserializer.load_stream(tempFile): yield item os.unlink(tempFile.name)\n6. Apache Spark: User 'davies' has created a pull request for this issue: https://github.com/apache/spark/pull/4923\n7. Josh Rosen: This was fixed for 1.3.1, 1.2.2, and 1.4.0. I don't think that we'l do a 1.1.x backport, so I'm going to mark this as resolved.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "4fd93f1b72957485dee9b8c31687d1c0", "issue_key": "SPARK-678", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add an example that does a roll-up on log data", "description": "People don't always understand how you can aggregate multiple statistics in parallel with Spark. Since it's such a common thing, would be good to have an example in the examples directory.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-02-04T11:27:00.000+0000", "updated": "2013-02-12T14:00:11.000+0000", "resolved": "2013-02-12T14:00:11.000+0000", "labels": [], "components": [], "comments": [{"author": "Nicholas Pentreath", "body": "I'm happy to work up an example if someone can point me to a small sample dataset for log data?", "created": "2013-02-07T07:00:55.051+0000"}, {"author": "Nicholas Pentreath", "body": "Oh I see you already did it in 446", "created": "2013-02-07T07:12:07.591+0000"}], "num_comments": 2, "text": "Issue: SPARK-678\nSummary: Add an example that does a roll-up on log data\nDescription: People don't always understand how you can aggregate multiple statistics in parallel with Spark. Since it's such a common thing, would be good to have an example in the examples directory.\n\nComments (2):\n1. Nicholas Pentreath: I'm happy to work up an example if someone can point me to a small sample dataset for log data?\n2. Nicholas Pentreath: Oh I see you already did it in 446", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "642b93ebd8d9e7a3e871378976f847df", "issue_key": "SPARK-679", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Have a DSL or other language support for OLAP expressions", "description": "One common use of Spark is to do roll-ups that collect multiple statistics in parallel. Right now, this requires writing a custom aggregator do do things like mean/min/max/percentiles/total/etc. It wouldn't be hard to write some language support for this. This needs some more thought, but something like this would be cool:  rdd.stats(average(.numPosts), percentile(95)(.size), …)  Sort of a scala version of MDX. This avoids people having to re-invent the wheel with custom aggregators all the time.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-02-04T11:35:59.000+0000", "updated": "2014-03-25T15:09:55.000+0000", "resolved": "2014-03-25T15:09:55.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "I like this idea!", "created": "2013-02-04T11:38:38.341+0000"}, {"author": "Patrick McFadin", "body": "Fixed by SPARK-1251... many years later.", "created": "2014-03-25T15:09:55.767+0000"}], "num_comments": 2, "text": "Issue: SPARK-679\nSummary: Have a DSL or other language support for OLAP expressions\nDescription: One common use of Spark is to do roll-ups that collect multiple statistics in parallel. Right now, this requires writing a custom aggregator do do things like mean/min/max/percentiles/total/etc. It wouldn't be hard to write some language support for this. This needs some more thought, but something like this would be cool:  rdd.stats(average(.numPosts), percentile(95)(.size), …)  Sort of a scala version of MDX. This avoids people having to re-invent the wheel with custom aggregators all the time.\n\nComments (2):\n1. Reynold Xin: I like this idea!\n2. Patrick McFadin: Fixed by SPARK-1251... many years later.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "6c5178ee7788c4c7c25d59dee0354b52", "issue_key": "SPARK-680", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "broadcast hangs spark cluster", "description": "Reported by Netease. Running logistics regression, in which the master repeatedly broadcast 30M of data to 100 slaves will hang the cluster.", "reporter": "Reynold Xin", "assignee": "Mosharaf Chowdhury", "created": "2013-02-04T13:39:59.000+0000", "updated": "2014-02-10T12:41:10.000+0000", "resolved": "2014-02-10T12:41:10.000+0000", "labels": [], "components": [], "comments": [{"author": "Mosharaf Chowdhury", "body": "Couldn't reproduce the bug while transferring the same amount of data hundreds of times. Contacted Netease; waiting for updates.", "created": "2013-02-14T23:38:07.262+0000"}, {"author": "Mosharaf Chowdhury", "body": "Never heard back from Netease regarding this issue. Current broadcast implementations should be easy able to handle the scenarios they mentioned. Closing this issue.", "created": "2014-02-10T12:41:00.631+0000"}], "num_comments": 2, "text": "Issue: SPARK-680\nSummary: broadcast hangs spark cluster\nDescription: Reported by Netease. Running logistics regression, in which the master repeatedly broadcast 30M of data to 100 slaves will hang the cluster.\n\nComments (2):\n1. Mosharaf Chowdhury: Couldn't reproduce the bug while transferring the same amount of data hundreds of times. Contacted Netease; waiting for updates.\n2. Mosharaf Chowdhury: Never heard back from Netease regarding this issue. Current broadcast implementations should be easy able to handle the scenarios they mentioned. Closing this issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "791efdbe6dda45f0a4b08f1ad52bd377", "issue_key": "SPARK-681", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Optimize hashtables used in Spark", "description": "The hash tables used in cogroup, join, etc take up a lot more space than they need to because they're using linked data structures. It would be nice to write a custom open hashtable class to use instead, especially since these tables are \"append-only\". A custom one would likely run better than fastutil as well.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-02-04T14:51:05.000+0000", "updated": "2014-11-06T17:35:01.000+0000", "resolved": "2014-11-06T17:35:01.000+0000", "labels": [], "components": [], "comments": [{"author": "Evan Chan", "body": "I see that fastutil's Object2LongOpenHashMap is used everywhere. Is this the data structure you want to replace? This is an open-addressing hash table, so it's not using linked data structures, but rather an array whose size is preset and grows 2x every time it gets kind of full. Is it possible that the initial size chosen is too large (see SHARK-27)?", "created": "2014-02-06T15:23:49.551+0000"}, {"author": "Reynold Xin", "body": "I think Matei was referring to the hash map (java.util.HashMap) used in aggregation and cogroup.", "created": "2014-02-06T15:40:37.484+0000"}, {"author": "Evan Chan", "body": "Ok. How about we just replace java.util.HashMap with the one from fastutil? I honestly don't think our own implementation would be much faster (what makes the append-case expensive for an open hash map is growing the size of the backing array, which is unavoidable) other than that fastutil is a huge unwieldy dependency.", "created": "2014-02-13T15:17:29.257+0000"}, {"author": "Reynold Xin", "body": "Actually there are multiple benefits. 1. As you mentioned, fastutil is a huge dependency. 2. Our hashmap allows us to find and update within a single hash lookup. 3. It can be specialized using Scala specialization, whereas fastutil would require writing separate Scala/Java code to use. 4. Some other perf gains from not having to deal with deletes.", "created": "2014-02-13T16:29:47.166+0000"}, {"author": "Aaron Davidson", "body": "External sorting has gotten its grubby paws on our new AppendOnlyMap in order to efficiently track the size of the cogroup and aggregation tables as they grow and to sort without using extra memory. It would be difficult (and performance-impacting) to remove it now, at least for the case where external sorting is enabled.", "created": "2014-02-14T10:06:55.462+0000"}], "num_comments": 5, "text": "Issue: SPARK-681\nSummary: Optimize hashtables used in Spark\nDescription: The hash tables used in cogroup, join, etc take up a lot more space than they need to because they're using linked data structures. It would be nice to write a custom open hashtable class to use instead, especially since these tables are \"append-only\". A custom one would likely run better than fastutil as well.\n\nComments (5):\n1. Evan Chan: I see that fastutil's Object2LongOpenHashMap is used everywhere. Is this the data structure you want to replace? This is an open-addressing hash table, so it's not using linked data structures, but rather an array whose size is preset and grows 2x every time it gets kind of full. Is it possible that the initial size chosen is too large (see SHARK-27)?\n2. Reynold Xin: I think Matei was referring to the hash map (java.util.HashMap) used in aggregation and cogroup.\n3. Evan Chan: Ok. How about we just replace java.util.HashMap with the one from fastutil? I honestly don't think our own implementation would be much faster (what makes the append-case expensive for an open hash map is growing the size of the backing array, which is unavoidable) other than that fastutil is a huge unwieldy dependency.\n4. Reynold Xin: Actually there are multiple benefits. 1. As you mentioned, fastutil is a huge dependency. 2. Our hashmap allows us to find and update within a single hash lookup. 3. It can be specialized using Scala specialization, whereas fastutil would require writing separate Scala/Java code to use. 4. Some other perf gains from not having to deal with deletes.\n5. Aaron Davidson: External sorting has gotten its grubby paws on our new AppendOnlyMap in order to efficiently track the size of the cogroup and aggregation tables as they grow and to sort without using extra memory. It would be difficult (and performance-impacting) to remove it now, at least for the case where external sorting is enabled.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "96e2452672f86af3a7c8b20f8e82c211", "issue_key": "SPARK-682", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Memoize results of getPreferredLocations", "description": "In certain lineage graphs, DAGScheduler.getPreferredLocations might explore an RDD multiple times if there are multiple paths from it to the root of a job, causing potentially exponential blowup.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-02-04T14:52:17.000+0000", "updated": "2014-11-06T07:02:13.000+0000", "resolved": "2014-11-06T07:02:13.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-682\nSummary: Memoize results of getPreferredLocations\nDescription: In certain lineage graphs, DAGScheduler.getPreferredLocations might explore an RDD multiple times if there are multiple paths from it to the root of a job, causing potentially exponential blowup.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "96f4217a807434b6c0b6a87fc88e25d2", "issue_key": "SPARK-683", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation", "description": "A simple saveAsObjectFile() leads to the following error. org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, boolean, boolean, short, long) at java.lang.Class.getMethod(Class.java:1622) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:416) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)", "reporter": "Tathagata Das", "assignee": null, "created": "2013-02-04T17:13:37.000+0000", "updated": "2014-10-29T09:17:12.000+0000", "resolved": "2014-09-11T08:59:09.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Patrick McFadin", "body": "I think the default hadoop version might have changed in the build. Are you sure you compiled spark with the same hadoop version as is on the AMI?", "created": "2013-02-04T20:37:09.789+0000"}, {"author": "Shivaram Venkataraman", "body": "That was the problem. We changed the default hadoop version to 1.0 in 0.7.0 -- We should either change the AMI to run HDFS v1.0 or change Spark on the AMI to make sure users don't run into this.", "created": "2013-02-04T20:42:51.310+0000"}, {"author": "Patrick McFadin", "body": "Ya I ran into this testing streaming code. Probably same as TD.", "created": "2013-02-04T20:51:04.732+0000"}, {"author": "Shivaram Venkataraman", "body": "Hopefully this will help whoever makes the AMI for 0.7. I tried setting up Hadoop 1.0.3 on the existing AMI and the configuration we have right now works fine out of the box. All I had to do was: wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.3/hadoop-1.0.3.tar.gz tar -xf hadoop-1.0.3.tar.gz # Copy conf files from existing hdfs setup", "created": "2013-02-21T15:07:18.950+0000"}, {"author": "Sean R. Owen", "body": "I think this is likely long since obsolete or fixed, since Spark, Hadoop and AMI Hadoop versions have moved forward, and have not heard of this issue in recent memory.", "created": "2014-09-11T08:59:09.954+0000"}, {"author": "Sean R. Owen", "body": "PS I think this also turns out to be the same as SPARK-4078", "created": "2014-10-29T09:17:12.673+0000"}], "num_comments": 6, "text": "Issue: SPARK-683\nSummary: Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation\nDescription: A simple saveAsObjectFile() leads to the following error. org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, boolean, boolean, short, long) at java.lang.Class.getMethod(Class.java:1622) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:416) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\nComments (6):\n1. Patrick McFadin: I think the default hadoop version might have changed in the build. Are you sure you compiled spark with the same hadoop version as is on the AMI?\n2. Shivaram Venkataraman: That was the problem. We changed the default hadoop version to 1.0 in 0.7.0 -- We should either change the AMI to run HDFS v1.0 or change Spark on the AMI to make sure users don't run into this.\n3. Patrick McFadin: Ya I ran into this testing streaming code. Probably same as TD.\n4. Shivaram Venkataraman: Hopefully this will help whoever makes the AMI for 0.7. I tried setting up Hadoop 1.0.3 on the existing AMI and the configuration we have right now works fine out of the box. All I had to do was: wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.3/hadoop-1.0.3.tar.gz tar -xf hadoop-1.0.3.tar.gz # Copy conf files from existing hdfs setup\n5. Sean R. Owen: I think this is likely long since obsolete or fixed, since Spark, Hadoop and AMI Hadoop versions have moved forward, and have not heard of this issue in recent memory.\n6. Sean R. Owen: PS I think this also turns out to be the same as SPARK-4078", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "641ca0dc399532346f95cb48e39e1f2f", "issue_key": "SPARK-684", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Move downloads links on Spark website away from GitHub", "description": "GitHub is disabling its Downloads feature at the end of February, so we need to move them to our own server.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-02-05T12:30:08.000+0000", "updated": "2013-04-01T22:12:55.000+0000", "resolved": "2013-04-01T22:12:55.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Looks like this was fixed.", "created": "2013-04-01T22:12:55.484+0000"}], "num_comments": 1, "text": "Issue: SPARK-684\nSummary: Move downloads links on Spark website away from GitHub\nDescription: GitHub is disabling its Downloads feature at the end of February, so we need to move them to our own server.\n\nComments (1):\n1. Josh Rosen: Looks like this was fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "5e50e01e387c26b1325bb0bfbe24728d", "issue_key": "SPARK-685", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add an environment variable to launch PySpark with ipython", "description": "Right now users can do PYSPARK_PYTHON=\"ipython\" ./pyspark -i $SPARK_HOME/python/pyspark/shell.py But it's annoying.", "reporter": "Matei Alexandru Zaharia", "assignee": "Nicholas Pentreath", "created": "2013-02-05T15:21:38.000+0000", "updated": "2013-02-07T20:43:30.000+0000", "resolved": "2013-02-07T20:43:30.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Nicholas Pentreath", "body": "Opened PR for this: https://github.com/mesos/spark/pull/454 Usage: \"IPYTHON=1 ./pyspark\"", "created": "2013-02-07T06:56:59.058+0000"}], "num_comments": 1, "text": "Issue: SPARK-685\nSummary: Add an environment variable to launch PySpark with ipython\nDescription: Right now users can do PYSPARK_PYTHON=\"ipython\" ./pyspark -i $SPARK_HOME/python/pyspark/shell.py But it's annoying.\n\nComments (1):\n1. Nicholas Pentreath: Opened PR for this: https://github.com/mesos/spark/pull/454 Usage: \"IPYTHON=1 ./pyspark\"", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "8037a6946176b42c0f5f1accc5149408", "issue_key": "SPARK-686", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Port FT heartbeat and fixes from 0.6 branch to master", "description": "Commits https://github.com/mesos/spark/commit/4b53f145b49fd8228129edef9a1f4f3f3488b865 through https://github.com/mesos/spark/commit/f886b42cecc8097ab33b2ba5ac39445c103a9ba7 on branch 0.6 fix a few issues with fault tolerance, including detecting \"hard crashes\" of nodes faster than a TCP timeout in the standalone cluster, and properly removing block locations for failed nodes. These need to be ported to master, which change the code from using slave IDs to executor IDs.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-02-06T14:02:48.000+0000", "updated": "2013-12-07T14:14:25.000+0000", "resolved": "2013-12-07T14:14:25.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-686\nSummary: Port FT heartbeat and fixes from 0.6 branch to master\nDescription: Commits https://github.com/mesos/spark/commit/4b53f145b49fd8228129edef9a1f4f3f3488b865 through https://github.com/mesos/spark/commit/f886b42cecc8097ab33b2ba5ac39445c103a9ba7 on branch 0.6 fix a few issues with fault tolerance, including detecting \"hard crashes\" of nodes faster than a TCP timeout in the standalone cluster, and properly removing block locations for failed nodes. These need to be ported to master, which change the code from using slave IDs to executor IDs.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "62d8ab915ab5d42924eea2806428a39b", "issue_key": "SPARK-687", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Use separate SPARK_DAEMON_MEMORY setting in Windows run script too", "description": "Commit https://github.com/mesos/spark/commit/c0d2ea111c17d9dde579c1b3bd79e5a8098f011a switches the standalone worker and master to use a different environment variable for their memory, since allocating them a huge amount of memory is a common pitfall. The Windows run script needs to receive the corresponding changes.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-02-06T14:40:23.000+0000", "updated": "2013-02-07T21:51:01.000+0000", "resolved": "2013-02-07T21:51:01.000+0000", "labels": [], "components": ["Windows"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-687\nSummary: Use separate SPARK_DAEMON_MEMORY setting in Windows run script too\nDescription: Commit https://github.com/mesos/spark/commit/c0d2ea111c17d9dde579c1b3bd79e5a8098f011a switches the standalone worker and master to use a different environment variable for their memory, since allocating them a huge amount of memory is a common pitfall. The Windows run script needs to receive the corresponding changes.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "d0fa935ba9439b9b0ad976ef3f56bb79", "issue_key": "SPARK-688", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Task crashed when I do spark stress test", "description": "My spark test codes shows as following val data = spark.textFile(\"hdfs://...\") val data_map = Data.groupby(3).map{line => Thread.sleep(20000) //20s ...... } data_Map.saveAsTextFile(\"hdfs://....\") I run above application in standalone mode ,while task sleeps, I choose one worker node and kill StandaloneExecutorBackend daemon, then tasks will report crash stack as: 13/02/06 10:03:35 ERROR Executor: Exception in task ID 7 java.lang.NullPointerException at spark.MapOutputTracker$$anonfun$getServerStatuses$12.apply(MapOutputTracker.scala:179) at spark.MapOutputTracker$$anonfun$getServerStatuses$12.apply(MapOutputTracker.scala:178) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:38) at spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:178) at spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:20) at spark.rdd.ShuffledAggregatedRDD.compute(ShuffledRDD.scala:118) at spark.RDD.iterator(RDD.scala:164) at spark.rdd.MappedRDD.compute(MappedRDD.scala:15) at spark.RDD.iterator(RDD.scala:164) at spark.scheduler.ResultTask.run(ResultTask.scala:19) at spark.executor.Executor$TaskRunner.run(Executor.scala:87) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:722)", "reporter": "xiajunluan", "assignee": null, "created": "2013-02-06T19:04:42.000+0000", "updated": "2020-05-17T18:30:14.000+0000", "resolved": "2013-12-07T14:15:16.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I believe we fixed this issue in 0.6.2. Can you try it out? We just released it today.", "created": "2013-02-07T21:50:13.151+0000"}, {"author": "xiajunluan", "body": "sure, I will try it in latest spark. thanks.", "created": "2013-02-19T18:42:49.757+0000"}], "num_comments": 2, "text": "Issue: SPARK-688\nSummary: Task crashed when I do spark stress test\nDescription: My spark test codes shows as following val data = spark.textFile(\"hdfs://...\") val data_map = Data.groupby(3).map{line => Thread.sleep(20000) //20s ...... } data_Map.saveAsTextFile(\"hdfs://....\") I run above application in standalone mode ,while task sleeps, I choose one worker node and kill StandaloneExecutorBackend daemon, then tasks will report crash stack as: 13/02/06 10:03:35 ERROR Executor: Exception in task ID 7 java.lang.NullPointerException at spark.MapOutputTracker$$anonfun$getServerStatuses$12.apply(MapOutputTracker.scala:179) at spark.MapOutputTracker$$anonfun$getServerStatuses$12.apply(MapOutputTracker.scala:178) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:38) at spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:178) at spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:20) at spark.rdd.ShuffledAggregatedRDD.compute(ShuffledRDD.scala:118) at spark.RDD.iterator(RDD.scala:164) at spark.rdd.MappedRDD.compute(MappedRDD.scala:15) at spark.RDD.iterator(RDD.scala:164) at spark.scheduler.ResultTask.run(ResultTask.scala:19) at spark.executor.Executor$TaskRunner.run(Executor.scala:87) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:722)\n\nComments (2):\n1. Matei Alexandru Zaharia: I believe we fixed this issue in 0.6.2. Can you try it out? We just released it today.\n2. xiajunluan: sure, I will try it in latest spark. thanks.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "9671099e585ad2b7596568519240e187", "issue_key": "SPARK-689", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Task will crash when setting SPARK_WORKER_CORES> 128", "description": "when I set SPARK_WORKER_CORES > 128(for example 200), and run a job in standalone mode that will allocate 200 tasks in one worker node, then task will crash(it seems that worker cores has been hard-code)  13/02/07 11:25:02 ERROR StandaloneExecutorBackend: Task spark.executor.Executor$TaskRunner@5367839e rejected from java.util.concurrent.ThreadPoolExecutor@30f224d9[Running, pool size = 128, active threads = 128, queued tasks = 0, completed tasks = 0] java.util.concurrent.RejectedExecutionException: Task spark.executor.Executor$TaskRunner@5367839e rejected from java.util.concurrent.ThreadPoolExecutor@30f224d9[Running, pool size = 128, active threads = 128, queued tasks = 0, completed tasks = 0] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2013) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:816) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1337) at spark.executor.Executor.launchTask(Executor.scala:59) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Connecting to master: akka://spark@10.0.2.19:60882/user/StandaloneScheduler 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Got assigned task 1929 13/02/07 11:25:02 INFO Executor: launch taskId: 1929 13/02/07 11:25:02 ERROR StandaloneExecutorBackend: java.lang.NullPointerException at spark.executor.Executor.launchTask(Executor.scala:59) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Connecting to master: akka://spark@10.0.2.19:60882/user/StandaloneScheduler 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Got assigned task 1930 13/02/07 11:25:02 INFO Executor: launch taskId: 1930 13/02/07 11:25:02 ERROR StandaloneExecutorBackend: java.lang.NullPointerException at spark.executor.Executor.launchTask(Executor.scala:59) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)", "reporter": "xiajunluan", "assignee": null, "created": "2013-02-06T19:47:57.000+0000", "updated": "2014-11-14T08:46:05.000+0000", "resolved": "2014-11-12T05:52:53.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "I attempted a repro on a one-node cluster (my laptop) and confirmed that this bug no longer exists on master. A code inspection reveals that there is no thread limit of 128 limit anymore on the Executor's threadpool from this stacktrace line: {{at spark.executor.Executor.launchTask(Executor.scala:59)}} Here's the outline of my repro attempt:  aash@aash-mbp ~/git/spark$ cat conf/spark-env.sh SPARK_WORKER_CORES=200 SPARK_MASTER_IP=aash-mbp.local SPARK_PUBLIC_DNS=aash-mbp.local aash@aash-mbp ~/git/spark$ cat conf/spark-defaults.sh spark.master spark://aash-mbp.local:7077 aash@aash-mbp ~/git/spark$ sbin/start-all.sh ... aash@aash-mbp ~/git/spark$ bin/spark-shell spark> sc.parallelize(1l to 100000000l,200).reduce(_+_) res0: Long = 5000000050000000 spark>  I'm now closing this ticket, but please reopen [~xiajunluan] if you're still having issues.", "created": "2014-11-12T05:51:54.561+0000"}], "num_comments": 1, "text": "Issue: SPARK-689\nSummary: Task will crash when setting SPARK_WORKER_CORES> 128\nDescription: when I set SPARK_WORKER_CORES > 128(for example 200), and run a job in standalone mode that will allocate 200 tasks in one worker node, then task will crash(it seems that worker cores has been hard-code)  13/02/07 11:25:02 ERROR StandaloneExecutorBackend: Task spark.executor.Executor$TaskRunner@5367839e rejected from java.util.concurrent.ThreadPoolExecutor@30f224d9[Running, pool size = 128, active threads = 128, queued tasks = 0, completed tasks = 0] java.util.concurrent.RejectedExecutionException: Task spark.executor.Executor$TaskRunner@5367839e rejected from java.util.concurrent.ThreadPoolExecutor@30f224d9[Running, pool size = 128, active threads = 128, queued tasks = 0, completed tasks = 0] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2013) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:816) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1337) at spark.executor.Executor.launchTask(Executor.scala:59) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Connecting to master: akka://spark@10.0.2.19:60882/user/StandaloneScheduler 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Got assigned task 1929 13/02/07 11:25:02 INFO Executor: launch taskId: 1929 13/02/07 11:25:02 ERROR StandaloneExecutorBackend: java.lang.NullPointerException at spark.executor.Executor.launchTask(Executor.scala:59) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Connecting to master: akka://spark@10.0.2.19:60882/user/StandaloneScheduler 13/02/07 11:25:02 INFO StandaloneExecutorBackend: Got assigned task 1930 13/02/07 11:25:02 INFO Executor: launch taskId: 1930 13/02/07 11:25:02 ERROR StandaloneExecutorBackend: java.lang.NullPointerException at spark.executor.Executor.launchTask(Executor.scala:59) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:57) at spark.executor.StandaloneExecutorBackend$$anonfun$receive$1.apply(StandaloneExecutorBackend.scala:46) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.executor.StandaloneExecutorBackend.apply(StandaloneExecutorBackend.scala:17) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\nComments (1):\n1. Andrew Ash: I attempted a repro on a one-node cluster (my laptop) and confirmed that this bug no longer exists on master. A code inspection reveals that there is no thread limit of 128 limit anymore on the Executor's threadpool from this stacktrace line: {{at spark.executor.Executor.launchTask(Executor.scala:59)}} Here's the outline of my repro attempt:  aash@aash-mbp ~/git/spark$ cat conf/spark-env.sh SPARK_WORKER_CORES=200 SPARK_MASTER_IP=aash-mbp.local SPARK_PUBLIC_DNS=aash-mbp.local aash@aash-mbp ~/git/spark$ cat conf/spark-defaults.sh spark.master spark://aash-mbp.local:7077 aash@aash-mbp ~/git/spark$ sbin/start-all.sh ... aash@aash-mbp ~/git/spark$ bin/spark-shell spark> sc.parallelize(1l to 100000000l,200).reduce(_+_) res0: Long = 5000000050000000 spark>  I'm now closing this ticket, but please reopen [~xiajunluan] if you're still having issues.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "0695b01c9f4ac8c64e1a92e77b628c28", "issue_key": "SPARK-690", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Stack overflow when running pagerank more than 10000 iterators", "description": "when I run PageRank example more than 10000 iterators, Job client will report stack overflow errors. 13/02/07 13:41:40 INFO CacheTracker: Registering RDD ID 57993 with cache Exception in thread \"DAGScheduler\" java.lang.StackOverflowError at java.util.concurrent.locks.ReentrantReadWriteLock$Sync.tryAcquireShared(ReentrantReadWriteLock.java:467) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1281) at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:731) at org.jboss.netty.akka.util.HashedWheelTimer.scheduleTimeout(HashedWheelTimer.java:277) at org.jboss.netty.akka.util.HashedWheelTimer.newTimeout(HashedWheelTimer.java:264) at akka.actor.DefaultScheduler.scheduleOnce(Scheduler.scala:186) at akka.pattern.PromiseActorRef$.apply(AskSupport.scala:274) at akka.pattern.AskSupport$class.ask(AskSupport.scala:83) at akka.pattern.package$.ask(package.scala:43) at akka.pattern.AskSupport$AskableActorRef.ask(AskSupport.scala:123) at spark.CacheTracker.askTracker(CacheTracker.scala:121) at spark.CacheTracker.communicate(CacheTracker.scala:131) at spark.CacheTracker.registerRDD(CacheTracker.scala:142) at spark.scheduler.DAGScheduler.visit$1(DAGScheduler.scala:149) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:155) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:150) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler.visit$1(DAGScheduler.scala:150) at spark.scheduler.DAGScheduler.getParentStages(DAGScheduler.scala:160) at spark.scheduler.DAGScheduler.newStage(DAGScheduler.scala:131) at spark.scheduler.DAGScheduler.getShuffleMapStage(DAGScheduler.scala:111) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:153) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:150) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)", "reporter": "xiajunluan", "assignee": null, "created": "2013-02-06T21:53:01.000+0000", "updated": "2014-09-21T16:56:02.000+0000", "resolved": "2014-09-21T16:56:02.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matthew Farrellee", "body": "[~andrew xia] this is reported against a very old version. i'm going to close it out, but if you can reproduce please re-open", "created": "2014-09-21T16:55:49.447+0000"}], "num_comments": 1, "text": "Issue: SPARK-690\nSummary: Stack overflow when running pagerank more than 10000 iterators\nDescription: when I run PageRank example more than 10000 iterators, Job client will report stack overflow errors. 13/02/07 13:41:40 INFO CacheTracker: Registering RDD ID 57993 with cache Exception in thread \"DAGScheduler\" java.lang.StackOverflowError at java.util.concurrent.locks.ReentrantReadWriteLock$Sync.tryAcquireShared(ReentrantReadWriteLock.java:467) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1281) at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:731) at org.jboss.netty.akka.util.HashedWheelTimer.scheduleTimeout(HashedWheelTimer.java:277) at org.jboss.netty.akka.util.HashedWheelTimer.newTimeout(HashedWheelTimer.java:264) at akka.actor.DefaultScheduler.scheduleOnce(Scheduler.scala:186) at akka.pattern.PromiseActorRef$.apply(AskSupport.scala:274) at akka.pattern.AskSupport$class.ask(AskSupport.scala:83) at akka.pattern.package$.ask(package.scala:43) at akka.pattern.AskSupport$AskableActorRef.ask(AskSupport.scala:123) at spark.CacheTracker.askTracker(CacheTracker.scala:121) at spark.CacheTracker.communicate(CacheTracker.scala:131) at spark.CacheTracker.registerRDD(CacheTracker.scala:142) at spark.scheduler.DAGScheduler.visit$1(DAGScheduler.scala:149) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:155) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:150) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler.visit$1(DAGScheduler.scala:150) at spark.scheduler.DAGScheduler.getParentStages(DAGScheduler.scala:160) at spark.scheduler.DAGScheduler.newStage(DAGScheduler.scala:131) at spark.scheduler.DAGScheduler.getShuffleMapStage(DAGScheduler.scala:111) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:153) at spark.scheduler.DAGScheduler$$anonfun$visit$1$2.apply(DAGScheduler.scala:150) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)\n\nComments (1):\n1. Matthew Farrellee: [~andrew xia] this is reported against a very old version. i'm going to close it out, but if you can reproduce please re-open", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "160a6d8b116b6a43797e169e78e78398", "issue_key": "SPARK-691", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Infinite recursion in doCheckpoint when running Bagel", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-02-08T14:20:11.000+0000", "updated": "2013-02-11T13:26:27.000+0000", "resolved": "2013-02-11T13:26:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Fixed in https://github.com/mesos/spark/commit/ea08537143d58b79b3ae5d083e9b3a5647257da8", "created": "2013-02-11T13:26:00.294+0000"}], "num_comments": 1, "text": "Issue: SPARK-691\nSummary: Infinite recursion in doCheckpoint when running Bagel\n\nComments (1):\n1. Matei Alexandru Zaharia: Fixed in https://github.com/mesos/spark/commit/ea08537143d58b79b3ae5d083e9b3a5647257da8", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "1bc3f2eb10dce9c614e81581db7cf2bd", "issue_key": "SPARK-692", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "spark-shell doesn't start in 0.6.2", "description": "Running ./spark-shell gives the error: error: error while loading <root>, error in opening zip file Failed to initialize compiler: object scala not found. ** Note that as of 2.8 scala does not assume use of the java classpath. ** For the old behavior pass -usejavacp to scala, or if using a Settings ** object programatically, settings.usejavacp.value = true. Downgrading to 0.6.1 fixes the problem.", "reporter": "David Chiang", "assignee": null, "created": "2013-02-08T21:58:29.000+0000", "updated": "2014-06-22T07:05:05.000+0000", "resolved": "2014-06-22T07:05:05.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "I haven't seen this issue yet on my machine (Mac OS 10.8.2 / Scala 2.9.2). It looks like some users have experienced this issue on 0.6.1, too: https://groups.google.com/d/topic/spark-users/u1HbQ2b71LA/discussion", "created": "2013-02-10T14:56:39.403+0000"}, {"author": "David Chiang", "body": "Under OS X it works for me too (I just realized). I encountered this problem, and still do, under Linux using the prebuilt version. When I compile from sources it works now. (Incidentally, the prebuilt archive also has some OS X resource forks (beginning with ._) that caused trouble under Linux at one point.)", "created": "2013-02-11T09:37:20.707+0000"}, {"author": "Josh Rosen", "body": "Someone else ran across the same problem in Shark using Spark 0.6.1: https://groups.google.com/d/msg/shark-users/S8xhlmUdv_I/pk9m7nXOFZ8J So maybe this issue isn't 0.6.2-specific.", "created": "2013-05-04T19:25:02.215+0000"}, {"author": "Evan Chan", "body": "I'm not sure if this solves the OP's problems, but I'm attempting to package Spark 0.7.2 as a fat assembly jar, including the spark-shell, and ran into the same error message: Using Scala version 2.9.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_17) Initializing interpreter... 13/06/07 10:14:19 INFO server.Server: jetty-7.x.y-SNAPSHOT 13/06/07 10:14:19 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:49018 Failed to initialize compiler: object scala not found. ** Note that as of 2.8 scala does not assume use of the java classpath. ** For the old behavior pass -usejavacp to scala, or if using a Settings ** object programatically, settings.usejavacp.value = true. Type in expressions to have them evaluated. I initially launched this using java -cp spark-repl-assembly-0.8.0-SNAPSHOT.jar spark.repl.Main The solution was to add CLASSPATH: CLASSPATH=repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar java -cp repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar spark.repl.Main", "created": "2013-06-07T10:43:26.349+0000"}, {"author": "Evan Chan", "body": "Also, you may want to use strace -f -e trace=file (or dtruss if you are on OSX) to figure out if it is caused by a permissions issue on one of your Java JRE or Scala jars.", "created": "2013-06-07T10:44:28.529+0000"}], "num_comments": 5, "text": "Issue: SPARK-692\nSummary: spark-shell doesn't start in 0.6.2\nDescription: Running ./spark-shell gives the error: error: error while loading <root>, error in opening zip file Failed to initialize compiler: object scala not found. ** Note that as of 2.8 scala does not assume use of the java classpath. ** For the old behavior pass -usejavacp to scala, or if using a Settings ** object programatically, settings.usejavacp.value = true. Downgrading to 0.6.1 fixes the problem.\n\nComments (5):\n1. Josh Rosen: I haven't seen this issue yet on my machine (Mac OS 10.8.2 / Scala 2.9.2). It looks like some users have experienced this issue on 0.6.1, too: https://groups.google.com/d/topic/spark-users/u1HbQ2b71LA/discussion\n2. David Chiang: Under OS X it works for me too (I just realized). I encountered this problem, and still do, under Linux using the prebuilt version. When I compile from sources it works now. (Incidentally, the prebuilt archive also has some OS X resource forks (beginning with ._) that caused trouble under Linux at one point.)\n3. Josh Rosen: Someone else ran across the same problem in Shark using Spark 0.6.1: https://groups.google.com/d/msg/shark-users/S8xhlmUdv_I/pk9m7nXOFZ8J So maybe this issue isn't 0.6.2-specific.\n4. Evan Chan: I'm not sure if this solves the OP's problems, but I'm attempting to package Spark 0.7.2 as a fat assembly jar, including the spark-shell, and ran into the same error message: Using Scala version 2.9.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_17) Initializing interpreter... 13/06/07 10:14:19 INFO server.Server: jetty-7.x.y-SNAPSHOT 13/06/07 10:14:19 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:49018 Failed to initialize compiler: object scala not found. ** Note that as of 2.8 scala does not assume use of the java classpath. ** For the old behavior pass -usejavacp to scala, or if using a Settings ** object programatically, settings.usejavacp.value = true. Type in expressions to have them evaluated. I initially launched this using java -cp spark-repl-assembly-0.8.0-SNAPSHOT.jar spark.repl.Main The solution was to add CLASSPATH: CLASSPATH=repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar java -cp repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar spark.repl.Main\n5. Evan Chan: Also, you may want to use strace -f -e trace=file (or dtruss if you are on OSX) to figure out if it is caused by a permissions issue on one of your Java JRE or Scala jars.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "74f2cee414774881b7dc57337f4f9195", "issue_key": "SPARK-693", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Let deploy scripts set alternate conf, work directories", "description": "Currently SPARK_CONF_DIR is overridden in spark-config.sh, and start-slaves.sh doesn't allow the user to pass a -d option in to set the work directory. Allowing this is a small change and makes it possible to have multiple clusters running at once.", "reporter": "David Chiang", "assignee": null, "created": "2013-02-08T22:03:03.000+0000", "updated": "2015-02-19T23:08:16.000+0000", "resolved": "2015-01-23T10:30:58.000+0000", "labels": [], "components": [], "comments": [{"author": "Albert Chu", "body": "We required this support in our environment. Attached is my patch to implement this for Spark 1.0.0. Git pull request will be sent too.", "created": "2014-04-22T00:20:14.393+0000"}, {"author": "Sean R. Owen", "body": "While I'm at it, I think this is either obsolete, or was later subsumed by the discussion in SPARK-4616.", "created": "2015-01-23T10:30:58.993+0000"}, {"author": "Apache Spark", "body": "User 'chu11' has created a pull request for this issue: https://github.com/apache/spark/pull/472", "created": "2015-02-19T23:08:16.852+0000"}], "num_comments": 3, "text": "Issue: SPARK-693\nSummary: Let deploy scripts set alternate conf, work directories\nDescription: Currently SPARK_CONF_DIR is overridden in spark-config.sh, and start-slaves.sh doesn't allow the user to pass a -d option in to set the work directory. Allowing this is a small change and makes it possible to have multiple clusters running at once.\n\nComments (3):\n1. Albert Chu: We required this support in our environment. Attached is my patch to implement this for Spark 1.0.0. Git pull request will be sent too.\n2. Sean R. Owen: While I'm at it, I think this is either obsolete, or was later subsumed by the discussion in SPARK-4616.\n3. Apache Spark: User 'chu11' has created a pull request for this issue: https://github.com/apache/spark/pull/472", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "0f3517e5ebf449c0c99171f7ea270d87", "issue_key": "SPARK-694", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "All references to [K, V] in JavaDStreamLike should be changed to [K2, V2]", "description": "The type identifiers K, V are also used in JavaPairDStream, which causes a conflict with some compilers. This reveals itself whenever you want to create a JavaPairDStream from another, where the resulting types are different. This may be related to SPARK-668, I'm not sure if that's the same problem. For example, trying to revers the key and value data on a stream doesn't compile correctly:  JavaPairDStream<Integer, String> reversed = pairStream.map( new PairFunction<Tuple2<String, Integer>, Integer, String>() { @Override public Tuple2<Integer, String> call(Tuple2<String, Integer> in) throws Exception { return new Tuple2(in._2(), in._1()); } });", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-02-11T10:01:57.000+0000", "updated": "2013-02-12T14:01:06.000+0000", "resolved": "2013-02-12T14:01:06.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Ah, interesting. In principle, type identifiers in different namespaces shouldn't cause conflicts. In practice, I've seen some weird issues. For example, I reported [SI-6067|https://issues.scala-lang.org/browse/SI-6057] while working on the original Java API. What compiler error did you see when trying to compile that example?", "created": "2013-02-11T10:19:46.840+0000"}, {"author": "Patrick McFadin", "body": "Here is the compiler error  [error] /home/patrick/Documents/spark/streaming/src/test/java/spark/streaming/JavaAPISuite.java:650: cannot find symbol [error] symbol : method map(<anonymous spark.api.java.function.PairFunction<scala.Tuple2<java.lang.String,java.lang.Integer>,java.lang.Integer,java.lang.String>>) [error] location: class spark.streaming.api.java.JavaPairDStream<java.lang.String,java.lang.Integer> [error] JavaPairDStream<Integer, String> reversed = pairStream.map(  Another note - you need to sbt/sbt clean after applying this patch. Whatever scala/sbt uses to detect whether the code has changed doesn't pick up on this naming difference.", "created": "2013-02-11T10:43:49.595+0000"}, {"author": "Josh Rosen", "body": "That's the same error that I saw in SPARK-668. I wrote a hack workaround for that issue because renaming type parameters didn't seem to work when I tried it. I tried again just now after running {{sbt/sbt clean}} and the renamed type parameters solved the problem, so I'll submit a separate pull request to remove my hack workaround.", "created": "2013-02-11T11:13:50.060+0000"}], "num_comments": 3, "text": "Issue: SPARK-694\nSummary: All references to [K, V] in JavaDStreamLike should be changed to [K2, V2]\nDescription: The type identifiers K, V are also used in JavaPairDStream, which causes a conflict with some compilers. This reveals itself whenever you want to create a JavaPairDStream from another, where the resulting types are different. This may be related to SPARK-668, I'm not sure if that's the same problem. For example, trying to revers the key and value data on a stream doesn't compile correctly:  JavaPairDStream<Integer, String> reversed = pairStream.map( new PairFunction<Tuple2<String, Integer>, Integer, String>() { @Override public Tuple2<Integer, String> call(Tuple2<String, Integer> in) throws Exception { return new Tuple2(in._2(), in._1()); } });\n\nComments (3):\n1. Josh Rosen: Ah, interesting. In principle, type identifiers in different namespaces shouldn't cause conflicts. In practice, I've seen some weird issues. For example, I reported [SI-6067|https://issues.scala-lang.org/browse/SI-6057] while working on the original Java API. What compiler error did you see when trying to compile that example?\n2. Patrick McFadin: Here is the compiler error  [error] /home/patrick/Documents/spark/streaming/src/test/java/spark/streaming/JavaAPISuite.java:650: cannot find symbol [error] symbol : method map(<anonymous spark.api.java.function.PairFunction<scala.Tuple2<java.lang.String,java.lang.Integer>,java.lang.Integer,java.lang.String>>) [error] location: class spark.streaming.api.java.JavaPairDStream<java.lang.String,java.lang.Integer> [error] JavaPairDStream<Integer, String> reversed = pairStream.map(  Another note - you need to sbt/sbt clean after applying this patch. Whatever scala/sbt uses to detect whether the code has changed doesn't pick up on this naming difference.\n3. Josh Rosen: That's the same error that I saw in SPARK-668. I wrote a hack workaround for that issue because renaming type parameters didn't seem to work when I tried it. I tried again just now after running {{sbt/sbt clean}} and the renamed type parameters solved the problem, so I'll submit a separate pull request to remove my hack workaround.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "08eeddf1b16c09a4decf4dfc337dc851", "issue_key": "SPARK-695", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Exponential recursion in getPreferredLocations", "description": "This was reported to happen in DAGScheduler for graphs with many paths from the root up, though I haven't yet found a good test case for it.", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Staple", "created": "2013-02-11T13:27:41.000+0000", "updated": "2014-11-06T07:02:13.000+0000", "resolved": "2014-08-01T19:06:02.000+0000", "labels": [], "components": [], "comments": [{"author": "Aaron Staple", "body": "Progress has been made on a PR here: https://github.com/apache/spark/pull/1362", "created": "2014-07-31T23:23:18.663+0000"}], "num_comments": 1, "text": "Issue: SPARK-695\nSummary: Exponential recursion in getPreferredLocations\nDescription: This was reported to happen in DAGScheduler for graphs with many paths from the root up, though I haven't yet found a good test case for it.\n\nComments (1):\n1. Aaron Staple: Progress has been made on a PR here: https://github.com/apache/spark/pull/1362", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "3b87a60dae256281500a51b8bb97f095", "issue_key": "SPARK-696", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "sortByKey(ascending: Boolean) ignores ascending parameter", "description": "It should pass the ascending parameter on. Instead, it always passes \"true\".  /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(ascending: Boolean): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] sortByKey(comp, true) }", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-02-11T17:42:18.000+0000", "updated": "2013-02-11T18:53:45.000+0000", "resolved": "2013-02-11T18:53:45.000+0000", "labels": [], "components": ["Java API"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-696\nSummary: sortByKey(ascending: Boolean) ignores ascending parameter\nDescription: It should pass the ascending parameter on. Instead, it always passes \"true\".  /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(ascending: Boolean): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] sortByKey(comp, true) }", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "5f1f911cc25cbc2c67a5248b4e898d06", "issue_key": "SPARK-697", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "RDD should be covariant in T", "description": "It's currently causing me a lot of pain, but I'm not sure whether architectural concerns make this impossible.", "reporter": "Ben Duffield", "assignee": null, "created": "2013-02-12T09:20:52.000+0000", "updated": "2014-06-27T01:44:23.000+0000", "resolved": "2014-06-27T01:44:23.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-697\nSummary: RDD should be covariant in T\nDescription: It's currently causing me a lot of pain, but I'm not sure whether architectural concerns make this impossible.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "1e871fa4266d2a68aadfd7f05279700e", "issue_key": "SPARK-698", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark Standalone Mode is leaving a java process \"spark.executor.StandaloneExecutorBackend\" open on Windows", "description": "The java process runnig \"spark.executor.StandaloneExecutorBackend\" fails to end after a task is finished. Under Max OS X and Unix, there is a single shell script \"run\" to start Spark master, worker, and executor. Under Windows, there is a cascade: \"run.cmd\" calls \"run2.cmd\" which calls java. So when the spark.deploy.worker.ExecutorRunner (which runs in the worker process) wants to kill the executor process via process.destroy(), it actually only kills the process of \"run.cmd\", and the process of \"run2.cmd\" (=> java running the executor) stays alive. See this thread on spark-users for all details: https://groups.google.com/forum/#!topic/spark-users/NrdhVlrUDtU/discussion", "reporter": "Christoph Grothaus", "assignee": "Christoph Grothaus", "created": "2013-02-13T02:11:31.000+0000", "updated": "2013-06-25T19:15:03.000+0000", "resolved": "2013-06-25T19:15:03.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Christoph Grothaus", "body": "Did some internet search on this: there is a bug open at Oracle: \"4770092 : (process) Process.destroy does not kill multiple child processes\" http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4770092 It affects Windows platforms. All solutions discussed on StackOverflow that I found involve JNA, like this one: http://stackoverflow.com/questions/4912282/java-tool-method-to-force-kill-a-child-process/6032734#6032734 Another suggestion: maybe it is possible to send a stop message to the StandaloneExecutorBackend, so that it can stop the actor system and initiate its own shutdown properly.", "created": "2013-02-14T08:14:42.710+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Thanks for looking into it. The stop message would work in many cases, but unfortunately it would fail if the StandaloneExecutorBackend somehow freezes. Another option I'd consider is to execute the Java child process directly by running {{java}} instead of {{run.cmd}}. We would need to replicate the code for setting environment variables and classpaths that's there, but it shouldn't be too bad (in fact we can consider exporting the same variables that were used to launch the Worker).", "created": "2013-02-14T23:31:41.879+0000"}, {"author": "Christoph Grothaus", "body": "Well, that would indeed work. Maybe it's the best solution in the face of the aforementioned Bug on Windows.", "created": "2013-02-14T23:34:58.843+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Merged Christoph's commit to fix this in 0.8.", "created": "2013-06-25T19:15:03.115+0000"}], "num_comments": 4, "text": "Issue: SPARK-698\nSummary: Spark Standalone Mode is leaving a java process \"spark.executor.StandaloneExecutorBackend\" open on Windows\nDescription: The java process runnig \"spark.executor.StandaloneExecutorBackend\" fails to end after a task is finished. Under Max OS X and Unix, there is a single shell script \"run\" to start Spark master, worker, and executor. Under Windows, there is a cascade: \"run.cmd\" calls \"run2.cmd\" which calls java. So when the spark.deploy.worker.ExecutorRunner (which runs in the worker process) wants to kill the executor process via process.destroy(), it actually only kills the process of \"run.cmd\", and the process of \"run2.cmd\" (=> java running the executor) stays alive. See this thread on spark-users for all details: https://groups.google.com/forum/#!topic/spark-users/NrdhVlrUDtU/discussion\n\nComments (4):\n1. Christoph Grothaus: Did some internet search on this: there is a bug open at Oracle: \"4770092 : (process) Process.destroy does not kill multiple child processes\" http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4770092 It affects Windows platforms. All solutions discussed on StackOverflow that I found involve JNA, like this one: http://stackoverflow.com/questions/4912282/java-tool-method-to-force-kill-a-child-process/6032734#6032734 Another suggestion: maybe it is possible to send a stop message to the StandaloneExecutorBackend, so that it can stop the actor system and initiate its own shutdown properly.\n2. Matei Alexandru Zaharia: Thanks for looking into it. The stop message would work in many cases, but unfortunately it would fail if the StandaloneExecutorBackend somehow freezes. Another option I'd consider is to execute the Java child process directly by running {{java}} instead of {{run.cmd}}. We would need to replicate the code for setting environment variables and classpaths that's there, but it shouldn't be too bad (in fact we can consider exporting the same variables that were used to launch the Worker).\n3. Christoph Grothaus: Well, that would indeed work. Maybe it's the best solution in the face of the aforementioned Bug on Windows.\n4. Matei Alexandru Zaharia: Merged Christoph's commit to fix this in 0.8.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "fd471a5ec61dbcbb63a23e2ebe5c833c", "issue_key": "SPARK-699", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add example of reading from HBase", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-02-15T20:58:58.000+0000", "updated": "2013-08-06T22:59:03.000+0000", "resolved": "2013-08-06T22:59:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Had a pull request for this at https://github.com/mesos/spark/pull/596, but it was reverted due to introducing a Netty incompability issue through its HBase dependency. A re-opened copy of this PR, which excludes the problematic Netty dependency, is at https://github.com/mesos/spark/pull/605", "created": "2013-05-14T15:35:44.200+0000"}], "num_comments": 1, "text": "Issue: SPARK-699\nSummary: Add example of reading from HBase\n\nComments (1):\n1. Josh Rosen: Had a pull request for this at https://github.com/mesos/spark/pull/596, but it was reverted due to introducing a Netty incompability issue through its HBase dependency. A re-opened copy of this PR, which excludes the problematic Netty dependency, is at https://github.com/mesos/spark/pull/605", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "11383357f82d0df999e0e3e4c3e2de2e", "issue_key": "SPARK-700", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add example of reading from Cassandra", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Rohit Rai", "created": "2013-02-15T20:59:40.000+0000", "updated": "2013-10-10T18:04:26.000+0000", "resolved": "2013-10-10T18:04:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Rohit Rai", "body": "We have a library, in very initial stage, salvaged from one of our previous projects to ease using cassandra with Spark. https://github.com/tuplejump/calliope So I understand the working pretty well. If anyone can suggest an example they'll like to see with C*, I can take up this task and put together an example and a document.", "created": "2013-04-07T00:06:31.935+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Very cool! The example can be very simple, even, say, a WordCount or something like that. Just show how to read something from Cassandra as a set of strings. By the way, if you guys would like to contribute that code to the official Spark repo, we can create a subproject called spark-cassandra for accessing Cassandra.", "created": "2013-04-07T18:11:35.481+0000"}, {"author": "Rohit Rai", "body": "Will get a sample this week. About Calliope, We will be happy to contribute it :) I think it needs quite some polishing and some reorganisation, before we do that. Can you suggest a way forward on this. Where do we put the code? Do we rename the package structure? Once I put it in our Spark repo, we can discuss about cleaning the API and things like that. Should we create a separate ticket to track this?", "created": "2013-04-08T03:20:51.193+0000"}, {"author": "Rohit Rai", "body": "Sorry for the delay... got stuck in some urgent deliverables. Have got a working example in my repo here - https://github.com/milliondreams/spark/commit/3be7bdcefda13d67633f9b9f6d901722fd5649de If there are no obvious changes/goofups, I can raise a pull request.", "created": "2013-06-01T07:08:43.521+0000"}, {"author": "Rohit Rai", "body": "Sent a pull request - https://github.com/mesos/spark/pull/637", "created": "2013-06-02T02:32:41.012+0000"}, {"author": "Rohit Rai", "body": "The pull is merged, guess we can close this one now.", "created": "2013-06-19T08:35:53.738+0000"}], "num_comments": 6, "text": "Issue: SPARK-700\nSummary: Add example of reading from Cassandra\n\nComments (6):\n1. Rohit Rai: We have a library, in very initial stage, salvaged from one of our previous projects to ease using cassandra with Spark. https://github.com/tuplejump/calliope So I understand the working pretty well. If anyone can suggest an example they'll like to see with C*, I can take up this task and put together an example and a document.\n2. Matei Alexandru Zaharia: Very cool! The example can be very simple, even, say, a WordCount or something like that. Just show how to read something from Cassandra as a set of strings. By the way, if you guys would like to contribute that code to the official Spark repo, we can create a subproject called spark-cassandra for accessing Cassandra.\n3. Rohit Rai: Will get a sample this week. About Calliope, We will be happy to contribute it :) I think it needs quite some polishing and some reorganisation, before we do that. Can you suggest a way forward on this. Where do we put the code? Do we rename the package structure? Once I put it in our Spark repo, we can discuss about cleaning the API and things like that. Should we create a separate ticket to track this?\n4. Rohit Rai: Sorry for the delay... got stuck in some urgent deliverables. Have got a working example in my repo here - https://github.com/milliondreams/spark/commit/3be7bdcefda13d67633f9b9f6d901722fd5649de If there are no obvious changes/goofups, I can raise a pull request.\n5. Rohit Rai: Sent a pull request - https://github.com/mesos/spark/pull/637\n6. Rohit Rai: The pull is merged, guess we can close this one now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "29f0a775c10a9075597814d54a8b58b4", "issue_key": "SPARK-701", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Wrong SPARK_MEM setting with different EC2 master and worker machine types", "description": "When launching a spark-ec2 cluster using different worker and master machine types, SPARK_MEM in spark-env.sh is set based on the master's memory instead of the worker's. This causes jobs to hang if the master has more memory than the workers (because jobs will request too much memory).", "reporter": "Josh Rosen", "assignee": "Shivaram Venkataraman", "created": "2013-02-21T16:33:33.000+0000", "updated": "2015-06-30T06:07:42.000+0000", "resolved": "2013-05-14T15:38:42.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Shivaram Venkataraman", "body": "Should be fixed by https://github.com/mesos/spark-ec2/pull/4", "created": "2013-02-22T10:21:02.727+0000"}, {"author": "Josh Rosen", "body": "I'm pretty sure that this made it into the 0.7 AMI. Could we maybe tag commits in the spark-ec2 repo whenever we release a new public AMI, so that it's easier to tell what gets released?", "created": "2013-04-13T10:03:46.236+0000"}, {"author": "Josh Rosen", "body": "Yep, this fix is present in the current 0.7 AMI. Marking as fixed.", "created": "2013-05-14T15:38:42.564+0000"}, {"author": "Stefano Parmesan", "body": "[~shivaram] it looks like this issue reappeared, in a way (talking about 1.4.0 now, not tested on previous versions): if you create a cluster with an {{m1.small}} master (1.7GB RAM) and one {{m1.large}} worker (7.5GB RAM), {{spark.executor.memory}} will be set to 512MB, and that's because of [system_ram_kb = min(slave_ram_kb, master_ram_kb)|https://github.com/mesos/spark-ec2/blob/e642aa362338e01efed62948ec0f063d5fce3242/deploy_templates.py#L32] Quite often you use a smaller master instance compared to workers; smaller means fewer RAM, and the line of code I linked above shows that the minimum between the master and the worker(s) memory is used as {{spark_mb}}, which in turn is used as {{default_spark_mem}} to generate the [spark-defaults.conf|https://github.com/mesos/spark-ec2/blob/e642aa362338e01efed62948ec0f063d5fce3242/templates/root/spark/conf/spark-defaults.conf]. If we read the title of this bug, it would seem like the issue reappeared, and we should reopen it; if we instead read the description, we'll notice it's not 100% the same issue: it says \"SPARK_MEM in spark-env.sh is set based on the master's memory instead of the worker's\", while now \"it's set on the minimum between the master's memory and the worker's, which is quite often the master's\". What do you suggest? Should we reopen this three-digits issue, or create a new one?", "created": "2015-06-29T22:53:16.677+0000"}, {"author": "Shivaram Venkataraman", "body": "Yeah so SPARK_MEM used to be used for both master and executors before. Right now we have two separate variables spark.executor.memory and spark.driver.memory that we can set. Lets open a new issue for this.", "created": "2015-06-29T23:09:31.325+0000"}, {"author": "Stefano Parmesan", "body": "Posting the link to the new issue for reference: https://issues.apache.org/jira/browse/SPARK-8726", "created": "2015-06-30T06:07:42.677+0000"}], "num_comments": 6, "text": "Issue: SPARK-701\nSummary: Wrong SPARK_MEM setting with different EC2 master and worker machine types\nDescription: When launching a spark-ec2 cluster using different worker and master machine types, SPARK_MEM in spark-env.sh is set based on the master's memory instead of the worker's. This causes jobs to hang if the master has more memory than the workers (because jobs will request too much memory).\n\nComments (6):\n1. Shivaram Venkataraman: Should be fixed by https://github.com/mesos/spark-ec2/pull/4\n2. Josh Rosen: I'm pretty sure that this made it into the 0.7 AMI. Could we maybe tag commits in the spark-ec2 repo whenever we release a new public AMI, so that it's easier to tell what gets released?\n3. Josh Rosen: Yep, this fix is present in the current 0.7 AMI. Marking as fixed.\n4. Stefano Parmesan: [~shivaram] it looks like this issue reappeared, in a way (talking about 1.4.0 now, not tested on previous versions): if you create a cluster with an {{m1.small}} master (1.7GB RAM) and one {{m1.large}} worker (7.5GB RAM), {{spark.executor.memory}} will be set to 512MB, and that's because of [system_ram_kb = min(slave_ram_kb, master_ram_kb)|https://github.com/mesos/spark-ec2/blob/e642aa362338e01efed62948ec0f063d5fce3242/deploy_templates.py#L32] Quite often you use a smaller master instance compared to workers; smaller means fewer RAM, and the line of code I linked above shows that the minimum between the master and the worker(s) memory is used as {{spark_mb}}, which in turn is used as {{default_spark_mem}} to generate the [spark-defaults.conf|https://github.com/mesos/spark-ec2/blob/e642aa362338e01efed62948ec0f063d5fce3242/templates/root/spark/conf/spark-defaults.conf]. If we read the title of this bug, it would seem like the issue reappeared, and we should reopen it; if we instead read the description, we'll notice it's not 100% the same issue: it says \"SPARK_MEM in spark-env.sh is set based on the master's memory instead of the worker's\", while now \"it's set on the minimum between the master's memory and the worker's, which is quite often the master's\". What do you suggest? Should we reopen this three-digits issue, or create a new one?\n5. Shivaram Venkataraman: Yeah so SPARK_MEM used to be used for both master and executors before. Right now we have two separate variables spark.executor.memory and spark.driver.memory that we can set. Lets open a new issue for this.\n6. Stefano Parmesan: Posting the link to the new issue for reference: https://issues.apache.org/jira/browse/SPARK-8726", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "1e44168338649243bc056dc02e33432d", "issue_key": "SPARK-702", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "All PairRDDFunctions should accept JFunction (not Function)", "description": "There are a few that accept Scala's Function instead of JFunction. This should be fixed.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-02-22T12:51:23.000+0000", "updated": "2013-02-22T14:59:51.000+0000", "resolved": "2013-02-22T14:59:51.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-702\nSummary: All PairRDDFunctions should accept JFunction (not Function)\nDescription: There are a few that accept Scala's Function instead of JFunction. This should be fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "97e247382600b9cf64cccef1c668af76", "issue_key": "SPARK-703", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "KafkaWordCount example crashes with java.lang.ArrayIndexOutOfBoundsException in CheckpointRDD.scala", "description": "This is a bad Spark Streaming bug. The KafkaWordCount example can be used to demonstrate the problem. After a few iterations (batches), the test crashes with this stack trace during the checkpointing attempt: 3/02/22 15:26:54 INFO streaming.JobManager: Total delay: 0.02100 s for job 12 (execution: 0.01300 s) 13/02/22 15:26:54 INFO rdd.CoGroupedRDD: Adding one-to-one dependency with MappedValuesRDD[87] at apply at TraversableLike.scala:239 13/02/22 15:26:54 INFO rdd.CoGroupedRDD: Adding one-to-one dependency with MapPartitionsRDD[56] at apply at TraversableLike.scala:239 13/02/22 15:26:54 INFO rdd.CoGroupedRDD: Adding one-to-one dependency with MapPartitionsRDD[99] at apply at TraversableLike.scala:239 13/02/22 15:26:54 ERROR streaming.JobManager: Running streaming job 13 @ 1361572014000 ms failed java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CheckpointRDD.getPartitions(CheckpointRDD.scala:27) at spark.RDD.partitions(RDD.scala:166) at spark.RDD.partitions(RDD.scala:166) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:71) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:65) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:63) at spark.RDD.partitions(RDD.scala:166) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:655) at spark.RDD.partitions(RDD.scala:166) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:71) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:65) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:63) at spark.RDD.partitions(RDD.scala:166) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:655) at spark.RDD.partitions(RDD.scala:166) at spark.RDD.take(RDD.scala:550) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:522) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:521) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:15) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) The only way I can get this test to work on a cluster is to disable checkpointing and to use reduceByKey() instead of reduceByKeyAndWindow(). Also the test works when run using \"local\" as the master.", "reporter": "Craig A. Vanderborgh", "assignee": null, "created": "2013-02-22T14:42:34.000+0000", "updated": "2014-04-25T20:54:23.000+0000", "resolved": "2014-04-25T20:54:23.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Gert Verbruggen", "body": "I'm having the same issue with version 0.7.2. You can further simplify the test: just take the KafkaWordCount example and instead of counting words you count the messages. // Variation A: val count = ssc.kafkaStream(zkQuorum, group, topicpMap).count() count.print() // Variation B: val count = ssc.kafkaStream(zkQuorum, group, topicpMap).countByWindow(Minutes(1), Seconds(2)) count.print() In local mode both variations work. When running on a cluster, after a few iterations variation B will throw this exception and then keep throwing it on every following iteration: ERROR JobManager: Running streaming job 10 @ 1375451975000 ms failed java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:692) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:9) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.take(RDD.scala:667) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:495) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:494) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:662)", "created": "2013-08-02T07:01:18.318+0000"}, {"author": "Gert Verbruggen", "body": "I have set up a fresh standalone Spark cluster with one master and two workers. Spark 2.7.3 and Scala 2.9.3 (like in the quick-start examples). In the class spark.rdd.CoGroupedRDD I have changed this function to avoid the ArrayIndexOutOfBoundsException and instead print out some logging: override def getPartitions: Array[Partition] = { val array = new Array[Partition](part.numPartitions) for (i <- 0 until array.size) { // Each CoGroupPartition will have a dependency per contributing RDD val splitDep = new ArrayBuffer[CoGroupSplitDep]() for (j <- 0 until rdds.size) { val rdd = rdds(j) // Assume each RDD contributed a single dependency, and get it dependencies(j) match { case s: ShuffleDependency[_, _] => splitDep += new ShuffleCoGroupSplitDep(s.shuffleId) case _ => try { splitDep += new NarrowCoGroupSplitDep(rdd, i, rdd.partitions(i)) } catch { case e: Exception => logError(\"RDD \" + j + \" doesn't have a partition \" + i + \", expected \" + part.numPartitions + \" partitions but got \" + rdd.partitions.size, e) } } } array(i) = new CoGroupPartition(i, splitDep.toArray) } array } Now when I run the simple example \"Variation B\" from my previous comment I get this error: 13/08/09 15:27:40 ERROR CoGroupedRDD: RDD 0 doesn't have a partition 0, expected 1 partitions but got 0 java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:85) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:9) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.take(RDD.scala:667) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:500) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:499) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:662) Exception in thread \"DAGScheduler\" java.lang.ArrayIndexOutOfBoundsException: 0 at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:683) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply$mcVI$sp(DAGScheduler.scala:698) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:695) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:695) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply$mcVI$sp(DAGScheduler.scala:698) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:695) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:695) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply$mcVI$sp(DAGScheduler.scala:698) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:695) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:695) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$submitMissingTasks$5.apply(DAGScheduler.scala:452) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$submitMissingTasks$5.apply(DAGScheduler.scala:450) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:743) at scala.collection.immutable.Range.foreach(Range.scala:81) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:742) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:450) at spark.scheduler.DAGScheduler$$anonfun$handleTaskCompletion$16.apply(DAGScheduler.scala:564) at spark.scheduler.DAGScheduler$$anonfun$handleTaskCompletion$16.apply(DAGScheduler.scala:562) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:562) at spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:300) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:364) at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:107)", "created": "2013-08-09T08:29:20.977+0000"}, {"author": "Gert Verbruggen", "body": "Like with spark.rdd.CoGroupedRDD.getPartitions() I also added extra assertions and logging to spark.scheduler.DAGScheduler.getPreferredLocs() Now I'm avoiding all ArrayIndexOutOfBoundsExceptions, instead returning an empty list or Nil where I can. Now this is the output of the example code: scala> ssc.start() ------------------------------------------- Time: 1376065185000 ms ------------------------------------------- 4 13/08/09 16:19:55 ERROR CoGroupedRDD: RDD 0 doesn't have a partition 0, expected 1 partitions but got 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: getCacheLocs(rdd): no partition 0, found 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: rdd.preferredLocations: RDD has no partition 0, found 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: getCacheLocs(rdd): no partition 0, found 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: rdd.preferredLocations: RDD has no partition 0, found 0 13/08/09 16:19:55 ERROR TaskSetManager: Task 8.0:0 failed more than 4 times; aborting job I have no idea how to properly recover from this because I don't understand the cause of the problem. Can anyone please help?", "created": "2013-08-09T09:26:40.845+0000"}, {"author": "Gert Verbruggen", "body": "I'd like to reiterate that everything works just fine in local mode. I've set up a 3-node Mesos cluster and a 3-node Spark Standalone cluster. On both I got this issue. To reproduce this take the standard streaming example for Kafka input and simplify it in this way: val count = ssc.kafkaStream(zkQuorum, group, topicpMap).countByWindow(Minutes(1), Seconds(2)) count.print()", "created": "2013-08-09T09:34:03.999+0000"}, {"author": "taoli", "body": "I'm having the same issue with version 0.7.3, with the following output java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.take(RDD.scala:667) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:500) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:499) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)", "created": "2013-09-16T20:50:20.773+0000"}, {"author": "Gert Verbruggen", "body": "At least for our case, we've figured out what caused the weird exceptions. Looking back it seems silly that we didn't think of it sooner, but it's HDFS. You see, with a lot of the Spark Streaming examples you don't read the input from HDFS and you often don't write the output to HDFS either. We naively thought that we could deploy a Spark cluster to do stream processing, apart from HDFS. But Spark always has to run on an HDFS cluster, so the workers can share the RDD. Always, except when in local mode. It's not so clear from the Spark Streaming documentation but you *have to* install the cluster on top of a HDFS cluster. That made the ArrayIndexOutOfBoundsExceptions go away. One of the committers to Spark confirmed this.", "created": "2013-09-20T02:16:25.340+0000"}, {"author": "Tathagata Das", "body": "Yes, that observation is correct. This is something we have to make it clearer in the documentation. Closing this JIRA for now.", "created": "2014-04-25T20:53:43.290+0000"}], "num_comments": 7, "text": "Issue: SPARK-703\nSummary: KafkaWordCount example crashes with java.lang.ArrayIndexOutOfBoundsException in CheckpointRDD.scala\nDescription: This is a bad Spark Streaming bug. The KafkaWordCount example can be used to demonstrate the problem. After a few iterations (batches), the test crashes with this stack trace during the checkpointing attempt: 3/02/22 15:26:54 INFO streaming.JobManager: Total delay: 0.02100 s for job 12 (execution: 0.01300 s) 13/02/22 15:26:54 INFO rdd.CoGroupedRDD: Adding one-to-one dependency with MappedValuesRDD[87] at apply at TraversableLike.scala:239 13/02/22 15:26:54 INFO rdd.CoGroupedRDD: Adding one-to-one dependency with MapPartitionsRDD[56] at apply at TraversableLike.scala:239 13/02/22 15:26:54 INFO rdd.CoGroupedRDD: Adding one-to-one dependency with MapPartitionsRDD[99] at apply at TraversableLike.scala:239 13/02/22 15:26:54 ERROR streaming.JobManager: Running streaming job 13 @ 1361572014000 ms failed java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CheckpointRDD.getPartitions(CheckpointRDD.scala:27) at spark.RDD.partitions(RDD.scala:166) at spark.RDD.partitions(RDD.scala:166) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:71) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:65) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:63) at spark.RDD.partitions(RDD.scala:166) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:655) at spark.RDD.partitions(RDD.scala:166) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:71) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:65) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:63) at spark.RDD.partitions(RDD.scala:166) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:655) at spark.RDD.partitions(RDD.scala:166) at spark.RDD.take(RDD.scala:550) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:522) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:521) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:15) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) The only way I can get this test to work on a cluster is to disable checkpointing and to use reduceByKey() instead of reduceByKeyAndWindow(). Also the test works when run using \"local\" as the master.\n\nComments (7):\n1. Gert Verbruggen: I'm having the same issue with version 0.7.2. You can further simplify the test: just take the KafkaWordCount example and instead of counting words you count the messages. // Variation A: val count = ssc.kafkaStream(zkQuorum, group, topicpMap).count() count.print() // Variation B: val count = ssc.kafkaStream(zkQuorum, group, topicpMap).countByWindow(Minutes(1), Seconds(2)) count.print() In local mode both variations work. When running on a cluster, after a few iterations variation B will throw this exception and then keep throwing it on every following iteration: ERROR JobManager: Running streaming job 10 @ 1375451975000 ms failed java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:692) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:9) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.take(RDD.scala:667) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:495) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:494) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:662)\n2. Gert Verbruggen: I have set up a fresh standalone Spark cluster with one master and two workers. Spark 2.7.3 and Scala 2.9.3 (like in the quick-start examples). In the class spark.rdd.CoGroupedRDD I have changed this function to avoid the ArrayIndexOutOfBoundsException and instead print out some logging: override def getPartitions: Array[Partition] = { val array = new Array[Partition](part.numPartitions) for (i <- 0 until array.size) { // Each CoGroupPartition will have a dependency per contributing RDD val splitDep = new ArrayBuffer[CoGroupSplitDep]() for (j <- 0 until rdds.size) { val rdd = rdds(j) // Assume each RDD contributed a single dependency, and get it dependencies(j) match { case s: ShuffleDependency[_, _] => splitDep += new ShuffleCoGroupSplitDep(s.shuffleId) case _ => try { splitDep += new NarrowCoGroupSplitDep(rdd, i, rdd.partitions(i)) } catch { case e: Exception => logError(\"RDD \" + j + \" doesn't have a partition \" + i + \", expected \" + part.numPartitions + \" partitions but got \" + rdd.partitions.size, e) } } } array(i) = new CoGroupPartition(i, splitDep.toArray) } array } Now when I run the simple example \"Variation B\" from my previous comment I get this error: 13/08/09 15:27:40 ERROR CoGroupedRDD: RDD 0 doesn't have a partition 0, expected 1 partitions but got 0 java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:85) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:9) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.take(RDD.scala:667) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:500) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:499) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:662) Exception in thread \"DAGScheduler\" java.lang.ArrayIndexOutOfBoundsException: 0 at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:683) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply$mcVI$sp(DAGScheduler.scala:698) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:695) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:695) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply$mcVI$sp(DAGScheduler.scala:698) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:695) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:695) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply$mcVI$sp(DAGScheduler.scala:698) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1$$anonfun$apply$1.apply(DAGScheduler.scala:697) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:697) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$getPreferredLocs$1.apply(DAGScheduler.scala:695) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59) at scala.collection.immutable.List.foreach(List.scala:76) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$getPreferredLocs(DAGScheduler.scala:695) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$submitMissingTasks$5.apply(DAGScheduler.scala:452) at spark.scheduler.DAGScheduler$$anonfun$spark$scheduler$DAGScheduler$$submitMissingTasks$5.apply(DAGScheduler.scala:450) at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:743) at scala.collection.immutable.Range.foreach(Range.scala:81) at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:742) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:450) at spark.scheduler.DAGScheduler$$anonfun$handleTaskCompletion$16.apply(DAGScheduler.scala:564) at spark.scheduler.DAGScheduler$$anonfun$handleTaskCompletion$16.apply(DAGScheduler.scala:562) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:562) at spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:300) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:364) at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:107)\n3. Gert Verbruggen: Like with spark.rdd.CoGroupedRDD.getPartitions() I also added extra assertions and logging to spark.scheduler.DAGScheduler.getPreferredLocs() Now I'm avoiding all ArrayIndexOutOfBoundsExceptions, instead returning an empty list or Nil where I can. Now this is the output of the example code: scala> ssc.start() ------------------------------------------- Time: 1376065185000 ms ------------------------------------------- 4 13/08/09 16:19:55 ERROR CoGroupedRDD: RDD 0 doesn't have a partition 0, expected 1 partitions but got 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: getCacheLocs(rdd): no partition 0, found 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: rdd.preferredLocations: RDD has no partition 0, found 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: getCacheLocs(rdd): no partition 0, found 0 13/08/09 16:19:55 ERROR DAGScheduler: getPreferredLocs: rdd.preferredLocations: RDD has no partition 0, found 0 13/08/09 16:19:55 ERROR TaskSetManager: Task 8.0:0 failed more than 4 times; aborting job I have no idea how to properly recover from this because I don't understand the cause of the problem. Can anyone please help?\n4. Gert Verbruggen: I'd like to reiterate that everything works just fine in local mode. I've set up a 3-node Mesos cluster and a 3-node Spark Standalone cluster. On both I got this issue. To reproduce this take the standard streaming example for Kafka input and simplify it in this way: val count = ssc.kafkaStream(zkQuorum, group, topicpMap).countByWindow(Minutes(1), Seconds(2)) count.print()\n5. taoli: I'm having the same issue with version 0.7.3, with the following output java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:89) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:83) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:81) at spark.RDD.partitions(RDD.scala:169) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:701) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.take(RDD.scala:667) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:500) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:499) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)\n6. Gert Verbruggen: At least for our case, we've figured out what caused the weird exceptions. Looking back it seems silly that we didn't think of it sooner, but it's HDFS. You see, with a lot of the Spark Streaming examples you don't read the input from HDFS and you often don't write the output to HDFS either. We naively thought that we could deploy a Spark cluster to do stream processing, apart from HDFS. But Spark always has to run on an HDFS cluster, so the workers can share the RDD. Always, except when in local mode. It's not so clear from the Spark Streaming documentation but you *have to* install the cluster on top of a HDFS cluster. That made the ArrayIndexOutOfBoundsExceptions go away. One of the committers to Spark confirmed this.\n7. Tathagata Das: Yes, that observation is correct. This is something we have to make it clearer in the documentation. Closing this JIRA for now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "c99f9a244667262ba50004c4943b8511", "issue_key": "SPARK-704", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ConnectionManager sometimes cannot detect loss of sending connections", "description": "ConnectionManager currently does not detect when SendingConnections disconnect except if it is trying to send through them. As a result, a node failure just after a connection is initiated but before any acknowledgement messages can be sent may result in a hang. ConnectionManager has code intended to detect this case by detecting the failure of a corresponding ReceivingConnection, but this code assumes that the remote host:port of the ReceivingConnection is the same as the ConnectionManagerId, which is almost never true. Additionally, there does not appear to be any reason to assume a corresponding ReceivingConnection will exist.", "reporter": "Charles Reiss", "assignee": null, "created": "2013-02-22T14:47:32.000+0000", "updated": "2015-03-14T19:49:31.000+0000", "resolved": "2015-03-14T19:49:22.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Henry Saputra", "body": "If no one object, I would like to take a look at this. Thx", "created": "2014-03-12T15:58:31.676+0000"}, {"author": "Henry Saputra", "body": "Trying to reproduce and understand the issue. After a new SendingConnection is created it is creating its own channel then register to the ConnectionManager#selector to listen to state changes. When SendingConnection is being ask to send message, it will call Connection#registerInterest to ready for write which later in the Detecting whether SendingConnection is disconnected will be done when there is an attempt to write to the channel which will throw an exception which I believe should be sufficient for the purpose of the issue? Just want to clarify if I understand the problem correctly.", "created": "2014-06-19T21:19:07.766+0000"}, {"author": "Charles Reiss", "body": "It's been a while since I reported this issue, so it may have been incidentally fixed. But this problem was with a remote node failure _after_ a message (or several messages) was successfully sent to that node but before a response was received. So, there would be no message to send to trigger a failing attempt to write to the channel. If there's a corresponding ReceivingConnection, then the remote node death would be detected via a failed read, but I believe the code in ConnectionManager#removeConnection would not reliably trigger the MessageStatuses.", "created": "2014-06-20T21:36:46.805+0000"}, {"author": "Mridul Muralidharan", "body": "If remote node goes down, SendingConnection would be notified since it is also registered for read events (to handle precisely this case actually). ReceivingConnection would be notified since it is waiting on reads on that socket. This, ofcourse, assumes that local node detects remote node failure at tcp layer. Problems come in when this is not detected due to no activity on the socket (at app and socket level - keepalive timeout, etc). Usually this is detected via application level ping/keepalive messages : not sure if we want to introduce that into spark ...", "created": "2014-06-21T09:09:01.186+0000"}, {"author": "Henry Saputra", "body": "Thanks a lot to [~woggle] and [~mridulm80] for clarifying the issue and add additional comments to help make it clear what is happening. Yes, since the NIO's channel for SendingConnection listen to both write and read (from for-loop detection in the ConnectionManager) any loss connection will be detected by the SendingConnection's channel. My concern is about \"hang\" issue that Charles mentioned in the issue description, I tried to reproduce by shutting down the node manually but could not really get that situation. Since this is async IO there is no way to know about failure of remote node when there is no activity at the socket, like Mridul, mentioned other than sending keepalive messages.", "created": "2014-06-23T18:27:16.049+0000"}, {"author": "Henry Saputra", "body": "Could someone re-assign the issue from me? With the new setting only committer can reassign JIRA.", "created": "2015-01-20T19:45:28.850+0000"}], "num_comments": 6, "text": "Issue: SPARK-704\nSummary: ConnectionManager sometimes cannot detect loss of sending connections\nDescription: ConnectionManager currently does not detect when SendingConnections disconnect except if it is trying to send through them. As a result, a node failure just after a connection is initiated but before any acknowledgement messages can be sent may result in a hang. ConnectionManager has code intended to detect this case by detecting the failure of a corresponding ReceivingConnection, but this code assumes that the remote host:port of the ReceivingConnection is the same as the ConnectionManagerId, which is almost never true. Additionally, there does not appear to be any reason to assume a corresponding ReceivingConnection will exist.\n\nComments (6):\n1. Henry Saputra: If no one object, I would like to take a look at this. Thx\n2. Henry Saputra: Trying to reproduce and understand the issue. After a new SendingConnection is created it is creating its own channel then register to the ConnectionManager#selector to listen to state changes. When SendingConnection is being ask to send message, it will call Connection#registerInterest to ready for write which later in the Detecting whether SendingConnection is disconnected will be done when there is an attempt to write to the channel which will throw an exception which I believe should be sufficient for the purpose of the issue? Just want to clarify if I understand the problem correctly.\n3. Charles Reiss: It's been a while since I reported this issue, so it may have been incidentally fixed. But this problem was with a remote node failure _after_ a message (or several messages) was successfully sent to that node but before a response was received. So, there would be no message to send to trigger a failing attempt to write to the channel. If there's a corresponding ReceivingConnection, then the remote node death would be detected via a failed read, but I believe the code in ConnectionManager#removeConnection would not reliably trigger the MessageStatuses.\n4. Mridul Muralidharan: If remote node goes down, SendingConnection would be notified since it is also registered for read events (to handle precisely this case actually). ReceivingConnection would be notified since it is waiting on reads on that socket. This, ofcourse, assumes that local node detects remote node failure at tcp layer. Problems come in when this is not detected due to no activity on the socket (at app and socket level - keepalive timeout, etc). Usually this is detected via application level ping/keepalive messages : not sure if we want to introduce that into spark ...\n5. Henry Saputra: Thanks a lot to [~woggle] and [~mridulm80] for clarifying the issue and add additional comments to help make it clear what is happening. Yes, since the NIO's channel for SendingConnection listen to both write and read (from for-loop detection in the ConnectionManager) any loss connection will be detected by the SendingConnection's channel. My concern is about \"hang\" issue that Charles mentioned in the issue description, I tried to reproduce by shutting down the node manually but could not really get that situation. Since this is async IO there is no way to know about failure of remote node when there is no activity at the socket, like Mridul, mentioned other than sending keepalive messages.\n6. Henry Saputra: Could someone re-assign the issue from me? With the new setting only committer can reassign JIRA.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "cc0e7f69b9b5506b74452700043b1e69", "issue_key": "SPARK-705", "issue_type": "New Feature", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Implement sortByKey() in PySpark", "description": "Implement sortByKey() in PySpark. This is blocked by porting sample(), because RangePartitioner relies on sampling to determine the ranges.", "reporter": "Josh Rosen", "assignee": "Andre Schumacher", "created": "2013-02-24T13:41:48.000+0000", "updated": "2013-10-09T12:41:33.000+0000", "resolved": "2013-10-09T12:41:33.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Andre Schumacher", "body": "Closed by merge of PR #38", "created": "2013-10-09T12:41:21.812+0000"}], "num_comments": 1, "text": "Issue: SPARK-705\nSummary: Implement sortByKey() in PySpark\nDescription: Implement sortByKey() in PySpark. This is blocked by porting sample(), because RangePartitioner relies on sampling to determine the ranges.\n\nComments (1):\n1. Andre Schumacher: Closed by merge of PR #38", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "16f78c7f20807c16738d0a550158c1e4", "issue_key": "SPARK-706", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Failures in block manager put leads to task hanging", "description": "Reported in this thread: https://groups.google.com/forum/?fromgroups=#!topic/shark-users/Q_SiIDzVtZw The following exception in block manager leaves the block marked as pending.  13/02/26 06:14:56 ERROR executor.Executor: Exception in task ID 39 com.esotericsoftware.kryo.SerializationException: Buffer limit exceeded writing object of type: shark.ColumnarWritable at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:492) at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:78) at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:58) at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:73) at spark.storage.DiskStore.putValues(DiskStore.scala:63) at spark.storage.BlockManager.dropFromMemory(BlockManager.scala:779) at spark.storage.MemoryStore.tryToPut(MemoryStore.scala:162) at spark.storage.MemoryStore.putValues(MemoryStore.scala:57) at spark.storage.BlockManager.put(BlockManager.scala:582) at spark.CacheTracker.getOrCompute(CacheTracker.scala:215) at spark.RDD.iterator(RDD.scala:159) at spark.scheduler.ResultTask.run(ResultTask.scala:18) at spark.executor.Executor$TaskRunner.run(Executor.scala:76) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:679)  When the block is read, the task is stuck in BlockInfo.waitForReady(). We should propagate the error back to the master instead of hanging the slave node.", "reporter": "Reynold Xin", "assignee": null, "created": "2013-02-28T12:55:49.000+0000", "updated": "2020-05-17T18:21:45.000+0000", "resolved": "2015-02-06T21:28:40.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Reynold Xin", "body": "These two are related but not entirely the same.", "created": "2013-02-28T12:56:51.135+0000"}, {"author": "Nicholas Chammas", "body": "[~rxin] Is this issue still valid?", "created": "2015-02-06T19:39:46.912+0000"}], "num_comments": 2, "text": "Issue: SPARK-706\nSummary: Failures in block manager put leads to task hanging\nDescription: Reported in this thread: https://groups.google.com/forum/?fromgroups=#!topic/shark-users/Q_SiIDzVtZw The following exception in block manager leaves the block marked as pending.  13/02/26 06:14:56 ERROR executor.Executor: Exception in task ID 39 com.esotericsoftware.kryo.SerializationException: Buffer limit exceeded writing object of type: shark.ColumnarWritable at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:492) at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:78) at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:58) at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:73) at spark.storage.DiskStore.putValues(DiskStore.scala:63) at spark.storage.BlockManager.dropFromMemory(BlockManager.scala:779) at spark.storage.MemoryStore.tryToPut(MemoryStore.scala:162) at spark.storage.MemoryStore.putValues(MemoryStore.scala:57) at spark.storage.BlockManager.put(BlockManager.scala:582) at spark.CacheTracker.getOrCompute(CacheTracker.scala:215) at spark.RDD.iterator(RDD.scala:159) at spark.scheduler.ResultTask.run(ResultTask.scala:18) at spark.executor.Executor$TaskRunner.run(Executor.scala:76) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:679)  When the block is read, the task is stuck in BlockInfo.waitForReady(). We should propagate the error back to the master instead of hanging the slave node.\n\nComments (2):\n1. Reynold Xin: These two are related but not entirely the same.\n2. Nicholas Chammas: [~rxin] Is this issue still valid?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "412b181566ac18636e5866142bba861a", "issue_key": "SPARK-707", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Port more of the Scala example programs to Java", "description": "Getting Pi, K-means and the log mining example, at the very least, would be nice.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-03-03T16:09:19.000+0000", "updated": "2013-04-03T14:24:11.000+0000", "resolved": "2013-04-03T14:24:11.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Patrick McFadin", "body": "There is a KMeans java example in the latest tutorial that we could directly borrow.", "created": "2013-03-05T15:09:27.772+0000"}, {"author": "Nicholas Pentreath", "body": "Added Java versions of LogQuery and SparkPi examples in https://github.com/mesos/spark/pull/528. Happy to take a look at the K-means code in the tutorial and see if that can be added, or alternatively try to port the existing Scala k-means.", "created": "2013-03-15T02:17:47.957+0000"}, {"author": "Nicholas Pentreath", "body": "This is resolved as per https://github.com/mesos/spark/pull/528", "created": "2013-04-03T06:51:51.812+0000"}], "num_comments": 3, "text": "Issue: SPARK-707\nSummary: Port more of the Scala example programs to Java\nDescription: Getting Pi, K-means and the log mining example, at the very least, would be nice.\n\nComments (3):\n1. Patrick McFadin: There is a KMeans java example in the latest tutorial that we could directly borrow.\n2. Nicholas Pentreath: Added Java versions of LogQuery and SparkPi examples in https://github.com/mesos/spark/pull/528. Happy to take a look at the K-means code in the tutorial and see if that can be added, or alternatively try to port the existing Scala k-means.\n3. Nicholas Pentreath: This is resolved as per https://github.com/mesos/spark/pull/528", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "c6d26cb568da4254fa0f0a40c8acf625", "issue_key": "SPARK-708", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "add a JobLogger for Spark", "description": "Current spark logs are outputted to console or one spark log file, which is not convenient for analysis of one single job. We would like to implement a JobLogger for Spark which output one history file for each job(ActiveJob). now the Spark has task metrics and summaries. the history file can be built on top of them. the job history contains: 1.additinal information from outside. for example: query plan from Shark 2.RDD graph for the job. 3.task's start/stop and shuffle information 4.stage information a new class named JobLogger does this job: 1.each SparkContext has one JobLogger, and one folder is created for every JobLogger 2.JobLogger manages all history files of activeJobs running in that SparkCOntext, create one history file for each activeJob, and the file name is the jobID 3.JobLogger generate job history and outputted it into the history file Job history generation: 1.additional information from outside For example: to get queryplan from Shark, the interface between shark and spark would be modified to pass the information from Shark to Spark. 2.record RDD graph for each Job The RDD graph is printed using a top-down approach, the RDD dependencies are outputted recursively from finalRDD, and the parent-child relationship is represented by indent. 3.task's start/stop and shuffle information can be gotten from TaskMetrics and TaskSetManager 4.stage information can be gotten from StageInfo and DAGScheduler", "reporter": "mingfei.shi", "assignee": "mingfei.shi", "created": "2013-03-06T21:59:13.000+0000", "updated": "2013-07-28T16:01:26.000+0000", "resolved": "2013-07-28T16:01:26.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Reynold Xin", "body": "Hi Mingfei - this is a great idea. I am all for more logging and more controllable logging. One thing to keep in mind is to not break the existing APIs.", "created": "2013-03-07T11:07:34.761+0000"}, {"author": "Jason Dai", "body": "Our goal is to have something similar to the job history log that Hadoop has, which can greatly help performance analysis. We have seen some strange behaviors when running TPC-H queries.", "created": "2013-03-07T19:46:11.242+0000"}], "num_comments": 2, "text": "Issue: SPARK-708\nSummary: add a JobLogger for Spark\nDescription: Current spark logs are outputted to console or one spark log file, which is not convenient for analysis of one single job. We would like to implement a JobLogger for Spark which output one history file for each job(ActiveJob). now the Spark has task metrics and summaries. the history file can be built on top of them. the job history contains: 1.additinal information from outside. for example: query plan from Shark 2.RDD graph for the job. 3.task's start/stop and shuffle information 4.stage information a new class named JobLogger does this job: 1.each SparkContext has one JobLogger, and one folder is created for every JobLogger 2.JobLogger manages all history files of activeJobs running in that SparkCOntext, create one history file for each activeJob, and the file name is the jobID 3.JobLogger generate job history and outputted it into the history file Job history generation: 1.additional information from outside For example: to get queryplan from Shark, the interface between shark and spark would be modified to pass the information from Shark to Spark. 2.record RDD graph for each Job The RDD graph is printed using a top-down approach, the RDD dependencies are outputted recursively from finalRDD, and the parent-child relationship is represented by indent. 3.task's start/stop and shuffle information can be gotten from TaskMetrics and TaskSetManager 4.stage information can be gotten from StageInfo and DAGScheduler\n\nComments (2):\n1. Reynold Xin: Hi Mingfei - this is a great idea. I am all for more logging and more controllable logging. One thing to keep in mind is to not break the existing APIs.\n2. Jason Dai: Our goal is to have something similar to the job history log that Hadoop has, which can greatly help performance analysis. We have seen some strange behaviors when running TPC-H queries.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "6e2f94250a54ff6dfa5260cb449ab734", "issue_key": "SPARK-709", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Dropping a block reports 0 bytes", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "2013-03-07T11:56:06.000+0000", "updated": "2014-09-21T16:48:00.000+0000", "resolved": "2014-09-21T16:48:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Matthew Farrellee", "body": "[~rxin] there isn't enough information to make progress on this, but feel free to re-open if you so desire.", "created": "2014-09-21T16:48:00.395+0000"}], "num_comments": 1, "text": "Issue: SPARK-709\nSummary: Dropping a block reports 0 bytes\n\nComments (1):\n1. Matthew Farrellee: [~rxin] there isn't enough information to make progress on this, but feel free to re-open if you so desire.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "38f84c77ab6aa69ea04b2f8277b01933", "issue_key": "SPARK-710", "issue_type": "Task", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Remove dependency on Twitter4J repository", "description": "This maven repository is blocked in China. We should get rid of that dependency so people in China can compile Spark.", "reporter": "Reynold Xin", "assignee": null, "created": "2013-03-07T13:40:55.000+0000", "updated": "2013-06-29T23:38:19.000+0000", "resolved": "2013-06-29T23:38:19.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "I think we definitely need to not have the build break for anyone in China. One option would be to manually include that JAR with Spark (and maybe risk having Spark be blocked as well). We could also just remove this from the API, since streaming is still in Alpha. The latter may be more clean cut.", "created": "2013-03-07T18:40:25.308+0000"}, {"author": "Nicholas Pentreath", "body": "Also, perhaps the approach used by Storm is a good one? There is a storm-contrib project which is separate from the main one. Each subproject within that is independent and can be pulled in as a dependency as needed. This also avoids the future case where the methods attached to a StreamingContext to create new input streams become very cluttered with fairly special cases (HBase, Cassandra, Redis, Mongo, SQS, etc etc). spark-contrib could then also contain some custom things like Hadoop InputFormats and convenience code around using them, etc etc.", "created": "2013-03-12T08:09:00.182+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This is now fixed because Twitter4J now publishes to Maven Central.", "created": "2013-06-29T23:38:19.081+0000"}], "num_comments": 3, "text": "Issue: SPARK-710\nSummary: Remove dependency on Twitter4J repository\nDescription: This maven repository is blocked in China. We should get rid of that dependency so people in China can compile Spark.\n\nComments (3):\n1. Patrick McFadin: I think we definitely need to not have the build break for anyone in China. One option would be to manually include that JAR with Spark (and maybe risk having Spark be blocked as well). We could also just remove this from the API, since streaming is still in Alpha. The latter may be more clean cut.\n2. Nicholas Pentreath: Also, perhaps the approach used by Storm is a good one? There is a storm-contrib project which is separate from the main one. Each subproject within that is independent and can be pulled in as a dependency as needed. This also avoids the future case where the methods attached to a StreamingContext to create new input streams become very cluttered with fairly special cases (HBase, Cassandra, Redis, Mongo, SQS, etc etc). spark-contrib could then also contain some custom things like Hadoop InputFormats and convenience code around using them, etc etc.\n3. Matei Alexandru Zaharia: This is now fixed because Twitter4J now publishes to Maven Central.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "2724f7cef7817dbe488db636e557c361", "issue_key": "SPARK-711", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Spark Streaming 0.7.0: ArrayIndexOutOfBoundsException in KafkaWordCount Example", "description": "The unmodified KafkaWordCount example is crashing when run under Mesos. It works fine when the master is \"local\". The KafkaWordCount job works for about 5 iterations, then the exceptions start. This problem is related to windowing. Here is the stack trace: 13/03/07 15:43:46 ERROR streaming.JobManager: Running streaming job 5 @ 1362696226000 ms failed java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:71) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:65) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:63) at spark.RDD.partitions(RDD.scala:168) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:646) at spark.RDD.partitions(RDD.scala:168) at spark.RDD.take(RDD.scala:579) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:495) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:494) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) Please let me know if I can help or provide additional information. Craig", "reporter": "Craig A. Vanderborgh", "assignee": null, "created": "2013-03-07T14:51:17.000+0000", "updated": "2014-04-25T22:09:05.000+0000", "resolved": "2014-04-25T22:09:05.000+0000", "labels": [], "components": ["DStreams"], "comments": [{"author": "Craig A. Vanderborgh", "body": "OK.. I have NO IDEA how I figured this out, just a gut feeling really.. The problem here is that the KafkaWordCount.scala code does this: ssc.checkpoint(\"checkpoint\") where \"checkpoint\" is a directory. The problem is this relative path is meaningless on the slave. So I configured a system-wide checkpointing directory on my test machine, and changed the above code to the following: ssc.checkpoint(\"/b/spark/checkpoint\") And BAM.. it's working :o)", "created": "2013-03-07T15:19:26.223+0000"}, {"author": "Tathagata Das", "body": "Duplicate issue: https://issues.apache.org/jira/browse/SPARK-703", "created": "2014-04-25T22:09:05.522+0000"}], "num_comments": 2, "text": "Issue: SPARK-711\nSummary: Spark Streaming 0.7.0: ArrayIndexOutOfBoundsException in KafkaWordCount Example\nDescription: The unmodified KafkaWordCount example is crashing when run under Mesos. It works fine when the master is \"local\". The KafkaWordCount job works for about 5 iterations, then the exceptions start. This problem is related to windowing. Here is the stack trace: 13/03/07 15:43:46 ERROR streaming.JobManager: Running streaming job 5 @ 1362696226000 ms failed java.lang.ArrayIndexOutOfBoundsException: 0 at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:71) at spark.rdd.CoGroupedRDD$$anonfun$getPartitions$1$$anonfun$apply$mcVI$sp$1.apply(CoGroupedRDD.scala:65) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.mutable.ArrayBuffer.map(ArrayBuffer.scala:47) at spark.rdd.CoGroupedRDD.getPartitions(CoGroupedRDD.scala:63) at spark.RDD.partitions(RDD.scala:168) at spark.MappedValuesRDD.getPartitions(PairRDDFunctions.scala:646) at spark.RDD.partitions(RDD.scala:168) at spark.RDD.take(RDD.scala:579) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:495) at spark.streaming.DStream$$anonfun$foreachFunc$2$1.apply(DStream.scala:494) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:22) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:21) at spark.streaming.Job.run(Job.scala:10) at spark.streaming.JobManager$JobHandler.run(JobManager.scala:17) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) Please let me know if I can help or provide additional information. Craig\n\nComments (2):\n1. Craig A. Vanderborgh: OK.. I have NO IDEA how I figured this out, just a gut feeling really.. The problem here is that the KafkaWordCount.scala code does this: ssc.checkpoint(\"checkpoint\") where \"checkpoint\" is a directory. The problem is this relative path is meaningless on the slave. So I configured a system-wide checkpointing directory on my test machine, and changed the above code to the following: ssc.checkpoint(\"/b/spark/checkpoint\") And BAM.. it's working :o)\n2. Tathagata Das: Duplicate issue: https://issues.apache.org/jira/browse/SPARK-703", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "6679babeaaeb295781bc8258196ef72b", "issue_key": "SPARK-712", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Kafka OutOfMemoryError", "description": "Using KafkaInputDStream causes OutOfMemoryErrors after running for a period of time (1-2 minutes in my case). The bug is not within Spark Streaming, but rather has something to do with how the provided jar under /streaming/lib/ was packaged. If it is helpful- We have packaged Kafka 0.7.2 /w scala 2.9.2 that we run in many prod environments that we would be happy to provide. The issue is easily reproducible, here is code I ran using the kafka jar that is bundled with Spark Streaming-  import java.util.concurrent.Executors import java.util.Properties import kafka.consumer._ import kafka.message.{Message, MessageSet, MessageAndMetadata} import kafka.serializer.StringDecoder import kafka.utils.{Utils, ZKGroupTopicDirs} private class MessageHandler(stream: KafkaStream[String]) extends Runnable { var index = 0 def run() { stream.takeWhile { msgAndMetadata => if (index%1000 == 0) { println(\"got: \" + index) } index += 1 true } } } object KafkaTest { def main(args: Array[String]) { val props = new Properties() props.put(\"zk.connect\", \"ozoo01.staging.dmz,ozoo02.staging.dmz,ozoo03.staging.dmz\") props.put(\"groupid\", \"my-cool-consumer-group\") props.put(\"zk.connectiontimeout.ms\", \"100000\") val executorPool = Executors.newFixedThreadPool(1) val consumerConfig = new ConsumerConfig(props) val consumerConnector: ZookeeperConsumerConnector = Consumer.create(consumerConfig).asInstanceOf[ZookeeperConsumerConnector] val topicMessageStreams = consumerConnector.createMessageStreams(Map(\"las_01_scsRawHits\" -> 1), new StringDecoder()) topicMessageStreams.values.foreach { streams => streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) } } } }  Here is the output: got: 0 got: 1000 got: 2000 got: 3000 got: 4000 ... got: 158000 got: 159000 got: 160000 got: 161000 got: 162000 13/03/12 22:47:14 ERROR network.BoundedByteBufferReceive: OOME with size 4194330 java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:14 ERROR consumer.FetcherRunnable: error in FetcherRunnable java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:14 INFO consumer.FetcherRunnable: stopping fetcher FetchRunnable-1 to host oagg01.staging.dmz got: 163000 13/03/12 22:47:16 ERROR network.BoundedByteBufferReceive: OOME with size 4194330 java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:16 ERROR consumer.FetcherRunnable: error in FetcherRunnable java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:18 INFO consumer.FetcherRunnable: stopping fetcher FetchRunnable-0 to host oagg02.staging.dmz", "reporter": "SeanM", "assignee": "SeanM", "created": "2013-03-12T22:22:37.000+0000", "updated": "2013-07-18T15:48:11.000+0000", "resolved": "2013-07-18T15:48:01.000+0000", "labels": [], "components": ["DStreams"], "comments": [{"author": "SeanM", "body": "The OOM issue is caused by using takeWhile. https://github.com/mesos/spark/pull/527 has the fix. Thanks", "created": "2013-03-14T23:22:41.741+0000"}, {"author": "SeanM", "body": "https://github.com/mesos/spark/pull/527 was merged in, and the issue is fixed. So this can be marked as resolved (I don't see an option to close this issue.)", "created": "2013-07-18T15:26:59.952+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I've marked it as resolved. Thanks!", "created": "2013-07-18T15:48:11.701+0000"}], "num_comments": 3, "text": "Issue: SPARK-712\nSummary: Kafka OutOfMemoryError\nDescription: Using KafkaInputDStream causes OutOfMemoryErrors after running for a period of time (1-2 minutes in my case). The bug is not within Spark Streaming, but rather has something to do with how the provided jar under /streaming/lib/ was packaged. If it is helpful- We have packaged Kafka 0.7.2 /w scala 2.9.2 that we run in many prod environments that we would be happy to provide. The issue is easily reproducible, here is code I ran using the kafka jar that is bundled with Spark Streaming-  import java.util.concurrent.Executors import java.util.Properties import kafka.consumer._ import kafka.message.{Message, MessageSet, MessageAndMetadata} import kafka.serializer.StringDecoder import kafka.utils.{Utils, ZKGroupTopicDirs} private class MessageHandler(stream: KafkaStream[String]) extends Runnable { var index = 0 def run() { stream.takeWhile { msgAndMetadata => if (index%1000 == 0) { println(\"got: \" + index) } index += 1 true } } } object KafkaTest { def main(args: Array[String]) { val props = new Properties() props.put(\"zk.connect\", \"ozoo01.staging.dmz,ozoo02.staging.dmz,ozoo03.staging.dmz\") props.put(\"groupid\", \"my-cool-consumer-group\") props.put(\"zk.connectiontimeout.ms\", \"100000\") val executorPool = Executors.newFixedThreadPool(1) val consumerConfig = new ConsumerConfig(props) val consumerConnector: ZookeeperConsumerConnector = Consumer.create(consumerConfig).asInstanceOf[ZookeeperConsumerConnector] val topicMessageStreams = consumerConnector.createMessageStreams(Map(\"las_01_scsRawHits\" -> 1), new StringDecoder()) topicMessageStreams.values.foreach { streams => streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) } } } }  Here is the output: got: 0 got: 1000 got: 2000 got: 3000 got: 4000 ... got: 158000 got: 159000 got: 160000 got: 161000 got: 162000 13/03/12 22:47:14 ERROR network.BoundedByteBufferReceive: OOME with size 4194330 java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:14 ERROR consumer.FetcherRunnable: error in FetcherRunnable java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:14 INFO consumer.FetcherRunnable: stopping fetcher FetchRunnable-1 to host oagg01.staging.dmz got: 163000 13/03/12 22:47:16 ERROR network.BoundedByteBufferReceive: OOME with size 4194330 java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:16 ERROR consumer.FetcherRunnable: error in FetcherRunnable java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:331) at kafka.network.BoundedByteBufferReceive.byteBufferAllocate(BoundedByteBufferReceive.scala:80) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:63) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.consumer.SimpleConsumer.getResponse(SimpleConsumer.scala:177) at kafka.consumer.SimpleConsumer.liftedTree2$1(SimpleConsumer.scala:117) at kafka.consumer.SimpleConsumer.multifetch(SimpleConsumer.scala:115) at kafka.consumer.FetcherRunnable.run(FetcherRunnable.scala:60) 13/03/12 22:47:18 INFO consumer.FetcherRunnable: stopping fetcher FetchRunnable-0 to host oagg02.staging.dmz\n\nComments (3):\n1. SeanM: The OOM issue is caused by using takeWhile. https://github.com/mesos/spark/pull/527 has the fix. Thanks\n2. SeanM: https://github.com/mesos/spark/pull/527 was merged in, and the issue is fixed. So this can be marked as resolved (I don't see an option to close this issue.)\n3. Matei Alexandru Zaharia: I've marked it as resolved. Thanks!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "99c7a6ea5f0639323fdbd3be54c73b0b", "issue_key": "SPARK-713", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Broken link in quick start guide", "description": "the link to \"Java Programming Guide\" in the quick start (i.e. docs/ links to http://spark-project.org/docs/0.6.2/\"java-programming-guide\" (with the quotes) when it should point to http://spark-project.org/docs/0.6.2/java-programming-guide.html", "reporter": "Andy Konwinski", "assignee": "Andy Konwinski", "created": "2013-03-13T02:04:15.000+0000", "updated": "2013-03-13T12:11:40.000+0000", "resolved": "2013-03-13T12:11:40.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Andy Konwinski", "body": "Just found another broken link in same document, this time to the Python programming guide.", "created": "2013-03-13T02:24:30.053+0000"}, {"author": "Andy Konwinski", "body": "Fixed in merge https://github.com/mesos/spark/pull/523", "created": "2013-03-13T12:11:40.700+0000"}], "num_comments": 2, "text": "Issue: SPARK-713\nSummary: Broken link in quick start guide\nDescription: the link to \"Java Programming Guide\" in the quick start (i.e. docs/ links to http://spark-project.org/docs/0.6.2/\"java-programming-guide\" (with the quotes) when it should point to http://spark-project.org/docs/0.6.2/java-programming-guide.html\n\nComments (2):\n1. Andy Konwinski: Just found another broken link in same document, this time to the Python programming guide.\n2. Andy Konwinski: Fixed in merge https://github.com/mesos/spark/pull/523", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "3c0931a8d9d98661c41026cfee9235c0", "issue_key": "SPARK-714", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Link to YARN document broken in \"Launching Spark on YARN\" doc", "description": "The first hyperlink in the body of the \"Launching Spark on YARN\" doc (see http://www.spark-project.org/docs/0.7.0/running-on-yarn.html) currently links to http://hadoop.apache.org/docs/r2.0.1-alpha/hadoop-yarn/hadoop-yarn-site/YARN.html which now returns a 404 page not found. Should we link to http://hadoop.apache.org/docs/r2.0.3-alpha/hadoop-yarn/hadoop-yarn-site/YARN.html instead (which does currently work)?", "reporter": "Andy Konwinski", "assignee": "Andy Konwinski", "created": "2013-03-13T02:35:24.000+0000", "updated": "2013-03-16T11:44:57.000+0000", "resolved": "2013-03-16T11:44:57.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Andy Konwinski", "body": "Fixed in https://github.com/mesos/spark/pull/524 Though the link we updated to use might break as well since it is still an alpha version.", "created": "2013-03-16T11:44:57.992+0000"}], "num_comments": 1, "text": "Issue: SPARK-714\nSummary: Link to YARN document broken in \"Launching Spark on YARN\" doc\nDescription: The first hyperlink in the body of the \"Launching Spark on YARN\" doc (see http://www.spark-project.org/docs/0.7.0/running-on-yarn.html) currently links to http://hadoop.apache.org/docs/r2.0.1-alpha/hadoop-yarn/hadoop-yarn-site/YARN.html which now returns a 404 page not found. Should we link to http://hadoop.apache.org/docs/r2.0.3-alpha/hadoop-yarn/hadoop-yarn-site/YARN.html instead (which does currently work)?\n\nComments (1):\n1. Andy Konwinski: Fixed in https://github.com/mesos/spark/pull/524 Though the link we updated to use might break as well since it is still an alpha version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "499f24fc4d8e704ea749a83404af6343", "issue_key": "SPARK-715", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add documentation for using Maven to build Spark using", "description": "Currently it is not obvious how to use Maven to build Spark out of the box. The instructions at https://github.com/mesos/spark/pull/310 should be added to the documentation (in the docs dir) and/or the README.", "reporter": "Andy Konwinski", "assignee": "Andy Konwinski", "created": "2013-03-16T04:48:24.000+0000", "updated": "2013-04-04T13:47:52.000+0000", "resolved": "2013-04-04T13:47:52.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Nicholas Pentreath", "body": "Is this not done in https://github.com/mesos/spark/pull/532? So can be closed?", "created": "2013-04-03T06:49:42.344+0000"}, {"author": "Andy Konwinski", "body": "Yep. Can be closed. Thanks for catching. On Apr 3, 2013 6:51 AM, \"Nick Pentreath (JIRA)\" <", "created": "2013-04-03T08:59:06.242+0000"}, {"author": "Andy Konwinski", "body": "Fixed in pull request #532 - https://github.com/mesos/spark/pull/532", "created": "2013-04-04T13:47:52.142+0000"}], "num_comments": 3, "text": "Issue: SPARK-715\nSummary: Add documentation for using Maven to build Spark using\nDescription: Currently it is not obvious how to use Maven to build Spark out of the box. The instructions at https://github.com/mesos/spark/pull/310 should be added to the documentation (in the docs dir) and/or the README.\n\nComments (3):\n1. Nicholas Pentreath: Is this not done in https://github.com/mesos/spark/pull/532? So can be closed?\n2. Andy Konwinski: Yep. Can be closed. Thanks for catching. On Apr 3, 2013 6:51 AM, \"Nick Pentreath (JIRA)\" <\n3. Andy Konwinski: Fixed in pull request #532 - https://github.com/mesos/spark/pull/532", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "238809de6ef955438c89e4a077ff62e6", "issue_key": "SPARK-716", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Unit tests fail out of the box", "description": "At commit c1e9cdc49f89222b366a14a20ffd937ca0fb9adc $ sbt/sbt test ... [info] DriverSuite: [info] - driver should exit after finishing *** FAILED *** [info] SparkException was thrown during property evaluation. (DriverSuite.scala:15) [info] Message: Process List(./run, spark.DriverWithoutCleanup, local-cluster[2,1,512]) exited with code 1 [info] Occurred at table row 1 (zero based, not counting headings), which had values ( [info] master = local-cluster[2,1,512] [info] ) ... [error] Failed: : Total 203, Failed 1, Errors 0, Passed 202, Skipped 0 [error] Failed tests: [error] spark.DriverSuite ... [error] {file:/Users/andyk/Development/spark/}core/test:test: Tests unsuccessful [error] Total time: 676 s, completed Mar 17, 2013 1:47:55 PM", "reporter": "Andy Konwinski", "assignee": null, "created": "2013-03-17T16:00:26.000+0000", "updated": "2014-02-19T13:44:32.000+0000", "resolved": "2013-12-07T14:16:40.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "We might just want to drop this test or convert it into a \"benchmark\" or something, so that failures don't mark the test suites as failed. I added this test after fixing a bug where some \"daemon\" threads weren't actually being constructed as daemon threads, which might have prevented clients from exiting properly without calling System.exit().", "created": "2013-04-04T15:59:53.667+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This probably failed because you need to create a spark-env.sh and set SCALA_HOME to run it. We should warn better about that situation. I want these tests to run, so disabling them isn't an option.", "created": "2013-04-07T18:18:24.193+0000"}, {"author": "Sean Owen", "body": "Hi all, Sandy and I are seeing the same error when running the Maven-based tests in the CDH integration environment. To reproduce, you can just run the DriverSuite test from within core/, without building 'sbt assembly'. It works after running 'sbt assembly' but the Maven build doesn't ensure that has been run. I suppose I would imagine the Maven build should not depend on sbt. If that's the case I think this test might be removed? i.e. I don't think it's just a matter of setting env variables. Or if the intent is that the Maven build needs to depend on 'sbt assembly', then we can try to figure out a way to execute that in the Maven build. What's the preferred solution?", "created": "2014-02-19T13:44:32.257+0000"}], "num_comments": 3, "text": "Issue: SPARK-716\nSummary: Unit tests fail out of the box\nDescription: At commit c1e9cdc49f89222b366a14a20ffd937ca0fb9adc $ sbt/sbt test ... [info] DriverSuite: [info] - driver should exit after finishing *** FAILED *** [info] SparkException was thrown during property evaluation. (DriverSuite.scala:15) [info] Message: Process List(./run, spark.DriverWithoutCleanup, local-cluster[2,1,512]) exited with code 1 [info] Occurred at table row 1 (zero based, not counting headings), which had values ( [info] master = local-cluster[2,1,512] [info] ) ... [error] Failed: : Total 203, Failed 1, Errors 0, Passed 202, Skipped 0 [error] Failed tests: [error] spark.DriverSuite ... [error] {file:/Users/andyk/Development/spark/}core/test:test: Tests unsuccessful [error] Total time: 676 s, completed Mar 17, 2013 1:47:55 PM\n\nComments (3):\n1. Josh Rosen: We might just want to drop this test or convert it into a \"benchmark\" or something, so that failures don't mark the test suites as failed. I added this test after fixing a bug where some \"daemon\" threads weren't actually being constructed as daemon threads, which might have prevented clients from exiting properly without calling System.exit().\n2. Matei Alexandru Zaharia: This probably failed because you need to create a spark-env.sh and set SCALA_HOME to run it. We should warn better about that situation. I want these tests to run, so disabling them isn't an option.\n3. Sean Owen: Hi all, Sandy and I are seeing the same error when running the Maven-based tests in the CDH integration environment. To reproduce, you can just run the DriverSuite test from within core/, without building 'sbt assembly'. It works after running 'sbt assembly' but the Maven build doesn't ensure that has been run. I suppose I would imagine the Maven build should not depend on sbt. If that's the case I think this test might be removed? i.e. I don't think it's just a matter of setting env variables. Or if the intent is that the Maven build needs to depend on 'sbt assembly', then we can try to figure out a way to execute that in the Maven build. What's the preferred solution?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "ee57992364925409d8ef9f74f70fc019", "issue_key": "SPARK-717", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Refactor Programming Guides in Documentation", "description": "Currently, many Spark fundamentals concepts that are language agnostic are part of the Scala programming guide. This makes the Java and Python guides feel like 2nd class citizens. We should either factor out these general concepts into a \"Spark Concepts\" doc and have the Scala guide be more parallel with the Java and Python guides, or switch to a programming guide that covers all three languages with a table showing the different languages supported at the top and tabs on the code samples for the different languages like the AMP Camp mini course http://ampcamp.berkeley.edu/big-data-mini-course/", "reporter": "Andy Konwinski", "assignee": "Andy Konwinski", "created": "2013-03-18T10:30:15.000+0000", "updated": "2014-07-26T20:26:30.000+0000", "resolved": "2014-07-26T20:26:30.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Jyotiska NK", "body": "I agree with this. I use only Python API for Spark. We need a better programming guide to encourage Python programmers. Is anyone working on this? I can help out.", "created": "2014-02-05T22:44:28.346+0000"}, {"author": "Reynold Xin", "body": "I don't think anybody is actively working on this at this point (others might in the future). If you have time, it would be great to submit them in the form of pull requests. Thanks [~jyotiska]", "created": "2014-02-05T22:46:08.780+0000"}], "num_comments": 2, "text": "Issue: SPARK-717\nSummary: Refactor Programming Guides in Documentation\nDescription: Currently, many Spark fundamentals concepts that are language agnostic are part of the Scala programming guide. This makes the Java and Python guides feel like 2nd class citizens. We should either factor out these general concepts into a \"Spark Concepts\" doc and have the Scala guide be more parallel with the Java and Python guides, or switch to a programming guide that covers all three languages with a table showing the different languages supported at the top and tabs on the code samples for the different languages like the AMP Camp mini course http://ampcamp.berkeley.edu/big-data-mini-course/\n\nComments (2):\n1. Jyotiska NK: I agree with this. I use only Python API for Spark. We need a better programming guide to encourage Python programmers. Is anyone working on this? I can help out.\n2. Reynold Xin: I don't think anybody is actively working on this at this point (others might in the future). If you have time, it would be great to submit them in the form of pull requests. Thanks [~jyotiska]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "3f0cb59329742182015d6a326efe219d", "issue_key": "SPARK-718", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "NPE when performing action during transformation", "description": "Running the spark shell: The following code fails with a NPE when trying to collect the resulting RDD:  val data = sc.parallelize(1 to 10) data.map(i => data.count).collect   ERROR local.LocalScheduler: Exception in task 0 java.lang.NullPointerException at spark.RDD.count(RDD.scala:490) at $line16.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply$mcJI$sp(<console>:15) at $line16.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:15) at $line16.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:15) at scala.collection.Iterator$$anon$19.next(Iterator.scala:401) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399) at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:102) at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:250) at scala.collection.Iterator$$anon$19.toBuffer(Iterator.scala:399) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:237) at scala.collection.Iterator$$anon$19.toArray(Iterator.scala:399) at spark.RDD$$anonfun$1.apply(RDD.scala:389) at spark.RDD$$anonfun$1.apply(RDD.scala:389) at spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:610) at spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:610) at spark.scheduler.ResultTask.run(ResultTask.scala:76) at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74) at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:722)", "reporter": "Krzywicki", "assignee": null, "created": "2013-03-20T06:03:26.000+0000", "updated": "2014-09-21T16:46:31.000+0000", "resolved": "2014-09-21T16:46:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "I don't think this is even a valid \"transformation\" - calling an action within a transformation.", "created": "2013-03-20T06:07:47.379+0000"}, {"author": "Krzywicki", "body": "Hmm, so you mean that an action can only be called from the driver?", "created": "2013-03-20T06:37:51.397+0000"}, {"author": "Reynold Xin", "body": "Yup - an action launches a task. Programs on the workers can't launch tasks. You can rewrite the program as  val data = sc.parallelize(1 to 10) val datacount = data.count data.map(i => datacount).collect", "created": "2013-03-20T06:40:53.736+0000"}, {"author": "Krzywicki", "body": "I see. Maybe that should be more emphasised in the docs, then. The code above was simplified - I actually wanted to so something like: data.map(x => x -> data.takeSample(false, n, rand.nextInt)) so that each element was mapped to some number of other elements draw randomly, at that each sample was computed in parallel. I suppose I will simply use some custom sampling on a broadcasted array of indices, then...", "created": "2013-03-20T07:29:02.843+0000"}, {"author": "Matthew Farrellee", "body": "Spark simply does not support nesting RDDs in this fashion. you'll get a more prompt response and information with the user list, see http://spark.apache.org/community.html. i'm going to close this issue, but if you want feel free to re-open it.", "created": "2014-09-21T16:46:14.994+0000"}], "num_comments": 5, "text": "Issue: SPARK-718\nSummary: NPE when performing action during transformation\nDescription: Running the spark shell: The following code fails with a NPE when trying to collect the resulting RDD:  val data = sc.parallelize(1 to 10) data.map(i => data.count).collect   ERROR local.LocalScheduler: Exception in task 0 java.lang.NullPointerException at spark.RDD.count(RDD.scala:490) at $line16.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply$mcJI$sp(<console>:15) at $line16.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:15) at $line16.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:15) at scala.collection.Iterator$$anon$19.next(Iterator.scala:401) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399) at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:102) at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:250) at scala.collection.Iterator$$anon$19.toBuffer(Iterator.scala:399) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:237) at scala.collection.Iterator$$anon$19.toArray(Iterator.scala:399) at spark.RDD$$anonfun$1.apply(RDD.scala:389) at spark.RDD$$anonfun$1.apply(RDD.scala:389) at spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:610) at spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:610) at spark.scheduler.ResultTask.run(ResultTask.scala:76) at spark.scheduler.local.LocalScheduler.runTask$1(LocalScheduler.scala:74) at spark.scheduler.local.LocalScheduler$$anon$1.run(LocalScheduler.scala:50) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) at java.util.concurrent.FutureTask.run(FutureTask.java:166) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:722)\n\nComments (5):\n1. Reynold Xin: I don't think this is even a valid \"transformation\" - calling an action within a transformation.\n2. Krzywicki: Hmm, so you mean that an action can only be called from the driver?\n3. Reynold Xin: Yup - an action launches a task. Programs on the workers can't launch tasks. You can rewrite the program as  val data = sc.parallelize(1 to 10) val datacount = data.count data.map(i => datacount).collect\n4. Krzywicki: I see. Maybe that should be more emphasised in the docs, then. The code above was simplified - I actually wanted to so something like: data.map(x => x -> data.takeSample(false, n, rand.nextInt)) so that each element was mapped to some number of other elements draw randomly, at that each sample was computed in parallel. I suppose I will simply use some custom sampling on a broadcasted array of indices, then...\n5. Matthew Farrellee: Spark simply does not support nesting RDDs in this fashion. you'll get a more prompt response and information with the user list, see http://spark.apache.org/community.html. i'm going to close this issue, but if you want feel free to re-open it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "86ce73801157a18c43453837e7c36506", "issue_key": "SPARK-719", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Add FAQ page to documentation or webpage", "description": "Lots of issues on the mailing list are redundant (e.g., Patrick mentioned this question has been asked/answered multiple times https://groups.google.com/d/msg/spark-users/-mYn6BF-Y5Y/8qeXuxs8_d0J). We should put the solutions to common problems on an FAQ page in the documentation or on the webpage.", "reporter": "Andy Konwinski", "assignee": "Andy Konwinski", "created": "2013-03-21T15:35:13.000+0000", "updated": "2014-09-21T14:27:07.000+0000", "resolved": "2014-09-21T14:27:07.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Josh Rosen", "body": "A few FAQ candidates, off the top of my head: - Issues related to hostnames, including binding Spark to the wrong address and issues with addresses and HDFS locality. - Running against the wrong version of Hadoop / a different version of Hadoop than Spark was compiled against. It might be nice to add a sentence or two about building against the CDH distributions, since I've seen several questions about that. - Building Spark in Eclipse / IntelliJ. We link to an external tutorial from the website, but it might be nice to fold this into Spark's documentation. - Forgetting to import the implicit conversions from Spark context. - Wrong / nonexistent SCALA_LIBRARY_PATH (for example, if Scala is not installed), which usually manifests itself as {{java.lang.NoSuchMethodError}}. - Problems installing the Mesos native library. - Compiling Spark on a machine without internet access. - Attempting to nest RDD transformations.", "created": "2013-04-05T20:09:46.138+0000"}, {"author": "Patrick McFadin", "body": "Another thing to add here: * What to do when an executor fails (go look in the work/ dir on the slaves)", "created": "2013-04-16T10:37:01.882+0000"}, {"author": "Andy Konwinski", "body": "I just noticed that we already have an FAQ available on the website (i.e. the wordpress site, not in the docs) at http://spark-project.org/faq/ Options: 1. Expand this one and leave it where it is. 2. Expand it and move it to the docs so that it will get versioned with the rest of the docs. 3. Leave the one at spark-project.org a general/high-level FAQ (possibly renaming it) about the project and add a 2nd faq to the docs that will get versioned with the rest of the docs about more technical issues like the ones Josh suggested above. If we have two, they should each state clearly the scope they cover and link to the other. What do people think?", "created": "2013-04-16T16:43:04.762+0000"}, {"author": "Josh Rosen", "body": "For Shark/Spark: - Make sure that you're using compatible versions of Spark and Shark. Using incompatible versions can lead to weird behavior, including Shark hanging while connecting to the Spark cluster.", "created": "2013-05-07T19:34:55.939+0000"}, {"author": "Josh Rosen", "body": "For Spark: - Random number generation (see https://groups.google.com/d/msg/spark-users/edPkZ6SBpyE/3SaObud3xIwJ)", "created": "2013-07-30T12:07:24.700+0000"}, {"author": "Matthew Farrellee", "body": "it looks like this has some good content, but it's stale and likely needs vetting. the new FAQ location is http://spark.apache.org/faq.html i'm going to close this since there has been no progress. note - it'll still be available via search feel free to re-open if you disagree.", "created": "2014-09-21T14:26:49.730+0000"}], "num_comments": 6, "text": "Issue: SPARK-719\nSummary: Add FAQ page to documentation or webpage\nDescription: Lots of issues on the mailing list are redundant (e.g., Patrick mentioned this question has been asked/answered multiple times https://groups.google.com/d/msg/spark-users/-mYn6BF-Y5Y/8qeXuxs8_d0J). We should put the solutions to common problems on an FAQ page in the documentation or on the webpage.\n\nComments (6):\n1. Josh Rosen: A few FAQ candidates, off the top of my head: - Issues related to hostnames, including binding Spark to the wrong address and issues with addresses and HDFS locality. - Running against the wrong version of Hadoop / a different version of Hadoop than Spark was compiled against. It might be nice to add a sentence or two about building against the CDH distributions, since I've seen several questions about that. - Building Spark in Eclipse / IntelliJ. We link to an external tutorial from the website, but it might be nice to fold this into Spark's documentation. - Forgetting to import the implicit conversions from Spark context. - Wrong / nonexistent SCALA_LIBRARY_PATH (for example, if Scala is not installed), which usually manifests itself as {{java.lang.NoSuchMethodError}}. - Problems installing the Mesos native library. - Compiling Spark on a machine without internet access. - Attempting to nest RDD transformations.\n2. Patrick McFadin: Another thing to add here: * What to do when an executor fails (go look in the work/ dir on the slaves)\n3. Andy Konwinski: I just noticed that we already have an FAQ available on the website (i.e. the wordpress site, not in the docs) at http://spark-project.org/faq/ Options: 1. Expand this one and leave it where it is. 2. Expand it and move it to the docs so that it will get versioned with the rest of the docs. 3. Leave the one at spark-project.org a general/high-level FAQ (possibly renaming it) about the project and add a 2nd faq to the docs that will get versioned with the rest of the docs about more technical issues like the ones Josh suggested above. If we have two, they should each state clearly the scope they cover and link to the other. What do people think?\n4. Josh Rosen: For Shark/Spark: - Make sure that you're using compatible versions of Spark and Shark. Using incompatible versions can lead to weird behavior, including Shark hanging while connecting to the Spark cluster.\n5. Josh Rosen: For Spark: - Random number generation (see https://groups.google.com/d/msg/spark-users/edPkZ6SBpyE/3SaObud3xIwJ)\n6. Matthew Farrellee: it looks like this has some good content, but it's stale and likely needs vetting. the new FAQ location is http://spark.apache.org/faq.html i'm going to close this since there has been no progress. note - it'll still be available via search feel free to re-open if you disagree.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "b875d75e7e5146a0046f41bb8ad5ca17", "issue_key": "SPARK-720", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Statically guarantee serialization will succeed", "description": "First, thanks for developing Spark. It's great. Maybe I'm trying to serialize weird objects (eg Shapeless constructs), but I tend to get quite a few NotSerializableExceptions. These are pretty annoying because they happen at runtime, lengthening my code/debug cycle. I'd like it if Spark could introduce a serialization system that could statically check that serialization will succeed. One approach is to use typeclasses, perhaps using Spray-Json as inspiration. An added benefit of typeclasses is they can be used to serialize objects that were not originally intended to be serialized.", "reporter": "Eric Christiansen", "assignee": null, "created": "2013-03-21T20:51:51.000+0000", "updated": "2016-01-08T12:12:04.000+0000", "resolved": "2016-01-08T12:12:04.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "A big design goal of Spark is that you don't have to have any type restrictions on the objects contained in an RDD. If the parametrized type of the RDD happens to implement Comparable then the shuffler can make optimizations, but it's not a requirement. It's also not a requirement that the type implements Serializable, because you could be using Kryo to serialize and transport an object out of the Spark user's control that doesn't have the Serializable marker interface. I think it would be a hard sell to add restrictions to the type parametrized type of an RDD. Getting over that, I think the intention of guaranteeing serialization via the type system could work for standard JVM serialization (Serializable and Externalizable interfaces) because a class's serializability is clearly marked with those interfaces already. But I'm concerned that it couldn't be made to work with other serializer systems such as Kryo where there is no convenient marker interface. Kryo does serialization by registering a serializer for each class, and using that Class->Serializer map for future serialization by reflectively looking at an object's type as it receives objects for serialization. The only way to know if a class is serializable is to know what classes a Kryo instance has registered at compile time, which I believe is impossible given that the Registrator comes from outside the Spark codebase. [~emchristiansen] do you see a way to implement this generically for JVM serialization + Kryo + other systems in the future? I think we may have to close this request for infeasibility.", "created": "2014-11-12T06:21:11.452+0000"}], "num_comments": 1, "text": "Issue: SPARK-720\nSummary: Statically guarantee serialization will succeed\nDescription: First, thanks for developing Spark. It's great. Maybe I'm trying to serialize weird objects (eg Shapeless constructs), but I tend to get quite a few NotSerializableExceptions. These are pretty annoying because they happen at runtime, lengthening my code/debug cycle. I'd like it if Spark could introduce a serialization system that could statically check that serialization will succeed. One approach is to use typeclasses, perhaps using Spray-Json as inspiration. An added benefit of typeclasses is they can be used to serialize objects that were not originally intended to be serialized.\n\nComments (1):\n1. Andrew Ash: A big design goal of Spark is that you don't have to have any type restrictions on the objects contained in an RDD. If the parametrized type of the RDD happens to implement Comparable then the shuffler can make optimizations, but it's not a requirement. It's also not a requirement that the type implements Serializable, because you could be using Kryo to serialize and transport an object out of the Spark user's control that doesn't have the Serializable marker interface. I think it would be a hard sell to add restrictions to the type parametrized type of an RDD. Getting over that, I think the intention of guaranteeing serialization via the type system could work for standard JVM serialization (Serializable and Externalizable interfaces) because a class's serializability is clearly marked with those interfaces already. But I'm concerned that it couldn't be made to work with other serializer systems such as Kryo where there is no convenient marker interface. Kryo does serialization by registering a serializer for each class, and using that Class->Serializer map for future serialization by reflectively looking at an object's type as it receives objects for serialization. The only way to know if a class is serializable is to know what classes a Kryo instance has registered at compile time, which I believe is impossible given that the Registrator comes from outside the Spark codebase. [~emchristiansen] do you see a way to implement this generically for JVM serialization + Kryo + other systems in the future? I think we may have to close this request for infeasibility.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "3902eedf6674e0d67efe33f7326b43f5", "issue_key": "SPARK-721", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Fix remaining deprecation warnings", "description": "The recent patch to re-enable deprecation warnings fixed many of them, but there's still a few left; it would be nice to fix them. For example, here's one in RDDSuite:  [warn] /Users/joshrosen/Documents/spark/spark/core/src/test/scala/spark/RDDSuite.scala:32: method mapPartitionsWithSplit in class RDD is deprecated: use mapPartitionsWithIndex [warn] val partitionSumsWithSplit = nums.mapPartitionsWithSplit { [warn] ^ [warn] one warning found  Also, it looks like Scala 2.9 added a second \"deprecatedSince\" parameter to @Deprecated. We didn't fill this in, which causes some additional warnings:  [warn] /Users/joshrosen/Documents/spark/spark/core/src/main/scala/spark/RDD.scala:370: @deprecated now takes two arguments; see the scaladoc. [warn] @deprecated(\"use mapPartitionsWithIndex\") [warn] ^ [warn] one warning found", "reporter": "Josh Rosen", "assignee": null, "created": "2013-03-29T16:18:30.000+0000", "updated": "2014-06-22T07:04:43.000+0000", "resolved": "2014-06-22T07:04:43.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Gary Struthers", "body": "Most warnings are due to mixing the old org.apache.hadoop.mapred with the new org.apache.hadoop.mapreduce. I hoped to localize it around HadoopWriter but now I'm trying to make versions of the affected files that use only the new package.", "created": "2013-08-10T18:06:48.279+0000"}, {"author": "Gary Struthers", "body": "Hadoop mapred.JobConf deprecation warnings have already been fixed and merged. Pull request 707 fixes another Hadoop deprecation warning; calls to org.apache.hadoop.mapred.OutputCommitter.cleanup() have been deleted in core {PairRDDFunctions, HadoopWriter}. The deleted cleanup() calls were always after OutputCommitter.commit(), which is just a wrapper for cleanup(). So, I'm backing out and stopping progress. The remaining deprecation warning is a last resort call to Thread.stop in repl/src/main/scala/spark/repl/SparkILoop.scala. Is this better than just having the user kill a runaway repl process?", "created": "2013-08-25T15:23:17.889+0000"}, {"author": "Reynold Xin", "body": "Gary - thanks for looking into this. Do you know what that thread.stop is doing?", "created": "2013-09-18T04:42:51.996+0000"}, {"author": "Gary Struthers", "body": "Reynold - Thread.stop was fixed in the 2.10 branch https://github.com/mesos/spark/pull/535/files as part of a larger rewrite. The override of onRunaway, which called it, is gone https://github.com/mesos/spark/blob/scala-2.10/repl/src/main/scala/spark/repl/SparkILoop.scala FYI Scala's repl used to call Thread.stop and that and the Star Trek joke were copied from there. Scala fixed this by 2.9. I haven't gone through the history to see just when but Cntl-C exits the repl without the joke.", "created": "2013-09-23T12:51:55.653+0000"}, {"author": "Sean R. Owen", "body": "This appears to be resolved as I don't think these warnings have been in the build for a while.", "created": "2014-06-21T21:01:12.039+0000"}], "num_comments": 5, "text": "Issue: SPARK-721\nSummary: Fix remaining deprecation warnings\nDescription: The recent patch to re-enable deprecation warnings fixed many of them, but there's still a few left; it would be nice to fix them. For example, here's one in RDDSuite:  [warn] /Users/joshrosen/Documents/spark/spark/core/src/test/scala/spark/RDDSuite.scala:32: method mapPartitionsWithSplit in class RDD is deprecated: use mapPartitionsWithIndex [warn] val partitionSumsWithSplit = nums.mapPartitionsWithSplit { [warn] ^ [warn] one warning found  Also, it looks like Scala 2.9 added a second \"deprecatedSince\" parameter to @Deprecated. We didn't fill this in, which causes some additional warnings:  [warn] /Users/joshrosen/Documents/spark/spark/core/src/main/scala/spark/RDD.scala:370: @deprecated now takes two arguments; see the scaladoc. [warn] @deprecated(\"use mapPartitionsWithIndex\") [warn] ^ [warn] one warning found\n\nComments (5):\n1. Gary Struthers: Most warnings are due to mixing the old org.apache.hadoop.mapred with the new org.apache.hadoop.mapreduce. I hoped to localize it around HadoopWriter but now I'm trying to make versions of the affected files that use only the new package.\n2. Gary Struthers: Hadoop mapred.JobConf deprecation warnings have already been fixed and merged. Pull request 707 fixes another Hadoop deprecation warning; calls to org.apache.hadoop.mapred.OutputCommitter.cleanup() have been deleted in core {PairRDDFunctions, HadoopWriter}. The deleted cleanup() calls were always after OutputCommitter.commit(), which is just a wrapper for cleanup(). So, I'm backing out and stopping progress. The remaining deprecation warning is a last resort call to Thread.stop in repl/src/main/scala/spark/repl/SparkILoop.scala. Is this better than just having the user kill a runaway repl process?\n3. Reynold Xin: Gary - thanks for looking into this. Do you know what that thread.stop is doing?\n4. Gary Struthers: Reynold - Thread.stop was fixed in the 2.10 branch https://github.com/mesos/spark/pull/535/files as part of a larger rewrite. The override of onRunaway, which called it, is gone https://github.com/mesos/spark/blob/scala-2.10/repl/src/main/scala/spark/repl/SparkILoop.scala FYI Scala's repl used to call Thread.stop and that and the Star Trek joke were copied from there. Scala fixed this by 2.9. I haven't gone through the history to see just when but Cntl-C exits the repl without the joke.\n5. Sean R. Owen: This appears to be resolved as I don't think these warnings have been in the build for a while.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "585cf9b8dd46a3559cdac9a62e9acbeb", "issue_key": "SPARK-722", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Prefix current Spark package names with \"org.spark-project.\"", "description": "I believe the standard practice is to have package names use the project domain name the same way the Maven groupId does. Currently we start the package name with spark, such as `package spark` or `package spark.streaming`. We should probably add the \"org.spark-project.\" prefix to all of our package names.", "reporter": "Andy Konwinski", "assignee": null, "created": "2013-04-01T10:59:12.000+0000", "updated": "2013-09-01T15:28:31.000+0000", "resolved": "2013-09-01T15:28:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "The current package naming can be problematic if you have a subpackage named {{spark}} as part of another project. If other code in that subpackage tries to import code from Spark, the name will conflict, requiring you to use the ugly workaround {{\\_root\\_.spark}} to access Spark. This cropped up for me today when trying to add a {{myproject.runtime.spark}} package to a different project using Spark.", "created": "2013-05-15T16:41:25.273+0000"}, {"author": "Josh Rosen", "body": "This is fixed in 0.8, where we moved to the org.apache.spark.* prefix.", "created": "2013-09-01T15:28:31.306+0000"}], "num_comments": 2, "text": "Issue: SPARK-722\nSummary: Prefix current Spark package names with \"org.spark-project.\"\nDescription: I believe the standard practice is to have package names use the project domain name the same way the Maven groupId does. Currently we start the package name with spark, such as `package spark` or `package spark.streaming`. We should probably add the \"org.spark-project.\" prefix to all of our package names.\n\nComments (2):\n1. Josh Rosen: The current package naming can be problematic if you have a subpackage named {{spark}} as part of another project. If other code in that subpackage tries to import code from Spark, the name will conflict, requiring you to use the ugly workaround {{\\_root\\_.spark}} to access Spark. This cropped up for me today when trying to add a {{myproject.runtime.spark}} package to a different project using Spark.\n2. Josh Rosen: This is fixed in 0.8, where we moved to the org.apache.spark.* prefix.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "9c215440e4110c7627133b10c72aa56b", "issue_key": "SPARK-723", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Upgrade from Scala 2.9.2 to 2.9.3", "description": "", "reporter": "Andy Konwinski", "assignee": "Andy Konwinski", "created": "2013-04-02T14:29:29.000+0000", "updated": "2013-04-07T18:18:52.000+0000", "resolved": "2013-04-07T18:18:52.000+0000", "labels": [], "components": ["Build"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-723\nSummary: Upgrade from Scala 2.9.2 to 2.9.3", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "97664f3b8d91036f251710f7c9ef6d95", "issue_key": "SPARK-724", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Have Akka logging enabled by default for standalone daemons", "description": "Earlier we disabled the akka logging because it is very verbose and gives some ERROR level messages when normal things occur (such as someone calling sc.close()). Unfortunately this log is necessary in some important cases - such as when akka is dropping messages because of a misconfiguration. I'd like to enable it by default for the standalone daemons because of this.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-04-03T14:28:05.000+0000", "updated": "2013-04-08T10:09:23.000+0000", "resolved": "2013-04-08T10:09:23.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-724\nSummary: Have Akka logging enabled by default for standalone daemons\nDescription: Earlier we disabled the akka logging because it is very verbose and gives some ERROR level messages when normal things occur (such as someone calling sc.close()). Unfortunately this log is necessary in some important cases - such as when akka is dropping messages because of a misconfiguration. I'd like to enable it by default for the standalone daemons because of this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "184f23bbca126c00afaaf31c8396eddf", "issue_key": "SPARK-725", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Ran out of disk space on EC2 master due to Ganglia logs", "description": "This morning, I started a Spark Standalone cluster on EC2 using 50 m1.medium instances. When I tried to rebuild Spark ~5.5 hours later, the build failed because the master ran out of disk space. It looks like {{/var/lib/ganglia/rrds/spark}} grew to 4.2 gigabytes, using over half of the AMI's EBS disk space. Is there a default setting that we can change to place a harder limit on the total amount of space used by Ganglia to prevent this from happening?", "reporter": "Josh Rosen", "assignee": null, "created": "2013-04-05T19:29:10.000+0000", "updated": "2015-02-26T00:05:28.000+0000", "resolved": "2015-02-26T00:05:28.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Shivaram Venkataraman", "body": "There are two problems here: 1. We have some rrds which are present in the Base AMI. These are from some cluster setup run while creating the AMI and should be deleted. 2. We should change the ganglia rrd storage directory to /mnt/ganglia or something like that. For (1) we can have a temporary fix by deleting all rrds when the cluster launches. It will probably look like https://github.com/shivaram/spark-ec2/commit/6b615e3b3092ab1d8d6c475bb16b13cdf43f5490 , but I haven't tested this yet.", "created": "2013-04-05T20:18:35.695+0000"}, {"author": "Josh Rosen", "body": "I deleted all of the rrds and continued to use the same cluster. Ganglia rebuilt the rrds for the active nodes and the dashboard is displaying new data. I haven't run out of disk space yet, so the leftover rrds from the Base AMI may have ben the culprit.", "created": "2013-04-07T22:00:07.433+0000"}, {"author": "Andrew Ash", "body": "[~joshrosen] is this still an issue for you? I believe the Ganglia feature was removed from the published binaries due to licensing concerns so should no longer be included in the spark_ec2 scripts. Setting an upper bound on Ganglia disk use is probably a setting on the Ganglia receiver rather than the sender (Spark) so I doubt we could do much for this in the general case from the Spark side.", "created": "2014-11-14T09:00:49.721+0000"}, {"author": "Sean R. Owen", "body": "Sounds like the best guess was that this wasn't a Spark issue.", "created": "2015-02-26T00:05:28.357+0000"}], "num_comments": 4, "text": "Issue: SPARK-725\nSummary: Ran out of disk space on EC2 master due to Ganglia logs\nDescription: This morning, I started a Spark Standalone cluster on EC2 using 50 m1.medium instances. When I tried to rebuild Spark ~5.5 hours later, the build failed because the master ran out of disk space. It looks like {{/var/lib/ganglia/rrds/spark}} grew to 4.2 gigabytes, using over half of the AMI's EBS disk space. Is there a default setting that we can change to place a harder limit on the total amount of space used by Ganglia to prevent this from happening?\n\nComments (4):\n1. Shivaram Venkataraman: There are two problems here: 1. We have some rrds which are present in the Base AMI. These are from some cluster setup run while creating the AMI and should be deleted. 2. We should change the ganglia rrd storage directory to /mnt/ganglia or something like that. For (1) we can have a temporary fix by deleting all rrds when the cluster launches. It will probably look like https://github.com/shivaram/spark-ec2/commit/6b615e3b3092ab1d8d6c475bb16b13cdf43f5490 , but I haven't tested this yet.\n2. Josh Rosen: I deleted all of the rrds and continued to use the same cluster. Ganglia rebuilt the rrds for the active nodes and the dashboard is displaying new data. I haven't run out of disk space yet, so the leftover rrds from the Base AMI may have ben the culprit.\n3. Andrew Ash: [~joshrosen] is this still an issue for you? I believe the Ganglia feature was removed from the published binaries due to licensing concerns so should no longer be included in the spark_ec2 scripts. Setting an upper bound on Ganglia disk use is probably a setting on the Ganglia receiver rather than the sender (Spark) so I doubt we could do much for this in the general case from the Spark side.\n4. Sean R. Owen: Sounds like the best guess was that this wasn't a Spark issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "f9aedebe46860a06bd1df4a343243e07", "issue_key": "SPARK-726", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Possible bugs in zip() transformation", "description": "A couple of bugs in the {{zip()}} transformation were reported on the mailing list, so I thought I'd link them here so they aren't forgotten: - https://groups.google.com/d/msg/spark-users/EofjLb_xm5Y/HPSNXBakZegJ - https://groups.google.com/d/msg/spark-users/demrmjHFnoc/oYaEOLqFsqYJ", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-04-05T20:12:26.000+0000", "updated": "2013-04-07T18:16:33.000+0000", "resolved": "2013-04-07T18:16:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Fixed this in https://github.com/mesos/spark/commit/054feb6448578de5542f9ef54d4cc88f706c22f5", "created": "2013-04-07T18:16:33.540+0000"}], "num_comments": 1, "text": "Issue: SPARK-726\nSummary: Possible bugs in zip() transformation\nDescription: A couple of bugs in the {{zip()}} transformation were reported on the mailing list, so I thought I'd link them here so they aren't forgotten: - https://groups.google.com/d/msg/spark-users/EofjLb_xm5Y/HPSNXBakZegJ - https://groups.google.com/d/msg/spark-users/demrmjHFnoc/oYaEOLqFsqYJ\n\nComments (1):\n1. Matei Alexandru Zaharia: Fixed this in https://github.com/mesos/spark/commit/054feb6448578de5542f9ef54d4cc88f706c22f5", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "6880a12a527c33ecde022b16c99c36e2", "issue_key": "SPARK-727", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Various improvements to Spark EC2 Scripts", "description": "This tracks a variety of improvements we should make to the EC2 scripts. Many of these are outside of the Spark repo, they may not be tied to version control. Some are: - Publish an AMI that uses the HVM architecture - Alter the spark-ec2 scripts to work well mounting drives on the newer images - Upgrade the HDFS version to support direct reads on both old and new images - Automatically infer AMI ids based on Region, HVM/Non-HVM etc (SPARK-728) - Remove Matei's public key from image.", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2013-04-08T10:49:24.000+0000", "updated": "2014-03-30T04:14:35.000+0000", "resolved": "2013-08-26T10:08:55.000+0000", "labels": [], "components": ["EC2"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-727\nSummary: Various improvements to Spark EC2 Scripts\nDescription: This tracks a variety of improvements we should make to the EC2 scripts. Many of these are outside of the Spark repo, they may not be tied to version control. Some are: - Publish an AMI that uses the HVM architecture - Alter the spark-ec2 scripts to work well mounting drives on the newer images - Upgrade the HDFS version to support direct reads on both old and new images - Automatically infer AMI ids based on Region, HVM/Non-HVM etc (SPARK-728) - Remove Matei's public key from image.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "29213c770263683e22ab2e553e8119b9", "issue_key": "SPARK-728", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Automatically infer AMI ids based on Region, HVM/Non-HVM etc.", "description": "Based the command line arguments for region,zone and instance type, we should automatically choose the AMI-ID. Also as an extension, we could move the S3 ami-id shortcuts to a github file on the spark-ec2 repository to get version control/visibility on ami ids.", "reporter": "Shivaram Venkataraman", "assignee": "Patrick McFadin", "created": "2013-04-08T11:03:46.000+0000", "updated": "2013-08-26T10:08:45.000+0000", "resolved": "2013-08-26T10:08:45.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Patrick McFadin", "body": "We should make sure this degrades gracefully when the fetch fails (client doesn't have access to github, github is down, etc).", "created": "2013-04-17T08:55:42.692+0000"}], "num_comments": 1, "text": "Issue: SPARK-728\nSummary: Automatically infer AMI ids based on Region, HVM/Non-HVM etc.\nDescription: Based the command line arguments for region,zone and instance type, we should automatically choose the AMI-ID. Also as an extension, we could move the S3 ami-id shortcuts to a github file on the spark-ec2 repository to get version control/visibility on ami ids.\n\nComments (1):\n1. Patrick McFadin: We should make sure this degrades gracefully when the fetch fails (client doesn't have access to github, github is down, etc).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "0d3a85355b15d948168a30ab47b27985", "issue_key": "SPARK-729", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Closures not always serialized at capture time", "description": "As seen in https://groups.google.com/forum/?fromgroups=#!topic/spark-users/8pTchwuP2Kk and its corresponding fix on https://github.com/mesos/spark/commit/adba773fab6294b5764d101d248815a7d3cb3558, it is possible for a closure referencing a var to see the latest version of that var, instead of the version that was there when the closure was passed to Spark. This is not good when failures or recomputations happen. We need to serialize the closures on capture if possible, perhaps as part of ClosureCleaner.clean.", "reporter": "Matei Alexandru Zaharia", "assignee": "William Benton", "created": "2013-04-08T14:45:52.000+0000", "updated": "2016-01-18T10:25:08.000+0000", "resolved": "2016-01-18T10:25:08.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "This seems like a pretty important issue because it relates to users' mental models of Spark programs. Ideally, I'd be able to understand the semantics of a Spark program as \"it's just like regular code that I'd write in Scala, except parallel\", without having to understand caching or lazy evaluation except as performance optimizations (accumulators are side-effects that can slightly break this model, but that's a separate discussion). Capturing the closures when RDDs are defined supports this model.", "created": "2013-04-14T18:40:37.865+0000"}, {"author": "William Benton", "body": "I'm interested in taking this; could someone assign it to me?", "created": "2014-03-18T07:22:45.792+0000"}, {"author": "William Benton", "body": "So the straightforward approach (immediately serializing and deserializing a closure in ClosureCleaner.clean) causes a couple of problems in Spark 1.0 that weren't obvious from the 0.9.1 test suite (that is, they existed but weren't exposed by the suite). Most notably, if we serialize closures immediately, we might replace the only reference to a broadcast variable object with a serialized copy of that object, the original could be cleaned up by ContextCleaner before the closure has a chance to execute. I've been looking at ways to solve this but thought I'd provide a status update here in the meantime.", "created": "2014-05-06T15:14:52.185+0000"}, {"author": "Sean R. Owen", "body": "I'm tentatively closing for lack of activity; it is problematic to implement and does change behavior. Although it's a problem it does also end up turning up at a reasonable time, when the closure is executed. The error is clear too.", "created": "2016-01-18T10:25:08.963+0000"}], "num_comments": 4, "text": "Issue: SPARK-729\nSummary: Closures not always serialized at capture time\nDescription: As seen in https://groups.google.com/forum/?fromgroups=#!topic/spark-users/8pTchwuP2Kk and its corresponding fix on https://github.com/mesos/spark/commit/adba773fab6294b5764d101d248815a7d3cb3558, it is possible for a closure referencing a var to see the latest version of that var, instead of the version that was there when the closure was passed to Spark. This is not good when failures or recomputations happen. We need to serialize the closures on capture if possible, perhaps as part of ClosureCleaner.clean.\n\nComments (4):\n1. Josh Rosen: This seems like a pretty important issue because it relates to users' mental models of Spark programs. Ideally, I'd be able to understand the semantics of a Spark program as \"it's just like regular code that I'd write in Scala, except parallel\", without having to understand caching or lazy evaluation except as performance optimizations (accumulators are side-effects that can slightly break this model, but that's a separate discussion). Capturing the closures when RDDs are defined supports this model.\n2. William Benton: I'm interested in taking this; could someone assign it to me?\n3. William Benton: So the straightforward approach (immediately serializing and deserializing a closure in ClosureCleaner.clean) causes a couple of problems in Spark 1.0 that weren't obvious from the 0.9.1 test suite (that is, they existed but weren't exposed by the suite). Most notably, if we serialize closures immediately, we might replace the only reference to a broadcast variable object with a serialized copy of that object, the original could be cleaned up by ContextCleaner before the closure has a chance to execute. I've been looking at ways to solve this but thought I'd provide a status update here in the meantime.\n4. Sean R. Owen: I'm tentatively closing for lack of activity; it is problematic to implement and does change behavior. Although it's a problem it does also end up turning up at a reasonable time, when the closure is executed. The error is clear too.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "ac8250290077079cabfcd077e2e56a92", "issue_key": "SPARK-730", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Unit testing failure", "description": "After, I believe, the switch to using sbt 0.12.x, multiple unit tests fail. Most of the failures are of the form: . . . org.jboss.netty.channel.ChannelException: Failed to bind to: /10.0.0.35:61448 . . . Cause: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) . . . *** And a meta-bug: 0.8.0 isn't yet available as an \"Affects Version/s\" option in JIRA.", "reporter": "Mark Hamstra", "assignee": "Matei Alexandru Zaharia", "created": "2013-04-09T12:05:20.000+0000", "updated": "2013-04-11T19:34:35.000+0000", "resolved": "2013-04-11T19:34:35.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Andy Konwinski", "body": "I've added 0.8.0 as an unreleased version.", "created": "2013-04-09T12:38:36.421+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I didn't see these failures myself when I updated. Are you sure they're not due to something like your hostname not mapping to the right IP?", "created": "2013-04-10T07:54:05.616+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Actually, the effect might be nondeterministic due to SBT's parallel execution, which changed slightly in 0.12: https://github.com/harrah/xsbt/wiki/Parallel-Execution. Maybe we need to limit the degree of parallelism more than the build file already does.", "created": "2013-04-10T07:56:41.954+0000"}, {"author": "Mark Hamstra", "body": "It really does look like a concurrency issue, since tests that fail using 'sbt/sbt test' succeed when run individually using, e.g, 'sbt/sbt \"test-only spark.AccumulatorSuite\"'.", "created": "2013-04-10T08:31:52.937+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I've fixed this in https://github.com/mesos/spark/commit/ed336e0d44d27e9be66adb0962f82af7d1ac4d87. It was slightly tricky because SBT itself had a bug in this setting until 0.12.3 (https://github.com/sbt/sbt/issues/692).", "created": "2013-04-11T19:34:35.521+0000"}], "num_comments": 5, "text": "Issue: SPARK-730\nSummary: Unit testing failure\nDescription: After, I believe, the switch to using sbt 0.12.x, multiple unit tests fail. Most of the failures are of the form: . . . org.jboss.netty.channel.ChannelException: Failed to bind to: /10.0.0.35:61448 . . . Cause: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) . . . *** And a meta-bug: 0.8.0 isn't yet available as an \"Affects Version/s\" option in JIRA.\n\nComments (5):\n1. Andy Konwinski: I've added 0.8.0 as an unreleased version.\n2. Matei Alexandru Zaharia: I didn't see these failures myself when I updated. Are you sure they're not due to something like your hostname not mapping to the right IP?\n3. Matei Alexandru Zaharia: Actually, the effect might be nondeterministic due to SBT's parallel execution, which changed slightly in 0.12: https://github.com/harrah/xsbt/wiki/Parallel-Execution. Maybe we need to limit the degree of parallelism more than the build file already does.\n4. Mark Hamstra: It really does look like a concurrency issue, since tests that fail using 'sbt/sbt test' succeed when run individually using, e.g, 'sbt/sbt \"test-only spark.AccumulatorSuite\"'.\n5. Matei Alexandru Zaharia: I've fixed this in https://github.com/mesos/spark/commit/ed336e0d44d27e9be66adb0962f82af7d1ac4d87. It was slightly tricky because SBT itself had a bug in this setting until 0.12.3 (https://github.com/sbt/sbt/issues/692).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "5ca25c7fb7656856fc7cf43c1e387008", "issue_key": "SPARK-731", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "CheckpointRDD with zero partitions test failure", "description": "In spark.CheckpointSuite, \"CheckpointRDD with zero partitions\" has begun failing. This appears to be distinct from SPARK-730 since this test is also failing in the Maven build (and is the only one to do so at the moment.)", "reporter": "Mark Hamstra", "assignee": "Matei Alexandru Zaharia", "created": "2013-04-09T14:25:33.000+0000", "updated": "2013-04-25T11:15:34.000+0000", "resolved": "2013-04-11T19:35:14.000+0000", "labels": [], "components": ["Build", "DStreams"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "What is the failure? Again, I don't see such a failure myself.", "created": "2013-04-10T07:57:25.548+0000"}, {"author": "Mark Hamstra", "body": "- CheckpointRDD with zero partitions *** FAILED *** java.io.FileNotFoundException: File /var/folders/3g/27h0b7m55ql9zb5qqh1m1jf80000gn/T/temp2384637511484111860/rdd-0 does not exist at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:321) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1341) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1381) at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:557) at spark.rdd.CheckpointRDD.getPartitions(CheckpointRDD.scala:24) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.partitions(RDD.scala:169) at spark.CheckpointSuite$$anonfun$12.apply$mcV$sp(CheckpointSuite.scala:172) at spark.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:165) at spark.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:165)", "created": "2013-04-10T08:25:43.184+0000"}, {"author": "Matei Alexandru Zaharia", "body": "My guess is that this might be due to parallel execution as in my comment in SPARK-730. Would be good to look at that and make sure we aren't running tests from two Spark subprojects in the same JVM in parallel.", "created": "2013-04-10T08:31:22.133+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Closing this as a duplicate of SPARK-730.", "created": "2013-04-11T19:35:14.777+0000"}, {"author": "Mark Hamstra", "body": "Real fix in 6e6b5204ea015fc7cc2c3e16e0032be3074413be", "created": "2013-04-25T10:10:56.368+0000"}], "num_comments": 5, "text": "Issue: SPARK-731\nSummary: CheckpointRDD with zero partitions test failure\nDescription: In spark.CheckpointSuite, \"CheckpointRDD with zero partitions\" has begun failing. This appears to be distinct from SPARK-730 since this test is also failing in the Maven build (and is the only one to do so at the moment.)\n\nComments (5):\n1. Matei Alexandru Zaharia: What is the failure? Again, I don't see such a failure myself.\n2. Mark Hamstra: - CheckpointRDD with zero partitions *** FAILED *** java.io.FileNotFoundException: File /var/folders/3g/27h0b7m55ql9zb5qqh1m1jf80000gn/T/temp2384637511484111860/rdd-0 does not exist at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:321) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1341) at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1381) at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:557) at spark.rdd.CheckpointRDD.getPartitions(CheckpointRDD.scala:24) at spark.RDD.partitions(RDD.scala:169) at spark.RDD.partitions(RDD.scala:169) at spark.CheckpointSuite$$anonfun$12.apply$mcV$sp(CheckpointSuite.scala:172) at spark.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:165) at spark.CheckpointSuite$$anonfun$12.apply(CheckpointSuite.scala:165)\n3. Matei Alexandru Zaharia: My guess is that this might be due to parallel execution as in my comment in SPARK-730. Would be good to look at that and make sure we aren't running tests from two Spark subprojects in the same JVM in parallel.\n4. Matei Alexandru Zaharia: Closing this as a duplicate of SPARK-730.\n5. Mark Hamstra: Real fix in 6e6b5204ea015fc7cc2c3e16e0032be3074413be", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "cf0487791e73027443825d1a3d36ecae", "issue_key": "SPARK-732", "issue_type": "Bug", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "Recomputation of RDDs may result in duplicated accumulator updates", "description": "Currently, Spark doesn't guard against duplicated updates to the same accumulator due to recomputations of an RDD. For example:  val acc = sc.accumulator(0) data.map(x => acc += 1; f(x)) data.count() // acc should equal data.count() here data.foreach{...} // Now, acc = 2 * data.count() because the map() was recomputed.  I think that this behavior is incorrect, especially because this behavior allows the additon or removal of a cache() call to affect the outcome of a computation. There's an old TODO to fix this duplicate update issue in the [DAGScheduler code|https://github.com/mesos/spark/blob/ec5e553b418be43aa3f0ccc24e0d5ca9d63504b2/core/src/main/scala/spark/scheduler/DAGScheduler.scala#L494]. I haven't tested whether recomputation due to blocks being dropped from the cache can trigger duplicate accumulator updates. Hypothetically someone could be relying on the current behavior to implement performance counters that track the actual number of computations performed (including recomputations). To be safe, we could add an explicit warning in the release notes that documents the change in behavior when we fix this. Ignoring duplicate updates shouldn't be too hard, but there are a few subtleties. Currently, we allow accumulators to be used in multiple transformations, so we'd need to detect duplicate updates at the per-transformation level. I haven't dug too deeply into the scheduler internals, but we might also run into problems where pipelining causes what is logically one set of accumulator updates to show up in two different tasks (e.g. rdd.map(accum += x; ...) and rdd.map(accum += x; ...).count() may cause what's logically the same accumulator update to be applied from two different contexts, complicating the detection of duplicate updates).", "reporter": "Josh Rosen", "assignee": "Nan Zhu", "created": "2013-04-14T22:50:16.000+0000", "updated": "2016-02-24T22:23:34.000+0000", "resolved": "2014-11-27T01:32:59.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "We might want to revisit this before the 1.0 release, since I think the current accumulator behavior is counter-intuitive.", "created": "2014-02-25T21:58:33.376+0000"}, {"author": "Nan Zhu", "body": "I'm interested in fixing this, I plan to start with the scheduler, i.e. detect the duplication when get the task result", "created": "2014-03-20T17:28:13.924+0000"}, {"author": "Nan Zhu", "body": "I finished the deduplication for resubmitted tasks but for transformation, any hint?", "created": "2014-03-24T07:39:30.933+0000"}, {"author": "Nan Zhu", "body": "made a PR: https://github.com/apache/spark/pull/228", "created": "2014-03-25T12:04:21.245+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38997862 Merged build triggered. Build is starting -or- tests failed to complete.", "created": "2014-03-29T14:57:29.677+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38997867 Merged build started. Build is starting -or- tests failed to complete.", "created": "2014-03-29T14:57:41.176+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38999177 Merged build triggered. Build is starting -or- tests failed to complete.", "created": "2014-03-29T15:42:25.163+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38999183 Merged build started. Build is starting -or- tests failed to complete.", "created": "2014-03-29T15:42:33.509+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39000586 Build is starting -or- tests failed to complete. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13573/", "created": "2014-03-29T16:28:37.461+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39000584 Merged build finished. Build is starting -or- tests failed to complete.", "created": "2014-03-29T16:28:37.589+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39002055 Build is starting -or- tests failed to complete. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13577/", "created": "2014-03-29T17:14:03.890+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39002053 Merged build finished. Build is starting -or- tests failed to complete.", "created": "2014-03-29T17:14:04.025+0000"}, {"author": "Mattmann, Chris A (388J)", "body": "Guys I fixed this by adding jira@apache.org to the mailing list, no more moderation required. Cheers, Chris", "created": "2014-03-29T17:23:15.799+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39002548 Merged build triggered. Build is starting -or- tests failed to complete.", "created": "2014-03-29T17:32:24.973+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39002551 Merged build started. Build is starting -or- tests failed to complete.", "created": "2014-03-29T17:32:29.271+0000"}, {"author": "ASF GitHub Bot", "body": "Github user mridulm commented on a diff in the pull request: https://github.com/apache/spark/pull/228#discussion_r11094324 --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala --- @@ -116,6 +116,9 @@ class DAGScheduler( private val metadataCleaner = new MetadataCleaner(MetadataCleanerType.DAG_SCHEDULER, this.cleanup, env.conf) + // stageId -> (splitId -> (acumulatorId, accumulatorValue)) + val stageIdToAccumulators = new HashMap[Int, HashMap[Int, ListBuffer[(Long, Any)]]] --- End diff -- Since this is mutable datastructure - can me make it private ? For testcase, expose some method which checks for the condition instead of needing to expose the entire stageIdToAccumulators ?", "created": "2014-03-29T17:52:41.667+0000"}, {"author": "ASF GitHub Bot", "body": "Github user mridulm commented on a diff in the pull request: https://github.com/apache/spark/pull/228#discussion_r11094329 --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala --- @@ -789,6 +799,25 @@ class DAGScheduler( } /** + * detect the duplicate accumulator value and save the accumulator values + * @param accumValue the accumulator values received from the task + * @param stage the stage which the task belongs to + * @param task the completed task + */ + private def saveAccumulatorValue(accumValue: Map[Long, Any], stage: Stage, task: Task[_]) { + if (accumValue != null && + (!stageIdToAccumulators.contains(stage.id) || + !stageIdToAccumulators(stage.id).contains(task.partitionId))) { + stageIdToAccumulators.getOrElseUpdate(stage.id, + new HashMap[Int, ListBuffer[(Long, Any)]]). + getOrElseUpdate(task.partitionId, new ListBuffer[(Long, Any)]) + for ((id, value) <- accumValue) { + stageIdToAccumulators(stage.id)(task.partitionId) += id -> value --- End diff -- nit: you can avoid the lookup within the loop by using the value returned in the getOrElseUpdate calls.", "created": "2014-03-29T17:54:45.533+0000"}, {"author": "ASF GitHub Bot", "body": "Github user mridulm commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39003471 looks good @CodingCat ! just made a few minor points. excellent change !!", "created": "2014-03-29T18:02:25.156+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39004142 Merged build finished. Build is starting -or- tests failed to complete.", "created": "2014-03-29T18:23:47.821+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39004143 Build is starting -or- tests failed to complete. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13579/", "created": "2014-03-29T18:23:47.978+0000"}, {"author": "ASF GitHub Bot", "body": "Github user kayousterhout commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39004325 Can we just add the accumulator update to TaskSetManager, in the handleSuccessfulTask() method? This seems much simpler because the TaskSetManager already has all of the state about which tasks are running, which ones have been resubmitted or speculated, etc. I think this change would be much simpler. Over time, a lot of functionality has leaked into the DAGScheduler, such that there's a lot of state that's kept in multiple places: in the DAGScheduler and in the TaskSetManager or the TaskSchedulerImpl. The abstraction is supposed to be that the DAGScheduler handles the high level semantics of scheduling stages and dealing with inter-stage dependencies, and the TaskSetManager handles the low-level details of the tasks for each stage. There are some parts of this abstraction that are currently broken (where the DAGScheduler knows too much about task-level details) and refactoring this is on my todo list, but in the meantime I think we should try not to make this problem any worse, because it makes the code much more complicated, more difficult to understand, and buggy.", "created": "2014-03-29T18:30:44.230+0000"}, {"author": "ASF GitHub Bot", "body": "Github user CodingCat commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39007448 Hi, Kay, I will think about it, and see if we can move accumulator related functionalities to tm entirely.", "created": "2014-03-29T20:12:41.716+0000"}, {"author": "ASF GitHub Bot", "body": "Github user kayousterhout commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39007883 Cool thanks!! On Sat, Mar 29, 2014 at 1:12 PM, Nan Zhu <notifications@github.com> wrote: > Hi, Kay, I will think about it, and see if we can move accumulator related > functionalities to tm entirely. > > -- > Reply to this email directly or view it on GitHub<https://github.com/apache/spark/pull/228#issuecomment-39007448> > . >", "created": "2014-03-29T20:27:05.375+0000"}, {"author": "Daniel Siegmann", "body": "Any update on this issue? As it currently stands, values of Accumulators simply can't be trusted.", "created": "2014-06-12T17:51:08.379+0000"}, {"author": "Nan Zhu", "body": "I actually made a PR long time ago https://github.com/apache/spark/pull/228", "created": "2014-06-12T17:54:29.286+0000"}, {"author": "Apache Spark", "body": "User 'CodingCat' has created a pull request for this issue: https://github.com/apache/spark/pull/2524", "created": "2014-10-23T13:24:20.177+0000"}, {"author": "Matei Alexandru Zaharia", "body": "As discussed on https://github.com/apache/spark/pull/2524 this is pretty hard to provide good semantics for in the general case (accumulator updates inside non-result stages), for the following reasons: - An RDD may be computed as part of multiple stages. For example, if you update an accumulator inside a MappedRDD and then shuffle it, that might be one stage. But if you then call map() again on the MappedRDD, and shuffle the result of that, you get a second stage where that map is pipeline. Do you want to count this accumulator update twice or not? - Entire stages may be resubmitted if shuffle files are deleted by the periodic cleaner or are lost due to a node failure, so anything that tracks RDDs would need to do so for long periods of time (as long as the RDD is referenceable in the user program), which would be pretty complicated to implement. So I'm going to mark this as \"won't fix\" for now, except for the part for result stages done in SPARK-3628.", "created": "2014-11-27T01:32:29.198+0000"}, {"author": "Daniel Siegmann", "body": "This is very disappointing. Essentially, Spark doesn't support any equivalent to Hadoop's counters. That's a major drawback to Spark. I hope you are at least planning to note this limitation prominently in the documentation (http://spark.apache.org/docs/latest/programming-guide.html#accumulators).", "created": "2014-11-30T23:38:46.666+0000"}, {"author": "koert kuipers", "body": "It is not clear to me what the usage of accumulators is without this", "created": "2015-09-09T04:03:59.651+0000"}, {"author": "Jim Lohse", "body": "Affects versions only goes to 1.1.0, presumably this is still an issue? Is it correct that this is only an issue in transformations, but in actions will work correctly? That idea seems to be supported by the docs under https://spark.apache.org/docs/latest/programming-guide.html#accumulators-a-nameaccumlinka: \"In Java, Spark also supports the more general Accumulable interface to accumulate data where the resulting type is not the same as the elements added (e.g. build a list by collecting together elements). For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.\"", "created": "2016-02-24T22:23:34.212+0000"}], "num_comments": 30, "text": "Issue: SPARK-732\nSummary: Recomputation of RDDs may result in duplicated accumulator updates\nDescription: Currently, Spark doesn't guard against duplicated updates to the same accumulator due to recomputations of an RDD. For example:  val acc = sc.accumulator(0) data.map(x => acc += 1; f(x)) data.count() // acc should equal data.count() here data.foreach{...} // Now, acc = 2 * data.count() because the map() was recomputed.  I think that this behavior is incorrect, especially because this behavior allows the additon or removal of a cache() call to affect the outcome of a computation. There's an old TODO to fix this duplicate update issue in the [DAGScheduler code|https://github.com/mesos/spark/blob/ec5e553b418be43aa3f0ccc24e0d5ca9d63504b2/core/src/main/scala/spark/scheduler/DAGScheduler.scala#L494]. I haven't tested whether recomputation due to blocks being dropped from the cache can trigger duplicate accumulator updates. Hypothetically someone could be relying on the current behavior to implement performance counters that track the actual number of computations performed (including recomputations). To be safe, we could add an explicit warning in the release notes that documents the change in behavior when we fix this. Ignoring duplicate updates shouldn't be too hard, but there are a few subtleties. Currently, we allow accumulators to be used in multiple transformations, so we'd need to detect duplicate updates at the per-transformation level. I haven't dug too deeply into the scheduler internals, but we might also run into problems where pipelining causes what is logically one set of accumulator updates to show up in two different tasks (e.g. rdd.map(accum += x; ...) and rdd.map(accum += x; ...).count() may cause what's logically the same accumulator update to be applied from two different contexts, complicating the detection of duplicate updates).\n\nComments (30):\n1. Josh Rosen: We might want to revisit this before the 1.0 release, since I think the current accumulator behavior is counter-intuitive.\n2. Nan Zhu: I'm interested in fixing this, I plan to start with the scheduler, i.e. detect the duplication when get the task result\n3. Nan Zhu: I finished the deduplication for resubmitted tasks but for transformation, any hint?\n4. Nan Zhu: made a PR: https://github.com/apache/spark/pull/228\n5. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38997862 Merged build triggered. Build is starting -or- tests failed to complete.\n6. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38997867 Merged build started. Build is starting -or- tests failed to complete.\n7. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38999177 Merged build triggered. Build is starting -or- tests failed to complete.\n8. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-38999183 Merged build started. Build is starting -or- tests failed to complete.\n9. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39000586 Build is starting -or- tests failed to complete. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13573/\n10. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/228#issuecomment-39000584 Merged build finished. Build is starting -or- tests failed to complete.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "5cc57f491f672e418617f74b62eae974", "issue_key": "SPARK-733", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Add documentation on use of accumulators in lazy transformation", "description": "Accumulators updates are side-effects of RDD computations. Unlike RDDs, accumulators do not carry lineage that would allow them to be computed when their values are accessed on the master. This can lead to confusion when accumulators are used in lazy transformations like `map`:  val acc = sc.accumulator(0) data.map(x => acc += x; f(x)) // Here, acc is 0 because no actions have cause the `map` to be computed.  As far as I can tell, our documentation only includes examples of using accumulators in `foreach`, for which this problem does not occur. This pattern of using accumulators in map() occurs in Bagel and other Spark code found in the wild. It might be nice to document this behavior in the accumulators section of the Spark programming guide.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-04-14T22:59:27.000+0000", "updated": "2015-01-16T21:33:51.000+0000", "resolved": "2015-01-16T21:33:51.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Apache Spark", "body": "User 'ilganeli' has created a pull request for this issue: https://github.com/apache/spark/pull/4022", "created": "2015-01-13T17:55:38.568+0000"}, {"author": "Imran Rashid", "body": "Fixed by https://github.com/apache/spark/pull/4022", "created": "2015-01-16T21:33:51.955+0000"}], "num_comments": 2, "text": "Issue: SPARK-733\nSummary: Add documentation on use of accumulators in lazy transformation\nDescription: Accumulators updates are side-effects of RDD computations. Unlike RDDs, accumulators do not carry lineage that would allow them to be computed when their values are accessed on the master. This can lead to confusion when accumulators are used in lazy transformations like `map`:  val acc = sc.accumulator(0) data.map(x => acc += x; f(x)) // Here, acc is 0 because no actions have cause the `map` to be computed.  As far as I can tell, our documentation only includes examples of using accumulators in `foreach`, for which this problem does not occur. This pattern of using accumulators in map() occurs in Bagel and other Spark code found in the wild. It might be nice to document this behavior in the accumulators section of the Spark programming guide.\n\nComments (2):\n1. Apache Spark: User 'ilganeli' has created a pull request for this issue: https://github.com/apache/spark/pull/4022\n2. Imran Rashid: Fixed by https://github.com/apache/spark/pull/4022", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "7519ba54e21158d64ddd45f6e226a8e6", "issue_key": "SPARK-734", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Adjust default timeouts to be more sane", "description": "Some of the timeouts in Spark are quite low (e.g. < 5 seconds) and this doesn't work well once larger datasets are involved and GC pauses happen that take a lot of time. We should bump all timeouts to be in the ~30s range.", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2013-04-17T09:02:16.000+0000", "updated": "2014-03-30T04:14:51.000+0000", "resolved": "2013-06-04T22:39:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Here is one set of values suggested on the user list: SPARK_JAVA_OPTS+=\" -Dspark.worker.timeout=30000 -Dspark.akka.timeout=30000 -Dspark.storage.blockManagerHeartBeatMs=30000 -Dspark.akka.retry.wait=30000 -Dspark.akka.frameSize=10000", "created": "2013-06-04T14:03:12.883+0000"}], "num_comments": 1, "text": "Issue: SPARK-734\nSummary: Adjust default timeouts to be more sane\nDescription: Some of the timeouts in Spark are quite low (e.g. < 5 seconds) and this doesn't work well once larger datasets are involved and GC pauses happen that take a lot of time. We should bump all timeouts to be in the ~30s range.\n\nComments (1):\n1. Patrick McFadin: Here is one set of values suggested on the user list: SPARK_JAVA_OPTS+=\" -Dspark.worker.timeout=30000 -Dspark.akka.timeout=30000 -Dspark.storage.blockManagerHeartBeatMs=30000 -Dspark.akka.retry.wait=30000 -Dspark.akka.frameSize=10000", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "7d728a6e2ccc527f604b0210026b3991", "issue_key": "SPARK-735", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "memory leak in KryoSerializer", "description": "KryoSerializer uses a ThreadLocal object to store a kryo buffer, which is never cleaned up. This becomes a serious problem in projects like Shark, where new threads are created constantly by the thrift server. As long as the new thread references the kryo serializer, it will create a new kryo buffer. A simple solution is to remove the ThreadLocal reference, and create a new buffer object every time a new kryo serializer instance is created. This is not very expensive because large writes actually go through the serialization stream interface, which reuse the buffer anyway.", "reporter": "Reynold Xin", "assignee": "Ram Sriharsha", "created": "2013-04-17T16:00:04.000+0000", "updated": "2014-10-21T07:47:00.000+0000", "resolved": "2014-10-21T07:47:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "Ram already has a fix and he will submit a PR to Spark.", "created": "2013-04-17T16:00:31.387+0000"}, {"author": "Patrick Wendell", "body": "I think this was fixed a long time ago.", "created": "2014-10-21T07:47:00.359+0000"}], "num_comments": 2, "text": "Issue: SPARK-735\nSummary: memory leak in KryoSerializer\nDescription: KryoSerializer uses a ThreadLocal object to store a kryo buffer, which is never cleaned up. This becomes a serious problem in projects like Shark, where new threads are created constantly by the thrift server. As long as the new thread references the kryo serializer, it will create a new kryo buffer. A simple solution is to remove the ThreadLocal reference, and create a new buffer object every time a new kryo serializer instance is created. This is not very expensive because large writes actually go through the serialization stream interface, which reuse the buffer anyway.\n\nComments (2):\n1. Reynold Xin: Ram already has a fix and he will submit a PR to Spark.\n2. Patrick Wendell: I think this was fixed a long time ago.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "b3ddf6928d183c756738cbc1095a3d37", "issue_key": "SPARK-736", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Killing tasks in spark", "description": "See the initial discussion at https://github.com/mesos/spark/pull/209", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-04-18T13:30:15.000+0000", "updated": "2013-12-07T13:44:30.000+0000", "resolved": "2013-12-07T13:44:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "There's a new pull request for this at https://github.com/mesos/spark/pull/665", "created": "2013-06-28T12:09:29.261+0000"}, {"author": "Josh Rosen", "body": "Added in https://github.com/apache/incubator-spark/pull/29 I think that pull request only added job cancellation to the Scala API and non-Mesos cluster backends. I'll open new issues to track the remaining pieces.", "created": "2013-12-07T13:44:30.101+0000"}], "num_comments": 2, "text": "Issue: SPARK-736\nSummary: Killing tasks in spark\nDescription: See the initial discussion at https://github.com/mesos/spark/pull/209\n\nComments (2):\n1. Josh Rosen: There's a new pull request for this at https://github.com/mesos/spark/pull/665\n2. Josh Rosen: Added in https://github.com/apache/incubator-spark/pull/29 I think that pull request only added job cancellation to the Scala API and non-Mesos cluster backends. I'll open new issues to track the remaining pieces.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "d696fa4ccaada2ef90460d34e8285353", "issue_key": "SPARK-737", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Silence expected exceptions for unit tests", "description": "The unit tests make a lot of noise, mostly from Akka logs. It would be good to figure out how to disable these so they don't confuse people.", "reporter": "Patrick McFadin", "assignee": "Shivaram Venkataraman", "created": "2013-04-22T10:55:30.000+0000", "updated": "2013-07-11T15:54:50.000+0000", "resolved": "2013-07-11T15:54:49.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "The  [INFO] [04/13/2013 09:48:09.453] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped  log messages are particularly annoying; I'd love to disable these if we can.", "created": "2013-04-22T14:52:21.625+0000"}, {"author": "Patrick McFadin", "body": "Spent some time on this today - unfortunately it's non-trivial. The Spray logging internally hooks into Akka's event logging. We have the Akka logging configured to delegate to slf4j which then delegates to log4j (since we include the log4j jar in our build). With a slight modification to the way Spray is initialized, it's possible to silence this with the Spark logging properties, as seen here: https://github.com/pwendell/spark/commit/b5a5c2562d32ab65669e94dfc19002ac2d2059bf The problem is that, while this works great for running Spark in the shell, it doesn't work for tests. For some reason, during the tests something breaks down in the Spray->Akka->Slf4j->Log4j pipeline, possibly because of the way the classloading happens inside of Scalatest. To actually silence this for the tests, I had to manually hook into the Spray logger and disable all info/debug level messages. This works but it's pretty brittle: https://github.com/pwendell/spark/commit/40410247d898cbde9299089ef92f79eb7991ff25 I'm going to have to put this aside for a bit, that's as far as I got for now.", "created": "2013-04-23T20:21:42.623+0000"}, {"author": "Patrick McFadin", "body": "Thanks Shivaram for finally cracking this. The last annoying bit has been fixed here: https://github.com/mesos/spark/pull/683", "created": "2013-07-11T15:54:50.774+0000"}], "num_comments": 3, "text": "Issue: SPARK-737\nSummary: Silence expected exceptions for unit tests\nDescription: The unit tests make a lot of noise, mostly from Akka logs. It would be good to figure out how to disable these so they don't confuse people.\n\nComments (3):\n1. Josh Rosen: The  [INFO] [04/13/2013 09:48:09.453] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped  log messages are particularly annoying; I'd love to disable these if we can.\n2. Patrick McFadin: Spent some time on this today - unfortunately it's non-trivial. The Spray logging internally hooks into Akka's event logging. We have the Akka logging configured to delegate to slf4j which then delegates to log4j (since we include the log4j jar in our build). With a slight modification to the way Spray is initialized, it's possible to silence this with the Spark logging properties, as seen here: https://github.com/pwendell/spark/commit/b5a5c2562d32ab65669e94dfc19002ac2d2059bf The problem is that, while this works great for running Spark in the shell, it doesn't work for tests. For some reason, during the tests something breaks down in the Spray->Akka->Slf4j->Log4j pipeline, possibly because of the way the classloading happens inside of Scalatest. To actually silence this for the tests, I had to manually hook into the Spray logger and disable all info/debug level messages. This works but it's pretty brittle: https://github.com/pwendell/spark/commit/40410247d898cbde9299089ef92f79eb7991ff25 I'm going to have to put this aside for a bit, that's as far as I got for now.\n3. Patrick McFadin: Thanks Shivaram for finally cracking this. The last annoying bit has been fixed here: https://github.com/mesos/spark/pull/683", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "fa5b33be90dfc54af1b0ab57deb60f96", "issue_key": "SPARK-738", "issue_type": "Bug", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Spark should detect and squash nonserializable exceptions", "description": "If user exception is thrown which a Spark executor cannot serialize, it causes the executor to crash. The executor should wrap the attempt to serialize the exception with a try/catch and return a default exception in the case where it can't be serialized. The fix should include a test case", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2013-04-24T21:38:07.000+0000", "updated": "2014-03-30T04:14:08.000+0000", "resolved": "2013-06-02T09:21:47.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "Pushing the priority to Critical - since a user level bug can actually destroy the cluster this way ...", "created": "2013-04-24T21:42:04.982+0000"}], "num_comments": 1, "text": "Issue: SPARK-738\nSummary: Spark should detect and squash nonserializable exceptions\nDescription: If user exception is thrown which a Spark executor cannot serialize, it causes the executor to crash. The executor should wrap the attempt to serialize the exception with a try/catch and return a default exception in the case where it can't be serialized. The fix should include a test case\n\nComments (1):\n1. Reynold Xin: Pushing the priority to Critical - since a user level bug can actually destroy the cluster this way ...", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "b441ad25960393cc8903ca5a77d8a199", "issue_key": "SPARK-739", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Have quickstart standalone read README instead of log file", "description": "This will make it work out of the box on Windows machines or situations where it is run on a cluster.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-04-25T10:39:40.000+0000", "updated": "2013-04-25T16:28:52.000+0000", "resolved": "2013-04-25T16:28:52.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-739\nSummary: Have quickstart standalone read README instead of log file\nDescription: This will make it work out of the box on Windows machines or situations where it is run on a cluster.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "33d657ae10f8bbebf6ee1e784e1b4223", "issue_key": "SPARK-740", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Spark block manager UI has bug when enabling Spark Streaming", "description": "currently Spark storage ui aggregate RDDInfo using block name, and in block manger, all the block name is rdd_*_*. But in Spark Streaming, block name changes to input-*-*, this will cause a exception when group rdd info using block name in StorageUtils.scala: val groupedRddBlocks = infos.groupBy { case(k, v) => k.substring(0,k.lastIndexOf('_')) }.mapValues(_.values.toArray) according to '_' to get rdd name will meet exception when using Spark Streaming. java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at java.lang.String.substring(String.java:1958) at spark.storage.StorageUtils$$anonfun$3.apply(StorageUtils.scala:49) at spark.storage.StorageUtils$$anonfun$3.apply(StorageUtils.scala:48) at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:315) at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:314) at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:178) at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:347) at scala.collection.TraversableLike$class.groupBy(TraversableLike.scala:314) at scala.collection.immutable.HashMap.groupBy(HashMap.scala:38) at spark.storage.StorageUtils$.rddInfoFromBlockStatusList(StorageUtils.scala:48) at spark.storage.StorageUtils$.rddInfoFromStorageStatus(StorageUtils.scala:40) at spark.storage.BlockManagerUI$$anonfun$5.apply(BlockManagerUI.scala:54) .... there has two methods: 1. filter out all the Spark Streaming's input block RDD. 2. treat Spark Streaming's input RDD as a special case, add code to support this case.", "reporter": "Saisai Shao", "assignee": "Saisai Shao", "created": "2013-04-25T19:40:58.000+0000", "updated": "2020-05-17T18:21:14.000+0000", "resolved": "2013-08-07T10:50:48.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Andy Petrella", "body": "The same goes with 0.7.3 (fyi ;-))", "created": "2013-07-28T06:51:34.645+0000"}, {"author": "Saisai Shao", "body": "It is fixed in master branch, but do not backport to 0.7.3.", "created": "2013-07-28T17:36:02.900+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Oh, can you show me the commit where it's fixed?", "created": "2013-07-29T00:02:56.380+0000"}, {"author": "Saisai Shao", "body": "hi Matei, this issue is fixed in in PR 581(https://github.com/mesos/spark/pull/581).", "created": "2013-07-29T00:09:33.553+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Thanks. I've now fixed this in branch-0.7 as well.", "created": "2013-08-07T10:49:53.068+0000"}], "num_comments": 5, "text": "Issue: SPARK-740\nSummary: Spark block manager UI has bug when enabling Spark Streaming\nDescription: currently Spark storage ui aggregate RDDInfo using block name, and in block manger, all the block name is rdd_*_*. But in Spark Streaming, block name changes to input-*-*, this will cause a exception when group rdd info using block name in StorageUtils.scala: val groupedRddBlocks = infos.groupBy { case(k, v) => k.substring(0,k.lastIndexOf('_')) }.mapValues(_.values.toArray) according to '_' to get rdd name will meet exception when using Spark Streaming. java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at java.lang.String.substring(String.java:1958) at spark.storage.StorageUtils$$anonfun$3.apply(StorageUtils.scala:49) at spark.storage.StorageUtils$$anonfun$3.apply(StorageUtils.scala:48) at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:315) at scala.collection.TraversableLike$$anonfun$groupBy$1.apply(TraversableLike.scala:314) at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:178) at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:347) at scala.collection.TraversableLike$class.groupBy(TraversableLike.scala:314) at scala.collection.immutable.HashMap.groupBy(HashMap.scala:38) at spark.storage.StorageUtils$.rddInfoFromBlockStatusList(StorageUtils.scala:48) at spark.storage.StorageUtils$.rddInfoFromStorageStatus(StorageUtils.scala:40) at spark.storage.BlockManagerUI$$anonfun$5.apply(BlockManagerUI.scala:54) .... there has two methods: 1. filter out all the Spark Streaming's input block RDD. 2. treat Spark Streaming's input RDD as a special case, add code to support this case.\n\nComments (5):\n1. Andy Petrella: The same goes with 0.7.3 (fyi ;-))\n2. Saisai Shao: It is fixed in master branch, but do not backport to 0.7.3.\n3. Matei Alexandru Zaharia: Oh, can you show me the commit where it's fixed?\n4. Saisai Shao: hi Matei, this issue is fixed in in PR 581(https://github.com/mesos/spark/pull/581).\n5. Matei Alexandru Zaharia: Thanks. I've now fixed this in branch-0.7 as well.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "7318e4af146c2f38c57f61de804ced1a", "issue_key": "SPARK-741", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "DiskStore should use > 8kB buffer when doing writes", "description": "Right now the DiskStore uses a buffered output stream with the default buffer size of 8kB. This can hurt disk throughput by a substantial amount when there are several shuffle files being output at once (either due to a large # of concurrent tasks or a large # of output splits). We should avoid increasing this buffer arbitrarily because it is instantiated (# tasks * # splits) times currently, which could be large. The best approach is probably to do something like this: - By default, give each task 10mB of total buffer space, divided up amongst its output partitions. - If this means each split buffer is < 8kB, bump up to at least 8kB (we'd rather OOM then have terrible disk throughput, so at least people can figure out what's wrong).", "reporter": "Patrick Wendell", "assignee": "Reynold Xin", "created": "2013-04-29T08:49:29.000+0000", "updated": "2014-03-30T04:14:24.000+0000", "resolved": "2013-05-06T16:12:57.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "It would make more sense to put this at the layer above DiskStore. Make the buffer size a parameter we passed into DiskStore so DiskStore can be dumb and doesn't have to understand the task logic.", "created": "2013-04-29T10:18:34.909+0000"}, {"author": "Patrick McFadin", "body": "Ya, but one issue is that the diskstore is accessed through the BlockStore interface (except for in the fork you are using), so we'd need to add a parameter there that is very specific to the disk store.", "created": "2013-04-29T10:29:24.326+0000"}, {"author": "Reynold Xin", "body": "It doesn't break polymorphism if it is a constructor parameter. That said, I actually don't think the common interface for DiskStore and MemoryStore makes sense. They serve very different purposes. In block manager, they are not interchangeable at all. There are tons of if statements to differentiate the usage of them. Polymorphism doesn't simplify the design of abstract anything away.", "created": "2013-04-29T10:37:35.266+0000"}, {"author": "Reynold Xin", "body": "Done as part of the following pull request: https://github.com/mesos/spark/pull/587", "created": "2013-05-06T16:12:57.905+0000"}], "num_comments": 4, "text": "Issue: SPARK-741\nSummary: DiskStore should use > 8kB buffer when doing writes\nDescription: Right now the DiskStore uses a buffered output stream with the default buffer size of 8kB. This can hurt disk throughput by a substantial amount when there are several shuffle files being output at once (either due to a large # of concurrent tasks or a large # of output splits). We should avoid increasing this buffer arbitrarily because it is instantiated (# tasks * # splits) times currently, which could be large. The best approach is probably to do something like this: - By default, give each task 10mB of total buffer space, divided up amongst its output partitions. - If this means each split buffer is < 8kB, bump up to at least 8kB (we'd rather OOM then have terrible disk throughput, so at least people can figure out what's wrong).\n\nComments (4):\n1. Reynold Xin: It would make more sense to put this at the layer above DiskStore. Make the buffer size a parameter we passed into DiskStore so DiskStore can be dumb and doesn't have to understand the task logic.\n2. Patrick McFadin: Ya, but one issue is that the diskstore is accessed through the BlockStore interface (except for in the fork you are using), so we'd need to add a parameter there that is very specific to the disk store.\n3. Reynold Xin: It doesn't break polymorphism if it is a constructor parameter. That said, I actually don't think the common interface for DiskStore and MemoryStore makes sense. They serve very different purposes. In block manager, they are not interchangeable at all. There are tons of if statements to differentiate the usage of them. Polymorphism doesn't simplify the design of abstract anything away.\n4. Reynold Xin: Done as part of the following pull request: https://github.com/mesos/spark/pull/587", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "fda2663651485c57b367080749bc9e2a", "issue_key": "SPARK-742", "issue_type": "Bug", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "Task Metrics should not employ per-record timing by default", "description": "The per-record timing inside of TimedIterator causes substantial performance overhead in certain cases. It should be an optional feature that is turned off by default, rather than hard-coded feature that is not possible to disable.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-04-29T08:51:19.000+0000", "updated": "2013-05-01T17:38:19.000+0000", "resolved": "2013-05-01T17:38:19.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-742\nSummary: Task Metrics should not employ per-record timing by default\nDescription: The per-record timing inside of TimedIterator causes substantial performance overhead in certain cases. It should be an optional feature that is turned off by default, rather than hard-coded feature that is not possible to disable.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "bd17487ce7b3806bf34b95e5528b6391", "issue_key": "SPARK-743", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "An Akka-cluster based masterless standalone mode for Spark.", "description": "Using akka-cluster for building a resource manager will simplify deployment and also provide features like high availability and may be partition tolerence. To summarize. 1. Auto joining of workers/seed nodes. 2. Masterless approach or leader election 3. Status reporting and state communication via gossip. UI 1. Akka-cluster comes with nice cluster metrics reporting.", "reporter": "Prashant Sharma", "assignee": "Prashant Sharma", "created": "2013-05-02T23:54:03.000+0000", "updated": "2020-02-07T17:28:00.000+0000", "resolved": "2016-01-04T14:51:51.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Evan Chan", "body": "Master vs Masterless: - I think Masterless is a cleaner design. It fits better with the whole Dynamo/Akka Cluster philosophy of peer-to-peer. It will be a bigger change though for sure. Client: - Instead of a URL to a single master, most likely the client would need a configuration pointing to multiple nodes, because it might be masterless. We would probably want to target Akka 2.2, as it will have a feature for \"roles\", where clients will join the cluster not as a worker member, but as a \"client\" role. Otherwise the remote Akka connection might have issues. Reusing Akka metrics will be good too.", "created": "2013-05-03T09:50:20.994+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Hey Evan, FYI, this recently merged PR adds ZooKeeper-based fault tolerance to the standalone mode: https://github.com/apache/incubator-spark/pull/19. When we move to Scala 2.10 and Akka 2.2, a similar approach based on Akka-cluster would be great. But this should at least be a start for cluster HA -- give it a whirl and tell us what you think.", "created": "2013-10-10T17:47:52.239+0000"}, {"author": "Prashant Sharma", "body": "Take a look at this preview. https://github.com/apache/incubator-spark/pull/60", "created": "2013-10-15T04:05:50.584+0000"}, {"author": "François Garillot", "body": "That last URL is https://github.com/apache/spark/pull/60", "created": "2014-11-03T10:24:43.198+0000"}], "num_comments": 4, "text": "Issue: SPARK-743\nSummary: An Akka-cluster based masterless standalone mode for Spark.\nDescription: Using akka-cluster for building a resource manager will simplify deployment and also provide features like high availability and may be partition tolerence. To summarize. 1. Auto joining of workers/seed nodes. 2. Masterless approach or leader election 3. Status reporting and state communication via gossip. UI 1. Akka-cluster comes with nice cluster metrics reporting.\n\nComments (4):\n1. Evan Chan: Master vs Masterless: - I think Masterless is a cleaner design. It fits better with the whole Dynamo/Akka Cluster philosophy of peer-to-peer. It will be a bigger change though for sure. Client: - Instead of a URL to a single master, most likely the client would need a configuration pointing to multiple nodes, because it might be masterless. We would probably want to target Akka 2.2, as it will have a feature for \"roles\", where clients will join the cluster not as a worker member, but as a \"client\" role. Otherwise the remote Akka connection might have issues. Reusing Akka metrics will be good too.\n2. Matei Alexandru Zaharia: Hey Evan, FYI, this recently merged PR adds ZooKeeper-based fault tolerance to the standalone mode: https://github.com/apache/incubator-spark/pull/19. When we move to Scala 2.10 and Akka 2.2, a similar approach based on Akka-cluster would be great. But this should at least be a start for cluster HA -- give it a whirl and tell us what you think.\n3. Prashant Sharma: Take a look at this preview. https://github.com/apache/incubator-spark/pull/60\n4. François Garillot: That last URL is https://github.com/apache/spark/pull/60", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "480876885062d941fa2e0079751d4333", "issue_key": "SPARK-744", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "BlockManagerUI with no RDD: java.lang.UnsupportedOperationException: empty.reduceLeft", "description": "Steps: 1. Start a Spark app, which starts Spark (I'm using Mesos) 2. Point browser to the BlockManager web UI before any RDD is created. 3. Exception!  2013-05-04 09:15:17,903 [spark-akka.actor.default-dispatcher-9201] ERROR cc.spray.HttpService - Error during processing of request HttpRequest(GET,/,List(Cookie: sessionid=\"d569ee15ea0b4dfebaa5eeca2465c051\"; csrftoken=\"a81cda0fc43c8de4097268d89096a026\"; session=\"4TujZqbfGWbBeJ8F1W+FIkxLw1E=?csrf=Uyc0NTFmYjI5ZjliMTBmZDlkZTZkZTZhMmUwZGZlZjJlZTZkOTQ5ZjUxJwpwMQou\", Accept-Language: en-US, en, Accept-Encoding: gzip, deflate, sdch, user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1490.2 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml, */*, Connection: keep-alive, Host: localhost:33971),None,HTTP/1.1) java.lang.UnsupportedOperationException: empty.reduceLeft at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:152) at scala.collection.mutable.ArrayOps.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayOps.scala:38) at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:69) at scala.collection.mutable.ArrayOps.reduceLeft(ArrayOps.scala:38) at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:180) at scala.collection.mutable.ArrayOps.reduce(ArrayOps.scala:38) at spark.storage.BlockManagerUI$$anonfun$5.apply(BlockManagerUI.scala:50) at spark.storage.BlockManagerUI$$anonfun$5.apply(BlockManagerUI.scala:46) at cc.spray.directives.MarshallingDirectives$$anonfun$completeWith$1.apply(MarshallingDirectives.scala:90) at cc.spray.directives.MarshallingDirectives$$anonfun$completeWith$1.apply(MarshallingDirectives.scala:90) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:104) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:102) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:195) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:194) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:195) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:194) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:104) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:102) at cc.spray.HttpServiceLogic$class.handle(HttpServiceLogic.scala:43) at cc.spray.HttpService.handle(HttpServiceActor.scala:49) at cc.spray.HttpServiceActor$$anonfun$receive$1.apply(HttpServiceActor.scala:41) at cc.spray.HttpServiceActor$$anonfun$receive$1.apply(HttpServiceActor.scala:40) at akka.actor.Actor$class.apply(Actor.scala:318) at cc.spray.HttpService.apply(HttpServiceActor.scala:49) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)", "reporter": "Hai-Anh Trinh", "assignee": null, "created": "2013-05-04T02:30:44.000+0000", "updated": "2014-09-17T04:07:44.000+0000", "resolved": "2014-09-17T04:07:44.000+0000", "labels": [], "components": [], "comments": [{"author": "Rohit Rai", "body": "I am not able to produce this with spark-shell or spark standalone cluster, on spark built from master. Is this a issue only with mesos cluster? or is it already fixed?", "created": "2013-06-02T03:34:20.288+0000"}, {"author": "Hai-Anh Trinh", "body": "As reported, this was Spark 0.7.0 on mesos. I'm not sure about the status of it on latest Spark.", "created": "2013-06-02T09:10:26.382+0000"}, {"author": "Rohit Rai", "body": "Thanks for confirming.", "created": "2013-06-02T12:12:29.220+0000"}], "num_comments": 3, "text": "Issue: SPARK-744\nSummary: BlockManagerUI with no RDD: java.lang.UnsupportedOperationException: empty.reduceLeft\nDescription: Steps: 1. Start a Spark app, which starts Spark (I'm using Mesos) 2. Point browser to the BlockManager web UI before any RDD is created. 3. Exception!  2013-05-04 09:15:17,903 [spark-akka.actor.default-dispatcher-9201] ERROR cc.spray.HttpService - Error during processing of request HttpRequest(GET,/,List(Cookie: sessionid=\"d569ee15ea0b4dfebaa5eeca2465c051\"; csrftoken=\"a81cda0fc43c8de4097268d89096a026\"; session=\"4TujZqbfGWbBeJ8F1W+FIkxLw1E=?csrf=Uyc0NTFmYjI5ZjliMTBmZDlkZTZkZTZhMmUwZGZlZjJlZTZkOTQ5ZjUxJwpwMQou\", Accept-Language: en-US, en, Accept-Encoding: gzip, deflate, sdch, user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1490.2 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml, */*, Connection: keep-alive, Host: localhost:33971),None,HTTP/1.1) java.lang.UnsupportedOperationException: empty.reduceLeft at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:152) at scala.collection.mutable.ArrayOps.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayOps.scala:38) at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:69) at scala.collection.mutable.ArrayOps.reduceLeft(ArrayOps.scala:38) at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:180) at scala.collection.mutable.ArrayOps.reduce(ArrayOps.scala:38) at spark.storage.BlockManagerUI$$anonfun$5.apply(BlockManagerUI.scala:50) at spark.storage.BlockManagerUI$$anonfun$5.apply(BlockManagerUI.scala:46) at cc.spray.directives.MarshallingDirectives$$anonfun$completeWith$1.apply(MarshallingDirectives.scala:90) at cc.spray.directives.MarshallingDirectives$$anonfun$completeWith$1.apply(MarshallingDirectives.scala:90) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:104) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:102) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:195) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:194) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:195) at cc.spray.directives.MiscDirectives$RouteConcatenation$$anonfun$$tilde$1.apply(MiscDirectives.scala:194) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:104) at cc.spray.directives.SprayRoute$$anonfun$fromRouting$1.apply(BasicDirectives.scala:102) at cc.spray.HttpServiceLogic$class.handle(HttpServiceLogic.scala:43) at cc.spray.HttpService.handle(HttpServiceActor.scala:49) at cc.spray.HttpServiceActor$$anonfun$receive$1.apply(HttpServiceActor.scala:41) at cc.spray.HttpServiceActor$$anonfun$receive$1.apply(HttpServiceActor.scala:40) at akka.actor.Actor$class.apply(Actor.scala:318) at cc.spray.HttpService.apply(HttpServiceActor.scala:49) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\nComments (3):\n1. Rohit Rai: I am not able to produce this with spark-shell or spark standalone cluster, on spark built from master. Is this a issue only with mesos cluster? or is it already fixed?\n2. Hai-Anh Trinh: As reported, this was Spark 0.7.0 on mesos. I'm not sure about the status of it on latest Spark.\n3. Rohit Rai: Thanks for confirming.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "5ccdc707ecb03089cce47fc1fba34b4f", "issue_key": "SPARK-745", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Document Scala environment configuration when Scala is installed from RPM", "description": "As https://github.com/mesos/spark/pull/292 points out, the Typesafe Scala RPM installs the {{scala}} executable in {{/usr/bin/}} and places the Scala library JARs in {{/usr/share/java/}}. That pull request added a SCALA_LIBRARY_PATH setting to support this method of installing Scala. We should document proper configuration of SCALA_LIBRARY_PATH and SCALA_HOME; right now, that pull request is the only documentation and it's implicit that the proper Debian / CentOS settings are SCALA_HOME=/usr/ and SCALA_LIBRARY_PATH=/usr/share/java/. I propose adding extra comments to the spark-conf.sh template that describe typical values for these settings. Maybe we also want to mention this in the docs somewhere?", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-05-07T18:20:45.000+0000", "updated": "2013-06-30T17:14:43.000+0000", "resolved": "2013-06-30T17:14:43.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I've now documented these: https://github.com/mesos/spark/commit/5bbd0eec84867937713ceb8438f25a943765a084", "created": "2013-06-30T17:14:43.692+0000"}], "num_comments": 1, "text": "Issue: SPARK-745\nSummary: Document Scala environment configuration when Scala is installed from RPM\nDescription: As https://github.com/mesos/spark/pull/292 points out, the Typesafe Scala RPM installs the {{scala}} executable in {{/usr/bin/}} and places the Scala library JARs in {{/usr/share/java/}}. That pull request added a SCALA_LIBRARY_PATH setting to support this method of installing Scala. We should document proper configuration of SCALA_LIBRARY_PATH and SCALA_HOME; right now, that pull request is the only documentation and it's implicit that the proper Debian / CentOS settings are SCALA_HOME=/usr/ and SCALA_LIBRARY_PATH=/usr/share/java/. I propose adding extra comments to the spark-conf.sh template that describe typical values for these settings. Maybe we also want to mention this in the docs somewhere?\n\nComments (1):\n1. Matei Alexandru Zaharia: I've now documented these: https://github.com/mesos/spark/commit/5bbd0eec84867937713ceb8438f25a943765a084", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "113b75c75a243191518369a14fa292ee", "issue_key": "SPARK-746", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Automatically Use Avro Serialization for Avro Objects", "description": "All generated objects extend org.apache.avro.specific.SpecificRecordBase (or there may be a higher up class as well). Since Avro records aren't JavaSerializable by default people currently have to wrap their records. It would be good if we could use an implicit conversion to do this for them.", "reporter": "Patrick McFadin", "assignee": "Joseph Batchik", "created": "2013-05-08T15:04:02.000+0000", "updated": "2021-02-20T02:23:05.000+0000", "resolved": "2015-07-29T19:03:11.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "Progress is being made to get this into Chill, so Spark may get this with just a future Chill library upgrade. https://github.com/twitter/chill/issues/171", "created": "2014-02-09T22:13:53.210+0000"}, {"author": "Andrew Ash", "body": "Progress being made on https://github.com/twitter/chill/pull/172", "created": "2014-02-14T12:20:16.741+0000"}, {"author": "Joseph Batchik", "body": "Spark can currently serialize the three type of Avro records if the user specifies Kryo. Specific and Reflect records serialize just fine, if the user registers them ahead of time, since Kryo can efficiently deal with serializing classes. The problem lies in generic records since Kryo cannot serialize them without a large amount of overhead. This causes issues for users who want to use Avro records during a shuffle. To alleviate this, I implemented a custom Kryo serializer for generic records that tries to reduce the amount of network IO. https://github.com/JDrit/spark/commit/6f1106bc20eb670e963d45a191dfc4517d46543b This works by sending a compressed form of the schema with each message over have Kryo serialize the in-memory representation itself. Since the same schema is going to be sent numerous times, it caches previously seen values as to reduce the computation needed. It also allows users to register their schemas ahead of time. This allows it to just send the schema’s unique ID with each message, over the entire schema itself. Could I get some feedback about this approach or let me know if I am missing anything important.", "created": "2015-06-23T17:11:54.117+0000"}, {"author": "Apache Spark", "body": "User 'JDrit' has created a pull request for this issue: https://github.com/apache/spark/pull/7004", "created": "2015-06-24T23:54:03.673+0000"}, {"author": "Joseph Batchik", "body": "I did some benchmarks for both the current implementation of serializing Avro records compared to this change, the results are located here: http://www.csh.rit.edu/~jd/spark/Avro%20Datapoints.xlsx", "created": "2015-07-23T00:19:56.269+0000"}, {"author": "Imran Rashid", "body": "Issue resolved by pull request 7004 [https://github.com/apache/spark/pull/7004]", "created": "2015-07-29T19:03:11.935+0000"}], "num_comments": 6, "text": "Issue: SPARK-746\nSummary: Automatically Use Avro Serialization for Avro Objects\nDescription: All generated objects extend org.apache.avro.specific.SpecificRecordBase (or there may be a higher up class as well). Since Avro records aren't JavaSerializable by default people currently have to wrap their records. It would be good if we could use an implicit conversion to do this for them.\n\nComments (6):\n1. Andrew Ash: Progress is being made to get this into Chill, so Spark may get this with just a future Chill library upgrade. https://github.com/twitter/chill/issues/171\n2. Andrew Ash: Progress being made on https://github.com/twitter/chill/pull/172\n3. Joseph Batchik: Spark can currently serialize the three type of Avro records if the user specifies Kryo. Specific and Reflect records serialize just fine, if the user registers them ahead of time, since Kryo can efficiently deal with serializing classes. The problem lies in generic records since Kryo cannot serialize them without a large amount of overhead. This causes issues for users who want to use Avro records during a shuffle. To alleviate this, I implemented a custom Kryo serializer for generic records that tries to reduce the amount of network IO. https://github.com/JDrit/spark/commit/6f1106bc20eb670e963d45a191dfc4517d46543b This works by sending a compressed form of the schema with each message over have Kryo serialize the in-memory representation itself. Since the same schema is going to be sent numerous times, it caches previously seen values as to reduce the computation needed. It also allows users to register their schemas ahead of time. This allows it to just send the schema’s unique ID with each message, over the entire schema itself. Could I get some feedback about this approach or let me know if I am missing anything important.\n4. Apache Spark: User 'JDrit' has created a pull request for this issue: https://github.com/apache/spark/pull/7004\n5. Joseph Batchik: I did some benchmarks for both the current implementation of serializing Avro records compared to this change, the results are located here: http://www.csh.rit.edu/~jd/spark/Avro%20Datapoints.xlsx\n6. Imran Rashid: Issue resolved by pull request 7004 [https://github.com/apache/spark/pull/7004]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "9947e50839b0b4d0ef82c5c9828a3c62", "issue_key": "SPARK-747", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Throw an exception on slaves if a message is sent that is larger than akka's max frame size", "description": "This can be really hard to debug for users, until we fix SPARK-669 we should at least warn users when this happens with an exception: https://groups.google.com/forum/#!msg/spark-users/WG87fG8rrKY/6VpJ1AFXS-cJ", "reporter": "Patrick McFadin", "assignee": "Josh Rosen", "created": "2013-05-10T22:29:06.000+0000", "updated": "2014-03-17T10:23:05.000+0000", "resolved": "2014-03-17T10:23:05.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "It looks like earlier versions of Akka had some bugs that make this issue very difficult to diagnose: https://www.assembla.com/spaces/akka/tickets/3038-silently-dropped-big-messages It looks like this has been fixed in Akka 2.1.1, so I'll see about upgrading to a newer Akka version.", "created": "2013-05-11T17:55:24.073+0000"}, {"author": "Reynold Xin", "body": "I saw that ticket before - the problem is Spark needs to be aware that this is happening too ... and hopefully remind the user. It is hard to diagnose problems right now, but it wouldn't be too hard to check how big the task result is. It doesn't have to be very accurate, but as long as it is close to the max frame size, we can at least give a warning. But yes, if you have time to just fix SPARK-669, it might be easier to just do that one.", "created": "2013-05-11T17:58:49.726+0000"}, {"author": "Josh Rosen", "body": "Pull request https://github.com/mesos/spark/pull/610 modifies Spark to abort the job if any task fails due to a TaskResult that exceeds the Akka frame size. Do we want to backport this fix into branch-0.7?", "created": "2013-05-19T13:18:18.753+0000"}, {"author": "Shivaram Venkataraman", "body": "I just saw a case where sc.parallelize failed when the array being parallelized is larger than 10MB. We should also print an error message or warning for that.", "created": "2013-06-12T15:08:42.726+0000"}, {"author": "Patrick McFadin", "body": "Subsumed by SPARK-669.", "created": "2014-03-17T10:23:05.344+0000"}], "num_comments": 5, "text": "Issue: SPARK-747\nSummary: Throw an exception on slaves if a message is sent that is larger than akka's max frame size\nDescription: This can be really hard to debug for users, until we fix SPARK-669 we should at least warn users when this happens with an exception: https://groups.google.com/forum/#!msg/spark-users/WG87fG8rrKY/6VpJ1AFXS-cJ\n\nComments (5):\n1. Josh Rosen: It looks like earlier versions of Akka had some bugs that make this issue very difficult to diagnose: https://www.assembla.com/spaces/akka/tickets/3038-silently-dropped-big-messages It looks like this has been fixed in Akka 2.1.1, so I'll see about upgrading to a newer Akka version.\n2. Reynold Xin: I saw that ticket before - the problem is Spark needs to be aware that this is happening too ... and hopefully remind the user. It is hard to diagnose problems right now, but it wouldn't be too hard to check how big the task result is. It doesn't have to be very accurate, but as long as it is close to the max frame size, we can at least give a warning. But yes, if you have time to just fix SPARK-669, it might be easier to just do that one.\n3. Josh Rosen: Pull request https://github.com/mesos/spark/pull/610 modifies Spark to abort the job if any task fails due to a TaskResult that exceeds the Akka frame size. Do we want to backport this fix into branch-0.7?\n4. Shivaram Venkataraman: I just saw a case where sc.parallelize failed when the array being parallelized is larger than 10MB. We should also print an error message or warning for that.\n5. Patrick McFadin: Subsumed by SPARK-669.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "a592d3321b7adb27f75be82e7917ba06", "issue_key": "SPARK-748", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add documentation page describing interoperability with other software (e.g. HBase, JDBC, Kafka, etc.)", "description": "Spark seems to be gaining a lot of data input / output features for integrating with systems like HBase, Kafka, JDBC, Hadoop, etc. It might be a good idea to create a single documentation page that provides a list of all of the data sources that Spark supports and links to the relevant documentation / examples / {{spark-users}} threads. This would help prospective users to evaluate how easy it will be to integrate Spark with their existing systems.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-05-12T13:39:53.000+0000", "updated": "2016-01-18T10:24:06.000+0000", "resolved": "2016-01-18T10:24:05.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Andrew Ash", "body": "I agree this would be valuable -- almost like a \"Spark Cookbook\" of how to read and write data from various other systems. Step one is probably deciding what software to mention. Tentatively I propose: Spark Core - HDFS - HBase - Cassandra - Elasticsearch - JDBC, with examples for Postgres and MySQL - General Hadoop InputFormat Spark Streaming - Kafka - Flume - Storm For destination, this could go on the documentation included in the git repo and published to the Spark website, or on the Spark project wiki. I tend to prefer the former. A possible location for that could be http://spark.apache.org/docs/latest/programming-guide.html#external-datasets", "created": "2014-11-14T09:30:14.928+0000"}, {"author": "Sean R. Owen", "body": "Some of this exists in pieces in the docs. It's also committing to update and maintain docs for integration with this and more future external systems. Given that the integration example code isn't well maintained I suspect this won't happen and this hasn't moved in 15 months.", "created": "2016-01-18T10:24:06.068+0000"}], "num_comments": 2, "text": "Issue: SPARK-748\nSummary: Add documentation page describing interoperability with other software (e.g. HBase, JDBC, Kafka, etc.)\nDescription: Spark seems to be gaining a lot of data input / output features for integrating with systems like HBase, Kafka, JDBC, Hadoop, etc. It might be a good idea to create a single documentation page that provides a list of all of the data sources that Spark supports and links to the relevant documentation / examples / {{spark-users}} threads. This would help prospective users to evaluate how easy it will be to integrate Spark with their existing systems.\n\nComments (2):\n1. Andrew Ash: I agree this would be valuable -- almost like a \"Spark Cookbook\" of how to read and write data from various other systems. Step one is probably deciding what software to mention. Tentatively I propose: Spark Core - HDFS - HBase - Cassandra - Elasticsearch - JDBC, with examples for Postgres and MySQL - General Hadoop InputFormat Spark Streaming - Kafka - Flume - Storm For destination, this could go on the documentation included in the git repo and published to the Spark website, or on the Spark project wiki. I tend to prefer the former. A possible location for that could be http://spark.apache.org/docs/latest/programming-guide.html#external-datasets\n2. Sean R. Owen: Some of this exists in pieces in the docs. It's also committing to update and maintain docs for integration with this and more future external systems. Given that the integration example code isn't well maintained I suspect this won't happen and this hasn't moved in 15 months.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "ca876682fb6422c0bb21f8a2b5a95855", "issue_key": "SPARK-749", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark-ec2 fails to detect cluster after ssh error during launch", "description": "I tried to launch an EC2 cluster using Patrick's new version of the EC2 script, running this command  ./spark-ec2 -i ~/.ssh/id_rsa -s 10 -t m1.large --spot-price 0.25 --region us-west-1 launch test_cluster  This launched a cluster, but ssh failed because I didn't configure things properly  Setting up security groups... Searching for existing cluster test_cluster... Spark AMI: ami-61ffd024 Launching instances... Requesting 10 slaves as spot instances with price $0.250 Waiting for spot instances to be granted... 0 of 10 slaves granted, waiting longer ... All 10 slaves granted Launched master in us-west-1a, regid = r-790f2320 Waiting for instances to start up... Waiting 120 more seconds... Copying SSH key /Users/joshrosen/.ssh/id_rsa to master... Warning: Permanently added 'ec2-54-241-179-230.us-west-1.compute.amazonaws.com,54.241.179.230' (RSA) to the list of known hosts. Permission denied (publickey).  When I tried to shut this cluster down, the EC2 script claims that it couldn't find a cluster. I see the same message when using the 0.7 version of spark-ec2. If I view the EC2 administration console, it tells me that I have a number of machines running in us-west-1 with the right security groups. Although the script finds the active instance requests, it can't find any security group names for those requests, so it fails to identify those machines as belonging to our spark-ec2 cluster. I think the culprit is the line  group_names = [g.name for g in res.groups]  in {{get_existing_cluster()}}. If I replace this with  group_names = list(set(g.name for g in i.groups for i in res.instances))  then it finds the group names and detects the cluster. Does anyone know what's going on here? I'm not familiar enough with Boto to explain why the Instance would have groups attribute while its Request's groups are empty.", "reporter": "Josh Rosen", "assignee": "Aaron Davidson", "created": "2013-05-14T15:11:37.000+0000", "updated": "2013-09-19T14:58:23.000+0000", "resolved": "2013-09-19T14:58:23.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Shivaram Venkataraman", "body": "From my limited knowledge of boto, the first command looks like it retrieves all the groups for a user, while the second one lists the groups for instances. So my guess is that the while the instances were assigned a group, the group was not created in the list of all groups for a user (maybe an effect of eventual consistency ?).", "created": "2013-05-19T11:41:17.405+0000"}, {"author": "Aaron Davidson", "body": "Fixed in [938|https://github.com/mesos/spark/pull/938]. The exact cause of this discrepancy is still not known, but this PR should fix it nevertheless.", "created": "2013-09-19T14:58:23.723+0000"}], "num_comments": 2, "text": "Issue: SPARK-749\nSummary: spark-ec2 fails to detect cluster after ssh error during launch\nDescription: I tried to launch an EC2 cluster using Patrick's new version of the EC2 script, running this command  ./spark-ec2 -i ~/.ssh/id_rsa -s 10 -t m1.large --spot-price 0.25 --region us-west-1 launch test_cluster  This launched a cluster, but ssh failed because I didn't configure things properly  Setting up security groups... Searching for existing cluster test_cluster... Spark AMI: ami-61ffd024 Launching instances... Requesting 10 slaves as spot instances with price $0.250 Waiting for spot instances to be granted... 0 of 10 slaves granted, waiting longer ... All 10 slaves granted Launched master in us-west-1a, regid = r-790f2320 Waiting for instances to start up... Waiting 120 more seconds... Copying SSH key /Users/joshrosen/.ssh/id_rsa to master... Warning: Permanently added 'ec2-54-241-179-230.us-west-1.compute.amazonaws.com,54.241.179.230' (RSA) to the list of known hosts. Permission denied (publickey).  When I tried to shut this cluster down, the EC2 script claims that it couldn't find a cluster. I see the same message when using the 0.7 version of spark-ec2. If I view the EC2 administration console, it tells me that I have a number of machines running in us-west-1 with the right security groups. Although the script finds the active instance requests, it can't find any security group names for those requests, so it fails to identify those machines as belonging to our spark-ec2 cluster. I think the culprit is the line  group_names = [g.name for g in res.groups]  in {{get_existing_cluster()}}. If I replace this with  group_names = list(set(g.name for g in i.groups for i in res.instances))  then it finds the group names and detects the cluster. Does anyone know what's going on here? I'm not familiar enough with Boto to explain why the Instance would have groups attribute while its Request's groups are empty.\n\nComments (2):\n1. Shivaram Venkataraman: From my limited knowledge of boto, the first command looks like it retrieves all the groups for a user, while the second one lists the groups for instances. So my guess is that the while the instances were assigned a group, the group was not created in the list of all groups for a user (maybe an effect of eventual consistency ?).\n2. Aaron Davidson: Fixed in [938|https://github.com/mesos/spark/pull/938]. The exact cause of this discrepancy is still not known, but this PR should fix it nevertheless.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "a6094079a09ae34f7dc13ba2ba3ba5be", "issue_key": "SPARK-750", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "LocalSparkContext should be included in Spark JAR", "description": "To aid third-party developers in writing unit tests with Spark, LocalSparkContext should be included in the Spark JAR. Right now, it appears to be excluded because it is located in one of the Spark test directories.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-05-15T15:56:14.000+0000", "updated": "2015-01-20T15:46:48.000+0000", "resolved": "2015-01-20T15:46:48.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Actually, the right solution might be to use Maven to publish a separate 'test-jar' artifact that contains the test classes (I think Hadoop does something like this with artifacts like 'hadoop-yarn-server-tests').", "created": "2013-06-10T22:43:17.217+0000"}, {"author": "Andy Petrella", "body": "+1 for the test-jar!", "created": "2014-02-19T01:38:26.207+0000"}, {"author": "Nathan McCarthy", "body": "+1 This shouldnt be hard, in maven its a plugin to add in the spark/core/pom.xml file like describe here http://maven.apache.org/plugins/maven-jar-plugin/examples/create-test-jar.html", "created": "2014-11-13T03:08:06.938+0000"}, {"author": "Sean R. Owen", "body": "Can this be considered subsumed by https://issues.apache.org/jira/browse/SPARK-4442 now?", "created": "2014-11-21T22:20:59.348+0000"}, {"author": "Matthew Cornell", "body": "Please, as a new Spark (and Maven and SBT) user, having a jar I could simply drop into my IntelliJ project would be a life saver. Until then, would someone please sketch a little detail on how I could build the jar using the 1.2.0 sources? Thanks!", "created": "2015-01-20T15:23:53.383+0000"}, {"author": "Sean R. Owen", "body": "I'm going to boldly fold this into SPARK-4442 as a more general, related request to expose test utilities explicitly.", "created": "2015-01-20T15:46:48.835+0000"}], "num_comments": 6, "text": "Issue: SPARK-750\nSummary: LocalSparkContext should be included in Spark JAR\nDescription: To aid third-party developers in writing unit tests with Spark, LocalSparkContext should be included in the Spark JAR. Right now, it appears to be excluded because it is located in one of the Spark test directories.\n\nComments (6):\n1. Josh Rosen: Actually, the right solution might be to use Maven to publish a separate 'test-jar' artifact that contains the test classes (I think Hadoop does something like this with artifacts like 'hadoop-yarn-server-tests').\n2. Andy Petrella: +1 for the test-jar!\n3. Nathan McCarthy: +1 This shouldnt be hard, in maven its a plugin to add in the spark/core/pom.xml file like describe here http://maven.apache.org/plugins/maven-jar-plugin/examples/create-test-jar.html\n4. Sean R. Owen: Can this be considered subsumed by https://issues.apache.org/jira/browse/SPARK-4442 now?\n5. Matthew Cornell: Please, as a new Spark (and Maven and SBT) user, having a jar I could simply drop into my IntelliJ project would be a life saver. Until then, would someone please sketch a little detail on how I could build the jar using the 1.2.0 sources? Thanks!\n6. Sean R. Owen: I'm going to boldly fold this into SPARK-4442 as a more general, related request to expose test utilities explicitly.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.046724"}}
{"id": "e05c0742eaa5f136795df865dc3ef332", "issue_key": "SPARK-752", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "CoalescedRDD should maximize locality", "description": "CoalescedRDD simply ignores partition locality, which can be very inefficient if the parent RDD partitions are not clustered by locality.", "reporter": "Reynold Xin", "assignee": null, "created": "2013-05-20T19:59:36.000+0000", "updated": "2013-09-01T15:21:24.000+0000", "resolved": "2013-09-01T15:21:23.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Would the idea be to have a \"machine-local\" coalesce that merges all of the partitions co-located on the same machine into a single machine (i.e. combining an RDD into one partition per machine)? If we had that, then it might provide a clean way to perform per-machine local combining in reduce() (this would still be sub-optimal if it used a ShuffleDependency since we'd have to materialize the partitions before beginning to locally combine them). The problem is that we don't necessarily know which partitions are on which machines until runtime. There are also fault-tolerance issues: re-computations would have to coalesce the same partitions, even if the recomputed partitions are no longer on the same machine.", "created": "2013-05-27T10:33:07.621+0000"}, {"author": "Reynold Xin", "body": "It's not going to be that complicated. The parent RDDs of this coalesced RDD should have some locality constraint (even without cached blocks). E.g. HadoopRDD. This RDD can coalesce based on those locality constraints.", "created": "2013-05-27T12:04:52.168+0000"}, {"author": "Josh Rosen", "body": "Ali fixed this in https://github.com/mesos/spark/pull/832", "created": "2013-09-01T15:21:24.145+0000"}], "num_comments": 3, "text": "Issue: SPARK-752\nSummary: CoalescedRDD should maximize locality\nDescription: CoalescedRDD simply ignores partition locality, which can be very inefficient if the parent RDD partitions are not clustered by locality.\n\nComments (3):\n1. Josh Rosen: Would the idea be to have a \"machine-local\" coalesce that merges all of the partitions co-located on the same machine into a single machine (i.e. combining an RDD into one partition per machine)? If we had that, then it might provide a clean way to perform per-machine local combining in reduce() (this would still be sub-optimal if it used a ShuffleDependency since we'd have to materialize the partitions before beginning to locally combine them). The problem is that we don't necessarily know which partitions are on which machines until runtime. There are also fault-tolerance issues: re-computations would have to coalesce the same partitions, even if the recomputed partitions are no longer on the same machine.\n2. Reynold Xin: It's not going to be that complicated. The parent RDDs of this coalesced RDD should have some locality constraint (even without cached blocks). E.g. HadoopRDD. This RDD can coalesce based on those locality constraints.\n3. Josh Rosen: Ali fixed this in https://github.com/mesos/spark/pull/832", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.046724"}}
{"id": "c1baa8363ae1f9756b59e46c72f946af", "issue_key": "SPARK-751", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Consolidate shuffle files", "description": "Right now on each machine, we create M * R temporary files for shuffle, where M = number of map tasks, R = number of reduce tasks. This can be pretty high when there are lots of mappers and reducers (e.g. 1k map * 1k reduce = 1 million files for a single shuffle). The high number can cripple the file system and significantly slow the system down. We should cut this number down to O(R) instead of O(M*R).", "reporter": "Reynold Xin", "assignee": "Jason Dai", "created": "2013-05-21T00:31:34.000+0000", "updated": "2013-11-14T18:29:59.000+0000", "resolved": "2013-11-14T18:29:59.000+0000", "labels": [], "components": [], "comments": [{"author": "Jason Dai", "body": "A document that outlines the design", "created": "2013-07-03T01:46:11.796+0000"}, {"author": "Patrick McFadin", "body": "I also did some experiments to see how much having many smaller files impacts disk throughput. This is another \"cost\" of having M * R output files. The below image shows the result of running a small micro-benchmark [1] on a single node with 4 cores and 4 disks. The benchmark simulates running several concurrent maps (in this case 4) which each output several reduce splits. The total output amount (across all reducers) and the number of output splits (per reducer) are varied. Plotted is per-disk throughput in MB/s. This is a modified version of Spark that let's you fsync all the output files at the end of each map task. The big take away is that as you get to many smaller files, the disk throughput get much worse. This is my experience based on running benchmarks in clusters - although _there_ we see it as the write-back speed from the OS buffer cache (because we don't usually fsync). This PR should help this somewhat if it gets merged, but we also should be careful to warn people not to create a very large number of reduce splits if there isn't much data to crunch. In the past, at least, I've tended to tell people to use a large number of reducers as a default strategy, but there is some cost do doing that, at least if they are running many successive queries/jobs such that they eventually evict the buffer cache. !https://docs.google.com/spreadsheet/oimg?key=0Aj2sNEazX9XIdGZYak9xNE9fTVg0Zmd5OEtXYzBROFE&oid=2&zx=j6lulflsn0oa! [1] https://github.com/pwendell/spark/blob/shuffle-benchmark/core/src/main/scala/spark/storage/StoragePerfTester.scala", "created": "2013-08-17T00:29:40.930+0000"}, {"author": "Aaron Davidson", "body": "A more basic version of shuffle file consolidation was implemented here: https://github.com/apache/incubator-spark/pull/87 It accomplishes the goal of reducing overhead to O(R) instead of O(M*R), but leaves out some more potential optimizations. Marking this feature as fixed since I think this was the main issue, but feel free to file further feature improvements for unimplemented optimizations.", "created": "2013-11-14T18:29:59.674+0000"}], "num_comments": 3, "text": "Issue: SPARK-751\nSummary: Consolidate shuffle files\nDescription: Right now on each machine, we create M * R temporary files for shuffle, where M = number of map tasks, R = number of reduce tasks. This can be pretty high when there are lots of mappers and reducers (e.g. 1k map * 1k reduce = 1 million files for a single shuffle). The high number can cripple the file system and significantly slow the system down. We should cut this number down to O(R) instead of O(M*R).\n\nComments (3):\n1. Jason Dai: A document that outlines the design\n2. Patrick McFadin: I also did some experiments to see how much having many smaller files impacts disk throughput. This is another \"cost\" of having M * R output files. The below image shows the result of running a small micro-benchmark [1] on a single node with 4 cores and 4 disks. The benchmark simulates running several concurrent maps (in this case 4) which each output several reduce splits. The total output amount (across all reducers) and the number of output splits (per reducer) are varied. Plotted is per-disk throughput in MB/s. This is a modified version of Spark that let's you fsync all the output files at the end of each map task. The big take away is that as you get to many smaller files, the disk throughput get much worse. This is my experience based on running benchmarks in clusters - although _there_ we see it as the write-back speed from the OS buffer cache (because we don't usually fsync). This PR should help this somewhat if it gets merged, but we also should be careful to warn people not to create a very large number of reduce splits if there isn't much data to crunch. In the past, at least, I've tended to tell people to use a large number of reducers as a default strategy, but there is some cost do doing that, at least if they are running many successive queries/jobs such that they eventually evict the buffer cache. !https://docs.google.com/spreadsheet/oimg?key=0Aj2sNEazX9XIdGZYak9xNE9fTVg0Zmd5OEtXYzBROFE&oid=2&zx=j6lulflsn0oa! [1] https://github.com/pwendell/spark/blob/shuffle-benchmark/core/src/main/scala/spark/storage/StoragePerfTester.scala\n3. Aaron Davidson: A more basic version of shuffle file consolidation was implemented here: https://github.com/apache/incubator-spark/pull/87 It accomplishes the goal of reducing overhead to O(R) instead of O(M*R), but leaves out some more potential optimizations. Marking this feature as fixed since I think this was the main issue, but feel free to file further feature improvements for unimplemented optimizations.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.046724"}}
{"id": "a15c5203d642618be1ddbcc644c753ed", "issue_key": "SPARK-753", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "ClusterSchedulerSuite unit test will failed in some scenarios", "description": "ClusterSchedulerSuite unit test will failed in some scenarios [info] ClusterSchedulerSuite: [info] - FIFO Scheduler Test [INFO] [05/30/2013 14:56:15.332] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [info] - Fair Scheduler Test *** FAILED *** [info] 3 did not equal 0 (ClusterSchedulerSuite.scala:111) [INFO] [05/30/2013 14:56:15.540] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [info] - Nested Pool Test *** FAILED *** [info] 7 did not equal 0 (ClusterSchedulerSuite.scala:111) [INFO] [05/30/2013 14:56:15.712] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [error] Failed: : Total 3, Failed 2, Errors 0, Passed 1, Skipped 0 [error] Failed tests: [error] spark.scheduler.ClusterSchedulerSuite [error] (core/test:test-only) sbt.TestsFailedException: Tests unsuccessful [error] Total time: 5 s, completed May 30, 2013 2:56:15 PM", "reporter": "xiajunluan", "assignee": "xiajunluan", "created": "2013-05-31T00:53:08.000+0000", "updated": "2014-01-07T17:59:22.000+0000", "resolved": "2014-01-07T17:59:22.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Zhihong Yu", "body": "Was the test failure based on hadoop1 or hadoop2 ? Can you attach full test output ? Thanks", "created": "2014-01-07T16:35:24.069+0000"}, {"author": "xiajunluan", "body": "I have fixed this bug, it has nothing to do with Hadoop, thanks.", "created": "2014-01-07T17:58:04.261+0000"}, {"author": "xiajunluan", "body": "fix this bug by changing scheduler algorithm.", "created": "2014-01-07T17:59:22.232+0000"}], "num_comments": 3, "text": "Issue: SPARK-753\nSummary: ClusterSchedulerSuite unit test will failed in some scenarios\nDescription: ClusterSchedulerSuite unit test will failed in some scenarios [info] ClusterSchedulerSuite: [info] - FIFO Scheduler Test [INFO] [05/30/2013 14:56:15.332] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [info] - Fair Scheduler Test *** FAILED *** [info] 3 did not equal 0 (ClusterSchedulerSuite.scala:111) [INFO] [05/30/2013 14:56:15.540] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [info] - Nested Pool Test *** FAILED *** [info] 7 did not equal 0 (ClusterSchedulerSuite.scala:111) [INFO] [05/30/2013 14:56:15.712] [spray-io-worker-0] [IoWorker] IoWorker thread 'spray-io-worker-0' stopped [error] Failed: : Total 3, Failed 2, Errors 0, Passed 1, Skipped 0 [error] Failed tests: [error] spark.scheduler.ClusterSchedulerSuite [error] (core/test:test-only) sbt.TestsFailedException: Tests unsuccessful [error] Total time: 5 s, completed May 30, 2013 2:56:15 PM\n\nComments (3):\n1. Zhihong Yu: Was the test failure based on hadoop1 or hadoop2 ? Can you attach full test output ? Thanks\n2. xiajunluan: I have fixed this bug, it has nothing to do with Hadoop, thanks.\n3. xiajunluan: fix this bug by changing scheduler algorithm.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.046724"}}
{"id": "2d47d20776e472ca3cfb4e670c121ba9", "issue_key": "SPARK-754", "issue_type": "Bug", "status": "Closed", "priority": "Critical", "resolution": null, "summary": "Multiple Spark Contexts active in a single Spark Context", "description": "This may be no more than creating a unit test to ensure it can be done but it is not clear that one can instantiate multiple spark contexts within a single VM and use them concurrently (one thread in a context at a time).", "reporter": "Erik James Freed", "assignee": null, "created": "2013-05-31T12:06:03.000+0000", "updated": "2014-11-14T10:53:22.000+0000", "resolved": "2014-11-14T10:53:22.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Richard Low", "body": "I can think of at least two tasks: * Have a per-SparkContext configuration map, as discussed here: https://groups.google.com/forum/#!topic/spark-users/G9YXvOaQE84 * Remove System.exit calls so one context failure doesn't bring down the whole JVM. There should be a way of handling these errors to e.g. restart a context.", "created": "2013-08-05T13:27:35.385+0000"}, {"author": "Andrew Ash", "body": "This is actually currently unsupported, and a ticket to make this possible is being tracked at SPARK-2243 I'm going to close this ticket as a duplicate of that one, but please let me know if you feel there are subtleties here that keep these from being a duplicate. Thanks for the bug report Erik! Andrew", "created": "2014-11-14T10:53:02.144+0000"}], "num_comments": 2, "text": "Issue: SPARK-754\nSummary: Multiple Spark Contexts active in a single Spark Context\nDescription: This may be no more than creating a unit test to ensure it can be done but it is not clear that one can instantiate multiple spark contexts within a single VM and use them concurrently (one thread in a context at a time).\n\nComments (2):\n1. Richard Low: I can think of at least two tasks: * Have a per-SparkContext configuration map, as discussed here: https://groups.google.com/forum/#!topic/spark-users/G9YXvOaQE84 * Remove System.exit calls so one context failure doesn't bring down the whole JVM. There should be a way of handling these errors to e.g. restart a context.\n2. Andrew Ash: This is actually currently unsupported, and a ticket to make this possible is being tracked at SPARK-2243 I'm going to close this ticket as a duplicate of that one, but please let me know if you feel there are subtleties here that keep these from being a duplicate. Thanks for the bug report Erik! Andrew", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.046724"}}
{"id": "2e70149b6edec60d5f1ad3dc971f6089", "issue_key": "SPARK-755", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Kryo serialization failing - MLbase", "description": "When I turn on Kryo serialization, I get the following error as I increase the size of my input dataset. (From ~10GB to ~100GB). This issue does not manifest itself when I turn kryo off. I have code that successfully reads files, parses them into an RDD[(String,Vector)], which can then be .count()'ed. I then run a .flatMap on these, with a function that has the following signature:  def expandData(x: (String, Vector)): Seq[(String, Float, Vector)]  And running a .count() on that RDD crashes - stack trace of failed task looks like this:  13/05/31 00:16:53 INFO cluster.TaskSetManager: Finished TID 2024 in 23594 ms (progress: 10/1000) 13/05/31 00:16:53 INFO scheduler.DAGScheduler: Completed ResultTask(3, 24) 13/05/31 00:16:53 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_3,runningTasks:151 13/05/31 00:16:53 INFO cluster.TaskSetManager: Starting task 3.0:175 as TID 2161 on slave 14: ip-10-62-199-77.ec2.internal:40850 (NODE_LOCAL) 13/05/31 00:16:53 INFO cluster.TaskSetManager: Serialized task 3.0:175 as 2832 bytes in 0 ms 13/05/31 00:16:53 INFO cluster.TaskSetManager: Lost TID 2053 (task 3.0:49) 13/05/31 00:16:53 INFO cluster.TaskSetManager: Loss was due to com.esotericsoftware.kryo.KryoException com.esotericsoftware.kryo.KryoException: java.lang.ArrayIndexOutOfBoundsException Serialization trace: elements (org.mlbase.Vector) _3 (scala.Tuple3) at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:585) at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213) at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:504) at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564) at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213) at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:571) at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:26) at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:63) at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:21) at spark.storage.BlockManager.dataSerialize(BlockManager.scala:910) at spark.storage.MemoryStore.putValues(MemoryStore.scala:61) at spark.storage.BlockManager.liftedTree1$1(BlockManager.scala:584) at spark.storage.BlockManager.put(BlockManager.scala:580) at spark.CacheManager.getOrCompute(CacheManager.scala:55) at spark.RDD.iterator(RDD.scala:207) at spark.scheduler.ResultTask.run(ResultTask.scala:84) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/05/31 00:16:53 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_3,runningTasks:151 13/05/31 00:16:53 INFO cluster.TaskSetManager: Starting task 3.0:49 as TID 2162 on slave 12: ip-10-11-46-255.ec2.internal:38878 (NODE_LOCAL) 13/05/31 00:16:53 INFO cluster.TaskSetManager: Serialized task 3.0:49 as 2832 bytes in 0 ms 13/05/31 00:16:53 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_3,runningTasks:152 13/05/31 00:16:54 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Added rdd_7_257 in mem  My Kryo Registrator looks like this:  class MyRegistrator extends KryoRegistrator { override def registerClasses(kryo: Kryo) { kryo.register(classOf[Vector]) kryo.register(classOf[String]) kryo.register(classOf[Float]) kryo.register(classOf[Tuple3[String,Float,Vector]]) kryo.register(classOf[Seq[Tuple3[String,Float,Vector]]]) kryo.register(classOf[Map[String,Vector]]) } }  \"Vector\" in this case is an org.mlbase.Vector, which in this case is a slightly modified version of spark.util.Vector (uses floats instead of Doubles).", "reporter": "Evan Sparks", "assignee": null, "created": "2013-05-31T14:48:59.000+0000", "updated": "2015-02-08T23:27:56.000+0000", "resolved": "2015-02-08T23:27:56.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Reynold Xin", "body": "I wonder if the problem is you've exceeded the Java array size limit, which is only indexed using int types, which gives you ~ 2^31 bytes.", "created": "2013-05-31T14:56:12.117+0000"}, {"author": "Evan Sparks", "body": "Each record is a Tuple3[(String,Float,Vector)] where internally the vectors are all Array[Float] of size 160000. Is it possible that would Kryo try and serialize many of these vectors into the same Java Array?", "created": "2013-05-31T15:45:29.461+0000"}, {"author": "Andrew Ash", "body": "Quick check [~sparks], this hasn't been updated in quite some time; is it still a problem for you? I think you may have been bit by SPARK-2878 which also deals with Kryo serialization failing and has since been fixed.", "created": "2014-11-14T09:08:33.937+0000"}, {"author": "Evan Sparks", "body": "Hmm... the issue was that Kryo worked well on small objects but when those same objects got bigger, it crashed. The JIRA you link to looks more like classpath issues with different versions of a custom serializer. This seems more like a kryo configuration issue or something. (Similar to something like an akka max frame size). At any rate - I haven't tried to run this particular example in some time, so if someone wants to to try and recreate with an RDD[Tuple3(String,Float,Array[Float])] where all arrays are size 160,000 and there are 1.2m elements in the RDD, be my guest, otherwise feel free to close the ticket and we can keep our fingers crossed that this isn't in spark 1.0+.", "created": "2014-11-14T16:43:34.215+0000"}], "num_comments": 4, "text": "Issue: SPARK-755\nSummary: Kryo serialization failing - MLbase\nDescription: When I turn on Kryo serialization, I get the following error as I increase the size of my input dataset. (From ~10GB to ~100GB). This issue does not manifest itself when I turn kryo off. I have code that successfully reads files, parses them into an RDD[(String,Vector)], which can then be .count()'ed. I then run a .flatMap on these, with a function that has the following signature:  def expandData(x: (String, Vector)): Seq[(String, Float, Vector)]  And running a .count() on that RDD crashes - stack trace of failed task looks like this:  13/05/31 00:16:53 INFO cluster.TaskSetManager: Finished TID 2024 in 23594 ms (progress: 10/1000) 13/05/31 00:16:53 INFO scheduler.DAGScheduler: Completed ResultTask(3, 24) 13/05/31 00:16:53 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_3,runningTasks:151 13/05/31 00:16:53 INFO cluster.TaskSetManager: Starting task 3.0:175 as TID 2161 on slave 14: ip-10-62-199-77.ec2.internal:40850 (NODE_LOCAL) 13/05/31 00:16:53 INFO cluster.TaskSetManager: Serialized task 3.0:175 as 2832 bytes in 0 ms 13/05/31 00:16:53 INFO cluster.TaskSetManager: Lost TID 2053 (task 3.0:49) 13/05/31 00:16:53 INFO cluster.TaskSetManager: Loss was due to com.esotericsoftware.kryo.KryoException com.esotericsoftware.kryo.KryoException: java.lang.ArrayIndexOutOfBoundsException Serialization trace: elements (org.mlbase.Vector) _3 (scala.Tuple3) at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:585) at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213) at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:504) at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564) at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213) at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:571) at spark.KryoSerializationStream.writeObject(KryoSerializer.scala:26) at spark.serializer.SerializationStream$class.writeAll(Serializer.scala:63) at spark.KryoSerializationStream.writeAll(KryoSerializer.scala:21) at spark.storage.BlockManager.dataSerialize(BlockManager.scala:910) at spark.storage.MemoryStore.putValues(MemoryStore.scala:61) at spark.storage.BlockManager.liftedTree1$1(BlockManager.scala:584) at spark.storage.BlockManager.put(BlockManager.scala:580) at spark.CacheManager.getOrCompute(CacheManager.scala:55) at spark.RDD.iterator(RDD.scala:207) at spark.scheduler.ResultTask.run(ResultTask.scala:84) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/05/31 00:16:53 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_3,runningTasks:151 13/05/31 00:16:53 INFO cluster.TaskSetManager: Starting task 3.0:49 as TID 2162 on slave 12: ip-10-11-46-255.ec2.internal:38878 (NODE_LOCAL) 13/05/31 00:16:53 INFO cluster.TaskSetManager: Serialized task 3.0:49 as 2832 bytes in 0 ms 13/05/31 00:16:53 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_3,runningTasks:152 13/05/31 00:16:54 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Added rdd_7_257 in mem  My Kryo Registrator looks like this:  class MyRegistrator extends KryoRegistrator { override def registerClasses(kryo: Kryo) { kryo.register(classOf[Vector]) kryo.register(classOf[String]) kryo.register(classOf[Float]) kryo.register(classOf[Tuple3[String,Float,Vector]]) kryo.register(classOf[Seq[Tuple3[String,Float,Vector]]]) kryo.register(classOf[Map[String,Vector]]) } }  \"Vector\" in this case is an org.mlbase.Vector, which in this case is a slightly modified version of spark.util.Vector (uses floats instead of Doubles).\n\nComments (4):\n1. Reynold Xin: I wonder if the problem is you've exceeded the Java array size limit, which is only indexed using int types, which gives you ~ 2^31 bytes.\n2. Evan Sparks: Each record is a Tuple3[(String,Float,Vector)] where internally the vectors are all Array[Float] of size 160000. Is it possible that would Kryo try and serialize many of these vectors into the same Java Array?\n3. Andrew Ash: Quick check [~sparks], this hasn't been updated in quite some time; is it still a problem for you? I think you may have been bit by SPARK-2878 which also deals with Kryo serialization failing and has since been fixed.\n4. Evan Sparks: Hmm... the issue was that Kryo worked well on small objects but when those same objects got bigger, it crashed. The JIRA you link to looks more like classpath issues with different versions of a custom serializer. This seems more like a kryo configuration issue or something. (Similar to something like an akka max frame size). At any rate - I haven't tried to run this particular example in some time, so if someone wants to to try and recreate with an RDD[Tuple3(String,Float,Array[Float])] where all arrays are size 160,000 and there are 1.2m elements in the RDD, be my guest, otherwise feel free to close the ticket and we can keep our fingers crossed that this isn't in spark 1.0+.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.046724"}}
{"id": "91ab42492251738d38481ef2e7de4a25", "issue_key": "SPARK-756", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "JAR file appears corrupt to workers. - MLbase", "description": "Running Spark on EC2 with the default AMI and standard configuration. This bug affects the spark master branch (as of 2013-05-29) - notably, the same code does not crash under spark 0.7.0. I am creating a 55MB JAR file via the sbt-assembly plugin which is valid - code inside the JAR is callable and works fine if I rsync it to each worker and put that file on the SPARK_CLASSPATH. However, when I share the JAR by passing it to the SparkContext (e.g. : val sc = new SparkContext(\"spark://\"+ sys.env(\"SPARK_MASTER_IP\") + \":7077\", \"VisionClass\", \"/root/spark\", List(\"target/visionClass-assembly-1.0.jar\")) ) Worker tasks die - with a \"java.util.zip.ZipException\" when attempting to load the JAR file. The jar file in the spark worker temp directory seems complete (MD5 sums match what I have on the master) upon later inspection - so I suspect the Worker is attempting to open the file before it has been fully flushed to disk.", "reporter": "Evan Sparks", "assignee": null, "created": "2013-05-31T16:06:19.000+0000", "updated": "2015-02-26T11:30:59.000+0000", "resolved": "2015-02-26T11:30:59.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Sean R. Owen", "body": "Given the version and \"MLbase\", I'm guessing this is obsolete?", "created": "2015-02-17T18:53:58.542+0000"}], "num_comments": 1, "text": "Issue: SPARK-756\nSummary: JAR file appears corrupt to workers. - MLbase\nDescription: Running Spark on EC2 with the default AMI and standard configuration. This bug affects the spark master branch (as of 2013-05-29) - notably, the same code does not crash under spark 0.7.0. I am creating a 55MB JAR file via the sbt-assembly plugin which is valid - code inside the JAR is callable and works fine if I rsync it to each worker and put that file on the SPARK_CLASSPATH. However, when I share the JAR by passing it to the SparkContext (e.g. : val sc = new SparkContext(\"spark://\"+ sys.env(\"SPARK_MASTER_IP\") + \":7077\", \"VisionClass\", \"/root/spark\", List(\"target/visionClass-assembly-1.0.jar\")) ) Worker tasks die - with a \"java.util.zip.ZipException\" when attempting to load the JAR file. The jar file in the spark worker temp directory seems complete (MD5 sums match what I have on the master) upon later inspection - so I suspect the Worker is attempting to open the file before it has been fully flushed to disk.\n\nComments (1):\n1. Sean R. Owen: Given the version and \"MLbase\", I'm guessing this is obsolete?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.046724"}}
{"id": "f7c3b68d28c749a2491959309d84f750", "issue_key": "SPARK-757", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Deserialization Exception partway into long running job with Netty - MLbase", "description": "Using Netty for communication I see some derserialization errors that are crashing my job about 30% of the way through an iterative 10-step job. Happens reliably around the same point of the job after multiple attempts. Logs on master and a couple of affected workers attached per request from Shivaram. 13/05/31 23:19:12 INFO cluster.TaskSetManager: Serialized task 11.0:454 as 3414 bytes in 0 ms 13/05/31 23:19:14 INFO cluster.TaskSetManager: Finished TID 11344 in 55289 ms (progress: 312/1000) 13/05/31 23:19:14 INFO scheduler.DAGScheduler: Completed ResultTask(11, 344) 13/05/31 23:19:14 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_11,runningTasks:143 13/05/31 23:19:14 INFO cluster.TaskSetManager: Starting task 11.0:455 as TID 11455 on slave 8: ip-10-60-217-218.ec2.internal:56262 (NODE_LOCAL) 13/05/31 23:19:14 INFO cluster.TaskSetManager: Serialized task 11.0:455 as 3414 bytes in 0 ms 13/05/31 23:19:17 INFO cluster.TaskSetManager: Lost TID 11412 (task 11.0:412) 13/05/31 23:19:17 INFO cluster.TaskSetManager: Loss was due to java.io.EOFException java.io.EOFException at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322) at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2791) at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:798) at java.io.ObjectInputStream.<init>(ObjectInputStream.java:298) at spark.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:18) at spark.JavaDeserializationStream.<init>(JavaSerializer.scala:18) at spark.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:53) at spark.storage.BlockManager.dataDeserialize(BlockManager.scala:925) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:318) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:239) at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:9) at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451) at spark.Aggregator.combineCombinersByKey(Aggregator.scala:33) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:19) at spark.RDD.computeOrReadCheckpoint(RDD.scala:220) at spark.RDD.iterator(RDD.scala:209) at spark.scheduler.ResultTask.run(ResultTask.scala:84) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679)", "reporter": "Evan Sparks", "assignee": "Shivaram Venkataraman", "created": "2013-05-31T16:52:50.000+0000", "updated": "2015-02-17T19:19:09.000+0000", "resolved": "2015-02-17T19:19:09.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Shivaram Venkataraman", "body": "I found a couple of things which might explain the errors. It'll be great if you can use https://github.com/shivaram/spark/tree/netty-dbg and let me know if it fixes things. Some comments on changes I made: 1. I think the interrupted exceptions are a red herring. They are thrown as we right now have a race between the iterator finishing and the copiers finishing. I removed the call to stop copiers as the the threads will exit automatically when the iterator completes. 2. The connect timeout by default is 30 seconds. I am not sure why we should hit this timeout, but this could happen due to say a long GC pause or some other reason like that. I added a flag \"spark.shuffle.netty.connect.timeout\" which is by default set to 60 seconds -- It'll be great if you can see if setting this to say 2 minutes makes any difference. 3. When we do get an exception, I fixed the failure handling code to insert a failed fetch result. This should avoid the data deserialization errors and this also print which fetch from which machine failed.", "created": "2013-05-31T23:33:37.470+0000"}, {"author": "Evan Sparks", "body": "Thanks for looking into this Shivaram - I'm running the test now and so far it seems to be working well. At least I'm getting significantly further than I was before. Will let you know whether it finishes later on.", "created": "2013-06-01T22:31:50.900+0000"}, {"author": "Evan Sparks", "body": "Unfortunately it's still crashing in what looks like a similar part of the codebase - this time it gets ~60% through execution before dying: 13/06/02 05:59:58 INFO cluster.TaskSetManager: Finished TID 17756 in 10724 ms (progress: 689/1000) 13/06/02 05:59:58 INFO scheduler.DAGScheduler: Completed ResultTask(17, 756) 13/06/02 05:59:58 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_17,runningTasks:151 13/06/02 05:59:58 INFO cluster.TaskSetManager: Starting task 17.0:840 as TID 17840 on slave 0: ip-10-60-217-218.ec2.internal:44833 (NODE_LOCAL) 13/06/02 05:59:58 INFO cluster.TaskSetManager: Serialized task 17.0:840 as 3413 bytes in 0 ms 13/06/02 06:00:01 INFO cluster.TaskSetManager: Lost TID 17838 (task 17.0:838) 13/06/02 06:00:01 INFO cluster.TaskSetManager: Loss was due to java.io.EOFException java.io.EOFException at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322) at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2791) at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:798) at java.io.ObjectInputStream.<init>(ObjectInputStream.java:298) at spark.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:18) at spark.JavaDeserializationStream.<init>(JavaSerializer.scala:18) at spark.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:53) at spark.storage.BlockManager.dataDeserialize(BlockManager.scala:940) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:318) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:239) at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:9) at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451) at spark.Aggregator.combineCombinersByKey(Aggregator.scala:33) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:19) at spark.RDD.computeOrReadCheckpoint(RDD.scala:221) at spark.RDD.iterator(RDD.scala:210) at spark.scheduler.ResultTask.run(ResultTask.scala:84) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/06/02 06:00:01 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_17,runningTasks:151 :", "created": "2013-06-01T23:39:00.243+0000"}, {"author": "Shivaram Venkataraman", "body": "If you have the logs from the slave for Task 17838, could you attach them as well ?", "created": "2013-06-01T23:51:17.283+0000"}, {"author": "Evan Sparks", "body": "Attached as \"newworklog.log\"", "created": "2013-06-02T00:48:49.390+0000"}, {"author": "Shivaram Venkataraman", "body": "Hmm -- Are you sure that this is running with the patch ? (The InterruptedException in newworklog.log has FileClient.waitForClose at FileClient.java:60 which was moved in the patch). Just to clarify the changes I made are in a branch netty-dbg in http://github.com/shivaram/spark . I can also post a diff if that would be easier.", "created": "2013-06-02T09:55:20.119+0000"}, {"author": "Evan Sparks", "body": "You're right - I had negelected to select the correct branch in that repo (was running on that repository's master). I've rebuilt everything and am running against it now, but my first attempt started hanging pretty early in the job - so I've killed it and am running again. Will let you know", "created": "2013-06-02T12:35:11.004+0000"}, {"author": "Evan Sparks", "body": "That run failed with the following: 13/06/02 20:26:21 INFO cluster.TaskSetManager: Serialized task 4.1:20 as 3064 bytes in 0 ms 13/06/02 20:26:21 INFO cluster.TaskSetManager: Lost TID 15697 (task 4.1:1) 13/06/02 20:26:21 INFO cluster.TaskSetManager: Loss was due to java.lang.Exception java.lang.Exception: File for block shuffle_1_20_0 already exists on disk: /mnt2/spark/spark-local-20130602192855-c9b6/21/shuffle_1_20_0 at spark.storage.DiskStore.spark$storage$DiskStore$$createFile(DiskStore.scala:200) at spark.storage.DiskStore$DiskBlockObjectWriter.<init>(DiskStore.scala:31) at spark.storage.DiskStore.getBlockWriter(DiskStore.scala:99) at spark.storage.BlockManager.getDiskBlockWriter(BlockManager.scala:516) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:27) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:25) at scala.Array$.tabulate(Array.scala:303) at spark.storage.ShuffleBlockManager$$anon$1.acquireWriters(ShuffleBlockManager.scala:25) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:141) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:76) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/06/02 20:26:21 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_4,runningTasks:12 13/06/02 20:26:21 INFO cluster.TaskSetManager: Starting task 4.1:1 as TID 15699 on slave 5: ip-10-218-103-90.ec2.internal:53151 (NODE_LOCAL) 13/06/02 20:26:21 INFO cluster.TaskSetManager: Serialized task 4.1:1 as 3064 bytes in 1 ms 13/06/02 20:26:21 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_4,runningTasks:13 On the slave - I see things like this: 13/06/02 20:26:21 ERROR executor.Executor: Exception in task ID 15697 java.lang.Exception: File for block shuffle_1_20_0 already exists on disk: /mnt2/spark/spark-local-20130602192855-c9b6/21/shuffle_1_20_0 at spark.storage.DiskStore.spark$storage$DiskStore$$createFile(DiskStore.scala:200) at spark.storage.DiskStore$DiskBlockObjectWriter.<init>(DiskStore.scala:31) at spark.storage.DiskStore.getBlockWriter(DiskStore.scala:99) at spark.storage.BlockManager.getDiskBlockWriter(BlockManager.scala:516) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:27) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:25) at scala.Array$.tabulate(Array.scala:303) at spark.storage.ShuffleBlockManager$$anon$1.acquireWriters(ShuffleBlockManager.scala:25) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:141) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:76) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/06/02 20:26:21 INFO executor.Executor: Its generation is 17 Also, another java.io.EOFException exists on the worker but it's during the deserialization phase of a different task. Anyway - i've attached the master log and the worker log. (newerlogs.tgz)", "created": "2013-06-02T18:56:04.807+0000"}, {"author": "Evan Sparks", "body": "Logs from run on 2013-06-02 duplicate shuffle file issue.", "created": "2013-06-02T18:57:03.890+0000"}, {"author": "Shivaram Venkataraman", "body": "Ah this is unfortunate, but we have hit another bug while trying to debug the netty stuff. The first problem here came from: 13/06/02 20:26:15 INFO cluster.TaskSetManager: Lost TID 15170 (task 15.0:170) 13/06/02 20:26:15 INFO cluster.TaskSetManager: Loss was due to fetch failure from BlockManagerId(5, ip-10-218-103-90.ec2.internal, 40501, 55810) 13/06/02 20:26:15 INFO cluster.ClusterScheduler: Remove TaskSet 15.0 from pool 13/06/02 20:26:15 INFO scheduler.DAGScheduler: Marking Stage 15 (reduceByKey at MLogisticRegression.scala:102) for resubmision due to a fetch failure 13/06/02 20:26:15 INFO scheduler.DAGScheduler: The failed fetch was from Stage 16 (reduceByKey at MLogisticRegression.scala:102); marking it for resubmission 13/06/02 20:26:15 INFO scheduler.DAGScheduler: Executor lost: 5 (generation 8) 13/06/02 20:26:15 INFO storage.BlockManagerMasterActor: Trying to remove executor 5 from BlockManagerMaster. Do you still have logs from this machine ip-10-35-37-143.ec2.internal:60279 ? It would be interesting to find out why it was marked as failed or rather why the fetch failed. I'll try to fix the bug you did run into soon.", "created": "2013-06-03T10:32:04.723+0000"}, {"author": "Evan Sparks", "body": "yes - relevant stack trace is here: 13/06/02 20:25:54 INFO storage.BlockFetcherIterator$NettyBlockFetcherIterator: maxBytesInFlight: 50331648, minRequest: 10066329 13/06/02 20:25:54 INFO storage.BlockFetcherIterator$NettyBlockFetcherIterator: Getting 1000 non-zero-bytes blocks out of 1000 blocks 13/06/02 20:25:54 INFO storage.BlockFetcherIterator$NettyBlockFetcherIterator: Started 13 remote gets in 2 ms 13/06/02 20:26:14 ERROR netty.ShuffleCopier: Shuffle copy of block shuffle_7_732_170 from ip-10-218-103-90.ec2.internal:55810 failed io.netty.channel.ChannelException: java.net.ConnectException: Connection timed out: ip-10-218-103-90.ec2.internal/10.218.103.90:55810 at io.netty.channel.DefaultChannelPromise.rethrowIfFailed(DefaultChannelPromise.java:200) at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:175) at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:32) at spark.network.netty.FileClient.connect(FileClient.java:40) at spark.network.netty.ShuffleCopier.getBlock(ShuffleCopier.scala:25) at spark.network.netty.ShuffleCopier.getBlock(ShuffleCopier.scala:41) at spark.network.netty.ShuffleCopier$$anonfun$getBlocks$2.apply(ShuffleCopier.scala:49) at spark.network.netty.ShuffleCopier$$anonfun$getBlocks$2.apply(ShuffleCopier.scala:48) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.network.netty.ShuffleCopier.getBlocks(ShuffleCopier.scala:48) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.sendRequest(BlockFetcherIterator.scala:287) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$startCopiers$1$$anon$1.run(BlockFetcherIterator.scala:255) Caused by: java.net.ConnectException: Connection timed out: ip-10-218-103-90.ec2.internal/10.218.103.90:55810 at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:327) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:193) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:180) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:384) at java.net.Socket.connect(Socket.java:546) at io.netty.channel.socket.oio.OioSocketChannel.doConnect(OioSocketChannel.java:199) at io.netty.channel.oio.AbstractOioChannel$DefaultOioUnsafe.connect(AbstractOioChannel.java:67) at io.netty.channel.DefaultChannelPipeline$HeadHandler.connect(DefaultChannelPipeline.java:1246) at io.netty.channel.DefaultChannelHandlerContext.invokeConnect0(DefaultChannelHandlerContext.java:1109) at io.netty.channel.DefaultChannelHandlerContext.invokeConnect(DefaultChannelHandlerContext.java:1094) at io.netty.channel.DefaultChannelHandlerContext.connect(DefaultChannelHandlerContext.java:1087) at io.netty.channel.ChannelOperationHandlerAdapter.connect(ChannelOperationHandlerAdapter.java:43) at io.netty.channel.DefaultChannelHandlerContext.invokeConnect0(DefaultChannelHandlerContext.java:1109) at io.netty.channel.DefaultChannelHandlerContext.access$2200(DefaultChannelHandlerContext.java:38) at io.netty.channel.DefaultChannelHandlerContext$12.run(DefaultChannelHandlerContext.java:1099) at io.netty.channel.oio.OioEventLoop.run(OioEventLoop.java:62) at io.netty.channel.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:110) at java.lang.Thread.run(Thread.java:679) 13/06/02 20:26:15 INFO executor.StandaloneExecutorBackend: Got assigned task 15686 13/06/02 20:26:15 INFO executor.Executor: Running task ID 15686 13/06/02 20:26:15 INFO executor.Executor: Its generation is 17 13/06/02 20:26:15 INFO spark.MapOutputTracker: Updating generation to 17 and clearing cache 13/06/02 20:26:20 INFO executor.Executor: Serialized size of result for 15652 is 584 13/06/02 20:26:20 INFO executor.Executor: Finished task ID 15652 13/06/02 20:26:20 INFO executor.StandaloneExecutorBackend: Got assigned task 15693 13/06/02 20:26:20 INFO executor.Executor: Running task ID 15693 13/06/02 20:26:20 INFO executor.Executor: Its generation is 17 13/06/02 20:26:22 ERROR executor.StandaloneExecutorBackend: Driver terminated or disconnected! Shutting down. 13/06/02 20:26:22 WARN rdd.HadoopRDD: Exception in RecordReader.close() java.io.IOException: Filesystem closed at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:264) at org.apache.hadoop.hdfs.DFSClient.access$1100(DFSClient.java:74) at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.close(DFSClient.java:2135) at java.io.FilterInputStream.close(FilterInputStream.java:172) at org.apache.hadoop.util.LineReader.close(LineReader.java:83)", "created": "2013-06-03T11:16:18.523+0000"}, {"author": "Shivaram Venkataraman", "body": "Thanks a lot for sending the log. One last request -- If you have access, can you get logs from ip-10-218-103-90.ec2.internal around 13/06/02 20:26:14 ? It'll be interesting to know what causes ConnectionTimeoutException even when we have a 1 minute timeout. My suspicion is that the machine was busy with GC, but there might be other problems. Just to summarize, I can see a set of problems here which we'll need to fix: a. Machines get busy enough that connections timeout (This could also be because we have too many open connections ?) b. The executor wrongly removes/terminates machines -- This is mostly because of missed heart beats I guess and again points to machines being overloaded. c. I think re-executing a shuffle map task when a executor comes back leads to an exception for file already exists.", "created": "2013-06-03T11:30:49.016+0000"}, {"author": "Shivaram Venkataraman", "body": "Few flags that might help a. spark.storage.blockManagerSlaveTimeoutMs - The default value is 15000, lets try to set this to say 5 minutes = 300000 b. Worst case, you can try to set spark.test.disableBlockManagerHeartBeat to true c. spark.shuffle.netty.connect.timeout - the default is 60000, lets try to set this to say 5 minutes as well = 300000 d. spark.shuffle.copier.threads - the default is 6, lets try to set this to say 2 I'll also ping this issue once I fix the file already exists bug", "created": "2013-06-03T11:59:16.967+0000"}, {"author": "Shivaram Venkataraman", "body": "I also pushed a fix to the file already exists issue to the netty-dbg branch.", "created": "2013-06-03T12:30:33.393+0000"}, {"author": "Evan Sparks", "body": "Alright - I've run this on my 10% sample - and that works again (although a little slower than I'd expect) - I'm now giving it a go on the full dataset to see if it can get through it.", "created": "2013-06-03T21:26:04.600+0000"}, {"author": "Evan Sparks", "body": "Per discussion with Shivaram earlier, spark logs (master+workers) for my latest runs.", "created": "2013-06-05T21:40:34.727+0000"}, {"author": "Evan Sparks", "body": "And the all-important job log to go with those spark logs.", "created": "2013-06-05T21:41:49.798+0000"}, {"author": "Evan Sparks", "body": "Fixing bad master log.", "created": "2013-06-05T22:10:56.564+0000"}, {"author": "Sean R. Owen", "body": "Is this resolved now, or obsolete?", "created": "2015-02-17T18:54:35.039+0000"}, {"author": "Shivaram Venkataraman", "body": "Since the shuffle implementation has changed recently I think this can be marked as obsolete", "created": "2015-02-17T19:05:16.769+0000"}], "num_comments": 20, "text": "Issue: SPARK-757\nSummary: Deserialization Exception partway into long running job with Netty - MLbase\nDescription: Using Netty for communication I see some derserialization errors that are crashing my job about 30% of the way through an iterative 10-step job. Happens reliably around the same point of the job after multiple attempts. Logs on master and a couple of affected workers attached per request from Shivaram. 13/05/31 23:19:12 INFO cluster.TaskSetManager: Serialized task 11.0:454 as 3414 bytes in 0 ms 13/05/31 23:19:14 INFO cluster.TaskSetManager: Finished TID 11344 in 55289 ms (progress: 312/1000) 13/05/31 23:19:14 INFO scheduler.DAGScheduler: Completed ResultTask(11, 344) 13/05/31 23:19:14 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_11,runningTasks:143 13/05/31 23:19:14 INFO cluster.TaskSetManager: Starting task 11.0:455 as TID 11455 on slave 8: ip-10-60-217-218.ec2.internal:56262 (NODE_LOCAL) 13/05/31 23:19:14 INFO cluster.TaskSetManager: Serialized task 11.0:455 as 3414 bytes in 0 ms 13/05/31 23:19:17 INFO cluster.TaskSetManager: Lost TID 11412 (task 11.0:412) 13/05/31 23:19:17 INFO cluster.TaskSetManager: Loss was due to java.io.EOFException java.io.EOFException at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322) at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2791) at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:798) at java.io.ObjectInputStream.<init>(ObjectInputStream.java:298) at spark.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:18) at spark.JavaDeserializationStream.<init>(JavaSerializer.scala:18) at spark.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:53) at spark.storage.BlockManager.dataDeserialize(BlockManager.scala:925) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:318) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:239) at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:9) at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451) at spark.Aggregator.combineCombinersByKey(Aggregator.scala:33) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:19) at spark.RDD.computeOrReadCheckpoint(RDD.scala:220) at spark.RDD.iterator(RDD.scala:209) at spark.scheduler.ResultTask.run(ResultTask.scala:84) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679)\n\nComments (20):\n1. Shivaram Venkataraman: I found a couple of things which might explain the errors. It'll be great if you can use https://github.com/shivaram/spark/tree/netty-dbg and let me know if it fixes things. Some comments on changes I made: 1. I think the interrupted exceptions are a red herring. They are thrown as we right now have a race between the iterator finishing and the copiers finishing. I removed the call to stop copiers as the the threads will exit automatically when the iterator completes. 2. The connect timeout by default is 30 seconds. I am not sure why we should hit this timeout, but this could happen due to say a long GC pause or some other reason like that. I added a flag \"spark.shuffle.netty.connect.timeout\" which is by default set to 60 seconds -- It'll be great if you can see if setting this to say 2 minutes makes any difference. 3. When we do get an exception, I fixed the failure handling code to insert a failed fetch result. This should avoid the data deserialization errors and this also print which fetch from which machine failed.\n2. Evan Sparks: Thanks for looking into this Shivaram - I'm running the test now and so far it seems to be working well. At least I'm getting significantly further than I was before. Will let you know whether it finishes later on.\n3. Evan Sparks: Unfortunately it's still crashing in what looks like a similar part of the codebase - this time it gets ~60% through execution before dying: 13/06/02 05:59:58 INFO cluster.TaskSetManager: Finished TID 17756 in 10724 ms (progress: 689/1000) 13/06/02 05:59:58 INFO scheduler.DAGScheduler: Completed ResultTask(17, 756) 13/06/02 05:59:58 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_17,runningTasks:151 13/06/02 05:59:58 INFO cluster.TaskSetManager: Starting task 17.0:840 as TID 17840 on slave 0: ip-10-60-217-218.ec2.internal:44833 (NODE_LOCAL) 13/06/02 05:59:58 INFO cluster.TaskSetManager: Serialized task 17.0:840 as 3413 bytes in 0 ms 13/06/02 06:00:01 INFO cluster.TaskSetManager: Lost TID 17838 (task 17.0:838) 13/06/02 06:00:01 INFO cluster.TaskSetManager: Loss was due to java.io.EOFException java.io.EOFException at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322) at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2791) at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:798) at java.io.ObjectInputStream.<init>(ObjectInputStream.java:298) at spark.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:18) at spark.JavaDeserializationStream.<init>(JavaSerializer.scala:18) at spark.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:53) at spark.storage.BlockManager.dataDeserialize(BlockManager.scala:940) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator$$anonfun$5.apply(BlockFetcherIterator.scala:279) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:318) at spark.storage.BlockFetcherIterator$NettyBlockFetcherIterator.next(BlockFetcherIterator.scala:239) at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:9) at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451) at spark.Aggregator.combineCombinersByKey(Aggregator.scala:33) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:72) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:19) at spark.RDD.computeOrReadCheckpoint(RDD.scala:221) at spark.RDD.iterator(RDD.scala:210) at spark.scheduler.ResultTask.run(ResultTask.scala:84) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/06/02 06:00:01 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_17,runningTasks:151 :\n4. Shivaram Venkataraman: If you have the logs from the slave for Task 17838, could you attach them as well ?\n5. Evan Sparks: Attached as \"newworklog.log\"\n6. Shivaram Venkataraman: Hmm -- Are you sure that this is running with the patch ? (The InterruptedException in newworklog.log has FileClient.waitForClose at FileClient.java:60 which was moved in the patch). Just to clarify the changes I made are in a branch netty-dbg in http://github.com/shivaram/spark . I can also post a diff if that would be easier.\n7. Evan Sparks: You're right - I had negelected to select the correct branch in that repo (was running on that repository's master). I've rebuilt everything and am running against it now, but my first attempt started hanging pretty early in the job - so I've killed it and am running again. Will let you know\n8. Evan Sparks: That run failed with the following: 13/06/02 20:26:21 INFO cluster.TaskSetManager: Serialized task 4.1:20 as 3064 bytes in 0 ms 13/06/02 20:26:21 INFO cluster.TaskSetManager: Lost TID 15697 (task 4.1:1) 13/06/02 20:26:21 INFO cluster.TaskSetManager: Loss was due to java.lang.Exception java.lang.Exception: File for block shuffle_1_20_0 already exists on disk: /mnt2/spark/spark-local-20130602192855-c9b6/21/shuffle_1_20_0 at spark.storage.DiskStore.spark$storage$DiskStore$$createFile(DiskStore.scala:200) at spark.storage.DiskStore$DiskBlockObjectWriter.<init>(DiskStore.scala:31) at spark.storage.DiskStore.getBlockWriter(DiskStore.scala:99) at spark.storage.BlockManager.getDiskBlockWriter(BlockManager.scala:516) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:27) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:25) at scala.Array$.tabulate(Array.scala:303) at spark.storage.ShuffleBlockManager$$anon$1.acquireWriters(ShuffleBlockManager.scala:25) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:141) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:76) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/06/02 20:26:21 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_4,runningTasks:12 13/06/02 20:26:21 INFO cluster.TaskSetManager: Starting task 4.1:1 as TID 15699 on slave 5: ip-10-218-103-90.ec2.internal:53151 (NODE_LOCAL) 13/06/02 20:26:21 INFO cluster.TaskSetManager: Serialized task 4.1:1 as 3064 bytes in 1 ms 13/06/02 20:26:21 INFO cluster.ClusterScheduler: parentName:,name:TaskSet_4,runningTasks:13 On the slave - I see things like this: 13/06/02 20:26:21 ERROR executor.Executor: Exception in task ID 15697 java.lang.Exception: File for block shuffle_1_20_0 already exists on disk: /mnt2/spark/spark-local-20130602192855-c9b6/21/shuffle_1_20_0 at spark.storage.DiskStore.spark$storage$DiskStore$$createFile(DiskStore.scala:200) at spark.storage.DiskStore$DiskBlockObjectWriter.<init>(DiskStore.scala:31) at spark.storage.DiskStore.getBlockWriter(DiskStore.scala:99) at spark.storage.BlockManager.getDiskBlockWriter(BlockManager.scala:516) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:27) at spark.storage.ShuffleBlockManager$$anon$1$$anonfun$1.apply(ShuffleBlockManager.scala:25) at scala.Array$.tabulate(Array.scala:303) at spark.storage.ShuffleBlockManager$$anon$1.acquireWriters(ShuffleBlockManager.scala:25) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:141) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:76) at spark.executor.Executor$TaskRunner.run(Executor.scala:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:679) 13/06/02 20:26:21 INFO executor.Executor: Its generation is 17 Also, another java.io.EOFException exists on the worker but it's during the deserialization phase of a different task. Anyway - i've attached the master log and the worker log. (newerlogs.tgz)\n9. Evan Sparks: Logs from run on 2013-06-02 duplicate shuffle file issue.\n10. Shivaram Venkataraman: Ah this is unfortunate, but we have hit another bug while trying to debug the netty stuff. The first problem here came from: 13/06/02 20:26:15 INFO cluster.TaskSetManager: Lost TID 15170 (task 15.0:170) 13/06/02 20:26:15 INFO cluster.TaskSetManager: Loss was due to fetch failure from BlockManagerId(5, ip-10-218-103-90.ec2.internal, 40501, 55810) 13/06/02 20:26:15 INFO cluster.ClusterScheduler: Remove TaskSet 15.0 from pool 13/06/02 20:26:15 INFO scheduler.DAGScheduler: Marking Stage 15 (reduceByKey at MLogisticRegression.scala:102) for resubmision due to a fetch failure 13/06/02 20:26:15 INFO scheduler.DAGScheduler: The failed fetch was from Stage 16 (reduceByKey at MLogisticRegression.scala:102); marking it for resubmission 13/06/02 20:26:15 INFO scheduler.DAGScheduler: Executor lost: 5 (generation 8) 13/06/02 20:26:15 INFO storage.BlockManagerMasterActor: Trying to remove executor 5 from BlockManagerMaster. Do you still have logs from this machine ip-10-35-37-143.ec2.internal:60279 ? It would be interesting to find out why it was marked as failed or rather why the fetch failed. I'll try to fix the bug you did run into soon.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.048729"}}
{"id": "5b02d3bca83b95cdf8e448d022468a47", "issue_key": "SPARK-758", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "ALS scalability for MLbase", "description": "We have implemented distributed Alternating Least Squares (called \"Join-ALS\") with Spark for the MLbase project (this algorithm performs matrix factorization / collaborative filtering). We've gone through effort to optimize the code, perform narrow joins, and minimize communication. Nonetheless, we are facing three issues: 1) it runs significantly slower than similar methods on other platforms (i.e. GraphLab) 2) it scales very poorly when increasing the rank parameter or dataset size 3) with large rank or dataset sizes, it sometimes runs into memory errors The following repo (https://github.com/gingsmith/ALS_debug.git) contains * the code (Join_ALS.scala), * scripts that illustrate the scaling and memory issues, * a README explaining how to run the scripts and where to download data, * a script to setup an EC2 cluster with the codebase, * breakdown of expected computing and communication costs * log files from previous runs. Detailed description of scripts: 1) rankscaling.sh: this script runs Join-ALS on a dataset using four different values for the rank parameter. We theoretically expect ALS computation and communication to scale quadratically with the rank parameter. Running ALS on Spark with the netflix dataset (2GB) on a 4-node cluster we see: - rank=10: the task completes in 410s - rank=20: the task completes in 4,353s - rank=30: the task completes in 11,956s - rank=100: the task runs for 18,106s and then runs into the error: java.lang.OutOfMemoryError: GC overhead limit exceeded and then java.io.FileNotFoundException: /mnt/spark/spark-local-20130531030438-7c49/34/shuffle_22_0_0 In contrast, using GraphLab, ALS runs in 93s on a 4-node cluster with rank=10, and 194s on a 4-node cluster with rank=20. 2) datascaling.sh: this script runs Join-ALS on a dataset, and then on a dataset 4x larger. We theoretically expect ALS computation and communication to scale linearly with the dimensions of the matrix. Running ALS on Spark with the netflix dataset (2GB), we complete the initial task in 350s using 4 nodes. When using the 4x larger dataset, the task completes after 11,211s.", "reporter": "Ginger Smith", "assignee": null, "created": "2013-05-31T17:22:34.000+0000", "updated": "2013-06-07T22:25:21.000+0000", "resolved": "2013-06-02T09:25:04.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Hi Ginger, This is an interesting report - we should definitely try to pinpoint what is going on and see if there are some simple optimizations in the code or in Spark which would improve performance. That said, it would be good to migrate this to a mailing list discussion instead of a JIRA. The purpose of JIRA is to do fine-grained issue tracking, not to report high-level observations. The place to start with an issue like this is the mailing list or discussing with people in the project. Then, if specific actionable improvements come out of those discussions, we can file JIRA issues to fix them. With that in mind, would you mind sending this to the spark developers or user list to initiate a discussion there? I may close this issue in the next day or so here in order to help move the discussion. - Patrick", "created": "2013-06-01T06:48:10.536+0000"}, {"author": "Josh Rosen", "body": "I think that `run-main` and your environment configuration could be part of your performance problems. It looks like you never `source ~/spark/conf/spark-env.sh` or otherwise set SPARK_MEM, so your program uses executors with only 512 megabytes of memory. Looking through your logs, most of your task failures were due to OutOfMemoryExceptions due to GC overhead limits being exceeded. I think we could do a better job of clarifying this in our documentation. The current documentation is confusing: our own AMP Camp scripts use run-main, even though that won't necessarily configure things correctly. Some folks on the mailing list have discouraged users from using `run` to run their own Spark jobs. I don't understand why we don't promote the use of the `run` script, since I've always thought that it should behave roughly like Hadoop's `hadoop` script: users will declare Spark as a 'provided' dependency in their build systems, then invoke `run` to run their code with Spark's jars addd to the classpath. By marking Spark as provided dependency, we can avoid user programs breaking when Spark is updated to a new point release (which is API-compatible from user-code, but not necessarily RPC or binary compatible). Whether to promote `run` is a separate discussion from this issue, so I'd be glad to take it to `spark-developers` if anyone is interested.", "created": "2013-06-01T17:47:02.806+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I agree that this would better be discussed by email since it's not a specific bug. In general though, one issue in the code here is that it represents the data as pairs of ((Int, Int), Double) or (Int, Array[Double]) objects. These have a lot of overhead in terms of both memory usage and time. For this type of numerical computation, it would be better to split the matrix into blocks and have RDDs of objects that represent those blocks. For example, divide the matrix into blocks of 64x64, and have an entry for the ratings in each such block (the value would probably be two int arrays and one double array to represent the sparse data in there); and divide the users into chunks of 64. Please follow up by email if you want to try this -- it would be cool to get this right and include it as an example.", "created": "2013-06-01T18:13:10.894+0000"}, {"author": "Josh Rosen", "body": "There may be a bug in how you co-partition the factors and ratings. Here's your original code:  // Initialize user and movie factors deterministically. Map partitions is used to // preserve the partitioning achieved when using reduceByKey. var users = ratingsByUser.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (uid, deg) => (uid, makeInitialFactor(uid)) } ).cache // Note I use the negative of the movie id to initialize movie factors var movies = ratingsByMovie.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (mid, deg) => (mid, makeInitialFactor(-mid)) } ).cache  The problem is that {{mapPartitions}} only preserves partitioning if you pass {{true}} to its optional {{preservesPartitioning}} argument. Replacing that code with  // Initialize user and movie factors deterministically. Map partitions is used to // preserve the partitioning achieved when using reduceByKey. var users = ratingsByUser.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (uid, deg) => (uid, makeInitialFactor(uid)) }, preservesPartitioning = true ).cache // Note I use the negative of the movie id to initialize movie factors var movies = ratingsByMovie.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (mid, deg) => (mid, makeInitialFactor(-mid)) }, preservesPartitioning = true ).cache  may improve performance.", "created": "2013-06-01T18:19:36.397+0000"}, {"author": "Patrick McFadin", "body": "These issues have migrated to an e-mail discussion, so I'm going to close.", "created": "2013-06-02T09:24:55.701+0000"}, {"author": "Nicholas Pentreath", "body": "Hi would you mind including me in this email discussion? Is it on the dev or user mailing list, since I can't seem to find it? I'd love to get involved since I've actually just finished a port of Mahout's ALS algorithms to Spark (well the implicit feedback one still needs some work on efficiency). It also uses Breeze for linear algebra but is more or less a straight port of the Mahout version. I would love to test it out on larger datasets and in cluster mode, and see the differences vs this implementation...", "created": "2013-06-07T02:01:09.968+0000"}, {"author": "Ginger Smith", "body": "Hi Nick, We've temporarily moved our discussions offline as we prepare for our MLbase release within a month. Our release will include all of the matrix factorization code (along with scripts to run on test data on ec2). We'll certainly let you know when we release the code, and it would be great to talk more and compare implementations at that point.", "created": "2013-06-07T22:25:21.210+0000"}], "num_comments": 7, "text": "Issue: SPARK-758\nSummary: ALS scalability for MLbase\nDescription: We have implemented distributed Alternating Least Squares (called \"Join-ALS\") with Spark for the MLbase project (this algorithm performs matrix factorization / collaborative filtering). We've gone through effort to optimize the code, perform narrow joins, and minimize communication. Nonetheless, we are facing three issues: 1) it runs significantly slower than similar methods on other platforms (i.e. GraphLab) 2) it scales very poorly when increasing the rank parameter or dataset size 3) with large rank or dataset sizes, it sometimes runs into memory errors The following repo (https://github.com/gingsmith/ALS_debug.git) contains * the code (Join_ALS.scala), * scripts that illustrate the scaling and memory issues, * a README explaining how to run the scripts and where to download data, * a script to setup an EC2 cluster with the codebase, * breakdown of expected computing and communication costs * log files from previous runs. Detailed description of scripts: 1) rankscaling.sh: this script runs Join-ALS on a dataset using four different values for the rank parameter. We theoretically expect ALS computation and communication to scale quadratically with the rank parameter. Running ALS on Spark with the netflix dataset (2GB) on a 4-node cluster we see: - rank=10: the task completes in 410s - rank=20: the task completes in 4,353s - rank=30: the task completes in 11,956s - rank=100: the task runs for 18,106s and then runs into the error: java.lang.OutOfMemoryError: GC overhead limit exceeded and then java.io.FileNotFoundException: /mnt/spark/spark-local-20130531030438-7c49/34/shuffle_22_0_0 In contrast, using GraphLab, ALS runs in 93s on a 4-node cluster with rank=10, and 194s on a 4-node cluster with rank=20. 2) datascaling.sh: this script runs Join-ALS on a dataset, and then on a dataset 4x larger. We theoretically expect ALS computation and communication to scale linearly with the dimensions of the matrix. Running ALS on Spark with the netflix dataset (2GB), we complete the initial task in 350s using 4 nodes. When using the 4x larger dataset, the task completes after 11,211s.\n\nComments (7):\n1. Patrick McFadin: Hi Ginger, This is an interesting report - we should definitely try to pinpoint what is going on and see if there are some simple optimizations in the code or in Spark which would improve performance. That said, it would be good to migrate this to a mailing list discussion instead of a JIRA. The purpose of JIRA is to do fine-grained issue tracking, not to report high-level observations. The place to start with an issue like this is the mailing list or discussing with people in the project. Then, if specific actionable improvements come out of those discussions, we can file JIRA issues to fix them. With that in mind, would you mind sending this to the spark developers or user list to initiate a discussion there? I may close this issue in the next day or so here in order to help move the discussion. - Patrick\n2. Josh Rosen: I think that `run-main` and your environment configuration could be part of your performance problems. It looks like you never `source ~/spark/conf/spark-env.sh` or otherwise set SPARK_MEM, so your program uses executors with only 512 megabytes of memory. Looking through your logs, most of your task failures were due to OutOfMemoryExceptions due to GC overhead limits being exceeded. I think we could do a better job of clarifying this in our documentation. The current documentation is confusing: our own AMP Camp scripts use run-main, even though that won't necessarily configure things correctly. Some folks on the mailing list have discouraged users from using `run` to run their own Spark jobs. I don't understand why we don't promote the use of the `run` script, since I've always thought that it should behave roughly like Hadoop's `hadoop` script: users will declare Spark as a 'provided' dependency in their build systems, then invoke `run` to run their code with Spark's jars addd to the classpath. By marking Spark as provided dependency, we can avoid user programs breaking when Spark is updated to a new point release (which is API-compatible from user-code, but not necessarily RPC or binary compatible). Whether to promote `run` is a separate discussion from this issue, so I'd be glad to take it to `spark-developers` if anyone is interested.\n3. Matei Alexandru Zaharia: I agree that this would better be discussed by email since it's not a specific bug. In general though, one issue in the code here is that it represents the data as pairs of ((Int, Int), Double) or (Int, Array[Double]) objects. These have a lot of overhead in terms of both memory usage and time. For this type of numerical computation, it would be better to split the matrix into blocks and have RDDs of objects that represent those blocks. For example, divide the matrix into blocks of 64x64, and have an entry for the ratings in each such block (the value would probably be two int arrays and one double array to represent the sparse data in there); and divide the users into chunks of 64. Please follow up by email if you want to try this -- it would be cool to get this right and include it as an example.\n4. Josh Rosen: There may be a bug in how you co-partition the factors and ratings. Here's your original code:  // Initialize user and movie factors deterministically. Map partitions is used to // preserve the partitioning achieved when using reduceByKey. var users = ratingsByUser.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (uid, deg) => (uid, makeInitialFactor(uid)) } ).cache // Note I use the negative of the movie id to initialize movie factors var movies = ratingsByMovie.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (mid, deg) => (mid, makeInitialFactor(-mid)) } ).cache  The problem is that {{mapPartitions}} only preserves partitioning if you pass {{true}} to its optional {{preservesPartitioning}} argument. Replacing that code with  // Initialize user and movie factors deterministically. Map partitions is used to // preserve the partitioning achieved when using reduceByKey. var users = ratingsByUser.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (uid, deg) => (uid, makeInitialFactor(uid)) }, preservesPartitioning = true ).cache // Note I use the negative of the movie id to initialize movie factors var movies = ratingsByMovie.mapValues(x => 1).reduceByKey(_ + _) .mapPartitions( _.map{ case (mid, deg) => (mid, makeInitialFactor(-mid)) }, preservesPartitioning = true ).cache  may improve performance.\n5. Patrick McFadin: These issues have migrated to an e-mail discussion, so I'm going to close.\n6. Nicholas Pentreath: Hi would you mind including me in this email discussion? Is it on the dev or user mailing list, since I can't seem to find it? I'd love to get involved since I've actually just finished a port of Mahout's ALS algorithms to Spark (well the implicit feedback one still needs some work on efficiency). It also uses Breeze for linear algebra but is more or less a straight port of the Mahout version. I would love to test it out on larger datasets and in cluster mode, and see the differences vs this implementation...\n7. Ginger Smith: Hi Nick, We've temporarily moved our discussions offline as we prepare for our MLbase release within a month. Our release will include all of the matrix factorization code (along with scripts to run on test data on ec2). We'll certainly let you know when we release the code, and it would be great to talk more and compare implementations at that point.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.048729"}}
{"id": "26e5f37c41158bf2214280bdf74978bd", "issue_key": "SPARK-759", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Change how we track AMI ids in the EC2 scripts", "description": "I think we should change how we track AMI ids in the EC2 scripts. I don't like the current approach of using a URL to track the latest AMI id for each major version number: 1. There's no versioning for the contents of these URLs or mechanism to look up old AMI ids. There's no repository or feed to watch to be notified when the AMI ids are updated. 2. Updating Spark to a point release can break things for some users. The Spark API is backwards-compatible in point releases, but user code that's linked against one release may not work when connecting to a cluster running a newer API-compatible version of Spark (it would work if users marked Spark as a 'provided' dependency and used something like Spark's `run` script to add the cluster's Spark JARs to the classpath). This message from spark-users illustrates both problems: https://groups.google.com/d/msg/spark-users/T-Ug8G03Ctk/qV-YfE6Ws8MJ Patrick has a pull request that moved the AMIs into the `spark-ec2` GitHub repository: https://github.com/shivaram/spark-ec2/pull/1. Can we extract this idea and apply it in the next release?", "reporter": "Josh Rosen", "assignee": null, "created": "2013-06-03T23:35:44.000+0000", "updated": "2013-08-26T10:08:36.000+0000", "resolved": "2013-08-26T10:08:36.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Patrick McFadin", "body": "Hey Josh, This is already the plan, as described (briefly) in SPARK-728 which is nested under SPARK-727. Are you proposing something substantially different than is what in that pull request? That's our current plan for now, I just need to push it through now that the release is out. Maybe we should nest this under SPARK-727 as well.", "created": "2013-06-04T14:01:18.290+0000"}, {"author": "Josh Rosen", "body": "I was going to argue that maybe AMI ids should be tied to Spark point releases, rather than entire branches (e.g. separate URLS for 0.7.0 and 0.7.1 AMIs instead of one 0.7 AMI that's used by all 0.7.x launch scripts). But I guess the issue of the 'latest' URL changing is maybe more of a communication / documentation issue: it would be fine to have that URL change so long as users are aware that it changes and informed that they should manually pass a specific AMI id if they want things to remain stable.", "created": "2013-06-04T14:37:41.068+0000"}, {"author": "Patrick McFadin", "body": "Yes I was going to have it at the granularity of individual releases. If you look, the new scripts have a version you pass to it e.g. v0.7.1 and what we're going to do is that fetches the binary release for that version and the correct Hadoop version. This is important since some people might still want to run e.g. 0.7.0 when 0.7.2 is the \"latest\". - Patrick", "created": "2013-06-04T14:41:27.891+0000"}], "num_comments": 3, "text": "Issue: SPARK-759\nSummary: Change how we track AMI ids in the EC2 scripts\nDescription: I think we should change how we track AMI ids in the EC2 scripts. I don't like the current approach of using a URL to track the latest AMI id for each major version number: 1. There's no versioning for the contents of these URLs or mechanism to look up old AMI ids. There's no repository or feed to watch to be notified when the AMI ids are updated. 2. Updating Spark to a point release can break things for some users. The Spark API is backwards-compatible in point releases, but user code that's linked against one release may not work when connecting to a cluster running a newer API-compatible version of Spark (it would work if users marked Spark as a 'provided' dependency and used something like Spark's `run` script to add the cluster's Spark JARs to the classpath). This message from spark-users illustrates both problems: https://groups.google.com/d/msg/spark-users/T-Ug8G03Ctk/qV-YfE6Ws8MJ Patrick has a pull request that moved the AMIs into the `spark-ec2` GitHub repository: https://github.com/shivaram/spark-ec2/pull/1. Can we extract this idea and apply it in the next release?\n\nComments (3):\n1. Patrick McFadin: Hey Josh, This is already the plan, as described (briefly) in SPARK-728 which is nested under SPARK-727. Are you proposing something substantially different than is what in that pull request? That's our current plan for now, I just need to push it through now that the release is out. Maybe we should nest this under SPARK-727 as well.\n2. Josh Rosen: I was going to argue that maybe AMI ids should be tied to Spark point releases, rather than entire branches (e.g. separate URLS for 0.7.0 and 0.7.1 AMIs instead of one 0.7 AMI that's used by all 0.7.x launch scripts). But I guess the issue of the 'latest' URL changing is maybe more of a communication / documentation issue: it would be fine to have that URL change so long as users are aware that it changes and informed that they should manually pass a specific AMI id if they want things to remain stable.\n3. Patrick McFadin: Yes I was going to have it at the granularity of individual releases. If you look, the new scripts have a version you pass to it e.g. v0.7.1 and what we're going to do is that fetches the binary release for that version and the correct Hadoop version. This is important since some people might still want to run e.g. 0.7.0 when 0.7.2 is the \"latest\". - Patrick", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.048729"}}
{"id": "9944b1cbb88f234cb1eca27ab36a0cf5", "issue_key": "SPARK-760", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Include a simple PageRank example in Spark", "description": "Lots of people ask about it. It would be nice to have one in Scala, Java and Python.", "reporter": "Matei Alexandru Zaharia", "assignee": "Chu Tong", "created": "2013-06-05T11:20:33.000+0000", "updated": "2013-08-12T21:27:28.000+0000", "resolved": "2013-08-12T21:27:28.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Nicholas Pentreath", "body": "Is anyone looking at this? I can try to take a crack at it (seems Spark examples are my thing :-)", "created": "2013-06-07T02:08:46.698+0000"}, {"author": "Chu Tong", "body": "Hi Nick, are you still working on this?", "created": "2013-07-28T23:41:58.446+0000"}, {"author": "Nicholas Pentreath", "body": "Hi, yes I still intend to to put something together, I just haven't had a chance just yet.", "created": "2013-07-30T22:32:35.645+0000"}, {"author": "Chu Tong", "body": "Why don't you do Scala and I do Java then?", "created": "2013-07-30T22:35:07.747+0000"}, {"author": "Nicholas Pentreath", "body": "Fair enough - looks like there is a quite simple version here in Scala that I will adapt: http://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-part-2-amp-camp-2012-standalone-programs.pdf Could also be used as a base for the Java version", "created": "2013-07-30T23:21:49.091+0000"}, {"author": "Chu Tong", "body": "thank you Nick.", "created": "2013-07-30T23:24:09.840+0000"}, {"author": "Nicholas Pentreath", "body": "Also Scalding has some PageRank examples that should be fairly straightforward to port over: https://github.com/twitter/scalding/tree/develop/scalding-core/src/main/scala/com/twitter/scalding/examples", "created": "2013-07-30T23:28:07.427+0000"}, {"author": "Chu Tong", "body": "Java version of PageRank is merged and I am working on the Python part.", "created": "2013-08-06T15:26:04.071+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Cool, a Python version would be great.", "created": "2013-08-06T17:07:29.979+0000"}, {"author": "Nicholas Pentreath", "body": "Added the Scala version (based on the Java version PR): https://github.com/mesos/spark/pull/789", "created": "2013-08-07T07:42:45.640+0000"}, {"author": "Chu Tong", "body": "Added Python version at: https://github.com/mesos/spark/pull/802", "created": "2013-08-10T23:24:39.106+0000"}], "num_comments": 11, "text": "Issue: SPARK-760\nSummary: Include a simple PageRank example in Spark\nDescription: Lots of people ask about it. It would be nice to have one in Scala, Java and Python.\n\nComments (11):\n1. Nicholas Pentreath: Is anyone looking at this? I can try to take a crack at it (seems Spark examples are my thing :-)\n2. Chu Tong: Hi Nick, are you still working on this?\n3. Nicholas Pentreath: Hi, yes I still intend to to put something together, I just haven't had a chance just yet.\n4. Chu Tong: Why don't you do Scala and I do Java then?\n5. Nicholas Pentreath: Fair enough - looks like there is a quite simple version here in Scala that I will adapt: http://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-part-2-amp-camp-2012-standalone-programs.pdf Could also be used as a base for the Java version\n6. Chu Tong: thank you Nick.\n7. Nicholas Pentreath: Also Scalding has some PageRank examples that should be fairly straightforward to port over: https://github.com/twitter/scalding/tree/develop/scalding-core/src/main/scala/com/twitter/scalding/examples\n8. Chu Tong: Java version of PageRank is merged and I am working on the Python part.\n9. Matei Alexandru Zaharia: Cool, a Python version would be great.\n10. Nicholas Pentreath: Added the Scala version (based on the Java version PR): https://github.com/mesos/spark/pull/789", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "0ab6c06996412a1aec43178c9e0dbbb7", "issue_key": "SPARK-761", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Print a nicer error message when incompatible Spark binaries try to talk", "description": "As a starter task, it would be good to audit the current behavior for different client <-> server pairs with respect to how exceptions occur.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-06-06T10:29:09.000+0000", "updated": "2019-05-21T04:15:18.000+0000", "resolved": "2019-05-21T04:15:18.000+0000", "labels": ["bulk-closed", "starter"], "components": ["Spark Core"], "comments": [{"author": "Nicholas Chammas", "body": "Not sure what component this falls under, or if this is still an issue. [~pwendell] / [~matei]?", "created": "2015-02-06T19:44:40.319+0000"}, {"author": "Sean R. Owen", "body": "I don't know how possible this is, as incompatibilities manifest in many and low-level ways.", "created": "2015-02-08T12:57:27.752+0000"}, {"author": "Patrick Wendell", "body": "I think the main thing to catch would be Akka. I.e. try connecting different versions and seeing what happens as an exploratory step. For instance, if akka has a standard exception which says you had an incompatible message type, we can wrap that and give an outer exception explaining that the spark version is likely wrong. So maybe we can see if someone wants to explore this a bit as a starter task.", "created": "2015-02-08T18:31:18.895+0000"}, {"author": "Andrew Ash", "body": "Another thing could be a basic check for version number mismatches. E.g. a warning log from both server and client could say: \"Version mismatch between server (1.2.0) and client (1.1.1); proceeding anyway\"", "created": "2015-02-08T18:44:55.160+0000"}, {"author": "Patrick Wendell", "body": "[~aash] right now we don't explicitly encode the spark version anywhere in the RPC. The best possible thing is to give an explicit version number like you said, but we don't have the plumbing to do that at the moment and IMO that's worth punting until we decide to standardize the RPC format.", "created": "2015-02-08T18:51:56.933+0000"}, {"author": "Harsh Gupta", "body": "Can I take up this as a starter task ? I am very much keen on contributing so this would get me going very well. [~matei] [~pwendell] [~aash]", "created": "2015-04-11T08:07:23.861+0000"}, {"author": "Sean R. Owen", "body": "[~harshg] Just go ahead, but, what do you propose to do? one of the issues here is it's not clear how to even detect this.", "created": "2015-04-11T11:52:25.400+0000"}, {"author": "Harsh Gupta", "body": "Isn't it supposed to be a check at the time of startup of application . i.e when your cluster is ready the client and servers should talk and check their compatibility ? Something like how zookeeper creates a quorum . [~srowen]", "created": "2015-04-11T12:02:42.336+0000"}, {"author": "Sean R. Owen", "body": "Yes, but, what would you check? The goal is not to forbid mismatched versions from talking -- we don't even have a version to check anyway -- but somehow detect when an error is due to this and print something nicer. I don't think it's possible in general; might be reliably possible in a few situations.", "created": "2015-04-11T12:05:15.210+0000"}, {"author": "Harsh Gupta", "body": "[~srowen] That makes sense. But is this an issue then ? What are the possibilities you can see?", "created": "2015-04-11T12:15:28.165+0000"}, {"author": "Andrew Ash", "body": "An implementation I can imagine would be to do an exchange of the Spark binary versions (e.g. 1.2.2, 1.3.1, 1.4.0, etc) on startup and log errors. The catch though is that Spark maintains forward compatibility between versions, so when used with the spark-submit script, a 1.2.2-compiled job jar will run on a 1.3.1 cluster. So we don't want to do a compatibility check on the Spark binary versions directly, but rather on the API that they speak, which is not versioned independently of the Spark binaries.", "created": "2015-04-11T16:28:44.023+0000"}], "num_comments": 11, "text": "Issue: SPARK-761\nSummary: Print a nicer error message when incompatible Spark binaries try to talk\nDescription: As a starter task, it would be good to audit the current behavior for different client <-> server pairs with respect to how exceptions occur.\n\nComments (11):\n1. Nicholas Chammas: Not sure what component this falls under, or if this is still an issue. [~pwendell] / [~matei]?\n2. Sean R. Owen: I don't know how possible this is, as incompatibilities manifest in many and low-level ways.\n3. Patrick Wendell: I think the main thing to catch would be Akka. I.e. try connecting different versions and seeing what happens as an exploratory step. For instance, if akka has a standard exception which says you had an incompatible message type, we can wrap that and give an outer exception explaining that the spark version is likely wrong. So maybe we can see if someone wants to explore this a bit as a starter task.\n4. Andrew Ash: Another thing could be a basic check for version number mismatches. E.g. a warning log from both server and client could say: \"Version mismatch between server (1.2.0) and client (1.1.1); proceeding anyway\"\n5. Patrick Wendell: [~aash] right now we don't explicitly encode the spark version anywhere in the RPC. The best possible thing is to give an explicit version number like you said, but we don't have the plumbing to do that at the moment and IMO that's worth punting until we decide to standardize the RPC format.\n6. Harsh Gupta: Can I take up this as a starter task ? I am very much keen on contributing so this would get me going very well. [~matei] [~pwendell] [~aash]\n7. Sean R. Owen: [~harshg] Just go ahead, but, what do you propose to do? one of the issues here is it's not clear how to even detect this.\n8. Harsh Gupta: Isn't it supposed to be a check at the time of startup of application . i.e when your cluster is ready the client and servers should talk and check their compatibility ? Something like how zookeeper creates a quorum . [~srowen]\n9. Sean R. Owen: Yes, but, what would you check? The goal is not to forbid mismatched versions from talking -- we don't even have a version to check anyway -- but somehow detect when an error is due to this and print something nicer. I don't think it's possible in general; might be reliably possible in a few situations.\n10. Harsh Gupta: [~srowen] That makes sense. But is this an issue then ? What are the possibilities you can see?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "8617f2a371315df4ad8f6a76a32bdb6d", "issue_key": "SPARK-762", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "The simple tutorial doesn't say anything about including the Spark jar or the Spark codepath", "description": "Fairly confusing to users. I am not even sure if this is a valid item, but you can certainly find it in the compiled html. http://spark-project.org/docs/latest/spark-simple-tutorial.html", "reporter": "Reynold Xin", "assignee": "Andy Konwinski", "created": "2013-06-06T12:02:33.000+0000", "updated": "2013-08-11T20:40:08.000+0000", "resolved": "2013-08-11T20:40:08.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Josh Rosen", "body": "It also doesn't mention setting SPARK_MEM (or any of the other settings from spark-env.sh).", "created": "2013-06-06T12:56:05.055+0000"}, {"author": "Patrick McFadin", "body": "Hey [~rxin] [~andyk] [~joshrosen], This document that's linked to here - I'm not sure it's actually referenced anywhere in the docs. This is also nearly completely subsumed by the Quickstart guide. Am I missing something? Probably we should just delete the doc.", "created": "2013-08-11T20:01:12.650+0000"}, {"author": "Reynold Xin", "body": "Yes - we should just delete it.", "created": "2013-08-11T20:11:10.623+0000"}], "num_comments": 3, "text": "Issue: SPARK-762\nSummary: The simple tutorial doesn't say anything about including the Spark jar or the Spark codepath\nDescription: Fairly confusing to users. I am not even sure if this is a valid item, but you can certainly find it in the compiled html. http://spark-project.org/docs/latest/spark-simple-tutorial.html\n\nComments (3):\n1. Josh Rosen: It also doesn't mention setting SPARK_MEM (or any of the other settings from spark-env.sh).\n2. Patrick McFadin: Hey [~rxin] [~andyk] [~joshrosen], This document that's linked to here - I'm not sure it's actually referenced anywhere in the docs. This is also nearly completely subsumed by the Quickstart guide. Am I missing something? Probably we should just delete the doc.\n3. Reynold Xin: Yes - we should just delete it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "ee60f382e6e88fb3aa43dcf1112afad4", "issue_key": "SPARK-763", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Built-in support for saving with compression", "description": "Many people store HDFS datasets in compressed form. It would be nice to add an Option[org.apache.hadoop.io.compress.CompressionCodec] (default None) as a parameter to various saveAsXXX functions. This is nicer than having people manually muck with a custom JobConf when they want to compress outputs.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-06-07T11:19:20.000+0000", "updated": "2013-06-13T17:48:29.000+0000", "resolved": "2013-06-13T17:48:13.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-763\nSummary: Built-in support for saving with compression\nDescription: Many people store HDFS datasets in compressed form. It would be nice to add an Option[org.apache.hadoop.io.compress.CompressionCodec] (default None) as a parameter to various saveAsXXX functions. This is nicer than having people manually muck with a custom JobConf when they want to compress outputs.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "7e4e33d94dd08f1ec8e85f375ba61668", "issue_key": "SPARK-764", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fix SPARK_EXAMPLES_JAR in 0.7.2", "description": "Users are reporting that SPARK_EXAMPLES_JAR is not set right in Spark 0.7.2 (see https://groups.google.com/d/msg/spark-users/nQ6wB2lcFN8/gWfBd6fLWHQJ for a recent example). A new post in https://groups.google.com/d/msg/spark-users/x5UczgI-Xm8/qInsC0ww-NAJ reports that SPARK_EXAMPLES_JAR is not set because the run script searches for the wrong filename. Here's the suggested fix in that message:  144 if [ -e \"$EXAMPLES_DIR/target/scala-$SCALA_VERSION/spark-examples_\"!(*sources |*javadoc) ]; then 145 # Use the JAR from the SBT build 146 export SPARK_EXAMPLES_JAR=`ls \"$EXAMPLES_DIR/target/scala-$SCALA_VERSION/spark-examples_\"!(*sources|*javadoc).jar` 147 fi  The new part here is the addition of the underscore between \"spark-examples\" and the Scala version, e.g. `spark-examples_2.9.3-0.7.2.jar`. I'm not sure whether the real problem is with the run script or the Maven or SBT assembly scripts; could someone more familiar with Spark's packaging look into this?", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-06-07T13:26:50.000+0000", "updated": "2013-06-22T10:22:33.000+0000", "resolved": "2013-06-22T10:22:33.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Just curious, did this happen with SBT or Maven?", "created": "2013-06-14T10:53:13.021+0000"}, {"author": "Josh Rosen", "body": "In the first thread, they say that they compiled Spark using sbt/sbt. I haven't checked whether we have this bug with the pre-compiled 0.7.2 binaries. It's worth confirming that Maven and sbt produce build artifacts with the same filenames.", "created": "2013-06-14T10:58:08.550+0000"}, {"author": "Christopher Nguyen", "body": "I can confirm that I get this when using \"sbt package\":  -rw-rw-r-- 1 ctn ctn 347561 Jun 14 17:47 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar  -- Separately, the run script breaks when there are more than one matching jar files, e.g.:  ctn@ctnu:~/src/spark$ ls -l examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.* -rw-rw-r-- 1 ctn ctn 347561 Apr 17 16:02 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar -rw-rw-r-- 1 ctn ctn 347561 Jun 14 17:47 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar ctn@ctnu:~/src/spark$ ./run spark.examples.SparkPi local + '[' -e /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar ']' ./run: line 145: [: /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar: binary operator expected + '[' -e '/home/ctn/src/spark/examples/target/spark-examples-!(*sources|*javadoc).jar' ']' + exit 0", "created": "2013-06-14T18:19:01.240+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I've fixed this here: https://github.com/mesos/spark/commit/e6a14e73b93fe30fcae7913262506b4a03fedd11. It looks like it was broken on a commit that existed in 0.7.x but not in master.", "created": "2013-06-22T10:22:19.285+0000"}], "num_comments": 4, "text": "Issue: SPARK-764\nSummary: Fix SPARK_EXAMPLES_JAR in 0.7.2\nDescription: Users are reporting that SPARK_EXAMPLES_JAR is not set right in Spark 0.7.2 (see https://groups.google.com/d/msg/spark-users/nQ6wB2lcFN8/gWfBd6fLWHQJ for a recent example). A new post in https://groups.google.com/d/msg/spark-users/x5UczgI-Xm8/qInsC0ww-NAJ reports that SPARK_EXAMPLES_JAR is not set because the run script searches for the wrong filename. Here's the suggested fix in that message:  144 if [ -e \"$EXAMPLES_DIR/target/scala-$SCALA_VERSION/spark-examples_\"!(*sources |*javadoc) ]; then 145 # Use the JAR from the SBT build 146 export SPARK_EXAMPLES_JAR=`ls \"$EXAMPLES_DIR/target/scala-$SCALA_VERSION/spark-examples_\"!(*sources|*javadoc).jar` 147 fi  The new part here is the addition of the underscore between \"spark-examples\" and the Scala version, e.g. `spark-examples_2.9.3-0.7.2.jar`. I'm not sure whether the real problem is with the run script or the Maven or SBT assembly scripts; could someone more familiar with Spark's packaging look into this?\n\nComments (4):\n1. Matei Alexandru Zaharia: Just curious, did this happen with SBT or Maven?\n2. Josh Rosen: In the first thread, they say that they compiled Spark using sbt/sbt. I haven't checked whether we have this bug with the pre-compiled 0.7.2 binaries. It's worth confirming that Maven and sbt produce build artifacts with the same filenames.\n3. Christopher Nguyen: I can confirm that I get this when using \"sbt package\":  -rw-rw-r-- 1 ctn ctn 347561 Jun 14 17:47 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar  -- Separately, the run script breaks when there are more than one matching jar files, e.g.:  ctn@ctnu:~/src/spark$ ls -l examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.* -rw-rw-r-- 1 ctn ctn 347561 Apr 17 16:02 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar -rw-rw-r-- 1 ctn ctn 347561 Jun 14 17:47 examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar ctn@ctnu:~/src/spark$ ./run spark.examples.SparkPi local + '[' -e /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3-SNAPSHOT.jar ']' ./run: line 145: [: /home/ctn/src/spark/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.1-SNAPSHOT.jar: binary operator expected + '[' -e '/home/ctn/src/spark/examples/target/spark-examples-!(*sources|*javadoc).jar' ']' + exit 0\n4. Matei Alexandru Zaharia: I've fixed this here: https://github.com/mesos/spark/commit/e6a14e73b93fe30fcae7913262506b4a03fedd11. It looks like it was broken on a commit that existed in 0.7.x but not in master.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "27da9444049d78d827e34f077ab56340", "issue_key": "SPARK-765", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Test suite should run Spark example programs", "description": "The Spark test suite should also run each of the Spark example programs (the PySpark suite should do the same). This should be done through a shell script or other mechanism to simulate the environment setup used by end users that run those scripts. This would prevent problems like SPARK-764 from making it into releases.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-06-07T17:55:18.000+0000", "updated": "2019-05-21T05:36:35.000+0000", "resolved": "2019-05-21T05:36:35.000+0000", "labels": ["bulk-closed"], "components": ["Examples"], "comments": [{"author": "Nicholas Chammas", "body": "Seems like a good idea. [~joshrosen] I assume this is still to be done, right?", "created": "2015-02-23T00:10:18.158+0000"}, {"author": "Josh Rosen", "body": "Yep, this still needs to be done. It's more of an issue for the Python examples than the Scala / Java ones, since at least the JVM ones are guaranteed to compile. It's unlikely that the examples are broken, though, given that we have compatibility guarantees, so this is probably a lower priority relative to other test automation tasks.", "created": "2015-02-23T19:17:46.302+0000"}, {"author": "Yu Ishikawa", "body": "Hi [~joshrosen], Should we add a maven dependency in the pom.xml of examples at first? It seems that we can't use FunSuite trait under examples at Spark 1.3.0. First of all, I think we should add test suites for the example programs.", "created": "2015-03-31T08:40:36.996+0000"}, {"author": "Josh Rosen", "body": "Hi [~yuu.ishikawa@gmail.com], It should be fined to add a {{test}}-scoped ScalaTest dependency to the examples subproject.", "created": "2015-04-02T17:19:27.000+0000"}, {"author": "Yu Ishikawa", "body": "Hi [~joshrosen], In order to avoid misunderstanding, please let me confirm. Is it OK if we add the ScalaTest dependency?", "created": "2015-04-03T01:57:43.043+0000"}, {"author": "Yu Ishikawa", "body": "I'm very sorry. I could run a test in spark.examples. Because my IntelliJ setting was wrong. You know, we don't need to add the dependency. Thanks.", "created": "2015-04-09T09:47:10.531+0000"}, {"author": "Yu Ishikawa", "body": "[~joshrosen] sorry, one more thing. Are we allowed to add test suites for spark.examples? We are discussing deplicating static train() method in Scala/Java on SPARK-6682. I think it is a good timing to add test suites to spark.examples.", "created": "2015-04-13T05:46:12.384+0000"}], "num_comments": 7, "text": "Issue: SPARK-765\nSummary: Test suite should run Spark example programs\nDescription: The Spark test suite should also run each of the Spark example programs (the PySpark suite should do the same). This should be done through a shell script or other mechanism to simulate the environment setup used by end users that run those scripts. This would prevent problems like SPARK-764 from making it into releases.\n\nComments (7):\n1. Nicholas Chammas: Seems like a good idea. [~joshrosen] I assume this is still to be done, right?\n2. Josh Rosen: Yep, this still needs to be done. It's more of an issue for the Python examples than the Scala / Java ones, since at least the JVM ones are guaranteed to compile. It's unlikely that the examples are broken, though, given that we have compatibility guarantees, so this is probably a lower priority relative to other test automation tasks.\n3. Yu Ishikawa: Hi [~joshrosen], Should we add a maven dependency in the pom.xml of examples at first? It seems that we can't use FunSuite trait under examples at Spark 1.3.0. First of all, I think we should add test suites for the example programs.\n4. Josh Rosen: Hi [~yuu.ishikawa@gmail.com], It should be fined to add a {{test}}-scoped ScalaTest dependency to the examples subproject.\n5. Yu Ishikawa: Hi [~joshrosen], In order to avoid misunderstanding, please let me confirm. Is it OK if we add the ScalaTest dependency?\n6. Yu Ishikawa: I'm very sorry. I could run a test in spark.examples. Because my IntelliJ setting was wrong. You know, we don't need to add the dependency. Thanks.\n7. Yu Ishikawa: [~joshrosen] sorry, one more thing. Are we allowed to add test suites for spark.examples? We are discussing deplicating static train() method in Scala/Java on SPARK-6682. I think it is a good timing to add test suites to spark.examples.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "5dc56d6b5d654c5047d474c012227017", "issue_key": "SPARK-766", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add executor info for the task completed message", "description": "It would be nice to add the executor id (host and port) when we log the task completion message. Should do this after the fair scheduler pull request is merged since the two changes would collide.", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-09T23:15:27.000+0000", "updated": "2013-07-29T14:16:41.000+0000", "resolved": "2013-07-29T14:16:41.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "+1. If the task start and task finished logs both contain the executor ID, this makes it way easier to use the logs to compute the number of concurrent tasks per executor over time. With the current logs, my Splunk concurrency query [turned into a giant mess|https://github.com/JoshRosen/spark-splunk/wiki#chart-maximum-concurrent-tasks-by-executor] because I had to correlate task completion events with task starts to extract the executor IDs for the finished tasks.", "created": "2013-06-10T11:47:29.763+0000"}], "num_comments": 1, "text": "Issue: SPARK-766\nSummary: Add executor info for the task completed message\nDescription: It would be nice to add the executor id (host and port) when we log the task completion message. Should do this after the fair scheduler pull request is merged since the two changes would collide.\n\nComments (1):\n1. Josh Rosen: +1. If the task start and task finished logs both contain the executor ID, this makes it way easier to use the logs to compute the number of concurrent tasks per executor over time. With the current logs, my Splunk concurrency query [turned into a giant mess|https://github.com/JoshRosen/spark-splunk/wiki#chart-maximum-concurrent-tasks-by-executor] because I had to correlate task completion events with task starts to extract the executor IDs for the finished tasks.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "6019ee91c660a7124adb985219fd2d1a", "issue_key": "SPARK-767", "issue_type": "Sub-task", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Standardize web ui port", "description": "The block manager UI randomly finds a port and bind to that. This port number is too confusing and makes it hard to control. We should standardize it with a configuration, and perhaps just do linear increment of the UI when a single node has multiple Spark processes.", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-13T14:40:05.000+0000", "updated": "2013-07-08T12:59:19.000+0000", "resolved": "2013-07-08T12:59:19.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "This is partially supported in master by setting: spark.ui.port But it doesn't work if you want to have multiple SparkContext's.", "created": "2013-06-13T17:34:03.304+0000"}], "num_comments": 1, "text": "Issue: SPARK-767\nSummary: Standardize web ui port\nDescription: The block manager UI randomly finds a port and bind to that. This port number is too confusing and makes it hard to control. We should standardize it with a configuration, and perhaps just do linear increment of the UI when a single node has multiple Spark processes.\n\nComments (1):\n1. Patrick McFadin: This is partially supported in master by setting: spark.ui.port But it doesn't work if you want to have multiple SparkContext's.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "13e2a3f64041d5f64a5694296b5528d5", "issue_key": "SPARK-768", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fail a task when the remote block it is fetching is not serializable", "description": "When a task is fetching a remote block (e.g. locality wait exceeded), and if the block is not serializable, the task would hang. The block manager should fail the task instead of hanging the task ... once the task fails, eventually it will get scheduled to the local node to be executed successfully.", "reporter": "Reynold Xin", "assignee": "Raymond Liu", "created": "2013-06-13T14:47:49.000+0000", "updated": "2014-06-20T06:46:50.000+0000", "resolved": "2014-06-20T06:46:50.000+0000", "labels": [], "components": [], "comments": [{"author": "Raymond Liu", "body": "Hi Reynold I am trying to figure out this issue. Here is my understanding: when the situation you mentioned happen. it means: the block is stored in memory level without serialization. otherwise, the execption alread been thown in previous steps. So under this condition, I can figure out two cases which might run into this problem : 1. the rdd is cached in memory, and as you mentioned, it got run on other node, in this case, it seems to me that the remote fetch operation of blockmanager will catch the exception in connectionManager and return None to cachemanager, then the task go to compute code path, though this lead to over compute and a second copy of block is stored. But this do not hang the task. and the job eventually got done. And I have write some cases to verify this. This case, we might find some solution to optimize it? 2. you are using BlockRDD in DStream case, and the storage level is Memory, Then upon compute of the BlockRDD on another node, the exception is thown, while in this case, I think the Task Executor will catch the exception and fail the task? So, either case seems to me will eventually finish the job. I am wondering which kind of case I am missing here which will lead to the hanging of the task, Can you kindly give me an example?", "created": "2014-06-20T02:34:37.063+0000"}, {"author": "Raymond Liu", "body": "And for case 2, the problem is that current code seems not make difference between the NonSerializableException been thrown by fetch remote block during computation and the exception been thrown during serialization of the task resut. it wll take it all as the task result is not serializable and abort the whole taskset. Thus the job will fail in the end I think. Is this what you mean hanging?", "created": "2014-06-20T03:01:42.220+0000"}, {"author": "Reynold Xin", "body": "I think it was the first case. It used to be the case that when a block was kept in memory in deserialized form, and a task got scheduled to a remote node and tried to fetch the block, if the block was not serializable, the whole thing would hang. Maybe we have already fixed it. If you can verify this is no longer a problem, we can close the ticket. Thanks!", "created": "2014-06-20T06:30:54.449+0000"}, {"author": "Raymond Liu", "body": "Hi Reynold If this is the first case, then I think, yes, it won't hang, at least from what I observe from my test and the code in this path. Only that recompute might be a problem? If I do the same thing on the cached RDD for a lot of iterations, eventually, all partitions will have a local block stored in each node. We can either accept this behavior, or need to modify the block ack message to identify this specific case other than return None as block not found.", "created": "2014-06-20T06:44:11.671+0000"}, {"author": "Reynold Xin", "body": "Thanks for confirming. I'm going to close this issue then.", "created": "2014-06-20T06:46:35.439+0000"}], "num_comments": 5, "text": "Issue: SPARK-768\nSummary: Fail a task when the remote block it is fetching is not serializable\nDescription: When a task is fetching a remote block (e.g. locality wait exceeded), and if the block is not serializable, the task would hang. The block manager should fail the task instead of hanging the task ... once the task fails, eventually it will get scheduled to the local node to be executed successfully.\n\nComments (5):\n1. Raymond Liu: Hi Reynold I am trying to figure out this issue. Here is my understanding: when the situation you mentioned happen. it means: the block is stored in memory level without serialization. otherwise, the execption alread been thown in previous steps. So under this condition, I can figure out two cases which might run into this problem : 1. the rdd is cached in memory, and as you mentioned, it got run on other node, in this case, it seems to me that the remote fetch operation of blockmanager will catch the exception in connectionManager and return None to cachemanager, then the task go to compute code path, though this lead to over compute and a second copy of block is stored. But this do not hang the task. and the job eventually got done. And I have write some cases to verify this. This case, we might find some solution to optimize it? 2. you are using BlockRDD in DStream case, and the storage level is Memory, Then upon compute of the BlockRDD on another node, the exception is thown, while in this case, I think the Task Executor will catch the exception and fail the task? So, either case seems to me will eventually finish the job. I am wondering which kind of case I am missing here which will lead to the hanging of the task, Can you kindly give me an example?\n2. Raymond Liu: And for case 2, the problem is that current code seems not make difference between the NonSerializableException been thrown by fetch remote block during computation and the exception been thrown during serialization of the task resut. it wll take it all as the task result is not serializable and abort the whole taskset. Thus the job will fail in the end I think. Is this what you mean hanging?\n3. Reynold Xin: I think it was the first case. It used to be the case that when a block was kept in memory in deserialized form, and a task got scheduled to a remote node and tried to fetch the block, if the block was not serializable, the whole thing would hang. Maybe we have already fixed it. If you can verify this is no longer a problem, we can close the ticket. Thanks!\n4. Raymond Liu: Hi Reynold If this is the first case, then I think, yes, it won't hang, at least from what I observe from my test and the code in this path. Only that recompute might be a problem? If I do the same thing on the cached RDD for a lot of iterations, eventually, all partitions will have a local block stored in each node. We can either accept this behavior, or need to modify the block ack message to identify this specific case other than return None as block not found.\n5. Reynold Xin: Thanks for confirming. I'm going to close this issue then.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "a6ad949b3429be3bc3645eef2cb036a4", "issue_key": "SPARK-769", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "block manager UI should contain the locations for each RDD block", "description": "Right now for each partition, only the size is reported. We should also report the nodes containing those partitions.", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-13T15:13:33.000+0000", "updated": "2013-07-12T23:18:08.000+0000", "resolved": "2013-07-12T23:18:08.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-769\nSummary: block manager UI should contain the locations for each RDD block\nDescription: Right now for each partition, only the size is reported. We should also report the nodes containing those partitions.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "aac7824d1b9cf59388b087a43dffe6a8", "issue_key": "SPARK-770", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Extend and Consolidate Spark Web UI", "description": "Spark would benefit from a visual monitoring dashboard which exposes task progress and instrumentation data. Currently, users have to look at log data in order to understand details about task progress and performance. This proposal outlines a new UI and identifies what code changes are necessary in various pieces of Spark. This document gives a proposal of how this might look: https://docs.google.com/document/d/1r5e_4IiEenqUiHEFqVFk8g6HPEMYf-60ybyjY8ceSgc/edit# There several sub-tasks which will need to happen as a result of this change. I'll flesh this list out more over time: - Upgrade existing UI's to newest version of Spray/Twirl or use alternative framework - Add plumbing to pass additional metrics to TaskListener - Create consolidated UI - Standardize UI port ranges (related to SPARK-769)", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-06-13T15:15:09.000+0000", "updated": "2013-08-02T20:56:31.000+0000", "resolved": "2013-08-02T20:56:30.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-770\nSummary: Extend and Consolidate Spark Web UI\nDescription: Spark would benefit from a visual monitoring dashboard which exposes task progress and instrumentation data. Currently, users have to look at log data in order to understand details about task progress and performance. This proposal outlines a new UI and identifies what code changes are necessary in various pieces of Spark. This document gives a proposal of how this might look: https://docs.google.com/document/d/1r5e_4IiEenqUiHEFqVFk8g6HPEMYf-60ybyjY8ceSgc/edit# There several sub-tasks which will need to happen as a result of this change. I'll flesh this list out more over time: - Upgrade existing UI's to newest version of Spray/Twirl or use alternative framework - Add plumbing to pass additional metrics to TaskListener - Create consolidated UI - Standardize UI port ranges (related to SPARK-769)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "4dc45170eadaf07eac7a9078633b5239", "issue_key": "SPARK-771", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Throw a more meaning message when SparkContext cannot connect to the master", "description": "The current error message is bash-4.1$ Exception in thread \"Thread-24\" java.util.concurrent.TimeoutException: Futures timed out after [10000] milliseconds at akka.dispatch.DefaultPromise.ready(Future.scala:870) at akka.dispatch.DefaultPromise.result(Future.scala:874) at akka.dispatch.Await$.result(Future.scala:74) at spark.deploy.client.Client.stop(Client.scala:118) at spark.scheduler.cluster.SparkDeploySchedulerBackend.stop(SparkDeploySchedulerBackend.scala:43) at spark.scheduler.cluster.ClusterScheduler.stop(ClusterScheduler.scala:396) at spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:740) at spark.SparkContext.stop(SparkContext.scala:565) at shark.SharkEnv$.stop(SharkEnv.scala:126) at shark.SharkServer$$anon$2.run(SharkServer.scala:107) We should say something more meaningful than that. E.g. \"Cannot connect to ...\" or \"Connection to ... timed out\"", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-13T17:22:12.000+0000", "updated": "2013-07-29T14:17:00.000+0000", "resolved": "2013-07-29T14:17:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Hey [~rxin] I can't reproduce this. What was the exact scenario under which this happened? In master if I try from Shark or Spark to connect to a disconnected master I get this appropriate message:  13/07/13 16:49:46 ERROR client.Client$ClientActor: Connection to master failed; stopping client  It's possible this is triggered in some other scenario, such as when the master and SC have different versions, I'm not sure.", "created": "2013-07-13T16:51:40.315+0000"}, {"author": "Patrick McFadin", "body": "I also confirmed things work if the Context connects to a master which is killed.", "created": "2013-07-13T16:54:06.090+0000"}, {"author": "Reynold Xin", "body": "Looks like the exception stack was from stopping a client? In client.scala:  def stop() { if (actor != null) { try { val timeout = Duration.create(System.getProperty(\"spark.akka.askTimeout\", \"10\").toLong, \"seconds\") val future = actor.ask(StopClient)(timeout) Await.result(future, timeout) } catch { case e: AskTimeoutException => // Ignore it, maybe master went away } actor = null } }", "created": "2013-07-13T17:49:37.566+0000"}, {"author": "Patrick McFadin", "body": "Okay [~rxin], my guess is that this means we should be catching {{TimeoutException}} rather than {{AskTimeoutException}} since it seems like the more general of the two types can be thrown (the latter is extends the former).", "created": "2013-07-14T18:32:03.139+0000"}], "num_comments": 4, "text": "Issue: SPARK-771\nSummary: Throw a more meaning message when SparkContext cannot connect to the master\nDescription: The current error message is bash-4.1$ Exception in thread \"Thread-24\" java.util.concurrent.TimeoutException: Futures timed out after [10000] milliseconds at akka.dispatch.DefaultPromise.ready(Future.scala:870) at akka.dispatch.DefaultPromise.result(Future.scala:874) at akka.dispatch.Await$.result(Future.scala:74) at spark.deploy.client.Client.stop(Client.scala:118) at spark.scheduler.cluster.SparkDeploySchedulerBackend.stop(SparkDeploySchedulerBackend.scala:43) at spark.scheduler.cluster.ClusterScheduler.stop(ClusterScheduler.scala:396) at spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:740) at spark.SparkContext.stop(SparkContext.scala:565) at shark.SharkEnv$.stop(SharkEnv.scala:126) at shark.SharkServer$$anon$2.run(SharkServer.scala:107) We should say something more meaningful than that. E.g. \"Cannot connect to ...\" or \"Connection to ... timed out\"\n\nComments (4):\n1. Patrick McFadin: Hey [~rxin] I can't reproduce this. What was the exact scenario under which this happened? In master if I try from Shark or Spark to connect to a disconnected master I get this appropriate message:  13/07/13 16:49:46 ERROR client.Client$ClientActor: Connection to master failed; stopping client  It's possible this is triggered in some other scenario, such as when the master and SC have different versions, I'm not sure.\n2. Patrick McFadin: I also confirmed things work if the Context connects to a master which is killed.\n3. Reynold Xin: Looks like the exception stack was from stopping a client? In client.scala:  def stop() { if (actor != null) { try { val timeout = Duration.create(System.getProperty(\"spark.akka.askTimeout\", \"10\").toLong, \"seconds\") val future = actor.ask(StopClient)(timeout) Await.result(future, timeout) } catch { case e: AskTimeoutException => // Ignore it, maybe master went away } actor = null } }\n4. Patrick McFadin: Okay [~rxin], my guess is that this means we should be catching {{TimeoutException}} rather than {{AskTimeoutException}} since it seems like the more general of the two types can be thrown (the latter is extends the former).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "2e108ec2f85304e01c826d200111350d", "issue_key": "SPARK-772", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "groupByKey should disable map side combine", "description": "Map side combine in group by key case does not reduce the amount of data shuffled. Instead, it forces a lot more objects to go into old gen, and leads to worse GC.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-06-13T23:43:22.000+0000", "updated": "2020-06-10T03:54:17.000+0000", "resolved": "2013-06-14T12:56:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "See https://github.com/mesos/spark/pull/651", "created": "2013-06-14T00:12:08.451+0000"}], "num_comments": 1, "text": "Issue: SPARK-772\nSummary: groupByKey should disable map side combine\nDescription: Map side combine in group by key case does not reduce the amount of data shuffled. Instead, it forces a lot more objects to go into old gen, and leads to worse GC.\n\nComments (1):\n1. Reynold Xin: See https://github.com/mesos/spark/pull/651", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "1330bdd9a19932f6db11a959ef2353ca", "issue_key": "SPARK-773", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add fair scheduler pool information UI similar with hadoop", "description": "I think that I should add web ui for scheduler info for FIFO and fair scheduler, for each pool, we could show the information such as stage number, running tasks, finished task for each stage, we could show the job id, pool id, running tasks, finished tasks ...", "reporter": "xiajunluan", "assignee": null, "created": "2013-06-13T23:48:28.000+0000", "updated": "2014-10-21T07:40:51.000+0000", "resolved": "2014-10-21T07:40:51.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick Wendell", "body": "This has existing in Spark for a while - it is a stale issue.", "created": "2014-10-21T07:40:51.978+0000"}], "num_comments": 1, "text": "Issue: SPARK-773\nSummary: Add fair scheduler pool information UI similar with hadoop\nDescription: I think that I should add web ui for scheduler info for FIFO and fair scheduler, for each pool, we could show the information such as stage number, running tasks, finished task for each stage, we could show the job id, pool id, running tasks, finished tasks ...\n\nComments (1):\n1. Patrick Wendell: This has existing in Spark for a while - it is a stale issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "546317f0d2b156a0a81a30b690839c1d", "issue_key": "SPARK-774", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "cogroup should also disable map side combine by default", "description": "", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-06-14T00:00:38.000+0000", "updated": "2013-06-14T12:56:35.000+0000", "resolved": "2013-06-14T12:56:35.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "See https://github.com/mesos/spark/pull/651", "created": "2013-06-14T00:12:12.267+0000"}], "num_comments": 1, "text": "Issue: SPARK-774\nSummary: cogroup should also disable map side combine by default\n\nComments (1):\n1. Reynold Xin: See https://github.com/mesos/spark/pull/651", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "e23a6ee109902980554e5200ee22a692", "issue_key": "SPARK-776", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support adding jars to Spark shell", "description": "We should add a mechanism to add additional jars to jobs run in the Spark shell, since addJar() doesn't work there (see https://github.com/mesos/spark/pull/359). There's a proposal/patch at https://groups.google.com/forum/?fromgroups#!searchin/spark-users/ADD_JAR/spark-users/IBgbLoFWbxw/9AzTrN_iwz4J, but someone needs to test it and submit it as a pull request. Spark should also emit warnings / errors when trying to call addJar() within spark-shell.", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-06-14T10:27:40.000+0000", "updated": "2013-06-22T17:17:37.000+0000", "resolved": "2013-06-22T17:17:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I've now fixed this in https://github.com/mesos/spark/commit/9494ed525b4bfb087f1cdf1448e8029c695dba06. The patch adds an ADD_JARS environment variable that can be set to a comma-separated list of JARs. These JARs will be added to both the interpreter's local classpath and the SparkContext.", "created": "2013-06-22T17:17:25.423+0000"}], "num_comments": 1, "text": "Issue: SPARK-776\nSummary: Support adding jars to Spark shell\nDescription: We should add a mechanism to add additional jars to jobs run in the Spark shell, since addJar() doesn't work there (see https://github.com/mesos/spark/pull/359). There's a proposal/patch at https://groups.google.com/forum/?fromgroups#!searchin/spark-users/ADD_JAR/spark-users/IBgbLoFWbxw/9AzTrN_iwz4J, but someone needs to test it and submit it as a pull request. Spark should also emit warnings / errors when trying to call addJar() within spark-shell.\n\nComments (1):\n1. Matei Alexandru Zaharia: I've now fixed this in https://github.com/mesos/spark/commit/9494ed525b4bfb087f1cdf1448e8029c695dba06. The patch adds an ADD_JARS environment variable that can be set to a comma-separated list of JARs. These JARs will be added to both the interpreter's local classpath and the SparkContext.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "81cbd4201750d56cc5e62323aa7fece1", "issue_key": "SPARK-777", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "spark.util.AkkaUtils not usable out of spark package", "description": "When I want to use spark.util.AkkaUtils.createActorSystem() to instanciate actorSystem like in the examples but in my application not in the spark package, the spark.util.AkkaUtils Object is not visible. As a workaround, I to put my main() in a spark package to be able to use spark.util.AkkaUtils. I propose to replace : private[spark] object AkkaUtils { by object AkkaUtils { best regards", "reporter": "Fabien Tison", "assignee": null, "created": "2013-06-14T14:51:51.000+0000", "updated": "2013-08-06T23:01:49.000+0000", "resolved": "2013-08-06T23:01:49.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Hi Fabien, Thanks for reporting this, but we don't want to expose AkkaUtils to outside users because we may change it over time. If you like the class, you can copy it and maintain a copy yourself. It's not very large.", "created": "2013-08-06T23:01:49.329+0000"}], "num_comments": 1, "text": "Issue: SPARK-777\nSummary: spark.util.AkkaUtils not usable out of spark package\nDescription: When I want to use spark.util.AkkaUtils.createActorSystem() to instanciate actorSystem like in the examples but in my application not in the spark package, the spark.util.AkkaUtils Object is not visible. As a workaround, I to put my main() in a spark package to be able to use spark.util.AkkaUtils. I propose to replace : private[spark] object AkkaUtils { by object AkkaUtils { best regards\n\nComments (1):\n1. Matei Alexandru Zaharia: Hi Fabien, Thanks for reporting this, but we don't want to expose AkkaUtils to outside users because we may change it over time. If you like the class, you can copy it and maintain a copy yourself. It's not very large.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "8986ad757c50f6f73a5437ff348ee2ad", "issue_key": "SPARK-778", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "run script should try java executable from JAVA_HOME first", "description": "See https://groups.google.com/d/msg/spark-users/uuw2e9d1l74/_bCyDfq6dywJ; It looks like the script tries to run `java` first and falls back on `$JAVA_HOME/bin/java` only if `java` cannot be found. This should probably be the other way around, trying $JAVA_HOME first. This is important for supporting users with multiple JDKs. We should probably do the same thing for $SCALA_HOME, checking for a user-supplied SCALA_HOME before falling back on `scala`.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-06-17T11:28:35.000+0000", "updated": "2013-06-27T22:19:48.000+0000", "resolved": "2013-06-27T22:19:48.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Fixed in https://github.com/mesos/spark/commit/4974b658edf2716ff3c6f2e6863cddb2a4ddf891", "created": "2013-06-27T22:19:23.639+0000"}], "num_comments": 1, "text": "Issue: SPARK-778\nSummary: run script should try java executable from JAVA_HOME first\nDescription: See https://groups.google.com/d/msg/spark-users/uuw2e9d1l74/_bCyDfq6dywJ; It looks like the script tries to run `java` first and falls back on `$JAVA_HOME/bin/java` only if `java` cannot be found. This should probably be the other way around, trying $JAVA_HOME first. This is important for supporting users with multiple JDKs. We should probably do the same thing for $SCALA_HOME, checking for a user-supplied SCALA_HOME before falling back on `scala`.\n\nComments (1):\n1. Matei Alexandru Zaharia: Fixed in https://github.com/mesos/spark/commit/4974b658edf2716ff3c6f2e6863cddb2a4ddf891", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "4184fbd14a4183b77dda04057fb2a19e", "issue_key": "SPARK-779", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Monitoring and logging improvement", "description": "An overall tracking ticket for monitoring & logging improvements.", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-17T12:50:11.000+0000", "updated": "2013-08-10T16:17:24.000+0000", "resolved": "2013-08-10T16:17:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Hey, I'd like to track this with finer grained umbrella JIRA's we have one right now for the various Web UI monitoring tools: SPARK-770 Probably we also want a top level JIRA for introducing a metrics library.", "created": "2013-06-17T12:53:10.606+0000"}, {"author": "Patrick McFadin", "body": "So my recommendation is that we close this until we get a better idea of the high level initiatives. Then we can create companion JIRA's to SPARK-770.", "created": "2013-06-17T12:53:51.629+0000"}], "num_comments": 2, "text": "Issue: SPARK-779\nSummary: Monitoring and logging improvement\nDescription: An overall tracking ticket for monitoring & logging improvements.\n\nComments (2):\n1. Patrick McFadin: Hey, I'd like to track this with finer grained umbrella JIRA's we have one right now for the various Web UI monitoring tools: SPARK-770 Probably we also want a top level JIRA for introducing a metrics library.\n2. Patrick McFadin: So my recommendation is that we close this until we get a better idea of the high level initiatives. Then we can create companion JIRA's to SPARK-770.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "c5c7abf7ad9d06160e0cbb8e60037ff0", "issue_key": "SPARK-780", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "stdout log should contain the command to launch the worker JVM", "description": "It would be much easier to debug many configuration issues when the command to launch the worker jvm is logged in the stdout file.", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-17T13:05:14.000+0000", "updated": "2013-08-10T16:17:15.000+0000", "resolved": "2013-08-10T16:17:15.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-780\nSummary: stdout log should contain the command to launch the worker JVM\nDescription: It would be much easier to debug many configuration issues when the command to launch the worker jvm is logged in the stdout file.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "3da1df52d22cd637ffd26617df28745c", "issue_key": "SPARK-781", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Log the temp directory path when Spark says \"Failed to create temp directory\"", "description": "I am watching somebody setting this up and it is hard to figure out where the root temp directory is right now.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-06-17T13:17:06.000+0000", "updated": "2013-07-27T16:58:39.000+0000", "resolved": "2013-07-27T16:58:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "@rxin are you sure that this is broken in master? The DiskStore seems to log this pretty clearly now.", "created": "2013-07-27T15:35:55.151+0000"}, {"author": "Reynold Xin", "body": "I actually fixed this one myself on June 17 ...", "created": "2013-07-27T16:58:25.831+0000"}], "num_comments": 2, "text": "Issue: SPARK-781\nSummary: Log the temp directory path when Spark says \"Failed to create temp directory\"\nDescription: I am watching somebody setting this up and it is hard to figure out where the root temp directory is right now.\n\nComments (2):\n1. Patrick McFadin: @rxin are you sure that this is broken in master? The DiskStore seems to log this pretty clearly now.\n2. Reynold Xin: I actually fixed this one myself on June 17 ...", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "9720eed265bf6e1fab58331dcb4a054f", "issue_key": "SPARK-782", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Multiple versions of ASM being put on classpath", "description": "Since the update to ASM 4, something is still pulling in ASM 3. We need to exclude it in SBT or just find a better way to manage lib_managed.", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandy Ryza", "created": "2013-06-20T08:07:57.000+0000", "updated": "2014-04-04T20:51:10.000+0000", "resolved": "2014-03-09T13:18:18.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "This is now fixed in the SBT build but not Maven.", "created": "2013-06-23T16:21:34.668+0000"}, {"author": "Sanford Ryza", "body": "It looks like Hadoop depends on asm 3.2. I've seen this cause IncompatibleClassChangeErrors a couple times recently, depending on which order the jars are placed on the classpath (even when built with SBT). Would it make sense to shade asm in Spark?", "created": "2014-02-23T23:52:22.388+0000"}, {"author": "Sanford Ryza", "body": "https://github.com/apache/spark/pull/90", "created": "2014-03-05T20:44:07.378+0000"}, {"author": "Kevin Markey", "body": "Thank you! Not following this issue earlier, and having to deal with multiple ASM conflicts (Hadoop, Jersey servlet, and more), and not being able to wait for shaded Maven packages (spark-core), we needed to shade other instances in other libraries, but given how Spark is deployed, it was impossible for us to shade Spark independency.", "created": "2014-03-11T13:09:23.840+0000"}, {"author": "Tathagata Das", "body": "The PR that fixes this https://github.com/apache/spark/pull/100/", "created": "2014-03-24T16:49:17.255+0000"}, {"author": "Kevin Markey", "body": "Thanks. I understand there's a fix and pull for it but my comment was to include it in. 0.9.1. It's lots handier if it's in Maven central than making a special build and pushing it into our Maven repository and pushing the fixes to the clusters. Thanks again.", "created": "2014-03-25T08:13:22.338+0000"}, {"author": "Glenn Murray", "body": "I'd like to +1 Kevin's request to push this into 0.9.1. This week I actually had a coding exercise for a job application that requested deployment of the exercise to AWS, and having it fail with 0.9.0 was painful. Thanks, Glenn", "created": "2014-03-25T16:18:51.288+0000"}, {"author": "Tathagata Das", "body": "The PR (https://github.com/apache/spark/pull/100/) that fixed this for master branch did the following: 1. Made Spark use existing shaded asm (Kryo's) and removed Spark's direct dependency to asm 2. Excluded ASM from all Spark dependencies so that asm is not Spark binary any more. For branch 0.9, I am afraid to do (2), because there is a remote possibility that completely remove asm from Spark binaries may fail something. For example, Hadoop has asm as dependency and removing asm completely may cause Hadoop to fail in some scenario. Hence, I am porting only (1). This would allow downstream projects to safely exclude asm from Spark, as Spark in itself does not depend on asm (only shaded asm).", "created": "2014-03-25T17:39:25.841+0000"}, {"author": "Tathagata Das", "body": "I opened a PR with this https://github.com/apache/spark/pull/232", "created": "2014-03-25T17:47:22.424+0000"}, {"author": "Kevin Markey", "body": "It is precisely the Hadoop transitive dependencies on ASM that broke us first. It depends on what Hadoop version. We were using 2,2,0 which required 3.2 ASM. Hive, too. More recent Hadoop versions might be in line with 4.0 but I've no checked. If they are in conflict, we are sunk in any case, and your concern is moot. Hadoop shouldn't depend on Spark's transitive dependencies. Thanks Kevin", "created": "2014-03-25T18:07:20.593+0000"}, {"author": "Tathagata Das", "body": "But shouldnt this change in theh PR make it easier to deal with such conflicts as you can now exclude ASM from Spark, without affecting the execution of Spark (since Spark does not use ASM any more)? Since Spark is designed to build against many different version of Hadoop, short of blindly eliminating ASM completely from all dependencies, there isnt any super-smart thing that can be done in the Spark build to deal with ASM conflicts. Explicitly removing a transitive dependency without have a full knowledge of how it may affect dependencies (Hadoop in this case) is something that should not be done in a maintenance release.", "created": "2014-03-25T19:01:52.000+0000"}, {"author": "Tathagata Das", "body": "[~klmarkey] Since 0.9.1-RC2 with this fix is out, if possible, you can try compiling your project with ASM excluded from Spark.", "created": "2014-03-26T14:50:37.809+0000"}, {"author": "Kevin Markey", "body": "Thank you for including 782. Yes, I'm in the process of ramping up a test. Compilation is insufficient. I'll need to test runtime, too. Which will take a few days because some of the relevant code in flux. Kevin", "created": "2014-03-26T15:07:20.575+0000"}, {"author": "santhoma", "body": "Thanks for the fix. I was getting below exception while trying with CDH 5.2 beta. java.lang.IncompatibleClassChangeError: class org.apache.spark.util.InnerClosureFinder has interface org.objectweb.asm.ClassVisitor as super class at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:792) The embedded version with CDH 5.2b is spark 0.9, and after building from 0.9.1 branch version the above error was gone.", "created": "2014-03-28T02:32:53.352+0000"}, {"author": "Tathagata Das", "body": "Great!", "created": "2014-03-28T02:53:42.388+0000"}], "num_comments": 15, "text": "Issue: SPARK-782\nSummary: Multiple versions of ASM being put on classpath\nDescription: Since the update to ASM 4, something is still pulling in ASM 3. We need to exclude it in SBT or just find a better way to manage lib_managed.\n\nComments (15):\n1. Matei Alexandru Zaharia: This is now fixed in the SBT build but not Maven.\n2. Sanford Ryza: It looks like Hadoop depends on asm 3.2. I've seen this cause IncompatibleClassChangeErrors a couple times recently, depending on which order the jars are placed on the classpath (even when built with SBT). Would it make sense to shade asm in Spark?\n3. Sanford Ryza: https://github.com/apache/spark/pull/90\n4. Kevin Markey: Thank you! Not following this issue earlier, and having to deal with multiple ASM conflicts (Hadoop, Jersey servlet, and more), and not being able to wait for shaded Maven packages (spark-core), we needed to shade other instances in other libraries, but given how Spark is deployed, it was impossible for us to shade Spark independency.\n5. Tathagata Das: The PR that fixes this https://github.com/apache/spark/pull/100/\n6. Kevin Markey: Thanks. I understand there's a fix and pull for it but my comment was to include it in. 0.9.1. It's lots handier if it's in Maven central than making a special build and pushing it into our Maven repository and pushing the fixes to the clusters. Thanks again.\n7. Glenn Murray: I'd like to +1 Kevin's request to push this into 0.9.1. This week I actually had a coding exercise for a job application that requested deployment of the exercise to AWS, and having it fail with 0.9.0 was painful. Thanks, Glenn\n8. Tathagata Das: The PR (https://github.com/apache/spark/pull/100/) that fixed this for master branch did the following: 1. Made Spark use existing shaded asm (Kryo's) and removed Spark's direct dependency to asm 2. Excluded ASM from all Spark dependencies so that asm is not Spark binary any more. For branch 0.9, I am afraid to do (2), because there is a remote possibility that completely remove asm from Spark binaries may fail something. For example, Hadoop has asm as dependency and removing asm completely may cause Hadoop to fail in some scenario. Hence, I am porting only (1). This would allow downstream projects to safely exclude asm from Spark, as Spark in itself does not depend on asm (only shaded asm).\n9. Tathagata Das: I opened a PR with this https://github.com/apache/spark/pull/232\n10. Kevin Markey: It is precisely the Hadoop transitive dependencies on ASM that broke us first. It depends on what Hadoop version. We were using 2,2,0 which required 3.2 ASM. Hive, too. More recent Hadoop versions might be in line with 4.0 but I've no checked. If they are in conflict, we are sunk in any case, and your concern is moot. Hadoop shouldn't depend on Spark's transitive dependencies. Thanks Kevin", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "7ba5dbbc7aed86b11aa68b2b8a807000", "issue_key": "SPARK-783", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Web UI should report whether a job/task has failed", "description": "", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-20T12:44:17.000+0000", "updated": "2013-07-08T12:59:02.000+0000", "resolved": "2013-07-08T12:59:02.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-783\nSummary: Web UI should report whether a job/task has failed", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "7370fb44d1db9ae2e8bf3ebc4a036e6f", "issue_key": "SPARK-784", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "foldByKey does not clone the \"zero value\" for each key, leading to overwriting", "description": "This is not a problem with the normal fold() because each task is deserialized separately and gets a separate version of the \"zero value\". We probably need to do that manually same in foldByKey.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-06-22T13:37:09.000+0000", "updated": "2013-06-23T10:27:49.000+0000", "resolved": "2013-06-23T10:27:49.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Fixed here: https://github.com/mesos/spark/commit/78ffe164b33c6b11a2e511442605acd2f795a1b5 and in branch-0.7.", "created": "2013-06-23T10:27:37.213+0000"}], "num_comments": 1, "text": "Issue: SPARK-784\nSummary: foldByKey does not clone the \"zero value\" for each key, leading to overwriting\nDescription: This is not a problem with the normal fold() because each task is deserialized separately and gets a separate version of the \"zero value\". We probably need to do that manually same in foldByKey.\n\nComments (1):\n1. Matei Alexandru Zaharia: Fixed here: https://github.com/mesos/spark/commit/78ffe164b33c6b11a2e511442605acd2f795a1b5 and in branch-0.7.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "9ce095897968fe10d2f2fbe2583259ee", "issue_key": "SPARK-785", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ClosureCleaner not invoked on most PairRDDFunctions", "description": "It's pretty weird that we've missed this so far, but it seems to be the case. Unfortunately it may not be good to fix this in 0.7.3 because it could change behavior in unexpected ways; I haven't decided yet. But we should definitely do it for 0.8, and add tests.", "reporter": "Matei Alexandru Zaharia", "assignee": "Sean R. Owen", "created": "2013-06-23T16:21:05.000+0000", "updated": "2014-12-17T20:21:52.000+0000", "resolved": "2014-12-17T20:21:32.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Aaron Davidson", "body": "Is this still an open issue?", "created": "2013-11-14T18:25:56.586+0000"}, {"author": "Matei Alexandru Zaharia", "body": "[~adav] it still seems to be, weirdly enough: for instance combineByKey doesn't do it. Unless I'm missing something.", "created": "2014-11-06T07:04:34.648+0000"}, {"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3690", "created": "2014-12-14T01:10:17.144+0000"}, {"author": "Josh Rosen", "body": "I've merged https://github.com/apache/spark/pull/3690 to fix this in the maintenance branches and have tagged this for a 1.2.1 backport.", "created": "2014-12-16T00:10:03.678+0000"}, {"author": "Josh Rosen", "body": "I've merged this into {{branch-1.2}}, so it will be included in Spark 1.2.1. Since this was the last backport, I'm marking this as Fixed.", "created": "2014-12-17T20:21:32.047+0000"}], "num_comments": 5, "text": "Issue: SPARK-785\nSummary: ClosureCleaner not invoked on most PairRDDFunctions\nDescription: It's pretty weird that we've missed this so far, but it seems to be the case. Unfortunately it may not be good to fix this in 0.7.3 because it could change behavior in unexpected ways; I haven't decided yet. But we should definitely do it for 0.8, and add tests.\n\nComments (5):\n1. Aaron Davidson: Is this still an open issue?\n2. Matei Alexandru Zaharia: [~adav] it still seems to be, weirdly enough: for instance combineByKey doesn't do it. Unless I'm missing something.\n3. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3690\n4. Josh Rosen: I've merged https://github.com/apache/spark/pull/3690 to fix this in the maintenance branches and have tagged this for a 1.2.1 backport.\n5. Josh Rosen: I've merged this into {{branch-1.2}}, so it will be included in Spark 1.2.1. Since this was the last backport, I'm marking this as Fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "7c70d5e03c94b0012ff2cbd80a0398be", "issue_key": "SPARK-786", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Clean up old work directories in standalone worker", "description": "We should add a setting to clean old work directories after X days. Otherwise, the directory gets filled forever with shuffle files and logs.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-06-26T20:35:18.000+0000", "updated": "2015-02-26T01:01:57.000+0000", "resolved": "2015-02-26T01:01:57.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Anurag Tangri", "body": "Hi, We are also facing this issue. Could somebody assign this ticket to me ? I would like to work on this. Thanks, Anurag Tangri", "created": "2014-07-02T06:50:20.613+0000"}, {"author": "Anurag Tangri", "body": "Knock-knock :) Can somebody assign this ticket to me ? thanks, Anurag Tangri", "created": "2014-07-08T18:33:53.956+0000"}, {"author": "Andrew Or", "body": "I believe this was already fixed in https://github.com/apache/spark/pull/288?", "created": "2014-07-08T22:58:27.864+0000"}, {"author": "Andrew Ash", "body": "Agreed. With SPARK-1860 we could re-enable that the features from that PR by default and be good here (it was disabled after it had negative effects with long-running transactions). I think this ticket can be closed as a dupe of that one", "created": "2014-07-25T01:21:24.801+0000"}], "num_comments": 4, "text": "Issue: SPARK-786\nSummary: Clean up old work directories in standalone worker\nDescription: We should add a setting to clean old work directories after X days. Otherwise, the directory gets filled forever with shuffle files and logs.\n\nComments (4):\n1. Anurag Tangri: Hi, We are also facing this issue. Could somebody assign this ticket to me ? I would like to work on this. Thanks, Anurag Tangri\n2. Anurag Tangri: Knock-knock :) Can somebody assign this ticket to me ? thanks, Anurag Tangri\n3. Andrew Or: I believe this was already fixed in https://github.com/apache/spark/pull/288?\n4. Andrew Ash: Agreed. With SPARK-1860 we could re-enable that the features from that PR by default and be good here (it was disabled after it had negative effects with long-running transactions). I think this ticket can be closed as a dupe of that one", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "f4dcbd164bb1bf0250e1a29ad8ad12cf", "issue_key": "SPARK-787", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add EC2 Script Option to Push EC2 Credentials to Spark Nodes", "description": "", "reporter": "Patrick Wendell", "assignee": "Dan Osipov", "created": "2013-07-02T14:26:37.000+0000", "updated": "2014-09-16T20:41:40.000+0000", "resolved": "2014-09-16T20:41:40.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Patrick McFadin", "body": "Once we merge SPARK-727 this will be really easy to add. We can just include the credentials as environment variables in ec2-variables.sh and also add the appropriate settings in the templates for mapreduce and hdfs. This would just automatically install the credentials that are used to create the cluster.", "created": "2013-07-12T17:54:30.038+0000"}, {"author": "Dan Osipov", "body": "I added a PR for this bug in https://github.com/apache/spark/pull/1120", "created": "2014-08-18T23:02:48.579+0000"}, {"author": "Patrick Wendell", "body": "This is fixed in https://github.com/apache/spark/pull/1120", "created": "2014-09-16T20:41:40.787+0000"}], "num_comments": 3, "text": "Issue: SPARK-787\nSummary: Add EC2 Script Option to Push EC2 Credentials to Spark Nodes\n\nComments (3):\n1. Patrick McFadin: Once we merge SPARK-727 this will be really easy to add. We can just include the credentials as environment variables in ec2-variables.sh and also add the appropriate settings in the templates for mapreduce and hdfs. This would just automatically install the credentials that are used to create the cluster.\n2. Dan Osipov: I added a PR for this bug in https://github.com/apache/spark/pull/1120\n3. Patrick Wendell: This is fixed in https://github.com/apache/spark/pull/1120", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "8721b01e90c0b06914d860bc4d3350c7", "issue_key": "SPARK-788", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add spark metrics system", "description": "like hadoop system, we should export cluster detail metrics (jmx format, ganglia format, text format.....)to cluster administrator , and it will benefit administrator to monitor cluster status with tools like nagios, ganglia.", "reporter": "xiajunluan", "assignee": "xiajunluan", "created": "2013-07-04T01:05:07.000+0000", "updated": "2013-09-08T22:48:34.000+0000", "resolved": "2013-09-08T22:48:34.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Jie Huang", "body": "The patch file to enable ganglia sink for Spark.", "created": "2013-09-08T05:43:51.086+0000"}, {"author": "Jie Huang", "body": "Snapshot for parts of the enabled ganglia metrics in my test environment after the patch file applied.", "created": "2013-09-08T05:44:40.588+0000"}, {"author": "Jie Huang", "body": "Good idea. Ganglia is a very popular monitoring system for the distributed system. Since the codahale-metrics already supports ganglia reporter, I have enabled the ganglia sink for Spark with ease. More details can be found either in the attachment or the [github link| https://github.com/GraceH/spark/commit/c9717adfb97dacb97c34ad1fe94c8076843e5bf9]. Here also attaches a snapshot for some enabled metrics in my test environment. !GangliaSysSnapshot.png! Meanwhile, it would be better if we can improve the metric name to group the metrics according to its source or category. This really helps to present a better view in the new ganglia web page. According to the current implementation, the codahale-metrics chooses those characters before the last dot from its whole name as the group name.", "created": "2013-09-08T05:45:44.326+0000"}, {"author": "xiajunluan", "body": "Hi Grace Pwendell has implemented similar feature with you, and you could refer to SPARK-895", "created": "2013-09-08T18:54:23.945+0000"}, {"author": "Patrick McFadin", "body": "Hey Guys, The scope of this JIRA is now completed, so I'm going to close this. I didn't realize you were working on the Ganglia patch, we merged something similar today: https://github.com/mesos/spark/pull/906/files - Patrick", "created": "2013-09-08T22:48:14.574+0000"}], "num_comments": 5, "text": "Issue: SPARK-788\nSummary: Add spark metrics system\nDescription: like hadoop system, we should export cluster detail metrics (jmx format, ganglia format, text format.....)to cluster administrator , and it will benefit administrator to monitor cluster status with tools like nagios, ganglia.\n\nComments (5):\n1. Jie Huang: The patch file to enable ganglia sink for Spark.\n2. Jie Huang: Snapshot for parts of the enabled ganglia metrics in my test environment after the patch file applied.\n3. Jie Huang: Good idea. Ganglia is a very popular monitoring system for the distributed system. Since the codahale-metrics already supports ganglia reporter, I have enabled the ganglia sink for Spark with ease. More details can be found either in the attachment or the [github link| https://github.com/GraceH/spark/commit/c9717adfb97dacb97c34ad1fe94c8076843e5bf9]. Here also attaches a snapshot for some enabled metrics in my test environment. !GangliaSysSnapshot.png! Meanwhile, it would be better if we can improve the metric name to group the metrics according to its source or category. This really helps to present a better view in the new ganglia web page. According to the current implementation, the codahale-metrics chooses those characters before the last dot from its whole name as the group name.\n4. xiajunluan: Hi Grace Pwendell has implemented similar feature with you, and you could refer to SPARK-895\n5. Patrick McFadin: Hey Guys, The scope of this JIRA is now completed, so I'm going to close this. I didn't realize you were working on the Ganglia patch, we merged something similar today: https://github.com/mesos/spark/pull/906/files - Patrick", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "0e7f0a1dd7448587c1252c4ce14a6559", "issue_key": "SPARK-789", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "ApplicationRemoved message shouldn't be sent to terminated application driver actor", "description": "Once received a Terminated message, the {{Master}} actor: # Calls {{finishApplication}} if the terminated actor is an app # The {{finishApplication}} method calls {{removeApplication(app, ApplicationState.FINISHED)}}. # Within {{removeApplication}}, an {{ApplicationRemoved}} message is sent to {{app.driver}} But {{app.driver}} has already terminated, thus causes Akka report log errors complaining about {{RemoteClientError}} and {{RemoteClientWriteFailed}} etc.. Within {{removeApplication}}, when the {{state}} argument is {{ApplicationState.FINISHED}}, we should not send the {{ApplicationRemoved}} message, since {{app.driver}} is terminated. A sample error log provided by Jason Dai is attached.", "reporter": "liancheng", "assignee": "liancheng", "created": "2013-07-04T09:46:52.000+0000", "updated": "2013-08-06T23:13:46.000+0000", "resolved": "2013-08-06T23:13:46.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "liancheng", "body": "Just sent a pull request to fix this issue: https://github.com/mesos/spark/pull/674", "created": "2013-07-04T10:01:22.750+0000"}], "num_comments": 1, "text": "Issue: SPARK-789\nSummary: ApplicationRemoved message shouldn't be sent to terminated application driver actor\nDescription: Once received a Terminated message, the {{Master}} actor: # Calls {{finishApplication}} if the terminated actor is an app # The {{finishApplication}} method calls {{removeApplication(app, ApplicationState.FINISHED)}}. # Within {{removeApplication}}, an {{ApplicationRemoved}} message is sent to {{app.driver}} But {{app.driver}} has already terminated, thus causes Akka report log errors complaining about {{RemoteClientError}} and {{RemoteClientWriteFailed}} etc.. Within {{removeApplication}}, when the {{state}} argument is {{ApplicationState.FINISHED}}, we should not send the {{ApplicationRemoved}} message, since {{app.driver}} is terminated. A sample error log provided by Jason Dai is attached.\n\nComments (1):\n1. liancheng: Just sent a pull request to fix this issue: https://github.com/mesos/spark/pull/674", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "7f7b76c40ae2b7ac51441d93dab8f151", "issue_key": "SPARK-790", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Implement the reregistered() callback in MesosScheduler to support master failover", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-07-04T10:37:40.000+0000", "updated": "2020-05-17T17:47:06.000+0000", "resolved": "2019-05-21T04:16:53.000+0000", "labels": ["bulk-closed"], "components": ["Mesos", "Scheduler", "Spark Core"], "comments": [{"author": "Apache Spark", "body": "User 'rekhajoshm' has created a pull request for this issue: https://github.com/apache/spark/pull/20418", "created": "2018-01-28T23:27:04.387+0000"}], "num_comments": 1, "text": "Issue: SPARK-790\nSummary: Implement the reregistered() callback in MesosScheduler to support master failover\n\nComments (1):\n1. Apache Spark: User 'rekhajoshm' has created a pull request for this issue: https://github.com/apache/spark/pull/20418", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "a94066fbf796d8b230dac7549c8ea502", "issue_key": "SPARK-791", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "[pyspark] operator.getattr not serialized", "description": "Using operator.itemgetter as a function in map seems to confuse the serialization process in pyspark. I'm using itemgetter to return tuples, which fails with a TypeError (details below). Using an equivalent lambda function returns the correct result. Use a test file:  echo 1,1 > test.txt  Then try mapping it to a tuple:  import csv sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(lambda l: (l[0],l[1])).first() Out[7]: ('1', '1')  But this does not work when using operator.itemgetter:  import operator sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first() # TypeError: list indices must be integers, not tuple  This is running with git master, commit 6d60fe571a405eb9306a2be1817901316a46f892 IPython 0.13.2 java version \"1.7.0_25\" Scala code runner version 2.9.1 Ubuntu 12.04 Full debug output:  In [9]: sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first() 13/07/04 16:19:49 INFO storage.MemoryStore: ensureFreeSpace(33632) called with curMem=201792, maxMem=339585269 13/07/04 16:19:49 INFO storage.MemoryStore: Block broadcast_6 stored as values to memory (estimated size 32.8 KB, free 323.6 MB) 13/07/04 16:19:49 INFO mapred.FileInputFormat: Total input paths to process : 1 13/07/04 16:19:49 INFO spark.SparkContext: Starting job: takePartition at NativeMethodAccessorImpl.java:-2 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Got job 4 (takePartition at NativeMethodAccessorImpl.java:-2) with 1 output partitions (allowLocal=true) 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Final stage: Stage 4 (PythonRDD at NativeConstructorAccessorImpl.java:-2) 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Parents of final stage: List() 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Missing parents: List() 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Computing the requested partition locally 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Failed to run takePartition at NativeMethodAccessorImpl.java:-2 --------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-9-1fdb3e7a8ac7> in <module>() ----> 1 sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first() /home/jim/src/spark/python/pyspark/rdd.pyc in first(self) 389 2 390 \"\"\" --> 391 return self.take(1)[0] 392 393 def saveAsTextFile(self, path): /home/jim/src/spark/python/pyspark/rdd.pyc in take(self, num) 372 items = [] 373 for partition in range(self._jrdd.splits().size()): --> 374 iterator = self.ctx._takePartition(self._jrdd.rdd(), partition) 375 # Each item in the iterator is a string, Python object, batch of 376 # Python objects. Regardless, it is sufficient to take `num` /home/jim/src/spark/python/lib/py4j0.7.egg/py4j/java_gateway.pyc in __call__(self, *args) 498 answer = self.gateway_client.send_command(command) 499 return_value = get_return_value(answer, self.gateway_client, --> 500 self.target_id, self.name) 501 502 for temp_arg in temp_args: /home/jim/src/spark/python/lib/py4j0.7.egg/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0}{1}{2}.\\n'. --> 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError( Py4JJavaError: An error occurred while calling z:spark.api.python.PythonRDD.takePartition. : spark.api.python.PythonException: Traceback (most recent call last): File \"/home/jim/src/spark/python/pyspark/worker.py\", line 53, in main for obj in func(split_index, iterator): File \"/home/jim/src/spark/python/pyspark/serializers.py\", line 24, in batched for item in iterator: TypeError: list indices must be integers, not tuple at spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:117) at spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:139) at spark.api.python.PythonRDD.compute(PythonRDD.scala:82) at spark.RDD.computeOrReadCheckpoint(RDD.scala:232) at spark.RDD.iterator(RDD.scala:221) at spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:423) at spark.scheduler.DAGScheduler$$anon$2.run(DAGScheduler.scala:410)", "reporter": "Jim Blomo", "assignee": "Davies Liu", "created": "2013-07-04T16:27:33.000+0000", "updated": "2014-07-29T08:18:47.000+0000", "resolved": "2014-07-29T08:18:33.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "I think this is a problem with how operator.itemgetter is serialized, because  sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(lambda x: operator.itemgetter(0, 1)(x)).first()  works for me, while  sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0, 1)).first()  gives the error that you saw. Interestingly, the example works fine if I only specify a single index, e.g. operator.itemgetter(0). To get a bit more insight into what's going on, I used pickletools.dis to log the pickled functions (specifically, I used pickletools.dis(pickletools.optimize(cloudpickle.dumps(func))) with the extra optimization added to remove unnecessary PUTs in order to make the output easier to read). For  sc.textFile(\"test.txt\").mapPartitions(csv.reader).cache().map(operator.itemgetter(0, 1)).first()  the second map function's pickled form disassembles into  0: \\x80 PROTO 2 2: ( MARK 3: c GLOBAL 'pyspark.cloudpickle _modules_to_main' 41: q BINPUT 0 43: ] EMPTY_LIST 44: U SHORT_BINSTRING 'pyspark.rdd' 57: q BINPUT 2 59: a APPEND 60: \\x85 TUPLE1 61: R REDUCE 62: 1 POP_MARK (MARK at 2) 63: c GLOBAL 'pyspark.cloudpickle _fill_function' 99: q BINPUT 4 101: ( MARK 102: c GLOBAL 'pyspark.cloudpickle _make_skel_func' 139: q BINPUT 5 141: c GLOBAL 'new code' 151: q BINPUT 6 153: ( MARK 154: K BININT1 2 156: K BININT1 2 158: K BININT1 4 160: K BININT1 19 162: U SHORT_BINSTRING 't\\x00\\x00\\x88\\x00\\x00|\\x00\\x00|\\x01\\x00\\x83\\x02\\x00\\x88\\x01\\x00\\x83\\x02\\x00S' 186: N NONE 187: \\x85 TUPLE1 188: U SHORT_BINSTRING 'batched' 197: q BINPUT 9 199: \\x85 TUPLE1 200: U SHORT_BINSTRING 'split' 207: q BINPUT 11 209: U SHORT_BINSTRING 'iterator' 219: q BINPUT 12 221: \\x86 TUPLE2 222: U SHORT_BINSTRING '/Users/joshrosen/Documents/spark/spark/python/pyspark/rdd.py' 284: U SHORT_BINSTRING 'batched_func' 298: M BININT2 725 301: U SHORT_BINSTRING '\\x00\\x01' 305: U SHORT_BINSTRING 'oldfunc' 314: U SHORT_BINSTRING 'batchSize' 325: \\x86 TUPLE2 326: ) EMPTY_TUPLE 327: t TUPLE (MARK at 153) 328: R REDUCE 329: K BININT1 2 331: } EMPTY_DICT 332: q BINPUT 22 334: \\x87 TUPLE3 335: R REDUCE 336: } EMPTY_DICT 337: h BINGET 9 339: c GLOBAL 'pyspark.serializers batched' 368: s SETITEM 369: N NONE 370: ] EMPTY_LIST 371: ( MARK 372: ( MARK 373: h BINGET 0 375: ] EMPTY_LIST 376: h BINGET 2 378: a APPEND 379: \\x85 TUPLE1 380: R REDUCE 381: 1 POP_MARK (MARK at 372) 382: h BINGET 4 384: ( MARK 385: h BINGET 5 387: h BINGET 6 389: ( MARK 390: K BININT1 2 392: K BININT1 2 394: K BININT1 3 396: K BININT1 19 398: U SHORT_BINSTRING 't\\x00\\x00\\x88\\x00\\x00|\\x01\\x00\\x83\\x02\\x00S' 413: N NONE 414: \\x85 TUPLE1 415: U SHORT_BINSTRING 'imap' 421: q BINPUT 32 423: \\x85 TUPLE1 424: h BINGET 11 426: h BINGET 12 428: \\x86 TUPLE2 429: U SHORT_BINSTRING '/Users/joshrosen/Documents/spark/spark/python/pyspark/rdd.py' 491: U SHORT_BINSTRING 'func' 497: K BININT1 87 499: U SHORT_BINSTRING '' 501: U SHORT_BINSTRING 'f' 504: \\x85 TUPLE1 505: ) EMPTY_TUPLE 506: t TUPLE (MARK at 389) 507: R REDUCE 508: K BININT1 1 510: h BINGET 22 512: \\x87 TUPLE3 513: R REDUCE 514: } EMPTY_DICT 515: h BINGET 32 517: c GLOBAL 'itertools imap' 533: s SETITEM 534: N NONE 535: ] EMPTY_LIST 536: c GLOBAL 'operator itemgetter' 557: K BININT1 0 559: K BININT1 1 561: \\x86 TUPLE2 562: \\x85 TUPLE1 563: R REDUCE 564: a APPEND 565: } EMPTY_DICT 566: t TUPLE (MARK at 384) 567: R REDUCE 568: M BININT2 1024 571: e APPENDS (MARK at 371) 572: } EMPTY_DICT 573: t TUPLE (MARK at 101) 574: R REDUCE 575: . STOP  It looks like something strange is happening to operator.itemgetter:  536: c GLOBAL 'operator itemgetter' 557: K BININT1 0 559: K BININT1 1 561: \\x86 TUPLE2 562: \\x85 TUPLE1 563: R REDUCE  It's supposed to be constructed using two arguments, 0 and 1, but instead it looks like it's being constructed with (0, 1). To verify this:  import pickle s = \"coperator\\nitemgetter\\nK\\x00K\\x01\\x86\\x85R.\" op = pickle.loads(s) t = [1, 2, 2, 3] print op(t)  gives \"TypeError: list indices must be integers, not tuple\". If I leave off the extra TUPLE1 opcode, then this works as expected:  import pickle s = \"coperator\\nitemgetter\\nK\\x00K\\x01\\x86R.\" op = pickle.loads(s) t = [1, 2, 2, 3] print op(t)  gives (1, 2). I'm guessing this is a problem in the CloudPickle library, but it might take me a while to find and fix it.", "created": "2013-07-28T21:00:45.947+0000"}, {"author": "Andre Schumacher", "body": "Just an observation: the same error occurs with the cloudpickler from cloud-2.8.5 from 2013-08-21. If it's a problem with the library, maybe one should file a bug report there?", "created": "2013-08-27T14:31:34.097+0000"}, {"author": "Josh Rosen", "body": "Quick update: It looks like the Dill serialization library handles this case properly, but there are a couple of issues to work out before we can consider switching to it: https://mail-archives.apache.org/mod_mbox/spark-dev/201312.mbox/%3CCAOEPXP5hu-dhGnjQq=RYdT35G-eeEYPVopgqQdf2NFxRB7vA_g@mail.gmail.com%3E", "created": "2014-01-23T16:56:12.879+0000"}, {"author": "Harry Brundage", "body": "Hey Josh, looks like that issue mentioned in that thread has been fixed in Dill: https://github.com/uqfoundation/dill/issues/18. Anecdotally playing around with Dill I've found it a bit more friendly than Cloudpickle and as you mentioned the actively-maintained status of it is really encouraging. Would you mind if I tried to rebase your port to Dill on top of current master and opened a PR?", "created": "2014-05-03T14:20:52.012+0000"}, {"author": "Mark Baker", "body": "I began porting Pyspark to Python 3, but with my modest Python-fu, hit a wall at cloudpickle. Dill supports Python 3, so seems like a big win in that direction too.", "created": "2014-06-18T03:52:58.259+0000"}, {"author": "Davies Liu", "body": "This will be fixed by PR-1627[1] [1] https://github.com/apache/spark/pull/1627", "created": "2014-07-29T01:10:37.855+0000"}], "num_comments": 6, "text": "Issue: SPARK-791\nSummary: [pyspark] operator.getattr not serialized\nDescription: Using operator.itemgetter as a function in map seems to confuse the serialization process in pyspark. I'm using itemgetter to return tuples, which fails with a TypeError (details below). Using an equivalent lambda function returns the correct result. Use a test file:  echo 1,1 > test.txt  Then try mapping it to a tuple:  import csv sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(lambda l: (l[0],l[1])).first() Out[7]: ('1', '1')  But this does not work when using operator.itemgetter:  import operator sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first() # TypeError: list indices must be integers, not tuple  This is running with git master, commit 6d60fe571a405eb9306a2be1817901316a46f892 IPython 0.13.2 java version \"1.7.0_25\" Scala code runner version 2.9.1 Ubuntu 12.04 Full debug output:  In [9]: sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first() 13/07/04 16:19:49 INFO storage.MemoryStore: ensureFreeSpace(33632) called with curMem=201792, maxMem=339585269 13/07/04 16:19:49 INFO storage.MemoryStore: Block broadcast_6 stored as values to memory (estimated size 32.8 KB, free 323.6 MB) 13/07/04 16:19:49 INFO mapred.FileInputFormat: Total input paths to process : 1 13/07/04 16:19:49 INFO spark.SparkContext: Starting job: takePartition at NativeMethodAccessorImpl.java:-2 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Got job 4 (takePartition at NativeMethodAccessorImpl.java:-2) with 1 output partitions (allowLocal=true) 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Final stage: Stage 4 (PythonRDD at NativeConstructorAccessorImpl.java:-2) 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Parents of final stage: List() 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Missing parents: List() 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Computing the requested partition locally 13/07/04 16:19:49 INFO scheduler.DAGScheduler: Failed to run takePartition at NativeMethodAccessorImpl.java:-2 --------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-9-1fdb3e7a8ac7> in <module>() ----> 1 sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0,1)).first() /home/jim/src/spark/python/pyspark/rdd.pyc in first(self) 389 2 390 \"\"\" --> 391 return self.take(1)[0] 392 393 def saveAsTextFile(self, path): /home/jim/src/spark/python/pyspark/rdd.pyc in take(self, num) 372 items = [] 373 for partition in range(self._jrdd.splits().size()): --> 374 iterator = self.ctx._takePartition(self._jrdd.rdd(), partition) 375 # Each item in the iterator is a string, Python object, batch of 376 # Python objects. Regardless, it is sufficient to take `num` /home/jim/src/spark/python/lib/py4j0.7.egg/py4j/java_gateway.pyc in __call__(self, *args) 498 answer = self.gateway_client.send_command(command) 499 return_value = get_return_value(answer, self.gateway_client, --> 500 self.target_id, self.name) 501 502 for temp_arg in temp_args: /home/jim/src/spark/python/lib/py4j0.7.egg/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0}{1}{2}.\\n'. --> 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError( Py4JJavaError: An error occurred while calling z:spark.api.python.PythonRDD.takePartition. : spark.api.python.PythonException: Traceback (most recent call last): File \"/home/jim/src/spark/python/pyspark/worker.py\", line 53, in main for obj in func(split_index, iterator): File \"/home/jim/src/spark/python/pyspark/serializers.py\", line 24, in batched for item in iterator: TypeError: list indices must be integers, not tuple at spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:117) at spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:139) at spark.api.python.PythonRDD.compute(PythonRDD.scala:82) at spark.RDD.computeOrReadCheckpoint(RDD.scala:232) at spark.RDD.iterator(RDD.scala:221) at spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:423) at spark.scheduler.DAGScheduler$$anon$2.run(DAGScheduler.scala:410)\n\nComments (6):\n1. Josh Rosen: I think this is a problem with how operator.itemgetter is serialized, because  sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(lambda x: operator.itemgetter(0, 1)(x)).first()  works for me, while  sc.textFile(\"test.txt\").mapPartitions(csv.reader).map(operator.itemgetter(0, 1)).first()  gives the error that you saw. Interestingly, the example works fine if I only specify a single index, e.g. operator.itemgetter(0). To get a bit more insight into what's going on, I used pickletools.dis to log the pickled functions (specifically, I used pickletools.dis(pickletools.optimize(cloudpickle.dumps(func))) with the extra optimization added to remove unnecessary PUTs in order to make the output easier to read). For  sc.textFile(\"test.txt\").mapPartitions(csv.reader).cache().map(operator.itemgetter(0, 1)).first()  the second map function's pickled form disassembles into  0: \\x80 PROTO 2 2: ( MARK 3: c GLOBAL 'pyspark.cloudpickle _modules_to_main' 41: q BINPUT 0 43: ] EMPTY_LIST 44: U SHORT_BINSTRING 'pyspark.rdd' 57: q BINPUT 2 59: a APPEND 60: \\x85 TUPLE1 61: R REDUCE 62: 1 POP_MARK (MARK at 2) 63: c GLOBAL 'pyspark.cloudpickle _fill_function' 99: q BINPUT 4 101: ( MARK 102: c GLOBAL 'pyspark.cloudpickle _make_skel_func' 139: q BINPUT 5 141: c GLOBAL 'new code' 151: q BINPUT 6 153: ( MARK 154: K BININT1 2 156: K BININT1 2 158: K BININT1 4 160: K BININT1 19 162: U SHORT_BINSTRING 't\\x00\\x00\\x88\\x00\\x00|\\x00\\x00|\\x01\\x00\\x83\\x02\\x00\\x88\\x01\\x00\\x83\\x02\\x00S' 186: N NONE 187: \\x85 TUPLE1 188: U SHORT_BINSTRING 'batched' 197: q BINPUT 9 199: \\x85 TUPLE1 200: U SHORT_BINSTRING 'split' 207: q BINPUT 11 209: U SHORT_BINSTRING 'iterator' 219: q BINPUT 12 221: \\x86 TUPLE2 222: U SHORT_BINSTRING '/Users/joshrosen/Documents/spark/spark/python/pyspark/rdd.py' 284: U SHORT_BINSTRING 'batched_func' 298: M BININT2 725 301: U SHORT_BINSTRING '\\x00\\x01' 305: U SHORT_BINSTRING 'oldfunc' 314: U SHORT_BINSTRING 'batchSize' 325: \\x86 TUPLE2 326: ) EMPTY_TUPLE 327: t TUPLE (MARK at 153) 328: R REDUCE 329: K BININT1 2 331: } EMPTY_DICT 332: q BINPUT 22 334: \\x87 TUPLE3 335: R REDUCE 336: } EMPTY_DICT 337: h BINGET 9 339: c GLOBAL 'pyspark.serializers batched' 368: s SETITEM 369: N NONE 370: ] EMPTY_LIST 371: ( MARK 372: ( MARK 373: h BINGET 0 375: ] EMPTY_LIST 376: h BINGET 2 378: a APPEND 379: \\x85 TUPLE1 380: R REDUCE 381: 1 POP_MARK (MARK at 372) 382: h BINGET 4 384: ( MARK 385: h BINGET 5 387: h BINGET 6 389: ( MARK 390: K BININT1 2 392: K BININT1 2 394: K BININT1 3 396: K BININT1 19 398: U SHORT_BINSTRING 't\\x00\\x00\\x88\\x00\\x00|\\x01\\x00\\x83\\x02\\x00S' 413: N NONE 414: \\x85 TUPLE1 415: U SHORT_BINSTRING 'imap' 421: q BINPUT 32 423: \\x85 TUPLE1 424: h BINGET 11 426: h BINGET 12 428: \\x86 TUPLE2 429: U SHORT_BINSTRING '/Users/joshrosen/Documents/spark/spark/python/pyspark/rdd.py' 491: U SHORT_BINSTRING 'func' 497: K BININT1 87 499: U SHORT_BINSTRING '' 501: U SHORT_BINSTRING 'f' 504: \\x85 TUPLE1 505: ) EMPTY_TUPLE 506: t TUPLE (MARK at 389) 507: R REDUCE 508: K BININT1 1 510: h BINGET 22 512: \\x87 TUPLE3 513: R REDUCE 514: } EMPTY_DICT 515: h BINGET 32 517: c GLOBAL 'itertools imap' 533: s SETITEM 534: N NONE 535: ] EMPTY_LIST 536: c GLOBAL 'operator itemgetter' 557: K BININT1 0 559: K BININT1 1 561: \\x86 TUPLE2 562: \\x85 TUPLE1 563: R REDUCE 564: a APPEND 565: } EMPTY_DICT 566: t TUPLE (MARK at 384) 567: R REDUCE 568: M BININT2 1024 571: e APPENDS (MARK at 371) 572: } EMPTY_DICT 573: t TUPLE (MARK at 101) 574: R REDUCE 575: . STOP  It looks like something strange is happening to operator.itemgetter:  536: c GLOBAL 'operator itemgetter' 557: K BININT1 0 559: K BININT1 1 561: \\x86 TUPLE2 562: \\x85 TUPLE1 563: R REDUCE  It's supposed to be constructed using two arguments, 0 and 1, but instead it looks like it's being constructed with (0, 1). To verify this:  import pickle s = \"coperator\\nitemgetter\\nK\\x00K\\x01\\x86\\x85R.\" op = pickle.loads(s) t = [1, 2, 2, 3] print op(t)  gives \"TypeError: list indices must be integers, not tuple\". If I leave off the extra TUPLE1 opcode, then this works as expected:  import pickle s = \"coperator\\nitemgetter\\nK\\x00K\\x01\\x86R.\" op = pickle.loads(s) t = [1, 2, 2, 3] print op(t)  gives (1, 2). I'm guessing this is a problem in the CloudPickle library, but it might take me a while to find and fix it.\n2. Andre Schumacher: Just an observation: the same error occurs with the cloudpickler from cloud-2.8.5 from 2013-08-21. If it's a problem with the library, maybe one should file a bug report there?\n3. Josh Rosen: Quick update: It looks like the Dill serialization library handles this case properly, but there are a couple of issues to work out before we can consider switching to it: https://mail-archives.apache.org/mod_mbox/spark-dev/201312.mbox/%3CCAOEPXP5hu-dhGnjQq=RYdT35G-eeEYPVopgqQdf2NFxRB7vA_g@mail.gmail.com%3E\n4. Harry Brundage: Hey Josh, looks like that issue mentioned in that thread has been fixed in Dill: https://github.com/uqfoundation/dill/issues/18. Anecdotally playing around with Dill I've found it a bit more friendly than Cloudpickle and as you mentioned the actively-maintained status of it is really encouraging. Would you mind if I tried to rebase your port to Dill on top of current master and opened a PR?\n5. Mark Baker: I began porting Pyspark to Python 3, but with my modest Python-fu, hit a wall at cloudpickle. Dill supports Python 3, so seems like a big win in that direction too.\n6. Davies Liu: This will be fixed by PR-1627[1] [1] https://github.com/apache/spark/pull/1627", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "adcc9f2aca840630aeba5e3be272ba34", "issue_key": "SPARK-792", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PairRDDFunctions should expect Product2 instead of Tuple2", "description": "PairRDDFunctions expect an RDD of Tuple2 (key, value) pair. If we change it to expect Product2, we can apply for optimizations such as reducing the number of temporary objects allocated and a faster runtime. Product2 is just a trait in Scala, and users can define classes that extend Product2. Two use cases in particular I have in mind: 1. In MappedRDD or MappedPartitionRDD, the iterator could reuse a concrete Product2 object and always return that; but in every next() call, the iterator updates the content of the Product2. 2. In some Spark cases (such as Shark, GraphX), the key field is only used to partition the data. A concrete implementation of Product2 can have the key field (_1) marked as transient, and thus reducing the amount of shuffle data sent across the network between map and reduce stages.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-07-05T14:11:50.000+0000", "updated": "2017-09-22T11:00:20.000+0000", "resolved": "2013-12-07T14:06:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "This was added in https://github.com/mesos/spark/pull/847, which was included in 0.8.", "created": "2013-12-07T14:06:39.217+0000"}, {"author": "Sergei Lebedev", "body": "It looks like the change in the title was never implemented. Does anyone know why?", "created": "2017-09-22T11:00:20.798+0000"}], "num_comments": 2, "text": "Issue: SPARK-792\nSummary: PairRDDFunctions should expect Product2 instead of Tuple2\nDescription: PairRDDFunctions expect an RDD of Tuple2 (key, value) pair. If we change it to expect Product2, we can apply for optimizations such as reducing the number of temporary objects allocated and a faster runtime. Product2 is just a trait in Scala, and users can define classes that extend Product2. Two use cases in particular I have in mind: 1. In MappedRDD or MappedPartitionRDD, the iterator could reuse a concrete Product2 object and always return that; but in every next() call, the iterator updates the content of the Product2. 2. In some Spark cases (such as Shark, GraphX), the key field is only used to partition the data. A concrete implementation of Product2 can have the key field (_1) marked as transient, and thus reducing the amount of shuffle data sent across the network between map and reduce stages.\n\nComments (2):\n1. Josh Rosen: This was added in https://github.com/mesos/spark/pull/847, which was included in 0.8.\n2. Sergei Lebedev: It looks like the change in the title was never implemented. Does anyone know why?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "8c5c51bacaf6f22c70c0507d89096047", "issue_key": "SPARK-793", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "DAGScheduler doesn't release ActiveJob inside idToActiveJob", "description": "In DAGScheduler, idToActiveJob is a HashMap to Store relationship between JobId and Active Job. But we found that there's only places that adding and reading ActiveJob from it, but never remove. 1. val idToActiveJob = new HashMap[Int, ActiveJob] 2. idToActiveJob(runId) = job 3. val properties = idToActiveJob(stage.priority).properties This will cause a memory leak in Master, when we call collect() method for multipal iterations. All ActiveJob will be kept in memory and can't not be return even with full GC, which will finally cause Master to die, no matter in Standalone cluster or Yarn cluster.", "reporter": "Andy Huang", "assignee": "wu zeming", "created": "2013-07-06T02:55:53.000+0000", "updated": "2014-01-23T23:37:11.000+0000", "resolved": "2014-01-23T23:37:11.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andy Huang", "body": "Here are the log of our job. On iteration 5, App master on Yarn lost... -------> Iteration 1 is running!! 194.375: [Full GC [PSYoungGen: 554284K->360219K(700544K)] [PSOldGen: 1527886K->1593407K(1768704K)] 2082171K->1953627K(2469248K) [PSPermGen: 47015K->47015K(55936K)], 6.5237410 secs] [Times: user=5.97 sys=0.55, real=6.52 secs] -----> Iteration 1 finished, loss is: 0.6931471805593069 381.753: [Full GC [PSYoungGen: 603580K->251971K(1054464K)] [PSOldGen: 3174220K->3352512K(3683264K)] 3777800K->3604483K(4737728K) [PSPermGen: 47161K->47161K(50880K)], 8.2145200 secs] [Times: user=7.65 sys=0.56, real=8.22 secs] -----> Iteration 2 finished, loss is: 0.6516189398401027 495.914: [Full GC [PSYoungGen: 547426K->348782K(1445440K)] [PSOldGen: 4412187K->4412351K(4831552K)] 4959614K->4761134K(6276992K) [PSPermGen: 47200K->47197K(49664K)], 13.8350190 secs] [Times: user=13.30 sys=0.51, real=13.83 secs] -----> Iteration 3 finished, loss is: 0.6219949536858732 -----> Iteration 4 finished, loss is: 0.5995576249445631 652.047: [Full GC [PSYoungGen: 1205541K->1053760K(1773120K)] [PSOldGen: 6138589K->6107303K(6558720K)] 7344130K->7161064K(8331840K) [PSPermGen: 47213K->47210K(48512K)], 20.3188240 secs] [Times: user=19.84 sys=0.45, real=20.31 secs] -------> Iteration 5 is running!! 1164.810: [Full GC [PSYoungGen: 600459K->597840K(1806080K)] [PSOldGen: 7233536K->7233536K(7233536K)] 7833995K->7831376K(9039616K) [PSPermGen: 47268K->47268K(47552K)], 15.7865770 secs] [Times: user=15.27 sys=0.48, real=15.79 secs]", "created": "2013-07-06T02:59:41.928+0000"}, {"author": "Josh Rosen", "body": "This was fixed in https://github.com/mesos/spark/pull/681", "created": "2014-01-23T23:37:11.592+0000"}], "num_comments": 2, "text": "Issue: SPARK-793\nSummary: DAGScheduler doesn't release ActiveJob inside idToActiveJob\nDescription: In DAGScheduler, idToActiveJob is a HashMap to Store relationship between JobId and Active Job. But we found that there's only places that adding and reading ActiveJob from it, but never remove. 1. val idToActiveJob = new HashMap[Int, ActiveJob] 2. idToActiveJob(runId) = job 3. val properties = idToActiveJob(stage.priority).properties This will cause a memory leak in Master, when we call collect() method for multipal iterations. All ActiveJob will be kept in memory and can't not be return even with full GC, which will finally cause Master to die, no matter in Standalone cluster or Yarn cluster.\n\nComments (2):\n1. Andy Huang: Here are the log of our job. On iteration 5, App master on Yarn lost... -------> Iteration 1 is running!! 194.375: [Full GC [PSYoungGen: 554284K->360219K(700544K)] [PSOldGen: 1527886K->1593407K(1768704K)] 2082171K->1953627K(2469248K) [PSPermGen: 47015K->47015K(55936K)], 6.5237410 secs] [Times: user=5.97 sys=0.55, real=6.52 secs] -----> Iteration 1 finished, loss is: 0.6931471805593069 381.753: [Full GC [PSYoungGen: 603580K->251971K(1054464K)] [PSOldGen: 3174220K->3352512K(3683264K)] 3777800K->3604483K(4737728K) [PSPermGen: 47161K->47161K(50880K)], 8.2145200 secs] [Times: user=7.65 sys=0.56, real=8.22 secs] -----> Iteration 2 finished, loss is: 0.6516189398401027 495.914: [Full GC [PSYoungGen: 547426K->348782K(1445440K)] [PSOldGen: 4412187K->4412351K(4831552K)] 4959614K->4761134K(6276992K) [PSPermGen: 47200K->47197K(49664K)], 13.8350190 secs] [Times: user=13.30 sys=0.51, real=13.83 secs] -----> Iteration 3 finished, loss is: 0.6219949536858732 -----> Iteration 4 finished, loss is: 0.5995576249445631 652.047: [Full GC [PSYoungGen: 1205541K->1053760K(1773120K)] [PSOldGen: 6138589K->6107303K(6558720K)] 7344130K->7161064K(8331840K) [PSPermGen: 47213K->47210K(48512K)], 20.3188240 secs] [Times: user=19.84 sys=0.45, real=20.31 secs] -------> Iteration 5 is running!! 1164.810: [Full GC [PSYoungGen: 600459K->597840K(1806080K)] [PSOldGen: 7233536K->7233536K(7233536K)] 7833995K->7831376K(9039616K) [PSPermGen: 47268K->47268K(47552K)], 15.7865770 secs] [Times: user=15.27 sys=0.48, real=15.79 secs]\n2. Josh Rosen: This was fixed in https://github.com/mesos/spark/pull/681", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "8d199c27f588f98b628f60fc0822f3e8", "issue_key": "SPARK-794", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Remove sleep() in ClusterScheduler.stop", "description": "This temporary change made a while back slows down the unit tests quite a bit.", "reporter": "Matei Alexandru Zaharia", "assignee": "Brennon York", "created": "2013-07-06T17:09:35.000+0000", "updated": "2020-05-17T17:47:12.000+0000", "resolved": "2015-02-26T22:08:38.000+0000", "labels": [], "components": ["Scheduler", "Spark Core"], "comments": [{"author": "Andrew Ash", "body": "I don't see a {{ClusterScheduler}} class on master -- was that refactored to {{TaskSchedulerImpl}}? There is still a {{Thread.sleep(1000)}} in that class's {{stop()}} though, which may be what this ticket refers to: https://github.com/apache/spark/blob/1df05a40ebf3493b0aff46d18c0f30d2d5256c7b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L399", "created": "2014-11-14T10:12:48.080+0000"}, {"author": "Apache Spark", "body": "User 'brennonyork' has created a pull request for this issue: https://github.com/apache/spark/pull/3851", "created": "2014-12-31T00:04:02.263+0000"}, {"author": "Josh Rosen", "body": "It looks like this was added by [~mridulm@yahoo-inc.com] all the way back in 2013: https://github.com/apache/spark/commit/d90d2af1036e909f81cf77c85bfe589993c4f9f3#diff-4e188f32951dc989d97fa7577858bc7cR397", "created": "2014-12-31T22:24:18.317+0000"}, {"author": "Josh Rosen", "body": "I'm merged [~boyork]'s PR into master and am going to monitor the pull request builder for a bit to see whether this change introduced any flakiness. If not, I'd consider backporting this change into other branches in order to speed up the tests.", "created": "2015-01-04T20:42:38.663+0000"}, {"author": "Brennon York", "body": "[~joshrosen] How is this PR holding up? I haven't seen any issues on the dev board. Think we can close this JIRA ticket? Trying to help prune the JIRA tree :)", "created": "2015-01-26T19:41:05.862+0000"}, {"author": "Brennon York", "body": "[~srowen] [~joshrosen] bump on this. Would assume things are stable with the removal of the sleep method, but want to double check. Thinking we can close this ticket out.", "created": "2015-02-23T22:41:37.893+0000"}, {"author": "Sean R. Owen", "body": "[~joshrosen] would it be OK if I tried my hand at backporting into 1.2? further?", "created": "2015-02-24T10:53:21.431+0000"}, {"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/4793", "created": "2015-02-26T19:43:59.753+0000"}, {"author": "Sean R. Owen", "body": "Backported to 1.2", "created": "2015-02-26T22:08:29.271+0000"}], "num_comments": 9, "text": "Issue: SPARK-794\nSummary: Remove sleep() in ClusterScheduler.stop\nDescription: This temporary change made a while back slows down the unit tests quite a bit.\n\nComments (9):\n1. Andrew Ash: I don't see a {{ClusterScheduler}} class on master -- was that refactored to {{TaskSchedulerImpl}}? There is still a {{Thread.sleep(1000)}} in that class's {{stop()}} though, which may be what this ticket refers to: https://github.com/apache/spark/blob/1df05a40ebf3493b0aff46d18c0f30d2d5256c7b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L399\n2. Apache Spark: User 'brennonyork' has created a pull request for this issue: https://github.com/apache/spark/pull/3851\n3. Josh Rosen: It looks like this was added by [~mridulm@yahoo-inc.com] all the way back in 2013: https://github.com/apache/spark/commit/d90d2af1036e909f81cf77c85bfe589993c4f9f3#diff-4e188f32951dc989d97fa7577858bc7cR397\n4. Josh Rosen: I'm merged [~boyork]'s PR into master and am going to monitor the pull request builder for a bit to see whether this change introduced any flakiness. If not, I'd consider backporting this change into other branches in order to speed up the tests.\n5. Brennon York: [~joshrosen] How is this PR holding up? I haven't seen any issues on the dev board. Think we can close this JIRA ticket? Trying to help prune the JIRA tree :)\n6. Brennon York: [~srowen] [~joshrosen] bump on this. Would assume things are stable with the removal of the sleep method, but want to double check. Thinking we can close this ticket out.\n7. Sean R. Owen: [~joshrosen] would it be OK if I tried my hand at backporting into 1.2? further?\n8. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/4793\n9. Sean R. Owen: Backported to 1.2", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.056751"}}
{"id": "83b49b78f5f0218e3a56f2368fe44071", "issue_key": "SPARK-795", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Numerous tests failing in maven build", "description": "After the merge of 1ffadb2d9e87169ccc406cd34dab6bd7beda70f1 pwendell/ui-updates, numerous tests are failing when run from maven (e.g. mvn -Phadoop2 test.) First test to fail is in core PartitionSuite, and is typical of the other test failures. Looks like a classpath issue... Run starting. Expected test count is: 245 DiscoverySuite: PartitioningSuite: *** RUN ABORTED *** java.lang.NoClassDefFoundError: scala/util/Try$ at spark.ui.JettyUtils$.connect$1(JettyUtils.scala:101) at spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:113) at spark.ui.SparkUI.bind(SparkUI.scala:33) at spark.SparkContext.<init>(SparkContext.scala:108) at spark.SharedSparkContext$class.beforeAll(SharedSparkContext.scala:14) at spark.PartitioningSuite.beforeAll(PartitioningSuite.scala:9) at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:150) at spark.PartitioningSuite.beforeAll(PartitioningSuite.scala:9) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:211) at spark.PartitioningSuite.run(PartitioningSuite.scala:9) ... Cause: java.lang.ClassNotFoundException: scala.util.Try$ at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:423) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:356) at spark.ui.JettyUtils$.connect$1(JettyUtils.scala:101) at spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:113) at spark.ui.SparkUI.bind(SparkUI.scala:33) ... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM .......................... SUCCESS [1.200s] [INFO] Spark Project Core ................................ FAILURE [8.966s] [INFO] Spark Project Bagel ............................... SKIPPED [INFO] Spark Project Streaming ........................... SKIPPED [INFO] Spark Project Examples ............................ SKIPPED [INFO] Spark Project REPL ................................ SKIPPED [INFO] Spark Project REPL binary packaging ............... SKIPPED [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------", "reporter": "Mark Hamstra", "assignee": "Mark Hamstra", "created": "2013-07-08T11:16:51.000+0000", "updated": "2013-12-07T14:23:28.000+0000", "resolved": "2013-12-07T14:23:28.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Mark Hamstra", "body": "See https://github.com/mesos/spark/pull/688", "created": "2013-07-08T16:00:28.782+0000"}], "num_comments": 1, "text": "Issue: SPARK-795\nSummary: Numerous tests failing in maven build\nDescription: After the merge of 1ffadb2d9e87169ccc406cd34dab6bd7beda70f1 pwendell/ui-updates, numerous tests are failing when run from maven (e.g. mvn -Phadoop2 test.) First test to fail is in core PartitionSuite, and is typical of the other test failures. Looks like a classpath issue... Run starting. Expected test count is: 245 DiscoverySuite: PartitioningSuite: *** RUN ABORTED *** java.lang.NoClassDefFoundError: scala/util/Try$ at spark.ui.JettyUtils$.connect$1(JettyUtils.scala:101) at spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:113) at spark.ui.SparkUI.bind(SparkUI.scala:33) at spark.SparkContext.<init>(SparkContext.scala:108) at spark.SharedSparkContext$class.beforeAll(SharedSparkContext.scala:14) at spark.PartitioningSuite.beforeAll(PartitioningSuite.scala:9) at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:150) at spark.PartitioningSuite.beforeAll(PartitioningSuite.scala:9) at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:211) at spark.PartitioningSuite.run(PartitioningSuite.scala:9) ... Cause: java.lang.ClassNotFoundException: scala.util.Try$ at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:423) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:356) at spark.ui.JettyUtils$.connect$1(JettyUtils.scala:101) at spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:113) at spark.ui.SparkUI.bind(SparkUI.scala:33) ... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM .......................... SUCCESS [1.200s] [INFO] Spark Project Core ................................ FAILURE [8.966s] [INFO] Spark Project Bagel ............................... SKIPPED [INFO] Spark Project Streaming ........................... SKIPPED [INFO] Spark Project Examples ............................ SKIPPED [INFO] Spark Project REPL ................................ SKIPPED [INFO] Spark Project REPL binary packaging ............... SKIPPED [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------\n\nComments (1):\n1. Mark Hamstra: See https://github.com/mesos/spark/pull/688", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.056751"}}
{"id": "fb6b33cb64b71979e0e0919e4ea01ef2", "issue_key": "SPARK-796", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Jobs are always marked as SUCCEEDED even it's actually failed on Yarn.", "description": "When I submit a spark job that will throw an exception on yarn, the job is marked as SUCCEEDED at FinalStatus. This will mislead some users.", "reporter": "wu zeming", "assignee": "wu zeming", "created": "2013-07-08T20:07:51.000+0000", "updated": "2013-08-06T23:13:23.000+0000", "resolved": "2013-08-06T23:13:23.000+0000", "labels": [], "components": ["Deploy"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-796\nSummary: Jobs are always marked as SUCCEEDED even it's actually failed on Yarn.\nDescription: When I submit a spark job that will throw an exception on yarn, the job is marked as SUCCEEDED at FinalStatus. This will mislead some users.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.056751"}}
{"id": "30050e0666a5160c7aa708e0423d5f39", "issue_key": "SPARK-797", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ML failures if libfortran is not installed", "description": "When running tests with the new ML package, I get:  [info] spark.SparkException: Job failed: Task 5.0:0 failed more than 4 times; aborting job java.lang.UnsatisfiedLinkError: org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V  Shivaram mentioned I should install a Fortran package:  sudo apt-get install libgfortran3  And it worked after that. We should figure out exactly what the dependencies are here list in the build instructions that they exist. Also, if these are runtime dependencies we should mention them clearly in the main Spark docs.", "reporter": "Patrick Wendell", "assignee": "Shivaram Venkataraman", "created": "2013-07-08T21:00:53.000+0000", "updated": "2015-01-23T12:47:11.000+0000", "resolved": "2015-01-23T12:47:10.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Hey [~shivaram] this would be something good to mention if you are going to add an MLLib doc.", "created": "2013-08-10T16:30:19.144+0000"}, {"author": "Andy Konwinski", "body": "Yeah, I also just saw this now when I tried building/testing Spark master on the Ubuntu Jenkins node (it was set to only build on the CentOS boxes before) - see https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT-Hadoop2/223/console It's a little bit of a shame to require this extra dependency, no?", "created": "2013-08-17T16:41:09.449+0000"}, {"author": "Andy Konwinski", "body": "BTW, here is the snippet of output including the point of failure: ... [info] RidgeRegressionSuite: -- org.jblas ERROR Couldn't load copied link file: java.lang.UnsatisfiedLinkError: /tmp/jblas6680700493603641651libjblas_arch_flavor.so: libgfortran.so.3: cannot open shared object file: No such file or directory. On Linux 64bit, you need additional support libraries. You need to install libgfortran3. For example for debian or Ubuntu, type \"sudo apt-get install libgfortran3\" For more information, see https://github.com/mikiobraun/jblas/wiki/Missing-Libraries [info] - multi-collinear variables *** FAILED *** [info] spark.SparkException: Job failed: Task 5.0:0 failed more than 4 times; aborting job java.lang.UnsatisfiedLinkError: org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V [info] at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:735) [info] at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:733) [info] at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) [info] at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) [info] at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:733) [info] at spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:363) [info] at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:425) [info] at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:135) [info] ...", "created": "2013-08-17T16:42:02.705+0000"}, {"author": "Shivaram Venkataraman", "body": "It is a bit of a shame -- but short of adding a layer of indirection over JBLAS I am not sure if there is an easy way to avoid this. FWIW, this shouldn't affect anybody not using ML Lib and the error message is pretty informative on what needs to be done. Any suggestions other than improving documentation and/or FAQs ?", "created": "2013-08-18T12:46:21.285+0000"}, {"author": "Patrick McFadin", "body": "This is the kind of thing that would be good to list in a \"requirements\" or \"dependencies\" section. We don't really have that for Spark right now, but if we add MLBase documentation, it would be good to list it up-front at the very beginning of the MLBase introduction.", "created": "2013-08-18T17:06:25.436+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yeah, let's add it when we add a doc page on MLlib. I don't think it's worth switching matrix libraries -- we'd have to try linking to some native code anyway and JBLAS went to a lot of trouble to make this easy.", "created": "2013-08-18T20:07:48.397+0000"}, {"author": "Matei Alexandru Zaharia", "body": "BTW, we may also want to make it \"fail early\" by trying to do a JBLAS operation in the driver and throwing a more meaningful exception if it fails. We can add a simple MLUtils.testJblas() call in the training objects.", "created": "2013-08-18T20:09:21.322+0000"}, {"author": "Shivaram Venkataraman", "body": "There is a `loadLibrariesAndCheckErrors()` call in JBLAS that we can use. https://github.com/mikiobraun/jblas/blob/master/src/main/java/org/jblas/NativeBlas.java#L77 Do we want to do this in the driver ? Programs which don't use ML Lib should just work without the libgfortran, so adding it to just the MLLib objects might be better.", "created": "2013-08-18T20:50:54.714+0000"}, {"author": "Patrick McFadin", "body": "Okay this is at least discussed now in the docs. It would be better to do some detection of this, especially for the tests.", "created": "2013-09-09T22:22:29.009+0000"}, {"author": "Sean R. Owen", "body": "I suggest this can be resolved. The dependency isn't going to be removed, and it's documented. I can't figure out where to put a check for these libs since not even every app that imports MLlib will end up touching jblas. A warning is a little nicer but the result is still that the program can't continue.", "created": "2015-01-23T12:47:11.969+0000"}], "num_comments": 10, "text": "Issue: SPARK-797\nSummary: ML failures if libfortran is not installed\nDescription: When running tests with the new ML package, I get:  [info] spark.SparkException: Job failed: Task 5.0:0 failed more than 4 times; aborting job java.lang.UnsatisfiedLinkError: org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V  Shivaram mentioned I should install a Fortran package:  sudo apt-get install libgfortran3  And it worked after that. We should figure out exactly what the dependencies are here list in the build instructions that they exist. Also, if these are runtime dependencies we should mention them clearly in the main Spark docs.\n\nComments (10):\n1. Patrick McFadin: Hey [~shivaram] this would be something good to mention if you are going to add an MLLib doc.\n2. Andy Konwinski: Yeah, I also just saw this now when I tried building/testing Spark master on the Ubuntu Jenkins node (it was set to only build on the CentOS boxes before) - see https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT-Hadoop2/223/console It's a little bit of a shame to require this extra dependency, no?\n3. Andy Konwinski: BTW, here is the snippet of output including the point of failure: ... [info] RidgeRegressionSuite: -- org.jblas ERROR Couldn't load copied link file: java.lang.UnsatisfiedLinkError: /tmp/jblas6680700493603641651libjblas_arch_flavor.so: libgfortran.so.3: cannot open shared object file: No such file or directory. On Linux 64bit, you need additional support libraries. You need to install libgfortran3. For example for debian or Ubuntu, type \"sudo apt-get install libgfortran3\" For more information, see https://github.com/mikiobraun/jblas/wiki/Missing-Libraries [info] - multi-collinear variables *** FAILED *** [info] spark.SparkException: Job failed: Task 5.0:0 failed more than 4 times; aborting job java.lang.UnsatisfiedLinkError: org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V [info] at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:735) [info] at spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:733) [info] at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) [info] at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) [info] at spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:733) [info] at spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:363) [info] at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:425) [info] at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:135) [info] ...\n4. Shivaram Venkataraman: It is a bit of a shame -- but short of adding a layer of indirection over JBLAS I am not sure if there is an easy way to avoid this. FWIW, this shouldn't affect anybody not using ML Lib and the error message is pretty informative on what needs to be done. Any suggestions other than improving documentation and/or FAQs ?\n5. Patrick McFadin: This is the kind of thing that would be good to list in a \"requirements\" or \"dependencies\" section. We don't really have that for Spark right now, but if we add MLBase documentation, it would be good to list it up-front at the very beginning of the MLBase introduction.\n6. Matei Alexandru Zaharia: Yeah, let's add it when we add a doc page on MLlib. I don't think it's worth switching matrix libraries -- we'd have to try linking to some native code anyway and JBLAS went to a lot of trouble to make this easy.\n7. Matei Alexandru Zaharia: BTW, we may also want to make it \"fail early\" by trying to do a JBLAS operation in the driver and throwing a more meaningful exception if it fails. We can add a simple MLUtils.testJblas() call in the training objects.\n8. Shivaram Venkataraman: There is a `loadLibrariesAndCheckErrors()` call in JBLAS that we can use. https://github.com/mikiobraun/jblas/blob/master/src/main/java/org/jblas/NativeBlas.java#L77 Do we want to do this in the driver ? Programs which don't use ML Lib should just work without the libgfortran, so adding it to just the MLLib objects might be better.\n9. Patrick McFadin: Okay this is at least discussed now in the docs. It would be better to do some detection of this, especially for the tests.\n10. Sean R. Owen: I suggest this can be resolved. The dependency isn't going to be removed, and it's documented. I can't figure out where to put a check for these libs since not even every app that imports MLlib will end up touching jblas. A warning is a little nicer but the result is still that the program can't continue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.056751"}}
{"id": "4051a2ec4a7a0e592ff1fcad847de32f", "issue_key": "SPARK-798", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "AMI: ami-530f7a3a and Mesos", "description": "Hi, I have some strange problems after new version of AMI out. The problem is, that when i create a Mesos cluster, i can't use it. I mean, the spark-console is frozen most of the time. Even if it is not frozen, the scheduled tasks are frozen. Steps to reproduce: 1) Start cluster: ./spark-ec2 -s 1 -w 200 -i [identity] -k [key-pair] --cluster-type=mesos launch spark-aalbul 2) SSH to the master node 3) go to \"spark\" dir 4) Execute: MASTER=`cat ~/spark-ec2/cluster-url` ./spark-shell Problems: 1) The most recent problem is that spark shell unable to start like this:  Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 0.7.2 /_/ Using Scala version 2.9.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_25) Initializing interpreter... 13/07/10 12:14:11 INFO server.Server: jetty-7.6.8.v20121106 13/07/10 12:14:11 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:33601 Creating SparkContext... 13/07/10 12:14:21 INFO slf4j.Slf4jEventHandler: Slf4jEventHandler started 13/07/10 12:14:22 INFO spark.SparkEnv: Registering BlockManagerMaster 13/07/10 12:14:22 INFO storage.MemoryStore: MemoryStore started with capacity 3.8 GB. 13/07/10 12:14:22 INFO storage.DiskStore: Created local directory at /mnt/spark/spark-local-20130710121422-61da 13/07/10 12:14:22 INFO storage.DiskStore: Created local directory at /mnt2/spark/spark-local-20130710121422-07d2 13/07/10 12:14:22 INFO network.ConnectionManager: Bound socket to port 49473 with id = ConnectionManagerId(ip-10-46-37-82.ec2.internal,49473) 13/07/10 12:14:22 INFO storage.BlockManagerMaster: Trying to register BlockManager 13/07/10 12:14:22 INFO storage.BlockManagerMaster: Registered BlockManager 13/07/10 12:14:22 INFO server.Server: jetty-7.6.8.v20121106 13/07/10 12:14:22 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:50518 13/07/10 12:14:22 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.46.37.82:50518 13/07/10 12:14:22 INFO spark.SparkEnv: Registering MapOutputTracker 13/07/10 12:14:22 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-6d5266c8-c9f8-4db6-958c-e4791bd8a81d 13/07/10 12:14:22 INFO server.Server: jetty-7.6.8.v20121106 13/07/10 12:14:22 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:47677 13/07/10 12:14:22 INFO io.IoWorker: IoWorker thread 'spray-io-worker-0' started 13/07/10 12:14:23 INFO server.HttpServer: akka://spark/user/BlockManagerHTTPServer started on /0.0.0.0:35347 13/07/10 12:14:23 INFO storage.BlockManagerUI: Started BlockManager web UI at http://ip-10-46-37-82.ec2.internal:35347  When i execute jstack on this process i see that one of the threads is trying to load mesos native library:  \"main\" prio=10 tid=0x00007fcfc800c000 nid=0x761 runnable [0x00007fcfcefd5000] java.lang.Thread.State: RUNNABLE at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1957) - locked <0x000000077fcb4fb8> (a java.util.Vector) - locked <0x000000077fd2b348> (a java.util.Vector) at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1882) at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1843) at java.lang.Runtime.load0(Runtime.java:795) - locked <0x000000077fcb66d8> (a java.lang.Runtime) at java.lang.System.load(System.java:1061) at org.apache.mesos.MesosNativeLibrary.load(MesosNativeLibrary.java:38) at spark.executor.MesosExecutorBackend$.main(MesosExecutorBackend.scala:73) at spark.executor.MesosExecutorBackend.main(MesosExecutorBackend.scala)  2) Scheduled task do not want to finish. Even when the console is started (rare case) i see this:  Spark context available as sc. Type in expressions to have them evaluated. Type :help for more information. scala> sc.parallelize(List(1,2,3)) res0: spark.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:13 scala> res0.collect() 13/07/11 09:06:59 INFO spark.SparkContext: Starting job: collect at <console>:15 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Got job 0 (collect at <console>:15) with 8 output partitions (allowLocal=false) 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Final stage: Stage 0 (parallelize at <console>:13) 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Parents of final stage: List() 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Missing parents: List() 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at <console>:13), which has no missing parents 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Submitting 8 missing tasks from Stage 0 (ParallelCollectionRDD[0] at parallelize at <console>:13) 13/07/11 09:06:59 INFO cluster.ClusterScheduler: Adding task set 0.0 with 8 tasks 13/07/11 09:06:59 INFO cluster.TaskSetManager: Starting task 0.0:0 as TID 0 on executor 201307110852-2871257866-5050-2301-0: ip-10-170-21-54.ec2.internal (preferred) 13/07/11 09:07:00 INFO cluster.TaskSetManager: Serialized task 0.0:0 as 975 bytes in 141 ms 13/07/11 09:07:00 INFO cluster.TaskSetManager: Starting task 0.0:1 as TID 1 on executor 201307110852-2871257866-5050-2301-0: ip-10-170-21-54.ec2.internal (preferred) 13/07/11 09:07:00 INFO cluster.TaskSetManager: Serialized task 0.0:1 as 975 bytes in 0 ms  Jstack tell me the same thing. It is trying to load Mesos native library and stuck with it. BTW: We downgraded to the previous version of AMI (unfortunately, i do not remember it's id), updated Java to 1.7 and spark + shark and everything is working like a charm", "reporter": "Alexander Albul", "assignee": null, "created": "2013-07-11T02:21:15.000+0000", "updated": "2014-11-06T17:37:46.000+0000", "resolved": "2014-11-06T17:37:46.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Sergey Vladimirov", "body": "Hi, I have the same issue, spark hang up on mesos library loading. mesos tests show it [ RUN ] SampleFrameworks.JavaFramework", "created": "2013-07-11T02:52:48.658+0000"}, {"author": "Bach Bui", "body": "I think it is the problem of libmesos being compile with different java versions. One data point is when I copy the libmesos on the old AMI to the new one, I was able to have spark running as normal.", "created": "2013-07-16T08:01:54.142+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Hey, so which libmesos did you copy to get it to wkr? I've tried rebuilding Mesos with JDK 1.7 on this AMI and unfortunately the rebuilt version still doesn't work.", "created": "2013-07-16T10:15:08.316+0000"}, {"author": "Hai-Anh Trinh", "body": "This one is surely a critical / blocker issue, no?", "created": "2013-07-16T19:45:35.160+0000"}, {"author": "Matei Alexandru Zaharia", "body": "It's an important issue, but because the default for the AMI is the Spark standalone mode instead of Mesos, most people are still able to use it fine. The other problem is that if the issue is Mesos 0.9 not working against Java 1.7 (which we recently had to upgrade the AMI to for Shark), we don't really have a choice for the Spark 0.7 branch; we can't increase the Mesos version we depend on in a minor release. We will definitely fix this in future releases if possible, or in a new version of the 0.7 AMI if it turns out that it is possible to build Mesos with this JDK (maybe I've just missed some things when I tried it). For Spark 0.8, we'll hopefully depend on Mesos 0.12 or later, and this should no longer be an issue. Also, the Mesos project will soon start making its own AMIs where they make sure that everything works together.", "created": "2013-07-16T19:50:39.119+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I guess one other option is to make the Spark 0.7 on the AMI build against Mesos 0.10 instead of 0.9 (i.e. make it different from the official package); that would also be fine if people are okay with it. But I first want to understand whether there's an older version of the Mesos library that works on JDK 1.7, or some build configuration I've missed. When I build Mesos against JDK 1.7 on the AMI and run `make check`, I also see Mesos's own Java unit test failing (hanging forever).", "created": "2013-07-16T19:53:37.191+0000"}, {"author": "Bach Bui", "body": "Matei, sorry for late reply. The libmesos that I used is the one from an older version of AMI (the AMI that is one version older than the current one). Unfortunately, I don't remember its number. I believe libmesos.so in that system is compiled with JDK1.6.", "created": "2013-07-30T07:46:38.178+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Okay, got it. Just FYI, we worked with the Mesos team to make Spark run on newer Mesos versions (turns out we just had to bump up the dependency and it will work fine) and I'm planning to update our Mesos dependency in 0.7.4 as well as 0.8. So hopefully we'll have functioning Mesos soon.", "created": "2013-07-30T12:12:21.019+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This is now about a pretty old AMI, so I'll close it. New versions of Spark use newer versions of Mesos.", "created": "2014-11-06T17:37:46.722+0000"}], "num_comments": 9, "text": "Issue: SPARK-798\nSummary: AMI: ami-530f7a3a and Mesos\nDescription: Hi, I have some strange problems after new version of AMI out. The problem is, that when i create a Mesos cluster, i can't use it. I mean, the spark-console is frozen most of the time. Even if it is not frozen, the scheduled tasks are frozen. Steps to reproduce: 1) Start cluster: ./spark-ec2 -s 1 -w 200 -i [identity] -k [key-pair] --cluster-type=mesos launch spark-aalbul 2) SSH to the master node 3) go to \"spark\" dir 4) Execute: MASTER=`cat ~/spark-ec2/cluster-url` ./spark-shell Problems: 1) The most recent problem is that spark shell unable to start like this:  Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 0.7.2 /_/ Using Scala version 2.9.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_25) Initializing interpreter... 13/07/10 12:14:11 INFO server.Server: jetty-7.6.8.v20121106 13/07/10 12:14:11 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:33601 Creating SparkContext... 13/07/10 12:14:21 INFO slf4j.Slf4jEventHandler: Slf4jEventHandler started 13/07/10 12:14:22 INFO spark.SparkEnv: Registering BlockManagerMaster 13/07/10 12:14:22 INFO storage.MemoryStore: MemoryStore started with capacity 3.8 GB. 13/07/10 12:14:22 INFO storage.DiskStore: Created local directory at /mnt/spark/spark-local-20130710121422-61da 13/07/10 12:14:22 INFO storage.DiskStore: Created local directory at /mnt2/spark/spark-local-20130710121422-07d2 13/07/10 12:14:22 INFO network.ConnectionManager: Bound socket to port 49473 with id = ConnectionManagerId(ip-10-46-37-82.ec2.internal,49473) 13/07/10 12:14:22 INFO storage.BlockManagerMaster: Trying to register BlockManager 13/07/10 12:14:22 INFO storage.BlockManagerMaster: Registered BlockManager 13/07/10 12:14:22 INFO server.Server: jetty-7.6.8.v20121106 13/07/10 12:14:22 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:50518 13/07/10 12:14:22 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.46.37.82:50518 13/07/10 12:14:22 INFO spark.SparkEnv: Registering MapOutputTracker 13/07/10 12:14:22 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-6d5266c8-c9f8-4db6-958c-e4791bd8a81d 13/07/10 12:14:22 INFO server.Server: jetty-7.6.8.v20121106 13/07/10 12:14:22 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:47677 13/07/10 12:14:22 INFO io.IoWorker: IoWorker thread 'spray-io-worker-0' started 13/07/10 12:14:23 INFO server.HttpServer: akka://spark/user/BlockManagerHTTPServer started on /0.0.0.0:35347 13/07/10 12:14:23 INFO storage.BlockManagerUI: Started BlockManager web UI at http://ip-10-46-37-82.ec2.internal:35347  When i execute jstack on this process i see that one of the threads is trying to load mesos native library:  \"main\" prio=10 tid=0x00007fcfc800c000 nid=0x761 runnable [0x00007fcfcefd5000] java.lang.Thread.State: RUNNABLE at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1957) - locked <0x000000077fcb4fb8> (a java.util.Vector) - locked <0x000000077fd2b348> (a java.util.Vector) at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1882) at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1843) at java.lang.Runtime.load0(Runtime.java:795) - locked <0x000000077fcb66d8> (a java.lang.Runtime) at java.lang.System.load(System.java:1061) at org.apache.mesos.MesosNativeLibrary.load(MesosNativeLibrary.java:38) at spark.executor.MesosExecutorBackend$.main(MesosExecutorBackend.scala:73) at spark.executor.MesosExecutorBackend.main(MesosExecutorBackend.scala)  2) Scheduled task do not want to finish. Even when the console is started (rare case) i see this:  Spark context available as sc. Type in expressions to have them evaluated. Type :help for more information. scala> sc.parallelize(List(1,2,3)) res0: spark.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:13 scala> res0.collect() 13/07/11 09:06:59 INFO spark.SparkContext: Starting job: collect at <console>:15 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Got job 0 (collect at <console>:15) with 8 output partitions (allowLocal=false) 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Final stage: Stage 0 (parallelize at <console>:13) 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Parents of final stage: List() 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Missing parents: List() 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at <console>:13), which has no missing parents 13/07/11 09:06:59 INFO scheduler.DAGScheduler: Submitting 8 missing tasks from Stage 0 (ParallelCollectionRDD[0] at parallelize at <console>:13) 13/07/11 09:06:59 INFO cluster.ClusterScheduler: Adding task set 0.0 with 8 tasks 13/07/11 09:06:59 INFO cluster.TaskSetManager: Starting task 0.0:0 as TID 0 on executor 201307110852-2871257866-5050-2301-0: ip-10-170-21-54.ec2.internal (preferred) 13/07/11 09:07:00 INFO cluster.TaskSetManager: Serialized task 0.0:0 as 975 bytes in 141 ms 13/07/11 09:07:00 INFO cluster.TaskSetManager: Starting task 0.0:1 as TID 1 on executor 201307110852-2871257866-5050-2301-0: ip-10-170-21-54.ec2.internal (preferred) 13/07/11 09:07:00 INFO cluster.TaskSetManager: Serialized task 0.0:1 as 975 bytes in 0 ms  Jstack tell me the same thing. It is trying to load Mesos native library and stuck with it. BTW: We downgraded to the previous version of AMI (unfortunately, i do not remember it's id), updated Java to 1.7 and spark + shark and everything is working like a charm\n\nComments (9):\n1. Sergey Vladimirov: Hi, I have the same issue, spark hang up on mesos library loading. mesos tests show it [ RUN ] SampleFrameworks.JavaFramework\n2. Bach Bui: I think it is the problem of libmesos being compile with different java versions. One data point is when I copy the libmesos on the old AMI to the new one, I was able to have spark running as normal.\n3. Matei Alexandru Zaharia: Hey, so which libmesos did you copy to get it to wkr? I've tried rebuilding Mesos with JDK 1.7 on this AMI and unfortunately the rebuilt version still doesn't work.\n4. Hai-Anh Trinh: This one is surely a critical / blocker issue, no?\n5. Matei Alexandru Zaharia: It's an important issue, but because the default for the AMI is the Spark standalone mode instead of Mesos, most people are still able to use it fine. The other problem is that if the issue is Mesos 0.9 not working against Java 1.7 (which we recently had to upgrade the AMI to for Shark), we don't really have a choice for the Spark 0.7 branch; we can't increase the Mesos version we depend on in a minor release. We will definitely fix this in future releases if possible, or in a new version of the 0.7 AMI if it turns out that it is possible to build Mesos with this JDK (maybe I've just missed some things when I tried it). For Spark 0.8, we'll hopefully depend on Mesos 0.12 or later, and this should no longer be an issue. Also, the Mesos project will soon start making its own AMIs where they make sure that everything works together.\n6. Matei Alexandru Zaharia: I guess one other option is to make the Spark 0.7 on the AMI build against Mesos 0.10 instead of 0.9 (i.e. make it different from the official package); that would also be fine if people are okay with it. But I first want to understand whether there's an older version of the Mesos library that works on JDK 1.7, or some build configuration I've missed. When I build Mesos against JDK 1.7 on the AMI and run `make check`, I also see Mesos's own Java unit test failing (hanging forever).\n7. Bach Bui: Matei, sorry for late reply. The libmesos that I used is the one from an older version of AMI (the AMI that is one version older than the current one). Unfortunately, I don't remember its number. I believe libmesos.so in that system is compiled with JDK1.6.\n8. Matei Alexandru Zaharia: Okay, got it. Just FYI, we worked with the Mesos team to make Spark run on newer Mesos versions (turns out we just had to bump up the dependency and it will work fine) and I'm planning to update our Mesos dependency in 0.7.4 as well as 0.8. So hopefully we'll have functioning Mesos soon.\n9. Matei Alexandru Zaharia: This is now about a pretty old AMI, so I'll close it. New versions of Spark use newer versions of Mesos.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.056751"}}
{"id": "d5861ff6ec3578ea796bbfaad37f1cc4", "issue_key": "SPARK-799", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Windows versions of the deploy scripts", "description": "Although the Spark daemons run fine on Windows with run.cmd, the deploy scripts (bin/start-all.sh and such) don't do so unless you have Cygwin. It would be nice to make .cmd versions of those.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-07-11T12:47:38.000+0000", "updated": "2019-05-21T05:36:57.000+0000", "resolved": "2019-05-21T05:36:57.000+0000", "labels": ["Starter", "bulk-closed"], "components": ["Deploy", "Windows"], "comments": [{"author": "Andrew Tweddle", "body": "Powershell is the modern Microsoft shell for Windows. Do you specifically want .cmd files rather than .ps1? What about .cmd files that delegate to .ps1 scripts?", "created": "2014-10-26T20:53:36.047+0000"}, {"author": "Masayoshi Tsuzuki", "body": "I think Powershell is better. Linux scripts, expecially like pyspark or utils.sh, are getting more complex and it's very painful (or sometimes impossible) to implement the equivalent script for Windows with .cmd because .cmd has very poor ability and inexplicable syntax. Powershell is much easier to implement than .cmd, but I wonder we might need to re-implement all of the scripts that already exist in bin or sbin folder.", "created": "2014-11-19T00:31:10.932+0000"}, {"author": "Steve Loughran", "body": "Proving python versions of the launcher scripts is probably a better approach to supporting windows than .cmd or .ps1 files # {{cmd}} is a painfully dated shell language, as you note. # Powershell is better, but not widely known in the java/scala dev space. # Neither ps1 nor cmd files can be tested except in Windows Python is cross-platform-ish enough that it can be tested on Unix systems too, and more likely to be maintained. It's not seamlessly cross-platform; propagating stdout/stderr from spawned java processes. It also provides the option of becoming the Unix entry point (the bash script simply invoking it), so that maintenance effort is shared, and testing becomes even more implicit.", "created": "2015-03-30T11:17:48.613+0000"}, {"author": "Masayoshi Tsuzuki", "body": "I tend not to think it is a good idea that we make python required for Windows systems (even if we don't use pyspark) just because of the deploy script. Enterprise users wouldn't like to install extra programs. Powershell is installed in all of the currently supported Windows systems.", "created": "2015-04-01T01:50:34.418+0000"}, {"author": "Joan Goyeau", "body": "I agree with [~tsudukim]. What about Scala scripting? Do you think we end up in the same situation as Python? See https://www.artima.com/pins1ed/scala-scripts-on-unix-and-windows.html I looked into that and we need at least to embed `scala` (which is ~25Mb) with the launch scripts. Pros: * Access to the Scala and Java lib * Scripts written in Scala * Multi-platform * Nothing more to install (if embedded) Cons: * Need to embed `scala`, which is ~25Mb", "created": "2016-03-15T10:21:20.349+0000"}, {"author": "Steve Loughran", "body": "If you want to go into the JVM. then there's a .JS script interpreter built into the JVM which can be called from the CLI. However (and perhaps the scala script has the same problem), the JVM is the wrong level to do things like setup env vars, classpaths, probe for native libs being on PATH, etc. Powershell is at the right layer in the code, it's just something which will need its own set of windows tests to validate; maybe someone setting up a Jenkins build somewhere (ASF Infra?) to nightly test them", "created": "2016-03-15T10:45:01.379+0000"}, {"author": "Joan Goyeau", "body": "Make sense", "created": "2016-03-16T13:31:46.113+0000"}], "num_comments": 7, "text": "Issue: SPARK-799\nSummary: Windows versions of the deploy scripts\nDescription: Although the Spark daemons run fine on Windows with run.cmd, the deploy scripts (bin/start-all.sh and such) don't do so unless you have Cygwin. It would be nice to make .cmd versions of those.\n\nComments (7):\n1. Andrew Tweddle: Powershell is the modern Microsoft shell for Windows. Do you specifically want .cmd files rather than .ps1? What about .cmd files that delegate to .ps1 scripts?\n2. Masayoshi Tsuzuki: I think Powershell is better. Linux scripts, expecially like pyspark or utils.sh, are getting more complex and it's very painful (or sometimes impossible) to implement the equivalent script for Windows with .cmd because .cmd has very poor ability and inexplicable syntax. Powershell is much easier to implement than .cmd, but I wonder we might need to re-implement all of the scripts that already exist in bin or sbin folder.\n3. Steve Loughran: Proving python versions of the launcher scripts is probably a better approach to supporting windows than .cmd or .ps1 files # {{cmd}} is a painfully dated shell language, as you note. # Powershell is better, but not widely known in the java/scala dev space. # Neither ps1 nor cmd files can be tested except in Windows Python is cross-platform-ish enough that it can be tested on Unix systems too, and more likely to be maintained. It's not seamlessly cross-platform; propagating stdout/stderr from spawned java processes. It also provides the option of becoming the Unix entry point (the bash script simply invoking it), so that maintenance effort is shared, and testing becomes even more implicit.\n4. Masayoshi Tsuzuki: I tend not to think it is a good idea that we make python required for Windows systems (even if we don't use pyspark) just because of the deploy script. Enterprise users wouldn't like to install extra programs. Powershell is installed in all of the currently supported Windows systems.\n5. Joan Goyeau: I agree with [~tsudukim]. What about Scala scripting? Do you think we end up in the same situation as Python? See https://www.artima.com/pins1ed/scala-scripts-on-unix-and-windows.html I looked into that and we need at least to embed `scala` (which is ~25Mb) with the launch scripts. Pros: * Access to the Scala and Java lib * Scripts written in Scala * Multi-platform * Nothing more to install (if embedded) Cons: * Need to embed `scala`, which is ~25Mb\n6. Steve Loughran: If you want to go into the JVM. then there's a .JS script interpreter built into the JVM which can be called from the CLI. However (and perhaps the scala script has the same problem), the JVM is the wrong level to do things like setup env vars, classpaths, probe for native libs being on PATH, etc. Powershell is at the right layer in the code, it's just something which will need its own set of windows tests to validate; maybe someone setting up a Jenkins build somewhere (ASF Infra?) to nightly test them\n7. Joan Goyeau: Make sense", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.056751"}}
{"id": "e650ccff1354fefdb4228bb924f8ae73", "issue_key": "SPARK-800", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Improve Quickstart Docs to Make Full Deployment More Clear", "description": "Some things are not super clear to people that should be in the quickstart standalone job section. 1. Explain that people need to package their dependencies either by making an uber jar (which is added to the SC) or by copying to each slave node if they build an actual project. This isn't clear since there are no dependencies in the quickstart itself. Warn them about issues creating an uber-jar with akka. 2. Explain that people need to set the required resources in an env variable before launching (or whatever new way Matei added to do this).", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "created": "2013-07-11T22:55:20.000+0000", "updated": "2014-04-23T02:18:29.000+0000", "resolved": "2014-04-23T02:18:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Another thing we should include here is the ugliness of dealing with Akka configurations inside of an uber jar. This is something [~joshrosen] ran into inside of the Spark build and I just ran into today. I independently arrived at the same fix Josh did. https://github.com/mesos/spark/pull/158", "created": "2013-07-25T13:07:14.328+0000"}, {"author": "Ian O Connell", "body": "Probably not the most common/best way, but since asked for ways: I mostly run on EMR clusters of mostly spot instances few hundred XL nodes normally, I use https://github.com/ianoc/SparkEMRBootstrap to build a custom bootstrap image with any extra global dependencies if they are large/common across work bundled into the RPM via the installer script. Then build assembly jars with projects like https://github.com/ianoc/ExampleSparkProject (excludes spark and scala from assembly),(does some stuff to probe the mesos cluster to figure out number of nodes at launch to set default parallelism) . And just used a simple script like https://github.com/ianoc/SparkEMRBootstrap/blob/master/sample_remote_deploy.sh to launch the job remotely inside a screen session on the cluster.", "created": "2013-08-03T21:13:42.192+0000"}, {"author": "SeanM", "body": "We use a simple bash script (2 lines of code) that finds and passes in the dependencies. It works great for us.", "created": "2013-08-04T21:24:32.892+0000"}, {"author": "Patrick Wendell", "body": "This has been fixed with the recent changes to the docs.", "created": "2014-04-23T02:18:29.974+0000"}], "num_comments": 4, "text": "Issue: SPARK-800\nSummary: Improve Quickstart Docs to Make Full Deployment More Clear\nDescription: Some things are not super clear to people that should be in the quickstart standalone job section. 1. Explain that people need to package their dependencies either by making an uber jar (which is added to the SC) or by copying to each slave node if they build an actual project. This isn't clear since there are no dependencies in the quickstart itself. Warn them about issues creating an uber-jar with akka. 2. Explain that people need to set the required resources in an env variable before launching (or whatever new way Matei added to do this).\n\nComments (4):\n1. Patrick McFadin: Another thing we should include here is the ugliness of dealing with Akka configurations inside of an uber jar. This is something [~joshrosen] ran into inside of the Spark build and I just ran into today. I independently arrived at the same fix Josh did. https://github.com/mesos/spark/pull/158\n2. Ian O Connell: Probably not the most common/best way, but since asked for ways: I mostly run on EMR clusters of mostly spot instances few hundred XL nodes normally, I use https://github.com/ianoc/SparkEMRBootstrap to build a custom bootstrap image with any extra global dependencies if they are large/common across work bundled into the RPM via the installer script. Then build assembly jars with projects like https://github.com/ianoc/ExampleSparkProject (excludes spark and scala from assembly),(does some stuff to probe the mesos cluster to figure out number of nodes at launch to set default parallelism) . And just used a simple script like https://github.com/ianoc/SparkEMRBootstrap/blob/master/sample_remote_deploy.sh to launch the job remotely inside a screen session on the cluster.\n3. SeanM: We use a simple bash script (2 lines of code) that finds and passes in the dependencies. It works great for us.\n4. Patrick Wendell: This has been fixed with the recent changes to the docs.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "b16d133234d06b72c7c252ae8270c058", "issue_key": "SPARK-801", "issue_type": "Bug", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Time columns in web UI tables don't sort properly", "description": "They seem to be sorted by the string value in them instead of the number of seconds, milliseconds, etc. It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-07-13T15:58:37.000+0000", "updated": "2013-08-06T23:03:14.000+0000", "resolved": "2013-08-06T23:03:14.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "Hey [~matei] - do you know which view and column was causing trouble? We use custom sort keys now in many places but maybe we missed one.", "created": "2013-07-13T18:17:17.101+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Looks like this was actually fine, so closing the issue.", "created": "2013-08-06T23:03:14.119+0000"}], "num_comments": 2, "text": "Issue: SPARK-801\nSummary: Time columns in web UI tables don't sort properly\nDescription: They seem to be sorted by the string value in them instead of the number of seconds, milliseconds, etc. It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by.\n\nComments (2):\n1. Patrick McFadin: Hey [~matei] - do you know which view and column was causing trouble? We use custom sort keys now in many places but maybe we missed one.\n2. Matei Alexandru Zaharia: Looks like this was actually fine, so closing the issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "ca1dc3b8d62f78e4a2ed2d65f25e2579", "issue_key": "SPARK-802", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Link to job UI from standalone deploy cluster web UI", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-13T15:59:03.000+0000", "updated": "2013-07-15T16:42:07.000+0000", "resolved": "2013-07-15T16:42:07.000+0000", "labels": [], "components": ["Deploy", "Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-802\nSummary: Link to job UI from standalone deploy cluster web UI", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "c1aaaf4658ccc9ee85cf7a8c41cc5380", "issue_key": "SPARK-803", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Open ports 33000-33010 on EC2 clusters for accessing job UI", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-13T16:00:12.000+0000", "updated": "2013-07-29T17:21:11.000+0000", "resolved": "2013-07-29T17:21:11.000+0000", "labels": [], "components": ["EC2"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-803\nSummary: Open ports 33000-33010 on EC2 clusters for accessing job UI", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "7b90696134aa72a62505991f5973b169", "issue_key": "SPARK-804", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Show task status in the Stage Details table on job UI", "description": "Right now it doesn't seem to show whether the task is still running, finished, killed or failed.", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-13T16:05:48.000+0000", "updated": "2013-07-24T14:35:37.000+0000", "resolved": "2013-07-24T14:35:37.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-804\nSummary: Show task status in the Stage Details table on job UI\nDescription: Right now it doesn't seem to show whether the task is still running, finished, killed or failed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "4ef1cfc49e50305fb502469a67fcfc5c", "issue_key": "SPARK-805", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "When determining the Spark method for a stage or RDD, look only in spark.* classes", "description": "Right now the UI and logs get confused when trying to determine the Spark method (e.g. map()) that created an RDD if the caller is in the spark package (e.g. examples, mllib). A simple rule is to only count the last class on the call stack that is directly inside spark or spark.api.java (but not any subpackages).", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-13T16:20:04.000+0000", "updated": "2013-07-23T15:56:43.000+0000", "resolved": "2013-07-23T15:56:43.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-805\nSummary: When determining the Spark method for a stage or RDD, look only in spark.* classes\nDescription: Right now the UI and logs get confused when trying to determine the Spark method (e.g. map()) that created an RDD if the caller is in the spark package (e.g. examples, mllib). A simple rule is to only count the last class on the call stack that is directly inside spark or spark.api.java (but not any subpackages).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "20f938b79f5aa1f407be3a2d89d771e3", "issue_key": "SPARK-806", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Show application name in HTML page titles on job web UI", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-13T16:36:15.000+0000", "updated": "2013-07-15T15:02:40.000+0000", "resolved": "2013-07-15T15:02:40.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-806\nSummary: Show application name in HTML page titles on job web UI", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "4458a5b1412719c379a7b069f56ae788", "issue_key": "SPARK-807", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Show totals for shuffle data and CPU time in Stage pages of UI", "description": "Might also be good to show it on the homepage overview of Stages.", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-13T17:42:18.000+0000", "updated": "2013-07-29T17:03:00.000+0000", "resolved": "2013-07-29T17:03:00.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "The reason it doesn't do this now is that computing this is `O(tasks)` and you could imagine a user with a very large number of tasks accounted for in the UI. For finished stages, we should just compute this once and keep it around.", "created": "2013-07-13T18:20:42.705+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yes, I think we should save it for finished stages and compute it on the fly for running ones. Another option is to keep a mutable variable for each stage that counts these (it's tougher for time if we want to sum the CPU-seconds used by all tasks so far I guess).", "created": "2013-07-13T18:49:06.672+0000"}], "num_comments": 2, "text": "Issue: SPARK-807\nSummary: Show totals for shuffle data and CPU time in Stage pages of UI\nDescription: Might also be good to show it on the homepage overview of Stages.\n\nComments (2):\n1. Patrick McFadin: The reason it doesn't do this now is that computing this is `O(tasks)` and you could imagine a user with a very large number of tasks accounted for in the UI. For finished stages, we should just compute this once and keep it around.\n2. Matei Alexandru Zaharia: Yes, I think we should save it for finished stages and compute it on the fly for running ones. Another option is to keep a mutable variable for each stage that counts these (it's tougher for time if we want to sum the CPU-seconds used by all tasks so far I guess).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "ad017f73f224472c9acf743c246dce20", "issue_key": "SPARK-808", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add an \"executors\" tab to job UI that shows executor stats", "description": "This could show the following: * Number of currently active tasks * How many tasks the executor ran in total * Number of RDD blocks on the executor * Current memory and disk usage", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-13T19:44:08.000+0000", "updated": "2013-07-24T14:09:54.000+0000", "resolved": "2013-07-24T14:09:54.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "Hey @matei having top-level tab called \"Executors\" is a little vague because it overlaps with what's now in \"Storage\". The current memory and disk usage per-executor is already reported in the Storage page. We could add the total number of blocks there. Maybe rip out those aggregate (per-executor) stats from the \"Storage\" page and then put them in a new page. So it would be like this: \"Jobs\" --> Listing of active jobs and stages \"Storage\" --> Listing of stored RDD's \"Executors\" --> Summary of both job and storage information for each executor.", "created": "2013-07-18T11:10:25.816+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yup, I think that sounds good.", "created": "2013-07-18T11:52:06.973+0000"}], "num_comments": 2, "text": "Issue: SPARK-808\nSummary: Add an \"executors\" tab to job UI that shows executor stats\nDescription: This could show the following: * Number of currently active tasks * How many tasks the executor ran in total * Number of RDD blocks on the executor * Current memory and disk usage\n\nComments (2):\n1. Patrick McFadin: Hey @matei having top-level tab called \"Executors\" is a little vague because it overlaps with what's now in \"Storage\". The current memory and disk usage per-executor is already reported in the Storage page. We could add the total number of blocks there. Maybe rip out those aggregate (per-executor) stats from the \"Storage\" page and then put them in a new page. So it would be like this: \"Jobs\" --> Listing of active jobs and stages \"Storage\" --> Listing of stored RDD's \"Executors\" --> Summary of both job and storage information for each executor.\n2. Matei Alexandru Zaharia: Yup, I think that sounds good.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "4cb3777e984aa0ed6358b5aecaccd9bc", "issue_key": "SPARK-809", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Give newly registered apps a set of executors right away", "description": "Right now, newly connected apps in the standalone cluster will not set a good defaultParallelism value if they create RDDs right after creating a SparkContext, because the executorAdded calls are asynchronous and happen after. It would be nice to wait for a few such calls before returning from the scheduler initializer.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-07-13T22:26:12.000+0000", "updated": "2016-01-12T14:27:12.000+0000", "resolved": "2016-01-12T14:27:12.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Andrew Ash", "body": "I believe this situation hasn't changed? Looking a little at the master branch (1.2) the standalone cluster still gets its {{defaultParallelism}} in {{CoarseGrainedSchedulerBackend}} from the {{totalCoreCount}} that is incremented/decremented as executors join/leave the driver's awareness.", "created": "2014-11-14T09:51:04.152+0000"}, {"author": "Sean R. Owen", "body": "I'm assuming this is WontFix at this point.", "created": "2016-01-12T14:27:12.868+0000"}], "num_comments": 2, "text": "Issue: SPARK-809\nSummary: Give newly registered apps a set of executors right away\nDescription: Right now, newly connected apps in the standalone cluster will not set a good defaultParallelism value if they create RDDs right after creating a SparkContext, because the executorAdded calls are asynchronous and happen after. It would be nice to wait for a few such calls before returning from the scheduler initializer.\n\nComments (2):\n1. Andrew Ash: I believe this situation hasn't changed? Looking a little at the master branch (1.2) the standalone cluster still gets its {{defaultParallelism}} in {{CoarseGrainedSchedulerBackend}} from the {{totalCoreCount}} that is incremented/decremented as executors join/leave the driver's awareness.\n2. Sean R. Owen: I'm assuming this is WontFix at this point.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "d05020e410ce7a566adbb686a9b60be3", "issue_key": "SPARK-810", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Ensure thread safety of Spark UI", "description": "The datastructures in the web ui may not be thread safe if a stage is active and the details of the stage are being mutated. I need to check this and possibly add synchronization.", "reporter": "Patrick Wendell", "assignee": "Kay Ousterhout", "created": "2013-07-13T22:59:11.000+0000", "updated": "2014-03-30T04:15:10.000+0000", "resolved": "2013-08-10T16:32:32.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-810\nSummary: Ensure thread safety of Spark UI\nDescription: The datastructures in the web ui may not be thread safe if a stage is active and the details of the stage are being mutated. I need to check this and possibly add synchronization.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "033d8db77e29391925f4a8ffc29efb96", "issue_key": "SPARK-811", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Job UI should show running tasks", "description": "Right now it only shows finished or failed ones, but often the challenge is figuring out where you have tasks running that are taking a long time", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-14T12:17:26.000+0000", "updated": "2013-07-24T14:36:07.000+0000", "resolved": "2013-07-24T14:36:07.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-811\nSummary: Job UI should show running tasks\nDescription: Right now it only shows finished or failed ones, but often the challenge is figuring out where you have tasks running that are taking a long time", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "3550217b69569bfe1340c9c315e40bf5", "issue_key": "SPARK-812", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Netty shuffle creates a lot of open file handles", "description": "I notice that the Netty shuffle implementation needs a very large ulimit even in cases where the default one works fine, which may mean that it's not closing files properly. Needs to be investigated for 0.8.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-07-14T17:12:34.000+0000", "updated": "2020-05-17T18:30:45.000+0000", "resolved": "2014-11-06T07:05:28.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "No longer a problem for new versions of the Netty shuffle", "created": "2014-11-06T07:05:28.067+0000"}], "num_comments": 1, "text": "Issue: SPARK-812\nSummary: Netty shuffle creates a lot of open file handles\nDescription: I notice that the Netty shuffle implementation needs a very large ulimit even in cases where the default one works fine, which may mean that it's not closing files properly. Needs to be investigated for 0.8.\n\nComments (1):\n1. Matei Alexandru Zaharia: No longer a problem for new versions of the Netty shuffle", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "917403cce0af93b3a0108ddfa1a66ee2", "issue_key": "SPARK-813", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Poor locality in master due to ClusterScheduler changes", "description": "The ClusterScheduler in master attempts to launch non-local tasks on a node if it has finished launching all local tasks for it, but unfortunately this causes it to miss the chance to assign those tasks to other nodes later. We should switch back to a delay scheduling strategy as before, with possibly two levels (process-local and then node- or rack-local depending on configuration), and a way to bypass process-local if it clearly won't be met for a stage.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-14T21:53:54.000+0000", "updated": "2013-09-09T16:19:07.000+0000", "resolved": "2013-09-09T16:19:07.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-813\nSummary: Poor locality in master due to ClusterScheduler changes\nDescription: The ClusterScheduler in master attempts to launch non-local tasks on a node if it has finished launching all local tasks for it, but unfortunately this causes it to miss the chance to assign those tasks to other nodes later. We should switch back to a delay scheduling strategy as before, with possibly two levels (process-local and then node- or rack-local depending on configuration), and a way to bypass process-local if it clearly won't be met for a stage.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "c9ae872c89850d52a4d7556c584fe996", "issue_key": "SPARK-814", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Result stages should be named after the action that invoked them", "description": "Right now it looks like jobs (as printed in the log) are named after this action, but result stages aren't. E.g. if you do sc.parallelize(1 to 10).count() there will be a stage called \"parallelize at <console>:1\" and a job called \"count at <console>:1\".", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-15T09:12:03.000+0000", "updated": "2013-07-16T11:02:34.000+0000", "resolved": "2013-07-16T11:02:34.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-814\nSummary: Result stages should be named after the action that invoked them\nDescription: Right now it looks like jobs (as printed in the log) are named after this action, but result stages aren't. E.g. if you do sc.parallelize(1 to 10).count() there will be a stage called \"parallelize at <console>:1\" and a job called \"count at <console>:1\".", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "2f48b7e7478f0dae02b0fae0b28a6232", "issue_key": "SPARK-815", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark's parallelize() should batch objects after partitioning (instead of before)", "description": "PySpark uses batching when serializing and deserializing Python objects. By default, it serializes objects in groups of 1024. The current batching code causes SparkContext.parallelize() to behave counterintuitively when parallelizing small datasets. The current code batches the objects, then parallelizes the batches, so calls to parallelize() with small inputs will be unaffected by the number of partitions:  >>> rdd = sc.parallelize([1, 2, 3, 4], 2) >>> rdd.glom().collect() [[], [1, 2, 3, 4]]  Instead, parallelize() should first partition the elements and then batch them:  >>> rdd = sc.parallelize([1, 2, 3, 4], 2) >>> rdd.glom().collect() [[1, 2], [3, 4]]  Maybe parallelize() should accept an option to control the batch size (right now, it can only be set when creating the SparkContext).", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-15T19:09:30.000+0000", "updated": "2014-03-30T23:33:02.000+0000", "resolved": "2013-07-28T23:56:37.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I've fixed this here: https://github.com/mesos/spark/commit/feba7ee540fca28872957120e5e39b9e36466953, though the solution is not completely perfect because it requires materializing the list if you pass a generator. On the other hand, that's what the Scala/Java parallelize does as well. Maybe a slightly better solution would be to materialize the first context.batchSize * numSplits elements from the generator, write those out with even splitting if you've reached the end of the generator, or write them out as full batches otherwise. But I'm not sure we want to go to this level of complexity now. A better thing to add by the way would be smart splitting support for xrange, similar to that of Range in Scala.", "created": "2013-07-28T23:56:17.913+0000"}, {"author": "Josh Rosen", "body": "Funny timing: I wrote my own fix for this tonight, but I like your fix better: https://github.com/JoshRosen/spark/compare/spark-815. In my branch, I cleaned up some old test code that was setting the batch size to 2 to work around this issue. We should probably clean that up in master now, too.", "created": "2013-07-29T00:03:08.847+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I tried the tests and they all work with this, but it could be nice to update them if you'd like.", "created": "2013-07-29T00:07:21.286+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Sorry for not giving a heads-up BTW, I just fixed a bunch of small issues since I was coming back from this conference: http://www.pydata.org/bos2013/ and had some time on the plane. One of the other annoying things I've fixed is that first() and take() now stop after returning the first few elements, instead of computing the rest of the partition anyway, which makes them quite a bit faster.", "created": "2013-07-29T00:10:23.014+0000"}], "num_comments": 4, "text": "Issue: SPARK-815\nSummary: PySpark's parallelize() should batch objects after partitioning (instead of before)\nDescription: PySpark uses batching when serializing and deserializing Python objects. By default, it serializes objects in groups of 1024. The current batching code causes SparkContext.parallelize() to behave counterintuitively when parallelizing small datasets. The current code batches the objects, then parallelizes the batches, so calls to parallelize() with small inputs will be unaffected by the number of partitions:  >>> rdd = sc.parallelize([1, 2, 3, 4], 2) >>> rdd.glom().collect() [[], [1, 2, 3, 4]]  Instead, parallelize() should first partition the elements and then batch them:  >>> rdd = sc.parallelize([1, 2, 3, 4], 2) >>> rdd.glom().collect() [[1, 2], [3, 4]]  Maybe parallelize() should accept an option to control the batch size (right now, it can only be set when creating the SparkContext).\n\nComments (4):\n1. Matei Alexandru Zaharia: I've fixed this here: https://github.com/mesos/spark/commit/feba7ee540fca28872957120e5e39b9e36466953, though the solution is not completely perfect because it requires materializing the list if you pass a generator. On the other hand, that's what the Scala/Java parallelize does as well. Maybe a slightly better solution would be to materialize the first context.batchSize * numSplits elements from the generator, write those out with even splitting if you've reached the end of the generator, or write them out as full batches otherwise. But I'm not sure we want to go to this level of complexity now. A better thing to add by the way would be smart splitting support for xrange, similar to that of Range in Scala.\n2. Josh Rosen: Funny timing: I wrote my own fix for this tonight, but I like your fix better: https://github.com/JoshRosen/spark/compare/spark-815. In my branch, I cleaned up some old test code that was setting the batch size to 2 to work around this issue. We should probably clean that up in master now, too.\n3. Matei Alexandru Zaharia: I tried the tests and they all work with this, but it could be nice to update them if you'd like.\n4. Matei Alexandru Zaharia: Sorry for not giving a heads-up BTW, I just fixed a bunch of small issues since I was coming back from this conference: http://www.pydata.org/bos2013/ and had some time on the plane. One of the other annoying things I've fixed is that first() and take() now stop after returning the first few elements, instead of computing the rest of the partition anyway, which makes them quite a bit faster.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "bec23b9e08006ce23485f3e2f9a204b8", "issue_key": "SPARK-816", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add Documentation Page for Building and Deploying with a CDH/HDP Cluster", "description": "Some initial to show would be: - Exact build specifications for each CDH/HDP version (for people building manually) - Information about deployment options (YARN, Standalone, etc) and the differences - Guidelines about physical infrastructure required for Spark", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-07-16T10:19:07.000+0000", "updated": "2013-09-22T15:35:10.000+0000", "resolved": "2013-09-22T15:35:10.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-816\nSummary: Add Documentation Page for Building and Deploying with a CDH/HDP Cluster\nDescription: Some initial to show would be: - Exact build specifications for each CDH/HDP version (for people building manually) - Information about deployment options (YARN, Standalone, etc) and the differences - Guidelines about physical infrastructure required for Spark", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "e5a23d98b0f4f66b03cc2c3c7e512d8c", "issue_key": "SPARK-1218", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Minibatch SGD with random sampling", "description": "Takes a gradient function as input. At each iteration, we run stochastic gradient descent locally on each worker with a fraction of the data points selected randomly and with replacement (i.e., sampled points may overlap across iterations).", "reporter": "Ameet Talwalkar", "assignee": "Shivaram Venkataraman", "created": "2013-07-16T19:17:23.000+0000", "updated": "2014-04-07T17:47:37.000+0000", "resolved": "2014-04-07T17:47:37.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "Fixed in 0.9.0 or an earlier version.", "created": "2014-04-07T17:47:37.925+0000"}], "num_comments": 1, "text": "Issue: SPARK-1218\nSummary: Minibatch SGD with random sampling\nDescription: Takes a gradient function as input. At each iteration, we run stochastic gradient descent locally on each worker with a fraction of the data points selected randomly and with replacement (i.e., sampled points may overlap across iterations).\n\nComments (1):\n1. Xiangrui Meng: Fixed in 0.9.0 or an earlier version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "3d93bd6f558f9c6a1b08dec5269875f2", "issue_key": "SPARK-1219", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Minibatch SGD with disjoint partitions", "description": "Takes a gradient function as input. At each iteration, we run stochastic gradient descent locally on each worker with a fraction (alpha) of the data points selected randomly and disjointly (i.e., we ensure that we touch all datapoints after at most 1/alpha iterations).", "reporter": "Ameet Talwalkar", "assignee": null, "created": "2013-07-16T19:20:09.000+0000", "updated": "2014-04-07T17:49:20.000+0000", "resolved": "2014-04-07T17:48:54.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Ameet Talwalkar", "body": "Addendum: proximal gradient method included as part of this feature (and thus a prox function should be included as an optional argument)", "created": "2013-07-16T20:29:50.422+0000"}, {"author": "Xiangrui Meng", "body": "Implemented in 0.9.0 or an earlier version.", "created": "2014-04-07T17:48:54.957+0000"}], "num_comments": 2, "text": "Issue: SPARK-1219\nSummary: Minibatch SGD with disjoint partitions\nDescription: Takes a gradient function as input. At each iteration, we run stochastic gradient descent locally on each worker with a fraction (alpha) of the data points selected randomly and disjointly (i.e., we ensure that we touch all datapoints after at most 1/alpha iterations).\n\nComments (2):\n1. Ameet Talwalkar: Addendum: proximal gradient method included as part of this feature (and thus a prox function should be included as an optional argument)\n2. Xiangrui Meng: Implemented in 0.9.0 or an earlier version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "4be6d916237de056556f45f27c425c55", "issue_key": "SPARK-1221", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "SVMs (+ regularized variants)", "description": "Implement Support Vector Machines using the SGD optimization primitives.", "reporter": "Ameet Talwalkar", "assignee": "Shivaram Venkataraman", "created": "2013-07-16T19:31:32.000+0000", "updated": "2014-04-07T17:49:10.000+0000", "resolved": "2014-04-07T17:49:09.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Martin Jaggi", "body": "is resolved, right?", "created": "2014-04-07T16:49:57.989+0000"}, {"author": "Xiangrui Meng", "body": "Implemented in 0.9.0 or an earlier version.", "created": "2014-04-07T17:49:10.024+0000"}], "num_comments": 2, "text": "Issue: SPARK-1221\nSummary: SVMs (+ regularized variants)\nDescription: Implement Support Vector Machines using the SGD optimization primitives.\n\nComments (2):\n1. Martin Jaggi: is resolved, right?\n2. Xiangrui Meng: Implemented in 0.9.0 or an earlier version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "a94fc1c6ccae18f6819c8d1a397bb955", "issue_key": "SPARK-1222", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Logistic Regression (+ regularized variants)", "description": "Implement Logistic Regression using the SGD optimization primitives.", "reporter": "Ameet Talwalkar", "assignee": "Shivaram Venkataraman", "created": "2013-07-16T19:32:29.000+0000", "updated": "2014-04-07T17:49:35.000+0000", "resolved": "2014-04-07T17:49:35.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Martin Jaggi", "body": "is resolved, right?", "created": "2014-04-07T16:49:51.408+0000"}, {"author": "Xiangrui Meng", "body": "Implemented in 0.9.0 or an earlier version.", "created": "2014-04-07T17:49:35.889+0000"}], "num_comments": 2, "text": "Issue: SPARK-1222\nSummary: Logistic Regression (+ regularized variants)\nDescription: Implement Logistic Regression using the SGD optimization primitives.\n\nComments (2):\n1. Martin Jaggi: is resolved, right?\n2. Xiangrui Meng: Implemented in 0.9.0 or an earlier version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "84f91f651c329c6bc52021462930bd50", "issue_key": "SPARK-1223", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Linear Regression (+ regularized variants)", "description": "Implement Linear regression using the SGD optimization primitives.", "reporter": "Ameet Talwalkar", "assignee": "Shivaram Venkataraman", "created": "2013-07-16T19:33:06.000+0000", "updated": "2014-04-07T17:49:49.000+0000", "resolved": "2014-04-07T17:49:49.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Martin Jaggi", "body": "is resolved, right?", "created": "2014-04-07T16:49:43.974+0000"}, {"author": "Xiangrui Meng", "body": "Implemented in 0.9.0 or an earlier version.", "created": "2014-04-07T17:49:49.058+0000"}], "num_comments": 2, "text": "Issue: SPARK-1223\nSummary: Linear Regression (+ regularized variants)\nDescription: Implement Linear regression using the SGD optimization primitives.\n\nComments (2):\n1. Martin Jaggi: is resolved, right?\n2. Xiangrui Meng: Implemented in 0.9.0 or an earlier version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "19daa8c41f2bcb1535e52fe7cb5d4093", "issue_key": "SPARK-817", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Consistently invoke bash with /usr/bin/env bash in scripts", "description": "Right now some scripts do that but some still do /bin/bash.", "reporter": "Matei Alexandru Zaharia", "assignee": "Chu Tong", "created": "2013-07-16T22:04:42.000+0000", "updated": "2013-07-17T22:27:46.000+0000", "resolved": "2013-07-17T22:27:46.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-817\nSummary: Consistently invoke bash with /usr/bin/env bash in scripts\nDescription: Right now some scripts do that but some still do /bin/bash.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.060501"}}
{"id": "57602e69886550418bed67f75a61393e", "issue_key": "SPARK-818", "issue_type": "New Feature", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Design Spark Job Server", "description": "We've been developing a generic REST job server here at Ooyala and would like to outline its goals, architecture, and API, so we could get feedback from the community and hopefully contribute it back.", "reporter": "Evan Chan", "assignee": null, "created": "2013-07-17T00:17:10.000+0000", "updated": "2014-11-14T10:43:30.000+0000", "resolved": "2014-11-14T10:43:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Evan Chan", "body": "Document explaining our current overall architecture / API / features for the job server.", "created": "2013-07-17T16:23:26.103+0000"}, {"author": "Henry Saputra", "body": "This is good start, thanks for driving this. Thanks. Some immediate questions: How would the API server communicate with the driver? Will it share the same context as the driver?", "created": "2013-07-19T13:25:59.615+0000"}, {"author": "Evan Chan", "body": "@Henry: > API server communicate with driver? I assume by \"driver\" you mean the SparkContext within which each job is running right? This is created by the job server itself. You can think of the workflow like this (we can post one we've been working on to make things more clear): - User does a POST /jobs to initiate a job. Either this is ad-hoc job (temporary context) or runs in a pre-created context. - Job server finds or creates the context - Job server loads the class for the job, which must implement a trait, and invokes a method, passing in the SparkContext instance. > Will it share the same context as the driver? Yes. So, all jobs passed to the job server should implement a trait, and the trait has a method like this: /** * This is the entry point for a Spark Job Server to execute Spark jobs. * This function should create or reuse RDDs and return the result at the end, which the * Job Server will cache or display. * @param sc a SparkContext for the job. May be reused across jobs. * @param config the Typesafe Config object passed into the job request * @return the job result */ def runJob(sc: SparkContext, config: Config): Any The user can submit multiple jobs to the same context -- for example, the first job can create a cached RDD, and the second one can query it. Hope that answers your questions, and looking forward to more feedback.", "created": "2013-07-19T23:46:12.631+0000"}, {"author": "Nicholas Pentreath", "body": "This sounds great and is something that I quite urgently need. I'm currently designing something similar for my fairly simple use case, and am running into issues with Jetty versioning (I think) between the job server and Spark. When do you think this might be ready for release / use?", "created": "2013-08-08T07:08:31.208+0000"}, {"author": "Henry Saputra", "body": "Hi Evan, \"driver\" means the application where construct SparkContext to submit request to Spark master where the job will be running. I assume we will not build full RESTful API server but more into HTTP based APIs. I am asking this because of the details of links and media representations as REST(ful) APIs. What is the backend proposed implementation to route the request? Use Akka remote Actor (directly) or Spray.io routing? Thx again for the effort.", "created": "2013-08-08T11:38:06.684+0000"}, {"author": "Evan Chan", "body": "@Nick: Thanks for the interest! We are hoping to submit this soon, in bits and pieces, with the first one coming in as soon as about a week. It is usable now, but it depends on some internal libraries which we need to factor out (or open source). We use Spray and put it first in the classpath, and haven't had any issues. @Henry: Yeah this server is RESTful in the sense that the route refers to resources - contexts or jobs or jars - and we use the HTTP verbs (GET, POST, DELETE) as the actions. \"What is the backend proposed implementation to route the request? Use Akka remote Actor (directly) or Spray.io routing?\" Well Spray handles the HTTP requests, and the job server internally routes the requests to actors which starts and stops contexts and jobs.", "created": "2013-08-09T00:43:34.628+0000"}, {"author": "Josh Rosen", "body": "There's now an open pull request for this: https://github.com/apache/incubator-spark/pull/222", "created": "2013-12-07T14:25:27.432+0000"}, {"author": "Evan Chan", "body": "An update: we have put up the final job server here: https://github.com/ooyala/spark-jobserver The plan is to have a spark-contrib repo/github account and this would be one of the first projects. See SPARK-1283 for the ticket to track spark-contrib.", "created": "2014-03-24T00:09:37.560+0000"}, {"author": "Andrew Ash", "body": "The community consensus was that the Spark job server should live in a separate repository from the Apache Spark code. It now lives in the repository that [~velvia] links above, and has continued to be quite active over the past 6 months. Closing this Apache Spark issue as Won't Fix -- the job server is a welcome addition to the growing Spark ecosystem but its issues don't belong in this issue tracker. Many thanks everyone!", "created": "2014-11-14T10:43:30.881+0000"}], "num_comments": 9, "text": "Issue: SPARK-818\nSummary: Design Spark Job Server\nDescription: We've been developing a generic REST job server here at Ooyala and would like to outline its goals, architecture, and API, so we could get feedback from the community and hopefully contribute it back.\n\nComments (9):\n1. Evan Chan: Document explaining our current overall architecture / API / features for the job server.\n2. Henry Saputra: This is good start, thanks for driving this. Thanks. Some immediate questions: How would the API server communicate with the driver? Will it share the same context as the driver?\n3. Evan Chan: @Henry: > API server communicate with driver? I assume by \"driver\" you mean the SparkContext within which each job is running right? This is created by the job server itself. You can think of the workflow like this (we can post one we've been working on to make things more clear): - User does a POST /jobs to initiate a job. Either this is ad-hoc job (temporary context) or runs in a pre-created context. - Job server finds or creates the context - Job server loads the class for the job, which must implement a trait, and invokes a method, passing in the SparkContext instance. > Will it share the same context as the driver? Yes. So, all jobs passed to the job server should implement a trait, and the trait has a method like this: /** * This is the entry point for a Spark Job Server to execute Spark jobs. * This function should create or reuse RDDs and return the result at the end, which the * Job Server will cache or display. * @param sc a SparkContext for the job. May be reused across jobs. * @param config the Typesafe Config object passed into the job request * @return the job result */ def runJob(sc: SparkContext, config: Config): Any The user can submit multiple jobs to the same context -- for example, the first job can create a cached RDD, and the second one can query it. Hope that answers your questions, and looking forward to more feedback.\n4. Nicholas Pentreath: This sounds great and is something that I quite urgently need. I'm currently designing something similar for my fairly simple use case, and am running into issues with Jetty versioning (I think) between the job server and Spark. When do you think this might be ready for release / use?\n5. Henry Saputra: Hi Evan, \"driver\" means the application where construct SparkContext to submit request to Spark master where the job will be running. I assume we will not build full RESTful API server but more into HTTP based APIs. I am asking this because of the details of links and media representations as REST(ful) APIs. What is the backend proposed implementation to route the request? Use Akka remote Actor (directly) or Spray.io routing? Thx again for the effort.\n6. Evan Chan: @Nick: Thanks for the interest! We are hoping to submit this soon, in bits and pieces, with the first one coming in as soon as about a week. It is usable now, but it depends on some internal libraries which we need to factor out (or open source). We use Spray and put it first in the classpath, and haven't had any issues. @Henry: Yeah this server is RESTful in the sense that the route refers to resources - contexts or jobs or jars - and we use the HTTP verbs (GET, POST, DELETE) as the actions. \"What is the backend proposed implementation to route the request? Use Akka remote Actor (directly) or Spray.io routing?\" Well Spray handles the HTTP requests, and the job server internally routes the requests to actors which starts and stops contexts and jobs.\n7. Josh Rosen: There's now an open pull request for this: https://github.com/apache/incubator-spark/pull/222\n8. Evan Chan: An update: we have put up the final job server here: https://github.com/ooyala/spark-jobserver The plan is to have a spark-contrib repo/github account and this would be one of the first projects. See SPARK-1283 for the ticket to track spark-contrib.\n9. Andrew Ash: The community consensus was that the Spark job server should live in a separate repository from the Apache Spark code. It now lives in the repository that [~velvia] links above, and has continued to be quite active over the past 6 months. Closing this Apache Spark issue as Won't Fix -- the job server is a welcome addition to the growing Spark ecosystem but its issues don't belong in this issue tracker. Many thanks everyone!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.062509"}}
{"id": "de30b575d07c14916783f193bc0ba66e", "issue_key": "SPARK-819", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "netty: ChannelInboundByteHandlerAdapter no longer exist in 4.0.3.Final", "description": "It appears the netty shuffle code uses netty version 4.0.0.Beta2, which by the tag was in beta. They now have 4.0.2.Final which doesn't include the api ChannelInboundByteHandlerAdapter which is used by the FileClientHandler. We should move to use a stable api. It looks like it was replaced with ChannelInboundHandlerAdapter.", "reporter": "Thomas Graves", "assignee": null, "created": "2013-07-18T13:07:58.000+0000", "updated": "2020-05-17T18:30:16.000+0000", "resolved": "2014-11-08T09:52:17.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Binh Nguyen", "body": "Should be fixed in https://github.com/apache/incubator-spark/pull/238", "created": "2013-12-31T17:37:50.980+0000"}, {"author": "Sean R. Owen", "body": "This must have been fixed along the way, as the class ChannelInboundByteHandlerAdapter is not used in the code now, nor is that version of Netty.", "created": "2014-11-08T09:52:17.544+0000"}], "num_comments": 2, "text": "Issue: SPARK-819\nSummary: netty: ChannelInboundByteHandlerAdapter no longer exist in 4.0.3.Final\nDescription: It appears the netty shuffle code uses netty version 4.0.0.Beta2, which by the tag was in beta. They now have 4.0.2.Final which doesn't include the api ChannelInboundByteHandlerAdapter which is used by the FileClientHandler. We should move to use a stable api. It looks like it was replaced with ChannelInboundHandlerAdapter.\n\nComments (2):\n1. Binh Nguyen: Should be fixed in https://github.com/apache/incubator-spark/pull/238\n2. Sean R. Owen: This must have been fixed along the way, as the class ChannelInboundByteHandlerAdapter is not used in the code now, nor is that version of Netty.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.062509"}}
{"id": "51c59cd57f4cad49b5ffc55fb75693c4", "issue_key": "SPARK-820", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Use Bootstrap progress bars in web UI", "description": "Instead of the custom progress bars we currently have, we should use the ones in Boostrap: http://twitter.github.io/bootstrap/components.html#progress.", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-18T16:55:40.000+0000", "updated": "2013-08-01T15:51:18.000+0000", "resolved": "2013-08-01T15:51:18.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-820\nSummary: Use Bootstrap progress bars in web UI\nDescription: Instead of the custom progress bars we currently have, we should use the ones in Boostrap: http://twitter.github.io/bootstrap/components.html#progress.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.062509"}}
{"id": "6ff9603b46b889d30e3b0be5b998d652", "issue_key": "SPARK-821", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Driver program should not put a block in memory", "description": "Often the driver/master node has ram allocated than the worker nodes. In the case the user runs take or first on a cached RDD, the task can get launched locally on the master, and then the master would attempt to put the first block (or first few blocks) in memory, leading to OOM on the master. Perhaps the simplest solution is to not put blocks in memory on the master node when Spark is running in cluster mode. See the related discussion on the mailing list: https://groups.google.com/forum/#!topic/spark-users/eu9RJc3nQng", "reporter": "Reynold Xin", "assignee": "Aaron Davidson", "created": "2013-07-18T17:37:17.000+0000", "updated": "2013-11-12T11:01:02.000+0000", "resolved": "2013-09-06T15:31:01.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Hey @rxin - just be clear, this will still have to _at least_ materialize the entire block in memory right? So if, e.g. the very first block exceeds the memory of the driver, we're still in trouble. So here it seems like the fix just avoids unnecessarily storing the first or first few blocks the Block Manager.", "created": "2013-09-05T13:46:39.025+0000"}, {"author": "Reynold Xin", "body": "We can just stream through the first block. No need to materialize anything.", "created": "2013-09-05T16:02:32.033+0000"}, {"author": "Brenton Partridge", "body": "It seems this has been marked as fixed for a while, but I've seen this exact problem (first() cache() first() fails) arise in 9f7b9bb (November 5) running on my group's cluster. Has a fix been pushed to the master branch on Github yet?", "created": "2013-11-12T01:24:05.694+0000"}, {"author": "Reynold Xin", "body": "Looks like it has: https://github.com/mesos/spark/commit/ddcb9d310abd258ba49ee764ff076dee9178f975", "created": "2013-11-12T10:45:40.558+0000"}, {"author": "Brenton Partridge", "body": "Thanks! Might be a different issue then. I'll keep trying to track it down and file another issue if I find anything.", "created": "2013-11-12T11:01:02.050+0000"}], "num_comments": 5, "text": "Issue: SPARK-821\nSummary: Driver program should not put a block in memory\nDescription: Often the driver/master node has ram allocated than the worker nodes. In the case the user runs take or first on a cached RDD, the task can get launched locally on the master, and then the master would attempt to put the first block (or first few blocks) in memory, leading to OOM on the master. Perhaps the simplest solution is to not put blocks in memory on the master node when Spark is running in cluster mode. See the related discussion on the mailing list: https://groups.google.com/forum/#!topic/spark-users/eu9RJc3nQng\n\nComments (5):\n1. Patrick McFadin: Hey @rxin - just be clear, this will still have to _at least_ materialize the entire block in memory right? So if, e.g. the very first block exceeds the memory of the driver, we're still in trouble. So here it seems like the fix just avoids unnecessarily storing the first or first few blocks the Block Manager.\n2. Reynold Xin: We can just stream through the first block. No need to materialize anything.\n3. Brenton Partridge: It seems this has been marked as fixed for a while, but I've seen this exact problem (first() cache() first() fails) arise in 9f7b9bb (November 5) running on my group's cluster. Has a fix been pushed to the master branch on Github yet?\n4. Reynold Xin: Looks like it has: https://github.com/mesos/spark/commit/ddcb9d310abd258ba49ee764ff076dee9178f975\n5. Brenton Partridge: Thanks! Might be a different issue then. I'll keep trying to track it down and file another issue if I find anything.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.062509"}}
{"id": "73916e19773997f45b11973e3bdd6ee3", "issue_key": "SPARK-822", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "defaultMinSplits can't be set higher than 2", "description": "{{SparkContext.defaultMinSplits}} is used to control the default level of parallelism when reading input from files. Unfortunately, the current implementation doesn't allow defaultMinSplits to be set higher than 2. [From 0.7.3|https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/SparkContext.scala#L707]:  /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */ def defaultParallelism: Int = taskScheduler.defaultParallelism /** Default min number of partitions for Hadoop RDDs when not given by user */ def defaultMinSplits: Int = math.min(defaultParallelism, 2)  I think that the {{math.min}} should be {{math.max}} instead. PySpark has the same problem.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-07-19T09:17:33.000+0000", "updated": "2015-01-25T19:30:00.000+0000", "resolved": "2013-07-28T10:24:20.000+0000", "labels": [], "components": ["PySpark", "Spark Core"], "comments": [{"author": "Josh Rosen", "body": "Resolving as \"Won't Fix\" for now, since this isn't actually a bug and needs better documentation. See https://github.com/mesos/spark/pull/718", "created": "2013-07-28T10:24:20.359+0000"}, {"author": "Josh Rosen", "body": "I merged a pull request that adds a link to the earlier discussion to the defaultMinPartitions documentation so that folks don't re-attempt to change this: https://github.com/apache/spark/pull/4102", "created": "2015-01-25T19:30:00.448+0000"}], "num_comments": 2, "text": "Issue: SPARK-822\nSummary: defaultMinSplits can't be set higher than 2\nDescription: {{SparkContext.defaultMinSplits}} is used to control the default level of parallelism when reading input from files. Unfortunately, the current implementation doesn't allow defaultMinSplits to be set higher than 2. [From 0.7.3|https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/SparkContext.scala#L707]:  /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */ def defaultParallelism: Int = taskScheduler.defaultParallelism /** Default min number of partitions for Hadoop RDDs when not given by user */ def defaultMinSplits: Int = math.min(defaultParallelism, 2)  I think that the {{math.min}} should be {{math.max}} instead. PySpark has the same problem.\n\nComments (2):\n1. Josh Rosen: Resolving as \"Won't Fix\" for now, since this isn't actually a bug and needs better documentation. See https://github.com/mesos/spark/pull/718\n2. Josh Rosen: I merged a pull request that adds a link to the earlier discussion to the defaultMinPartitions documentation so that folks don't re-attempt to change this: https://github.com/apache/spark/pull/4102", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.062509"}}
{"id": "14a0d729464c54b67bdefa74f81e04bc", "issue_key": "SPARK-823", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "spark.default.parallelism's default is inconsistent across scheduler backends", "description": "The [0.7.3 configuration guide|http://spark-project.org/docs/latest/configuration.html] says that {{spark.default.parallelism}}'s default is 8, but the default is actually max(totalCoreCount, 2) for the standalone scheduler backend, 8 for the Mesos scheduler, and {{threads}} for the local scheduler: https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/cluster/StandaloneSchedulerBackend.scala#L157 https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/mesos/MesosSchedulerBackend.scala#L317 https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/local/LocalScheduler.scala#L150 Should this be clarified in the documentation? Should the Mesos scheduler backend's default be revised?", "reporter": "Josh Rosen", "assignee": "Ilya Ganelin", "created": "2013-07-19T09:24:29.000+0000", "updated": "2020-05-17T17:48:31.000+0000", "resolved": "2015-02-09T16:31:45.000+0000", "labels": [], "components": ["Documentation", "PySpark", "Scheduler", "Spark Core"], "comments": [{"author": "Diana Carroll", "body": "Yes, please clarify the documentation, I just ran into this. the Configuration guide (http://spark.apache.org/docs/latest/configuration.html) says the default is 8. In testing this on Standalone Spark, there actually is no default value for the variable: >sc.getConf.contains(\"spark.default.parallelism\") >res1: Boolean = false It looks like if the variable is not set, then the default behavior is decided in code, e.g. Partitioner.scala:  if (rdd.context.conf.contains(\"spark.default.parallelism\")) { new HashPartitioner(rdd.context.defaultParallelism) } else { new HashPartitioner(bySize.head.partitions.size) }", "created": "2014-04-30T18:27:39.871+0000"}, {"author": "Diana Carroll", "body": "Okay, this is definitely more than a documentation bug, because PySpark and Scala work differently if spark.default.parallelism isn't set by the user. I'm testing using wordcount. Pyspark: reduceByKey will use the value of sc.defaultParallelism. That value is set to the number of threads when running locally. On my Spark Standalone \"cluster\" which has a single node with a single core, the value is 2. If I set spark.default.parallelism, it will set sc.defaultParallelism to that value and use that. Scala: reduceByKey will use the number of partitions in my file/map stage and ignore the value of sc.defaultParallelism. sc.defaultParallism is set by the same logic as pyspark (number of threads for local, 2 for my microcluster), it is just ignored. I'm not sure which approach is correct. Scala works as described here: http://spark.apache.org/docs/latest/tuning.html  Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to SparkContext.textFile, etc), and for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions. You can pass the level of parallelism as a second argument (see the spark.PairRDDFunctions documentation), or set the config property spark.default.parallelism to change the default. In general, we recommend 2-3 tasks per CPU core in your cluster.", "created": "2014-04-30T19:08:03.806+0000"}, {"author": "Ilya Ganelin", "body": "Hi [~joshrosen] I believe the documentation is up to date and I reviewed all usages of spark.default.parallelism and found no inconsistencies with the documentation. The only thing that is un-documented with regards to the usage of spark.default.parallelism is how it's used within the Partitioner class in both Spark and Python. If defined, the default number of partitions created is equal to spark.default.parallelism - otherwise, it's the local number of partitions. I think this issue can be closed - I don't think that particular case needs to be publicly documented (it's clearly evident in the code what is going on).", "created": "2015-02-09T16:07:11.547+0000"}], "num_comments": 3, "text": "Issue: SPARK-823\nSummary: spark.default.parallelism's default is inconsistent across scheduler backends\nDescription: The [0.7.3 configuration guide|http://spark-project.org/docs/latest/configuration.html] says that {{spark.default.parallelism}}'s default is 8, but the default is actually max(totalCoreCount, 2) for the standalone scheduler backend, 8 for the Mesos scheduler, and {{threads}} for the local scheduler: https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/cluster/StandaloneSchedulerBackend.scala#L157 https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/mesos/MesosSchedulerBackend.scala#L317 https://github.com/mesos/spark/blob/v0.7.3/core/src/main/scala/spark/scheduler/local/LocalScheduler.scala#L150 Should this be clarified in the documentation? Should the Mesos scheduler backend's default be revised?\n\nComments (3):\n1. Diana Carroll: Yes, please clarify the documentation, I just ran into this. the Configuration guide (http://spark.apache.org/docs/latest/configuration.html) says the default is 8. In testing this on Standalone Spark, there actually is no default value for the variable: >sc.getConf.contains(\"spark.default.parallelism\") >res1: Boolean = false It looks like if the variable is not set, then the default behavior is decided in code, e.g. Partitioner.scala:  if (rdd.context.conf.contains(\"spark.default.parallelism\")) { new HashPartitioner(rdd.context.defaultParallelism) } else { new HashPartitioner(bySize.head.partitions.size) }\n2. Diana Carroll: Okay, this is definitely more than a documentation bug, because PySpark and Scala work differently if spark.default.parallelism isn't set by the user. I'm testing using wordcount. Pyspark: reduceByKey will use the value of sc.defaultParallelism. That value is set to the number of threads when running locally. On my Spark Standalone \"cluster\" which has a single node with a single core, the value is 2. If I set spark.default.parallelism, it will set sc.defaultParallelism to that value and use that. Scala: reduceByKey will use the number of partitions in my file/map stage and ignore the value of sc.defaultParallelism. sc.defaultParallism is set by the same logic as pyspark (number of threads for local, 2 for my microcluster), it is just ignored. I'm not sure which approach is correct. Scala works as described here: http://spark.apache.org/docs/latest/tuning.html  Spark automatically sets the number of “map” tasks to run on each file according to its size (though you can control it through optional parameters to SparkContext.textFile, etc), and for distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest parent RDD’s number of partitions. You can pass the level of parallelism as a second argument (see the spark.PairRDDFunctions documentation), or set the config property spark.default.parallelism to change the default. In general, we recommend 2-3 tasks per CPU core in your cluster.\n3. Ilya Ganelin: Hi [~joshrosen] I believe the documentation is up to date and I reviewed all usages of spark.default.parallelism and found no inconsistencies with the documentation. The only thing that is un-documented with regards to the usage of spark.default.parallelism is how it's used within the Partitioner class in both Spark and Python. If defined, the default number of partitions created is equal to spark.default.parallelism - otherwise, it's the local number of partitions. I think this issue can be closed - I don't think that particular case needs to be publicly documented (it's clearly evident in the code what is going on).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.062509"}}
{"id": "ead3e5788ab50c09f7344fbe00e129dc", "issue_key": "SPARK-824", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Make less copies of blocks during remote reads", "description": "To satisfy a getRemote() when the source block is stored as in deserialized form, Spark will make at least two copies: it will create a bytebuffer of the entire deserialized block on the source node and one on the destination node. If the block consists of a single object (as occurs frequently in Shark and apparently PySpark), then the total effect is that remote reads require extra memory equal to three times the block size. As a result, especially with relatively coarse partitioning, Spark can require a surprisingly large amount of non-cache headroom. One copy could be avoided by serializing blocks as they are sent over the network. This would require connection manager and block manager API changes and might have negative effects if serialization is expensive relative to the connection speed. Similarly, a copy might be avoided by deserializing blocks as they are received over the network, but this probably would hurt some applications (e.g. if they process items from a block much slower than the network speed).", "reporter": "Charles Reiss", "assignee": null, "created": "2013-07-19T13:43:23.000+0000", "updated": "2020-05-17T18:21:11.000+0000", "resolved": "2014-11-06T07:07:31.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "This is a pretty old issue that no longer affects the newest block manager and Netty code.", "created": "2014-11-06T07:07:31.140+0000"}], "num_comments": 1, "text": "Issue: SPARK-824\nSummary: Make less copies of blocks during remote reads\nDescription: To satisfy a getRemote() when the source block is stored as in deserialized form, Spark will make at least two copies: it will create a bytebuffer of the entire deserialized block on the source node and one on the destination node. If the block consists of a single object (as occurs frequently in Shark and apparently PySpark), then the total effect is that remote reads require extra memory equal to three times the block size. As a result, especially with relatively coarse partitioning, Spark can require a surprisingly large amount of non-cache headroom. One copy could be avoided by serializing blocks as they are sent over the network. This would require connection manager and block manager API changes and might have negative effects if serialization is expensive relative to the connection speed. Similarly, a copy might be avoided by deserializing blocks as they are received over the network, but this probably would hurt some applications (e.g. if they process items from a block much slower than the network speed).\n\nComments (1):\n1. Matei Alexandru Zaharia: This is a pretty old issue that no longer affects the newest block manager and Netty code.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.064514"}}
{"id": "f4c414a8f75a2d0a092a65602dc0f725", "issue_key": "SPARK-825", "issue_type": "Bug", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "DoubleRDDFunctions.sampleStdev() actually computes regular stdev()", "description": "Due to a typo, DoubleRDDFunctions.sampleStdev() returns the regular, non-sample stdev():  /** * Compute the sample standard deviation of this RDD's elements (which corrects for bias in * estimating the standard deviation by dividing by N-1 instead of N). */ def sampleStdev(): Double = stats().stdev", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-07-22T13:09:12.000+0000", "updated": "2014-03-30T23:33:02.000+0000", "resolved": "2013-07-22T16:09:02.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-825\nSummary: DoubleRDDFunctions.sampleStdev() actually computes regular stdev()\nDescription: Due to a typo, DoubleRDDFunctions.sampleStdev() returns the regular, non-sample stdev():  /** * Compute the sample standard deviation of this RDD's elements (which corrects for bias in * estimating the standard deviation by dividing by N-1 instead of N). */ def sampleStdev(): Double = stats().stdev", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.064514"}}
{"id": "71b2b1387b1293a0bbaed96763cc3c5f", "issue_key": "SPARK-826", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "fold(), reduce(), collect() always attempt to use java serialization", "description": "For objects that support other than java serialization, a whole range of methods communicating task result to the front end fail because task result is always using closure serializer and ignoring proper object serializer. Relevant discussion: https://groups.google.com/forum/#!searchin/spark-users/parallelize$20kryo/spark-users/0NYbRS_ujkk/mfPueHz75fkJ", "reporter": "Dmitriy Lyubimov", "assignee": "Dmitriy Lyubimov", "created": "2013-07-22T13:16:19.000+0000", "updated": "2013-09-18T11:14:35.000+0000", "resolved": "2013-08-11T20:42:21.000+0000", "labels": [], "components": [], "comments": [{"author": "Dmitriy Lyubimov", "body": "note: the same problem also affects parallelize() but that is not using TaskResult. The suggested discussed solution suggests that we may need to use objectSerializer (instead of closure serializer) while serializing Task and TaskResult (see discussion on the group). However, I am dubious about this approach because it implies that TaskResult and Task _must_ support any type of serialization (which they can't possibly in case of any 3rd party serializer). More likely, and correctly, perhaps is when the instances of interest themselves inside task result are serialized using proper serializer, i.e. perhaps they need to be lazy and transient while their actual serialization must be backed by a byte array in a general case.", "created": "2013-07-22T13:22:25.261+0000"}, {"author": "Reynold Xin", "body": "That is a good point - TaskResult and Task might not be serializable with a 3rd party serializer. We can also work around it by implementing Externalizable in TaskResult and Task, and in their writeObject methods, use the objectSerializer to serialize the actual task result.", "created": "2013-07-22T13:43:41.448+0000"}, {"author": "Dmitriy Lyubimov", "body": "I kind of tried to wrap this in the TaskResult like this :  override def writeExternal(out: ObjectOutput) { val objectSer = SparkEnv.get.serializer.newInstance() val bb = objectSer.serialize(value) out.writeInt( bb.remaining()) if (bb.hasArray) { out.write(bb.array(), bb.arrayOffset() + bb.position(), bb.remaining()) } else { val bbval = new Array[Byte](bb.remaining()) bb.get(bbval) out.write(bbval) } ...  but similar unwinding code in the readExternal fails because SparkEnv is not initialized in that akka thread (and, perhaps, cannot be properly initialized). Other than that, it seems this should work.", "created": "2013-07-22T14:14:30.856+0000"}, {"author": "Dmitriy Lyubimov", "body": "Aha. so pushing environment to local thread seems to work for the kryo test i 've created. Not sure of side effects. Tested for local scheduler only so far.", "created": "2013-07-22T14:45:27.162+0000"}, {"author": "Dmitriy Lyubimov", "body": "here's the branch where i am tracking this for now : https://github.com/dlyubimov/spark/tree/SPARK-826 I will need to make sure this works with other schedulers..", "created": "2013-07-22T14:57:03.234+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Hey Dmitriy, I'd be okay changing the serializer for results to be our data serializer, but because this may break some applications, it's better to do it for 0.8 than 0.7.4, or at least make it only an option that's off by default in 0.7.4. By the way, do you mind submitting these as GitHub pull requests instead of patch files? You can just press the Pull Request button on your branch's page. It will make them easier to merge.", "created": "2013-07-22T16:45:15.225+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I should also say that in the long term I'd like to change how task results are fetched so that the master asks the worker to get them through the BlockManager, instead of shipping them in an Akka message. That's partly why we didn't optimize this. But it is a problem and it would be good to use the data serializer here in 0.8.", "created": "2013-07-22T16:46:12.500+0000"}, {"author": "Dmitriy Lyubimov", "body": "bq. I'd be okay changing the serializer for results to be our data serializer, but because this may break some applications, it's better to do it for 0.8 than 0.7.4, or at least make it only an option that's off by default in 0.7.4. (1) i guess it could be that there are applications relying on kryo but collecting thru java serialization and such java serialized objects happen not to be responsive to Kryo's FieldSerializer. It would seem to me that situation is not very natural (e.g. right now i collect kryo objects via artificial step of serializing them into byte arrays and deserializing them back in the front-end -- but that should work with kryo's field serializer just fine, not to mention i would be happy just to get rid of that ugly spaghetti once that is fixed). (2) I am working on top of master, which i assume to be towards 0.8 (since it bears 0.8.0-snapshot). So this should be fine. Actually i tried to put the issue on the 0.8 roadmap but for some reason was not able to do so in this jira setup (set fix version=0.8.0) bq. I should also say that in the long term I'd like to change how task results are fetched so that the master asks the worker to get them through the BlockManager, instead of shipping them in an Akka message. That's partly why we didn't optimize this. But it is a problem and it would be good to use the data serializer here in 0.8. this sounds great. I am pragmatically driven though and very much would like that in 0.8. The patch, if it works, does not seem to be a big deal to me.", "created": "2013-07-22T17:25:44.856+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yup, 0.8 is the current master.", "created": "2013-07-22T17:28:40.764+0000"}, {"author": "Dmitriy Lyubimov", "body": "bq. By the way, do you mind submitting these as GitHub pull requests instead of patch files? You can just press the Pull Request button on your branch's page. It will make them easier to merge. sure. that's been my intent. But this is WIP , patches are for illustration purposes only (to give an idea where it is all headed and get an early feedback). My experience working with other OSS projects tells me that it doesn't pay investing in patches that are not favorably looked at. One can have private patches but merging them to project ensures future versions will not require painful merges as committers will be required to run my tests too. So i tend to test the ground early.", "created": "2013-07-22T17:32:59.401+0000"}, {"author": "Dmitriy Lyubimov", "body": "Note: fold() still doesn't work, removed from scope of this issue. Perhaps maintenance releases will deal with that? otherwise, till 0.9", "created": "2013-08-12T12:38:14.943+0000"}, {"author": "Dmitriy Lyubimov", "body": "Note: fold() still doesn't work, removed from scope of this issue. Perhaps maintenance releases will deal with that? otherwise, till 0.9 On Sun, Aug 11, 2013 at 8:42 PM, Reynold Xin (JIRA) <", "created": "2013-08-12T12:38:53.501+0000"}, {"author": "Reynold Xin", "body": "I will open a new ticket for fold.", "created": "2013-08-12T12:40:03.879+0000"}, {"author": "Reynold Xin", "body": "Actually Dmitriy do you mind doing that? Since you are the original reporter of this :)", "created": "2013-08-12T12:40:29.612+0000"}, {"author": "Dmitriy Lyubimov", "body": "No i guess i don't mind but so far i am not sure what the best way to address closure-wrapped objects is. Otherwise i would've addressed it already. Need some input from you guys. Ideally, we'd want to rig the serialization of the closure objects themselves just like we did for TaskResult etc. but i am not sure if there's an elegant solution to that in terms of Scala (I am relatively a novice with scala, a bit < than 12 mos of hands-on experience yet) -d", "created": "2013-08-12T12:57:52.661+0000"}, {"author": "Matei Alexandru Zaharia", "body": "So is the problem just the \"zero value\" object that needs to go into the closure? For that, I'd actually use a byte array and deserialize it within the closure, using SparkEnv.get.serializer. For example, in fold(), you'd change the \"foldPartition\" function. Note that the \"mergeResult\" function runs locally on the driver program so it's fine.", "created": "2013-08-12T16:33:57.118+0000"}, {"author": "Dmitriy Lyubimov", "body": "Ok, yes, that's the easiest thing i thought. However, consistent thing to do with everything else we did in this issue would be use defaultWriteObject() for java serializer and then a nested stream for everything else. That way java serialization works exactly as before without creating a new stream header just for that zero object. Besides, zero object could be a collection itself as well, in which case we \"sort of\" getting back to that \"single big buffer\" problem potentially. Maybe not in this particular case, but maybe later in other api s. which is why i think maybe there's a trick in scala that allows to add custom writeObject to a closure. Behind the scenes they are most likely java anonymous classes so in theory it should be possible to equip them with custom serialization. On Mon, Aug 12, 2013 at 4:34 PM, Matei Zaharia (JIRA) <", "created": "2013-08-12T16:42:53.523+0000"}, {"author": "Dmitriy Lyubimov", "body": "Another possible angle of attach is perhaps to rely on kryo to serialize the whole closure with all the closure variables. but then again it locks as to serializers that would support such thing. So a little experimentation is required. Yet another angle which is kind of extension of tweaking closure serialization is possibly replace the whole closure with an actual anonymous object with a custom java serializer and the closure function.", "created": "2013-08-12T16:48:53.657+0000"}, {"author": "Dmitriy Lyubimov", "body": "I guess closure is really a Function1 instance. So i guess we could re-wrap it along the following lines: val closure = new Function1[Nothing, Int] with Serializable { def apply(v1: Nothing): Int = { a+1 } @throws(classOf[IOException]) private def writeObject(out:ObjectOutputStream):Unit = { // ... if java serializer out.defaultWriteObject() // ... else use writeNestedStream() } } perhaps re-wrap it into some utility class after that. There seems to be something similar in discussion about \"spores\" in scala.", "created": "2013-08-12T17:16:53.470+0000"}, {"author": "Dmitriy Lyubimov", "body": "i've created SPARK-869 to track this.", "created": "2013-08-12T18:44:53.528+0000"}, {"author": "Matei Alexandru Zaharia", "body": "That would probably work, though it's a bit scary-looking. The other issue is that it will not work if we make the closure serializer anything other than Java.. but we have that problem anyway with things like broadcast variables, so maybe we should just abandon making that pluggable.", "created": "2013-08-12T18:50:12.699+0000"}, {"author": "Dmitriy Lyubimov", "body": "On Mon, Aug 12, 2013 at 6:50 PM, Matei Zaharia (JIRA) Hm. This argument about locking sounds dubious. If scala allowed easy support for closure serializers, we would not have to have this workaround. But today,if we wanted to switch to a custom serializer for closures, it would actually have to be looking like this (unless i miss something about scala). Current code is just as locked to java serialization in this sense. If there's a clever and semantically \"non-scary\" way to provide custom closure serialization in scala one day, we'd just switch to it. Until then, it is either the workaround(which i am still not sure is working) or not use closures as serialization containers at all (IMO).", "created": "2013-08-12T22:30:53.605+0000"}, {"author": "Gary Malouf", "body": "Can a fix for this be added to 0.7.4 - or is the migration to 0.8 minimally painful? Ran into this today deserializing protobuf.", "created": "2013-09-18T11:07:23.851+0000"}, {"author": "Dmitriy Lyubimov", "body": "I don't think backporting to 0.7 is worth it. However be warned that the fix was only fixing parallelize(), collect() and reduce(). fold() and numerous other routines that ship front end parameters in a closure is still a pretty nasty case requiring (IMO) some rethinknig of how these things are done in spark. On Wed, Sep 18, 2013 at 11:08 AM, Gary Malouf Jr (JIRA) <", "created": "2013-09-18T11:14:35.345+0000"}], "num_comments": 24, "text": "Issue: SPARK-826\nSummary: fold(), reduce(), collect() always attempt to use java serialization\nDescription: For objects that support other than java serialization, a whole range of methods communicating task result to the front end fail because task result is always using closure serializer and ignoring proper object serializer. Relevant discussion: https://groups.google.com/forum/#!searchin/spark-users/parallelize$20kryo/spark-users/0NYbRS_ujkk/mfPueHz75fkJ\n\nComments (24):\n1. Dmitriy Lyubimov: note: the same problem also affects parallelize() but that is not using TaskResult. The suggested discussed solution suggests that we may need to use objectSerializer (instead of closure serializer) while serializing Task and TaskResult (see discussion on the group). However, I am dubious about this approach because it implies that TaskResult and Task _must_ support any type of serialization (which they can't possibly in case of any 3rd party serializer). More likely, and correctly, perhaps is when the instances of interest themselves inside task result are serialized using proper serializer, i.e. perhaps they need to be lazy and transient while their actual serialization must be backed by a byte array in a general case.\n2. Reynold Xin: That is a good point - TaskResult and Task might not be serializable with a 3rd party serializer. We can also work around it by implementing Externalizable in TaskResult and Task, and in their writeObject methods, use the objectSerializer to serialize the actual task result.\n3. Dmitriy Lyubimov: I kind of tried to wrap this in the TaskResult like this :  override def writeExternal(out: ObjectOutput) { val objectSer = SparkEnv.get.serializer.newInstance() val bb = objectSer.serialize(value) out.writeInt( bb.remaining()) if (bb.hasArray) { out.write(bb.array(), bb.arrayOffset() + bb.position(), bb.remaining()) } else { val bbval = new Array[Byte](bb.remaining()) bb.get(bbval) out.write(bbval) } ...  but similar unwinding code in the readExternal fails because SparkEnv is not initialized in that akka thread (and, perhaps, cannot be properly initialized). Other than that, it seems this should work.\n4. Dmitriy Lyubimov: Aha. so pushing environment to local thread seems to work for the kryo test i 've created. Not sure of side effects. Tested for local scheduler only so far.\n5. Dmitriy Lyubimov: here's the branch where i am tracking this for now : https://github.com/dlyubimov/spark/tree/SPARK-826 I will need to make sure this works with other schedulers..\n6. Matei Alexandru Zaharia: Hey Dmitriy, I'd be okay changing the serializer for results to be our data serializer, but because this may break some applications, it's better to do it for 0.8 than 0.7.4, or at least make it only an option that's off by default in 0.7.4. By the way, do you mind submitting these as GitHub pull requests instead of patch files? You can just press the Pull Request button on your branch's page. It will make them easier to merge.\n7. Matei Alexandru Zaharia: I should also say that in the long term I'd like to change how task results are fetched so that the master asks the worker to get them through the BlockManager, instead of shipping them in an Akka message. That's partly why we didn't optimize this. But it is a problem and it would be good to use the data serializer here in 0.8.\n8. Dmitriy Lyubimov: bq. I'd be okay changing the serializer for results to be our data serializer, but because this may break some applications, it's better to do it for 0.8 than 0.7.4, or at least make it only an option that's off by default in 0.7.4. (1) i guess it could be that there are applications relying on kryo but collecting thru java serialization and such java serialized objects happen not to be responsive to Kryo's FieldSerializer. It would seem to me that situation is not very natural (e.g. right now i collect kryo objects via artificial step of serializing them into byte arrays and deserializing them back in the front-end -- but that should work with kryo's field serializer just fine, not to mention i would be happy just to get rid of that ugly spaghetti once that is fixed). (2) I am working on top of master, which i assume to be towards 0.8 (since it bears 0.8.0-snapshot). So this should be fine. Actually i tried to put the issue on the 0.8 roadmap but for some reason was not able to do so in this jira setup (set fix version=0.8.0) bq. I should also say that in the long term I'd like to change how task results are fetched so that the master asks the worker to get them through the BlockManager, instead of shipping them in an Akka message. That's partly why we didn't optimize this. But it is a problem and it would be good to use the data serializer here in 0.8. this sounds great. I am pragmatically driven though and very much would like that in 0.8. The patch, if it works, does not seem to be a big deal to me.\n9. Matei Alexandru Zaharia: Yup, 0.8 is the current master.\n10. Dmitriy Lyubimov: bq. By the way, do you mind submitting these as GitHub pull requests instead of patch files? You can just press the Pull Request button on your branch's page. It will make them easier to merge. sure. that's been my intent. But this is WIP , patches are for illustration purposes only (to give an idea where it is all headed and get an early feedback). My experience working with other OSS projects tells me that it doesn't pay investing in patches that are not favorably looked at. One can have private patches but merging them to project ensures future versions will not require painful merges as committers will be required to run my tests too. So i tend to test the ground early.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.064514"}}
{"id": "0ba8029d4faed74ffb15821cbe474942", "issue_key": "SPARK-827", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "MAX_TASK_FAILURES not configurable", "description": "number of retries of a task is now hard-coded . We have a need for more flexibility to this end, and it is probably overalll justified.", "reporter": "Dmitriy Lyubimov", "assignee": "Dmitriy Lyubimov", "created": "2013-07-22T16:37:10.000+0000", "updated": "2013-12-07T14:26:54.000+0000", "resolved": "2013-12-07T14:26:54.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Dmitriy Lyubimov", "body": "trivial patch attached", "created": "2013-07-22T16:40:41.067+0000"}, {"author": "Dmitriy Lyubimov", "body": "this is now merged, can be closed.", "created": "2013-08-12T18:32:09.056+0000"}], "num_comments": 2, "text": "Issue: SPARK-827\nSummary: MAX_TASK_FAILURES not configurable\nDescription: number of retries of a task is now hard-coded . We have a need for more flexibility to this end, and it is probably overalll justified.\n\nComments (2):\n1. Dmitriy Lyubimov: trivial patch attached\n2. Dmitriy Lyubimov: this is now merged, can be closed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.064514"}}
{"id": "eab399edba8b85dc507a691ac78cdbbd", "issue_key": "SPARK-828", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Clean up web UI page headers", "description": "With the new Scala-only web UI, the HTML for the headers has changed a little, causing a bigger font size. In addition, the per-job UI has some info that doesn't fit even with a small font size. The two pictures attached show the problem.", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-23T11:59:43.000+0000", "updated": "2013-08-01T15:51:05.000+0000", "resolved": "2013-08-01T15:51:05.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Karen Feng", "body": "Is there any need for the URL to be in the header if it's already stated one line below? This seems somewhat redundant.", "created": "2013-07-29T14:55:50.946+0000"}], "num_comments": 1, "text": "Issue: SPARK-828\nSummary: Clean up web UI page headers\nDescription: With the new Scala-only web UI, the HTML for the headers has changed a little, causing a bigger font size. In addition, the per-job UI has some info that doesn't fit even with a small font size. The two pictures attached show the problem.\n\nComments (1):\n1. Karen Feng: Is there any need for the URL to be in the header if it's already stated one line below? This seems somewhat redundant.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.064514"}}
{"id": "da7f5a12f71bc4b799ab87b969dc9a42", "issue_key": "SPARK-829", "issue_type": "Bug", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "scheduler shouldn't hang if a task contains unserializable objects in its closure", "description": "Try  val a = new Object sc.parallelize(1 to 10, 2).map(x => a).count  The following exception is uncaught:  java.io.NotSerializableException: java.lang.Object at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:331) at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:28) at spark.scheduler.ResultTask$.serializeInfo(ResultTask.scala:43) at spark.scheduler.ResultTask.writeExternal(ResultTask.scala:115) at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1442) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1411) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:331) at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:28) at spark.JavaSerializerInstance.serialize(JavaSerializer.scala:48) at spark.scheduler.Task$.serializeWithDependencies(Task.scala:78) at spark.scheduler.local.LocalTaskSetManager.slaveOffer(LocalTaskSetManager.scala:115) at spark.scheduler.local.LocalScheduler$$anonfun$resourceOffer$2.apply(LocalScheduler.scala:134) at spark.scheduler.local.LocalScheduler$$anonfun$resourceOffer$2.apply(LocalScheduler.scala:131) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.local.LocalScheduler.resourceOffer(LocalScheduler.scala:131) at spark.scheduler.local.LocalActor$$anonfun$receive$1.apply(LocalScheduler.scala:46) at spark.scheduler.local.LocalActor$$anonfun$receive$1.apply(LocalScheduler.scala:44) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.scheduler.local.LocalActor.apply(LocalScheduler.scala:43) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-07-23T14:00:49.000+0000", "updated": "2013-08-05T16:31:26.000+0000", "resolved": "2013-08-05T16:31:26.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-829\nSummary: scheduler shouldn't hang if a task contains unserializable objects in its closure\nDescription: Try  val a = new Object sc.parallelize(1 to 10, 2).map(x => a).count  The following exception is uncaught:  java.io.NotSerializableException: java.lang.Object at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1531) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:331) at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:28) at spark.scheduler.ResultTask$.serializeInfo(ResultTask.scala:43) at spark.scheduler.ResultTask.writeExternal(ResultTask.scala:115) at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1442) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1411) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:331) at spark.JavaSerializationStream.writeObject(JavaSerializer.scala:28) at spark.JavaSerializerInstance.serialize(JavaSerializer.scala:48) at spark.scheduler.Task$.serializeWithDependencies(Task.scala:78) at spark.scheduler.local.LocalTaskSetManager.slaveOffer(LocalTaskSetManager.scala:115) at spark.scheduler.local.LocalScheduler$$anonfun$resourceOffer$2.apply(LocalScheduler.scala:134) at spark.scheduler.local.LocalScheduler$$anonfun$resourceOffer$2.apply(LocalScheduler.scala:131) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at spark.scheduler.local.LocalScheduler.resourceOffer(LocalScheduler.scala:131) at spark.scheduler.local.LocalActor$$anonfun$receive$1.apply(LocalScheduler.scala:46) at spark.scheduler.local.LocalActor$$anonfun$receive$1.apply(LocalScheduler.scala:44) at akka.actor.Actor$class.apply(Actor.scala:318) at spark.scheduler.local.LocalActor.apply(LocalScheduler.scala:43) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.064514"}}
{"id": "add057cf5080f080c5b33b9220414828", "issue_key": "SPARK-830", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "add DenseVector and SparseVector to mllib, and replace all Array[Double] with Vectors", "description": "currently machine learning models in mllib package use raw Array[Double] directly which is not portable and elegant. Replacing arrays with vectors can provide the following benefits: 1. Higher Performance. When the data are dense vectors, using array is fine, but when the data is sparse, using SparseVector can gain higher performance 2. Higher abstraction. Vectors can provide higher abstractions, which are elegant and intuitive, while Array[Double] is verbose.", "reporter": "Frank Dai", "assignee": "Xiangrui Meng", "created": "2013-07-25T18:42:51.000+0000", "updated": "2014-04-01T05:34:45.000+0000", "resolved": "2014-04-01T05:34:45.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Xiangrui Meng", "body": "MLlib v1.0 will support sparse data.", "created": "2014-04-01T05:34:45.318+0000"}], "num_comments": 1, "text": "Issue: SPARK-830\nSummary: add DenseVector and SparseVector to mllib, and replace all Array[Double] with Vectors\nDescription: currently machine learning models in mllib package use raw Array[Double] directly which is not portable and elegant. Replacing arrays with vectors can provide the following benefits: 1. Higher Performance. When the data are dense vectors, using array is fine, but when the data is sparse, using SparseVector can gain higher performance 2. Higher abstraction. Vectors can provide higher abstractions, which are elegant and intuitive, while Array[Double] is verbose.\n\nComments (1):\n1. Xiangrui Meng: MLlib v1.0 will support sparse data.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.064514"}}
{"id": "8079bb26687c3f8c0a29fbddc9ddcb2b", "issue_key": "SPARK-831", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Move certain classes into more appropriate packages", "description": "We will need to move the package name in Spark to org.apache.spark. We should take this opportunity to rename some classes and move them into more appropriate packages. Let's use this ticket to track what needs to move / have been moved.  RDD -> rdd.RDD PairRDDFunctions -> rdd.PairRDDFunctions MappedValuesRDD -> rdd.MappedValuesRDD FlatMappedValuesRDD -> rdd.FlatMappedValuesRDD OrderedRDDFunctions -> rdd.OrderedRDDFunctions DoubleRDDFunctions -> rdd.DoubleRDDFunctions KryoSerializer -> serializer.KryoSerializer JavaSerializer -> serializer.javaSerializer SizeEstimator -> util.SizeEstimator", "reporter": "Reynold Xin", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-26T17:07:51.000+0000", "updated": "2013-09-15T23:07:10.000+0000", "resolved": "2013-09-15T23:07:09.000+0000", "labels": [], "components": [], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Added these onto https://github.com/mesos/spark/pull/882 now. I also moved Utils and ClosureCleaner to spark.util.", "created": "2013-09-01T00:46:02.769+0000"}], "num_comments": 1, "text": "Issue: SPARK-831\nSummary: Move certain classes into more appropriate packages\nDescription: We will need to move the package name in Spark to org.apache.spark. We should take this opportunity to rename some classes and move them into more appropriate packages. Let's use this ticket to track what needs to move / have been moved.  RDD -> rdd.RDD PairRDDFunctions -> rdd.PairRDDFunctions MappedValuesRDD -> rdd.MappedValuesRDD FlatMappedValuesRDD -> rdd.FlatMappedValuesRDD OrderedRDDFunctions -> rdd.OrderedRDDFunctions DoubleRDDFunctions -> rdd.DoubleRDDFunctions KryoSerializer -> serializer.KryoSerializer JavaSerializer -> serializer.javaSerializer SizeEstimator -> util.SizeEstimator\n\nComments (1):\n1. Matei Alexandru Zaharia: Added these onto https://github.com/mesos/spark/pull/882 now. I also moved Utils and ClosureCleaner to spark.util.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "f4c0102e29d44cc8f167eed87a57eced", "issue_key": "SPARK-832", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark should set worker PYTHONPATH from SPARK_HOME instead of inheriting it from the master", "description": "In current versions of PySpark, the worker Python processes inherit the master's PYTHONPATH environment variable. This can lead to ImportErrors when running the PySpark worker processes if the master and workers use different SPARK_HOME paths. Instead, the workers should append SPARK_HOME/python/pyspark to their own PYTHONPATHs. To support customization of the PYTHONPATH on the workers (e.g. to add a NFS folder containing shared libraries), users would still be able to set a custom PYTHONPATH in spark-env.sh.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-07-28T17:52:52.000+0000", "updated": "2013-08-06T23:21:29.000+0000", "resolved": "2013-08-06T23:21:29.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-832\nSummary: PySpark should set worker PYTHONPATH from SPARK_HOME instead of inheriting it from the master\nDescription: In current versions of PySpark, the worker Python processes inherit the master's PYTHONPATH environment variable. This can lead to ImportErrors when running the PySpark worker processes if the master and workers use different SPARK_HOME paths. Instead, the workers should append SPARK_HOME/python/pyspark to their own PYTHONPATHs. To support customization of the PYTHONPATH on the workers (e.g. to add a NFS folder containing shared libraries), users would still be able to set a custom PYTHONPATH in spark-env.sh.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "74e8220d63dfa2356d6fc67f036ef081", "issue_key": "SPARK-833", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Web UI's \"Application Detail UI\" link may use internal EC2 hostname", "description": "In the new (0.8) web UI, the \"Application Detail UI\" link on the Application Info page is derived from Utils.getLocalHostName(). When running on EC2, this link will break if the local hostname isn't the public EC2 hostname. In SPARK-613, we fixed a similar problem by introducing a masterPublicAddress variable that's populated from SPARK_PUBLIC_DNS if it's set. We should probably do something similar here.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-07-29T10:54:14.000+0000", "updated": "2014-03-30T05:59:02.000+0000", "resolved": "2014-03-30T05:59:02.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Josh Rosen", "body": "This is still broken; some users ran across it during the AMP Camp 3 exercises.", "created": "2013-08-29T16:00:25.570+0000"}, {"author": "Matei Alexandru Zaharia", "body": "The code is still actually in SparkUI to use SPARK_PUBLIC_DNS; all that remains is to set it. I've sent a PR at https://github.com/mesos/spark-ec2/pull/18.", "created": "2013-09-02T12:15:30.261+0000"}, {"author": "Josh Rosen", "body": "This is still a problem, even in the {{0.8.0-incubating}} release. When I view the Master web UI, the application name link in the \"Running Applications\" row still links to an {{ip-*.ec2.internal}} address. Similarly, the link labeled \"Application Detail UI\" on the Master's application details page uses these internal addresses. I don't think that SPARK_PUBLIC_DNS is the problem here, since this is only happening with links added for the new per-application UI. My guess is that those links are using a different mechanism to get the master's hostname.", "created": "2013-09-25T22:04:15.515+0000"}, {"author": "Josh Rosen", "body": "Actually, the problem is a bit more complicated. I'm launching a PySpark session through IPython notebook, and it looks like the application WebUI is actually binding to the internal address:  13/09/26 04:54:15 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040 13/09/26 04:54:15 INFO ui.SparkUI: Started Spark Web UI at http://ip-10-182-188-66.ec2.internal:4040  This is because the UI binds to the either SPARK_PUBLIC_DNS or the local hostname:  val host = Option(System.getenv(\"SPARK_PUBLIC_DNS\")).getOrElse(Utils.localHostName())  One solution would be to have the web UI listen on all interfaces but generate links using SPARK_PUBLIC_DNS. Even if user applications (like my IPython notebook) launch without setting SPARK_PUBLIC_DNS, we can rely on the Spark master having it set, so the master web UI will still be able to generate correct links to the application UIs. However, this assumes that both web UIs are running on the same machine.", "created": "2013-09-25T22:31:03.647+0000"}, {"author": "Josh Rosen", "body": "This problem potentially affects all Spark driver programs, not just PySpark. For PySpark, I probably can fix this by setting SPARK_PUBLIC_DNS in the {{spark-class}} scripts, and we could do something similar in the {{run-example}} scripts. For arbitrary user programs, such as drivers launched through sbt, this wouldn't work. Maybe the best solution is to just export SPARK_PUBLIC_DNS in the {{.bash_profile}} script.", "created": "2013-09-27T14:46:58.290+0000"}, {"author": "Josh Rosen", "body": "This should (hopefully finally!) be fixed by https://github.com/mesos/spark-ec2/pull/23.", "created": "2013-10-04T22:01:04.852+0000"}, {"author": "Aaron Davidson", "body": "I am seeing this issue again. The \"Application Detail UI\" link is pointing to an EC2 internal. Other links are working, except for the \"Name\" column in the Master \"Running Applications\" table. Will attach screenshot.", "created": "2013-12-08T17:25:05.379+0000"}, {"author": "Aaron Davidson", "body": "Attached an image to show this bug is still at large. Note that I am hovering over the Application Detail UI link and the URL displayed in the status bar is an internal ec2 IP.", "created": "2013-12-08T17:26:43.414+0000"}, {"author": "Patrick Wendell", "body": "I don't think there is a general way to fix this other than setting SPARK_PUBLIC_DNS in provisioning frameworks such as the spark ec2 scripts and the amp camp scripts. AFAIK those have all been updated to do this correctly, so I'm going to close this for now.", "created": "2014-03-30T05:58:57.833+0000"}], "num_comments": 9, "text": "Issue: SPARK-833\nSummary: Web UI's \"Application Detail UI\" link may use internal EC2 hostname\nDescription: In the new (0.8) web UI, the \"Application Detail UI\" link on the Application Info page is derived from Utils.getLocalHostName(). When running on EC2, this link will break if the local hostname isn't the public EC2 hostname. In SPARK-613, we fixed a similar problem by introducing a masterPublicAddress variable that's populated from SPARK_PUBLIC_DNS if it's set. We should probably do something similar here.\n\nComments (9):\n1. Josh Rosen: This is still broken; some users ran across it during the AMP Camp 3 exercises.\n2. Matei Alexandru Zaharia: The code is still actually in SparkUI to use SPARK_PUBLIC_DNS; all that remains is to set it. I've sent a PR at https://github.com/mesos/spark-ec2/pull/18.\n3. Josh Rosen: This is still a problem, even in the {{0.8.0-incubating}} release. When I view the Master web UI, the application name link in the \"Running Applications\" row still links to an {{ip-*.ec2.internal}} address. Similarly, the link labeled \"Application Detail UI\" on the Master's application details page uses these internal addresses. I don't think that SPARK_PUBLIC_DNS is the problem here, since this is only happening with links added for the new per-application UI. My guess is that those links are using a different mechanism to get the master's hostname.\n4. Josh Rosen: Actually, the problem is a bit more complicated. I'm launching a PySpark session through IPython notebook, and it looks like the application WebUI is actually binding to the internal address:  13/09/26 04:54:15 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040 13/09/26 04:54:15 INFO ui.SparkUI: Started Spark Web UI at http://ip-10-182-188-66.ec2.internal:4040  This is because the UI binds to the either SPARK_PUBLIC_DNS or the local hostname:  val host = Option(System.getenv(\"SPARK_PUBLIC_DNS\")).getOrElse(Utils.localHostName())  One solution would be to have the web UI listen on all interfaces but generate links using SPARK_PUBLIC_DNS. Even if user applications (like my IPython notebook) launch without setting SPARK_PUBLIC_DNS, we can rely on the Spark master having it set, so the master web UI will still be able to generate correct links to the application UIs. However, this assumes that both web UIs are running on the same machine.\n5. Josh Rosen: This problem potentially affects all Spark driver programs, not just PySpark. For PySpark, I probably can fix this by setting SPARK_PUBLIC_DNS in the {{spark-class}} scripts, and we could do something similar in the {{run-example}} scripts. For arbitrary user programs, such as drivers launched through sbt, this wouldn't work. Maybe the best solution is to just export SPARK_PUBLIC_DNS in the {{.bash_profile}} script.\n6. Josh Rosen: This should (hopefully finally!) be fixed by https://github.com/mesos/spark-ec2/pull/23.\n7. Aaron Davidson: I am seeing this issue again. The \"Application Detail UI\" link is pointing to an EC2 internal. Other links are working, except for the \"Name\" column in the Master \"Running Applications\" table. Will attach screenshot.\n8. Aaron Davidson: Attached an image to show this bug is still at large. Note that I am hovering over the Application Detail UI link and the URL displayed in the status bar is an internal ec2 IP.\n9. Patrick Wendell: I don't think there is a general way to fix this other than setting SPARK_PUBLIC_DNS in provisioning frameworks such as the spark ec2 scripts and the amp camp scripts. AFAIK those have all been updated to do this correctly, so I'm going to close this for now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "3a3ffad220860a63ace4330d29f98bea", "issue_key": "SPARK-834", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "TaskMetrics should report compressed (not uncompressed) shuffle read bytes.", "description": "This should be consistent with the shuffle write bytes.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-07-29T16:07:01.000+0000", "updated": "2013-08-11T19:48:56.000+0000", "resolved": "2013-08-11T19:48:56.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-834\nSummary: TaskMetrics should report compressed (not uncompressed) shuffle read bytes.\nDescription: This should be consistent with the shuffle write bytes.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "82c73027c7b3a7f49bc22ab1fa1d67a0", "issue_key": "SPARK-835", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "RDD$parallelize() should use object serializer (not closure serializer) for collection objects", "description": "This is a twin issue for SPARK-826 encapsulating all use cases where collection of objects is transferred from front end to backend RDDs.", "reporter": "Dmitriy Lyubimov", "assignee": null, "created": "2013-07-29T17:21:54.000+0000", "updated": "2014-11-14T09:55:26.000+0000", "resolved": "2014-11-14T09:55:26.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Yup, sounds like a good idea.", "created": "2013-07-29T17:26:00.392+0000"}, {"author": "Dmitriy Lyubimov", "body": "i have a branch for it but i cannot issue a pull request until 826 is settled on master -- i had to merge 826 to re-use changes i made for kryo tests.", "created": "2013-07-29T18:30:21.696+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Ah, sorry about that -- will look at 826 first but I've been backlogged.", "created": "2013-07-29T19:10:16.145+0000"}, {"author": "Dmitriy Lyubimov", "body": "this is now merged as part of 826 PR. can be resolved.", "created": "2013-08-12T18:32:50.120+0000"}, {"author": "Andrew Ash", "body": "Closing per [~dlyubimov] with the Fix Version from SPARK-826", "created": "2014-11-14T09:55:26.119+0000"}], "num_comments": 5, "text": "Issue: SPARK-835\nSummary: RDD$parallelize() should use object serializer (not closure serializer) for collection objects\nDescription: This is a twin issue for SPARK-826 encapsulating all use cases where collection of objects is transferred from front end to backend RDDs.\n\nComments (5):\n1. Matei Alexandru Zaharia: Yup, sounds like a good idea.\n2. Dmitriy Lyubimov: i have a branch for it but i cannot issue a pull request until 826 is settled on master -- i had to merge 826 to re-use changes i made for kryo tests.\n3. Matei Alexandru Zaharia: Ah, sorry about that -- will look at 826 first but I've been backlogged.\n4. Dmitriy Lyubimov: this is now merged as part of 826 PR. can be resolved.\n5. Andrew Ash: Closing per [~dlyubimov] with the Fix Version from SPARK-826", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "e58acc83cf907d6c757301d98c2e7911", "issue_key": "SPARK-836", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ResultTask's serialization forget to handle generation", "description": "", "reporter": "Andy Huang", "assignee": null, "created": "2013-07-30T07:14:16.000+0000", "updated": "2013-12-07T14:37:30.000+0000", "resolved": "2013-12-07T14:37:30.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Aaron Davidson", "body": "Could you provide elaboration on this issue? If not, I think we'll have to close it.", "created": "2013-11-14T18:23:43.551+0000"}], "num_comments": 1, "text": "Issue: SPARK-836\nSummary: ResultTask's serialization forget to handle generation\n\nComments (1):\n1. Aaron Davidson: Could you provide elaboration on this issue? If not, I think we'll have to close it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "d1bc6dedaf8aecc86279309340570686", "issue_key": "SPARK-837", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "ResultTask's serialization forget about handling \"generation\" field, while ShuffleMapTask does", "description": "In ResultTask's serialization relative method: writeExternal and readExternal, they didn't do anything to generation. But in ShuffleMapTask's method, writeExternal and readExternal, they do something like \"partition = in.readInt()\" and \" out.writeLong(generation)\" to them. As we know ResultTask will be used after ShuffleMapTask, if right after ShuffleMapTask finish and the work failed for some reason, It will be recomputed, with a \"generation\" bigger than -1. The ResultTask can't get the right data again with default generation, that it will ask DAGScheduler to recompter ShuffleMapTask again. This will last until the whole job crash.", "reporter": "Andy Huang", "assignee": null, "created": "2013-07-30T07:29:52.000+0000", "updated": "2013-12-07T14:37:23.000+0000", "resolved": "2013-09-09T18:03:27.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "This does seem like a bug -- thanks for reporting it! Going to bump it up in priority.", "created": "2013-07-30T12:10:25.190+0000"}, {"author": "Patrick McFadin", "body": "This is now fixed as far as I can tell because the epoch is serialized with the ResultTask.", "created": "2013-09-09T18:03:19.628+0000"}], "num_comments": 2, "text": "Issue: SPARK-837\nSummary: ResultTask's serialization forget about handling \"generation\" field, while ShuffleMapTask does\nDescription: In ResultTask's serialization relative method: writeExternal and readExternal, they didn't do anything to generation. But in ShuffleMapTask's method, writeExternal and readExternal, they do something like \"partition = in.readInt()\" and \" out.writeLong(generation)\" to them. As we know ResultTask will be used after ShuffleMapTask, if right after ShuffleMapTask finish and the work failed for some reason, It will be recomputed, with a \"generation\" bigger than -1. The ResultTask can't get the right data again with default generation, that it will ask DAGScheduler to recompter ShuffleMapTask again. This will last until the whole job crash.\n\nComments (2):\n1. Matei Alexandru Zaharia: This does seem like a bug -- thanks for reporting it! Going to bump it up in priority.\n2. Patrick McFadin: This is now fixed as far as I can tell because the epoch is serialized with the ResultTask.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "eabe20b93c659ac9754568a3ae6f9818", "issue_key": "SPARK-838", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add DoubleRDDFunctions methods to PySpark", "description": "This is sum(), stats(), mean(), variance(), etc. Instead of trying to convert Python doubles to Scala, we'll probably have to reimplement the Scala/Java StatCounter in Python.", "reporter": "Matei Alexandru Zaharia", "assignee": "Andre Schumacher", "created": "2013-07-30T20:55:42.000+0000", "updated": "2013-08-22T16:24:55.000+0000", "resolved": "2013-08-21T18:00:02.000+0000", "labels": ["Starter"], "components": ["PySpark"], "comments": [{"author": "Andre Schumacher", "body": "Resolved by merged pull request #853.", "created": "2013-08-21T17:59:51.011+0000"}], "num_comments": 1, "text": "Issue: SPARK-838\nSummary: Add DoubleRDDFunctions methods to PySpark\nDescription: This is sum(), stats(), mean(), variance(), etc. Instead of trying to convert Python doubles to Scala, we'll probably have to reimplement the Scala/Java StatCounter in Python.\n\nComments (1):\n1. Andre Schumacher: Resolved by merged pull request #853.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "052c91cb6e687fdf2e0722dc04d0f4de", "issue_key": "SPARK-839", "issue_type": "Bug", "status": "Closed", "priority": "Critical", "resolution": null, "summary": "Bug in how failed executors are removed by ID from standalone cluster", "description": "ClearStory data reported the following issue, where some hashmaps are indexed by executorId and some by appId/executorId, and we use the wrong string to search for an executor: https://github.com/clearstorydata/spark/pull/9. This affects FT on the standalone mode.", "reporter": "Mark Hamstra", "assignee": null, "created": "2013-07-31T09:54:38.000+0000", "updated": "2015-02-09T02:51:58.000+0000", "resolved": "2015-02-09T02:51:57.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Mark Hamstra", "body": "Related user list report: https://groups.google.com/forum/?fromgroups=#!topic/spark-users/DXIOjT_DP0Y", "created": "2013-08-19T00:29:32.310+0000"}, {"author": "Mark Hamstra", "body": "Fixed long ago.", "created": "2015-02-09T02:51:40.548+0000"}], "num_comments": 2, "text": "Issue: SPARK-839\nSummary: Bug in how failed executors are removed by ID from standalone cluster\nDescription: ClearStory data reported the following issue, where some hashmaps are indexed by executorId and some by appId/executorId, and we use the wrong string to search for an executor: https://github.com/clearstorydata/spark/pull/9. This affects FT on the standalone mode.\n\nComments (2):\n1. Mark Hamstra: Related user list report: https://groups.google.com/forum/?fromgroups=#!topic/spark-users/DXIOjT_DP0Y\n2. Mark Hamstra: Fixed long ago.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "2a469838677321a589e0dc1e3c8a5426", "issue_key": "SPARK-840", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.", "description": "Even though the distribution has everything it needs invoking `spark-shell` now errors with: SCALA_HOME is not set and scala is not in PATH It looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1.", "reporter": "Benjamin Hindman", "assignee": "Benjamin Hindman", "created": "2013-07-31T10:45:41.000+0000", "updated": "2013-07-31T16:40:12.000+0000", "resolved": "2013-07-31T16:40:12.000+0000", "labels": [], "components": [], "comments": [{"author": "Benjamin Hindman", "body": "Add a fix for this into the pull request at https://github.com/mesos/spark/pull/749.", "created": "2013-07-31T12:52:27.748+0000"}, {"author": "Benjamin Hindman", "body": "This can be marked as resolved, it was included in https://github.com/mesos/spark/pull/749.", "created": "2013-07-31T16:32:15.787+0000"}], "num_comments": 2, "text": "Issue: SPARK-840\nSummary: Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.\nDescription: Even though the distribution has everything it needs invoking `spark-shell` now errors with: SCALA_HOME is not set and scala is not in PATH It looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1.\n\nComments (2):\n1. Benjamin Hindman: Add a fix for this into the pull request at https://github.com/mesos/spark/pull/749.\n2. Benjamin Hindman: This can be marked as resolved, it was included in https://github.com/mesos/spark/pull/749.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "47a1b38a4c60a10d417624d4c23157de", "issue_key": "SPARK-841", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Use a smaller job UI port than 33000 by default", "description": "33000 is in the ephemeral port range so it makes it more likely that your job sometimes binds to a higher port than expected. Note that this change should also be reflected in the EC2 scripts, as they open port 33000-33010 currently to let users view the job UI.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-31T11:04:14.000+0000", "updated": "2013-09-01T15:33:14.000+0000", "resolved": "2013-09-01T15:33:14.000+0000", "labels": ["Starter"], "components": ["Web UI"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/commit/498a26189b197bdaf4be47e6a8baca7b97fe9064 and https://github.com/mesos/spark/commit/793a722f8e14552b8d36f46cca39d336dc2df9dd", "created": "2013-09-01T15:33:14.088+0000"}], "num_comments": 1, "text": "Issue: SPARK-841\nSummary: Use a smaller job UI port than 33000 by default\nDescription: 33000 is in the ephemeral port range so it makes it more likely that your job sometimes binds to a higher port than expected. Note that this change should also be reflected in the EC2 scripts, as they open port 33000-33010 currently to let users view the job UI.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/commit/498a26189b197bdaf4be47e6a8baca7b97fe9064 and https://github.com/mesos/spark/commit/793a722f8e14552b8d36f46cca39d336dc2df9dd", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "6314428fcbd94b4e0c445ffd60509fbf", "issue_key": "SPARK-842", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Maven assembly is including examples libs and dependencies", "description": "According to this [email exchange|http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201307.mbox/%3C62DB7E7A-0547-4090-BB9A-0182829A0D19%40gmail.com%3E] final assembly has to include \"...libraries that users' client programs need to run. These are core, repl (needed if they use the shell), and likely bagel and streaming and mllib,\" Hence, current Maven assembly needs to be fixed accordinly to exclude examples/. This fix will also affect BIGTOP-715.", "reporter": "Konstantin I Boudnik", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-31T14:32:51.000+0000", "updated": "2013-10-10T18:05:02.000+0000", "resolved": "2013-10-10T18:05:02.000+0000", "labels": [], "components": ["Build"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-842\nSummary: Maven assembly is including examples libs and dependencies\nDescription: According to this [email exchange|http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201307.mbox/%3C62DB7E7A-0547-4090-BB9A-0182829A0D19%40gmail.com%3E] final assembly has to include \"...libraries that users' client programs need to run. These are core, repl (needed if they use the shell), and likely bagel and streaming and mllib,\" Hence, current Maven assembly needs to be fixed accordinly to exclude examples/. This fix will also affect BIGTOP-715.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "0af331d7ab8b6fb3b24f4829e8b0097f", "issue_key": "SPARK-843", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Show time the app has been running for in job UI", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-31T19:29:17.000+0000", "updated": "2013-08-05T10:15:21.000+0000", "resolved": "2013-08-05T10:15:21.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-843\nSummary: Show time the app has been running for in job UI", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "39e8778766b61f3f1931ea616bcc85da", "issue_key": "SPARK-844", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Occasional hang on shuffle fetches in master branch", "description": "In running some iterative jobs with lots of shuffles, I've occasionally seen a few tasks hung with this kind of stack trace:  \"pool-5-thread-1\" prio=10 tid=0x00007f6530049000 nid=0x1217 waiting on condition [0x00007f65d7cfa000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for <0x00007f6d17a1e298> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:246) at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:71) at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26) at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451) at spark.Aggregator.combineValuesByKey(Aggregator.scala:37) at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:32) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:161) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:93) at spark.executor.Executor$TaskRunner.run(Executor.scala:129) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:724)", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick McFadin", "created": "2013-07-31T22:59:38.000+0000", "updated": "2020-05-17T18:30:24.000+0000", "resolved": "2013-08-10T16:17:52.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Hey [~matei] did you look at the logs at all for these nodes? It would be helpful to know whether there was any log output.", "created": "2013-08-04T19:31:25.551+0000"}, {"author": "Haoyuan Li", "body": "Attach a hung machine's stderr and jstack.", "created": "2013-08-05T13:32:25.862+0000"}], "num_comments": 2, "text": "Issue: SPARK-844\nSummary: Occasional hang on shuffle fetches in master branch\nDescription: In running some iterative jobs with lots of shuffles, I've occasionally seen a few tasks hung with this kind of stack trace:  \"pool-5-thread-1\" prio=10 tid=0x00007f6530049000 nid=0x1217 waiting on condition [0x00007f65d7cfa000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for <0x00007f6d17a1e298> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:246) at spark.storage.BlockFetcherIterator$BasicBlockFetcherIterator.next(BlockFetcherIterator.scala:71) at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) at spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26) at scala.collection.Iterator$$anon$22.hasNext(Iterator.scala:457) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$22.foreach(Iterator.scala:451) at spark.Aggregator.combineValuesByKey(Aggregator.scala:37) at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) at spark.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) at spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:141) at spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:138) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:138) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.MappedValuesRDD.compute(PairRDDFunctions.scala:758) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.FlatMappedValuesRDD.compute(PairRDDFunctions.scala:768) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:32) at spark.RDD.computeOrReadCheckpoint(RDD.scala:252) at spark.RDD.iterator(RDD.scala:241) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:161) at spark.scheduler.ShuffleMapTask.run(ShuffleMapTask.scala:93) at spark.executor.Executor$TaskRunner.run(Executor.scala:129) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:724)\n\nComments (2):\n1. Patrick McFadin: Hey [~matei] did you look at the logs at all for these nodes? It would be helpful to know whether there was any log output.\n2. Haoyuan Li: Attach a hung machine's stderr and jstack.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "70a7b89a551ab42b097d095c4624ca6b", "issue_key": "SPARK-845", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Removing an executor can result in a negative number of cores used", "description": "Under some circumstances, an executor dying or being removed will result in a negative number of cores granted to and reported as used by an application. Still working on isolating the code paths that produce this result, but one know way to reproduce the problem is to kill a running executor with SIGKILL (kill -9). If the executor is killed with SIGTERM, then the correct number of cores are removed from running applications; but SIGKILL seems to result in spark.deploy.master.ApplicationInfo.removeExecutor being called multiple times. A proposed fix is available at https://github.com/clearstorydata/spark/commit/96f5e70fdbf21e188f4fc76a30957cc78302037f", "reporter": "Mark Hamstra", "assignee": null, "created": "2013-08-01T11:11:49.000+0000", "updated": "2013-11-14T18:22:39.000+0000", "resolved": "2013-11-14T18:22:39.000+0000", "labels": [], "components": ["Deploy", "Spark Core"], "comments": [{"author": "Aaron Davidson", "body": "Just marking as fixed -- thanks Mark!", "created": "2013-11-14T18:22:39.787+0000"}], "num_comments": 1, "text": "Issue: SPARK-845\nSummary: Removing an executor can result in a negative number of cores used\nDescription: Under some circumstances, an executor dying or being removed will result in a negative number of cores granted to and reported as used by an application. Still working on isolating the code paths that produce this result, but one know way to reproduce the problem is to kill a running executor with SIGKILL (kill -9). If the executor is killed with SIGTERM, then the correct number of cores are removed from running applications; but SIGKILL seems to result in spark.deploy.master.ApplicationInfo.removeExecutor being called multiple times. A proposed fix is available at https://github.com/clearstorydata/spark/commit/96f5e70fdbf21e188f4fc76a30957cc78302037f\n\nComments (1):\n1. Aaron Davidson: Just marking as fixed -- thanks Mark!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "8b4812005e5721ef4512e7e3d09b8c1b", "issue_key": "SPARK-846", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Set `spark.job.annotation` and display it in the web UI.", "description": "", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-08-01T13:31:15.000+0000", "updated": "2013-08-10T16:17:40.000+0000", "resolved": "2013-08-10T16:17:40.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-846\nSummary: Set `spark.job.annotation` and display it in the web UI.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "63d05ca610b5b490400c767c6db30b6c", "issue_key": "SPARK-847", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Zombie workers", "description": "spark.deploy.master.Worker contains 'workers', a HashSet[WorkerInfo]. The only place where a worker is ever removed from that set is within Worker.addWorker. Within addWorker, DEAD workers are only removed from the set if they were using both the same host address and port as the new worker being added. The result of this is that DEAD workers hang around forever if a new worker is never added or is added with a different host:port; and every WORKER_TIMEOUT interval, timeOutDeadWorkers again tries to remove them. It looks like one or both of two things needs to happen at least some of the time (proper conditions?): 1) Worker.removeWorker(worker) should remove the worker from the workers HashSet in addition to removing its associated entries in idToWorker, actorToWorker and addressToWorker; 2) addWorker should remove a DEAD worker from the set of workers even when there isn't an exact host:port match. Worker.removeWorker being called for zombies each WORKER_TIMEOUT is at least one of the reasons for core counts going negative: SPARK-845", "reporter": "Mark Hamstra", "assignee": null, "created": "2013-08-01T14:04:06.000+0000", "updated": "2013-11-14T18:21:48.000+0000", "resolved": "2013-11-14T18:21:48.000+0000", "labels": [], "components": ["Deploy", "Spark Core"], "comments": [{"author": "Mikhail Bautin", "body": "It looks like worker ports are random in practice, but in theory there could be more than one worker on the same node, which would probably make removing dead worker(s) based only on the hostname infeasible. However, a timeout-based approach to cleaning up dead workers sounds reasonable.", "created": "2013-08-01T15:07:06.879+0000"}, {"author": "Mark Hamstra", "body": "https://github.com/markhamstra/spark/commit/8e86fa5a11102e113559b7e200e594a05c9d16ef", "created": "2013-08-01T15:15:43.590+0000"}, {"author": "Aaron Davidson", "body": "Just marking as fixed -- thanks Mark!", "created": "2013-11-14T18:21:48.345+0000"}], "num_comments": 3, "text": "Issue: SPARK-847\nSummary: Zombie workers\nDescription: spark.deploy.master.Worker contains 'workers', a HashSet[WorkerInfo]. The only place where a worker is ever removed from that set is within Worker.addWorker. Within addWorker, DEAD workers are only removed from the set if they were using both the same host address and port as the new worker being added. The result of this is that DEAD workers hang around forever if a new worker is never added or is added with a different host:port; and every WORKER_TIMEOUT interval, timeOutDeadWorkers again tries to remove them. It looks like one or both of two things needs to happen at least some of the time (proper conditions?): 1) Worker.removeWorker(worker) should remove the worker from the workers HashSet in addition to removing its associated entries in idToWorker, actorToWorker and addressToWorker; 2) addWorker should remove a DEAD worker from the set of workers even when there isn't an exact host:port match. Worker.removeWorker being called for zombies each WORKER_TIMEOUT is at least one of the reasons for core counts going negative: SPARK-845\n\nComments (3):\n1. Mikhail Bautin: It looks like worker ports are random in practice, but in theory there could be more than one worker on the same node, which would probably make removing dead worker(s) based only on the hostname infeasible. However, a timeout-based approach to cleaning up dead workers sounds reasonable.\n2. Mark Hamstra: https://github.com/markhamstra/spark/commit/8e86fa5a11102e113559b7e200e594a05c9d16ef\n3. Aaron Davidson: Just marking as fixed -- thanks Mark!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "7c6b71e435c83b62ebcc4baef7b62ec5", "issue_key": "SPARK-848", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "The StageTable should sort by submitted time by default", "description": "I think you can set this using the Javascript sorting library. It would be good if it showed the little sort arrow when it's loaded so people know they can sort on other columns.", "reporter": "Patrick McFadin", "assignee": "Karen Feng", "created": "2013-08-02T15:12:11.000+0000", "updated": "2013-08-06T19:54:50.000+0000", "resolved": "2013-08-06T19:54:50.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-848\nSummary: The StageTable should sort by submitted time by default\nDescription: I think you can set this using the Javascript sorting library. It would be good if it showed the little sort arrow when it's loaded so people know they can sort on other columns.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "bab2adf70516d169bf3bb3cf726749ca", "issue_key": "SPARK-849", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Variety of small fixes in the Web UI", "description": "After discussion with Reynold we came up with some to-do's: Jobs Page: - Summary count at the top for active completed stages should link to corresponding sections in page - Remove `Stored RDD` Column - Should say \"Running/Succeeded\" instead of \"Running/Completed\" RDD Page - For some reason, header says \"Jobs\" in RDD storage page - Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part) Environment page: - Sort all tables by the first column (A -> Z) by default Header: - Move Jobs tab before storage - Right-align the \"Application name\" part if possible - Remove the executor count from the header We also had some other things I want to note here, but probably aren't in scope for this because they are trickier: - Clicking Spark logo should go to default page - Task progress should be overlaid with progress bar", "reporter": "Patrick McFadin", "assignee": "Karen Feng", "created": "2013-08-02T16:02:09.000+0000", "updated": "2013-08-06T19:55:14.000+0000", "resolved": "2013-08-06T19:55:14.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-849\nSummary: Variety of small fixes in the Web UI\nDescription: After discussion with Reynold we came up with some to-do's: Jobs Page: - Summary count at the top for active completed stages should link to corresponding sections in page - Remove `Stored RDD` Column - Should say \"Running/Succeeded\" instead of \"Running/Completed\" RDD Page - For some reason, header says \"Jobs\" in RDD storage page - Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part) Environment page: - Sort all tables by the first column (A -> Z) by default Header: - Move Jobs tab before storage - Right-align the \"Application name\" part if possible - Remove the executor count from the header We also had some other things I want to note here, but probably aren't in scope for this because they are trickier: - Clicking Spark logo should go to default page - Task progress should be overlaid with progress bar", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "b985d3c137bf805a73df713d71df2429", "issue_key": "SPARK-850", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered", "description": "When you running the spark job with SPARK_MEM set too large, you will receive the following error \"WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\" Could you give a better error message in the console. Step to recreate the issue: 1. set the SPARK_MEM in conf/spark-env.sh close to your memory limit on the box. 2. run the interactive Spark shell against the local cluster; for me is \"MASTER=spark://billz-retina.local:7077 ./spark-shell\" 3. run the spark job; i.e. \"val a = sc.parallelize(1 to 100)\", a.count(); you will see console error: 13/08/02 16:45:01 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered 13/08/02 16:45:16 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered In logs/spark-bill-spark.deploy.master.Master-1-billz-retina.local.out: 13/08/02 16:43:22 INFO master.Master: Registering app Spark shell 13/08/02 16:43:22 WARN master.Master: Could not find any workers with enough memory for app-20130802163627-0000 13/08/02 16:43:22 INFO master.Master: Registered app Spark shell with ID app-20130802164322-0002", "reporter": "William Zajac", "assignee": "William Zajac", "created": "2013-08-02T17:43:19.000+0000", "updated": "2013-08-05T12:10:51.000+0000", "resolved": "2013-08-05T12:10:51.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Hey Bill, It might be a good idea to append \"and have sufficient memory\" to the warning message at the driver. Would you mind submitting a pull request for this? - Patrick", "created": "2013-08-02T20:54:13.072+0000"}, {"author": "William Zajac", "body": "I will submit a pull request.", "created": "2013-08-05T09:53:24.738+0000"}], "num_comments": 2, "text": "Issue: SPARK-850\nSummary: WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\nDescription: When you running the spark job with SPARK_MEM set too large, you will receive the following error \"WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered\" Could you give a better error message in the console. Step to recreate the issue: 1. set the SPARK_MEM in conf/spark-env.sh close to your memory limit on the box. 2. run the interactive Spark shell against the local cluster; for me is \"MASTER=spark://billz-retina.local:7077 ./spark-shell\" 3. run the spark job; i.e. \"val a = sc.parallelize(1 to 100)\", a.count(); you will see console error: 13/08/02 16:45:01 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered 13/08/02 16:45:16 WARN cluster.ClusterScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered In logs/spark-bill-spark.deploy.master.Master-1-billz-retina.local.out: 13/08/02 16:43:22 INFO master.Master: Registering app Spark shell 13/08/02 16:43:22 WARN master.Master: Could not find any workers with enough memory for app-20130802163627-0000 13/08/02 16:43:22 INFO master.Master: Registered app Spark shell with ID app-20130802164322-0002\n\nComments (2):\n1. Patrick McFadin: Hey Bill, It might be a good idea to append \"and have sufficient memory\" to the warning message at the driver. Would you mind submitting a pull request for this? - Patrick\n2. William Zajac: I will submit a pull request.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "630e14da04624a400506539693609a22", "issue_key": "SPARK-851", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Summary count at the top for active completed stages should link to corresponding sections in page", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:10:01.000+0000", "updated": "2013-08-06T19:56:41.000+0000", "resolved": "2013-08-06T19:56:41.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-851\nSummary: Summary count at the top for active completed stages should link to corresponding sections in page", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "376f763e2b245aa0538472799775c5d8", "issue_key": "SPARK-852", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Remove `Stored RDD` Column", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:10:46.000+0000", "updated": "2013-08-06T19:56:49.000+0000", "resolved": "2013-08-06T19:56:49.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-852\nSummary: Remove `Stored RDD` Column", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "f47594d53d75eec28f6bded458b9fd55", "issue_key": "SPARK-853", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Should say \"Running/Succeeded\" instead of \"Running/Completed\"", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:11:01.000+0000", "updated": "2013-08-06T19:56:57.000+0000", "resolved": "2013-08-06T19:56:57.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-853\nSummary: Should say \"Running/Succeeded\" instead of \"Running/Completed\"", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "7bc9ed90531a3b987421f400eeb756e9", "issue_key": "SPARK-854", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "For some reason, header says \"Jobs\" in RDD storage page", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:11:12.000+0000", "updated": "2013-08-06T19:57:03.000+0000", "resolved": "2013-08-06T19:57:03.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-854\nSummary: For some reason, header says \"Jobs\" in RDD storage page", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.068525"}}
{"id": "9c9fadf599614d962e677f84c33c9140", "issue_key": "SPARK-855", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part)", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:11:21.000+0000", "updated": "2013-08-06T19:57:09.000+0000", "resolved": "2013-08-06T19:57:09.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-855\nSummary: Page should have two titles: \"Data Distribution Summary\" (top part), \"Partitions\" (bottom part)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "4f10d0b3887dc23c2e6f746baff3b4df", "issue_key": "SPARK-856", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Sort all tables by the first column (A -> Z) by default", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:11:33.000+0000", "updated": "2013-08-06T19:55:39.000+0000", "resolved": "2013-08-06T19:55:39.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-856\nSummary: Sort all tables by the first column (A -> Z) by default", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "f597e40fbd02617adb35900c0cafae94", "issue_key": "SPARK-857", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Move Jobs tab before storage", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:11:45.000+0000", "updated": "2013-08-06T19:57:15.000+0000", "resolved": "2013-08-06T19:57:15.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-857\nSummary: Move Jobs tab before storage", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "155b44a8975673b4e6161bc8d447d5b4", "issue_key": "SPARK-858", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Right-align the \"Application name\" part if possible", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:11:55.000+0000", "updated": "2013-08-06T19:57:21.000+0000", "resolved": "2013-08-06T19:57:21.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-858\nSummary: Right-align the \"Application name\" part if possible", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "0a619fe3dc070b4556d25877c95c2ffc", "issue_key": "SPARK-859", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Remove the executor count from the header", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:12:06.000+0000", "updated": "2013-08-06T19:57:27.000+0000", "resolved": "2013-08-06T19:57:27.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-859\nSummary: Remove the executor count from the header", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "fa0b4019656c647944237e95a34b6e5c", "issue_key": "SPARK-860", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Clicking Spark logo should go to default page", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:12:16.000+0000", "updated": "2013-08-06T19:55:49.000+0000", "resolved": "2013-08-06T19:55:48.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-860\nSummary: Clicking Spark logo should go to default page", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "fd9d50c401e35ec647a3ed04a7c6a4f8", "issue_key": "SPARK-861", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Task progress should be overlaid with progress bar", "description": "", "reporter": "Karen Feng", "assignee": "Karen Feng", "created": "2013-08-05T13:12:25.000+0000", "updated": "2013-08-06T19:55:29.000+0000", "resolved": "2013-08-06T19:55:29.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-861\nSummary: Task progress should be overlaid with progress bar", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "ff050354ecda682ce3f64f5dbf5c9410", "issue_key": "SPARK-862", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Could not use spark-ec2 to launch clusters with instance type 'cc1.4xlarge'", "description": "Error message: Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'. I believe this apply for other Cluster Compute instances as well, which must use HVM-based AMI, see: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cluster_computing.html  $ spark-ec2 -k mykey -i mykey.pem -s 4 --instance-type=cc1.4xlarge -w 120 --zone=us-east-1e --cluster-type=mesos launch spark3 Setting up security groups... Searching for existing cluster ada-spark3... Latest Spark AMI: ami-530f7a3a Launching instances... ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterCombination</Code><Message>Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'.</Message></Error></Errors><RequestID>c7bceb36-42f0-4f5d-95b1-8f6b50930cfb</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 761, in <module> main() File \"./spark_ec2.py\", line 621, in main conn, opts, cluster_name) File \"./spark_ec2.py\", line 329, in launch_cluster block_device_map = block_map) File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/image.py\", line 255, in run File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 678, in run_instances File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 925, in get_object boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterCombination</Code><Message>Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'.</Message></Error></Errors><RequestID>c7bceb36-42f0-4f5d-95b1-8f6b50930cfb</RequestID></Response>", "reporter": "Hai-Anh Trinh", "assignee": null, "created": "2013-08-05T22:43:09.000+0000", "updated": "2013-08-20T16:10:30.000+0000", "resolved": "2013-08-20T16:10:12.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Reynold Xin", "body": "This has been fixed in Patrick's latest EC2 scripts.", "created": "2013-08-05T22:47:09.130+0000"}, {"author": "Hai-Anh Trinh", "body": "The mentioned fix is at https://github.com/mesos/spark/pull/603, right ? Also, do you want to mark this issue as dup of https://spark-project.atlassian.net/browse/SPARK-728 ?", "created": "2013-08-08T02:23:42.916+0000"}, {"author": "Patrick McFadin", "body": "Yep - this duplicates SPARK-728", "created": "2013-08-20T16:10:30.991+0000"}], "num_comments": 3, "text": "Issue: SPARK-862\nSummary: Could not use spark-ec2 to launch clusters with instance type 'cc1.4xlarge'\nDescription: Error message: Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'. I believe this apply for other Cluster Compute instances as well, which must use HVM-based AMI, see: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cluster_computing.html  $ spark-ec2 -k mykey -i mykey.pem -s 4 --instance-type=cc1.4xlarge -w 120 --zone=us-east-1e --cluster-type=mesos launch spark3 Setting up security groups... Searching for existing cluster ada-spark3... Latest Spark AMI: ami-530f7a3a Launching instances... ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterCombination</Code><Message>Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'.</Message></Error></Errors><RequestID>c7bceb36-42f0-4f5d-95b1-8f6b50930cfb</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 761, in <module> main() File \"./spark_ec2.py\", line 621, in main conn, opts, cluster_name) File \"./spark_ec2.py\", line 329, in launch_cluster block_device_map = block_map) File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/image.py\", line 255, in run File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 678, in run_instances File \"/Users/aht/src/adatao/BigR/Installer/spark-ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 925, in get_object boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterCombination</Code><Message>Virtualization type 'hvm' is required for instances of type 'cc1.4xlarge'.</Message></Error></Errors><RequestID>c7bceb36-42f0-4f5d-95b1-8f6b50930cfb</RequestID></Response>\n\nComments (3):\n1. Reynold Xin: This has been fixed in Patrick's latest EC2 scripts.\n2. Hai-Anh Trinh: The mentioned fix is at https://github.com/mesos/spark/pull/603, right ? Also, do you want to mark this issue as dup of https://spark-project.atlassian.net/browse/SPARK-728 ?\n3. Patrick McFadin: Yep - this duplicates SPARK-728", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "15a86df65a9d7e3ab85c044eb7754828", "issue_key": "SPARK-863", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Have Synchronous Versions of `stop-all.sh` and `start-all.sh`", "description": "When doing things like restarting the cluster, it's sometimes necessary to know when stopping has completed (it can take a long time to delete temporary files). We should have a synchronous version of these scripts that won't return until the cluster is actually stopped.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-08-06T14:55:53.000+0000", "updated": "2015-02-26T01:26:55.000+0000", "resolved": "2015-02-26T01:26:55.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Sean R. Owen", "body": "Despite the title it seems like this is about stopping only, and {{stop-all.sh}} has a {{--wait}} flag now. I'm not as sure how you wait on startup since it would require knowing the services were up and healthy.", "created": "2015-02-26T01:26:55.815+0000"}], "num_comments": 1, "text": "Issue: SPARK-863\nSummary: Have Synchronous Versions of `stop-all.sh` and `start-all.sh`\nDescription: When doing things like restarting the cluster, it's sometimes necessary to know when stopping has completed (it can take a long time to delete temporary files). We should have a synchronous version of these scripts that won't return until the cluster is actually stopped.\n\nComments (1):\n1. Sean R. Owen: Despite the title it seems like this is about stopping only, and {{stop-all.sh}} has a {{--wait}} flag now. I'm not as sure how you wait on startup since it would require knowing the services were up and healthy.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "e41546a90c58ef2551486f2a858ce467", "issue_key": "SPARK-864", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "DAGScheduler Exception if A Node is Added then Deleted", "description": "According to [~markhamstra], if you run the UI tester locally and remove a slave, then add another slave, everything freezes. UPDATE: This appears to be caused by the DAGScheduler:  Exception in thread \"DAGScheduler\" java.util.NoSuchElementException: key not found: 2 at scala.collection.MapLike$class.default(MapLike.scala:225) at scala.collection.mutable.HashMap.default(HashMap.scala:45) at scala.collection.MapLike$class.apply(MapLike.scala:135) at scala.collection.mutable.HashMap.apply(HashMap.scala:45) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:515) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:481) at spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:383) at spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:382) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.scheduler.DAGScheduler.resubmitFailedStages(DAGScheduler.scala:382) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:433) at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:135)  This code is related to the FairScheduler change. Hey [~andrew xia] - could you take a look at this?", "reporter": "Patrick McFadin", "assignee": "xiajunluan", "created": "2013-08-06T20:41:25.000+0000", "updated": "2020-05-17T17:47:27.000+0000", "resolved": "2015-05-15T13:49:49.000+0000", "labels": [], "components": ["Scheduler", "Spark Core"], "comments": [{"author": "Mark Hamstra", "body": "Right, so the scenario is: ./run spark.deploy.master.Master ./run spark.deploy.worker.Worker spark://... ./run spark.ui.UIWorkloadGenerator spark://... ...everything looks lovely in the WebUI... ...kill the worker ...everything is dead in the WebUI (obviously, since there is no worker available) ./run spark.deploy.worker.Worker spark://... ...looks like UIWorkloadGenerator is trying to launch new jobs with the new Worker... ...but the application details page of the WebUI doesn't change from when the first Worker was killed I haven't worked out just how serious a problem this really is (i.e. whether something similar happens with real jobs, or whether this is just an artifact of how the UIWorkloadGenerator works); but regardless, I don't think it really shows what you want it to right now.", "created": "2013-08-06T21:42:15.051+0000"}, {"author": "xiajunluan", "body": "Sure, I will look into this issue.", "created": "2013-08-07T23:17:21.710+0000"}, {"author": "Sean R. Owen", "body": "Closing as it looks exceptionally stale at this point and haven't seen similar reports", "created": "2015-05-15T13:49:49.861+0000"}], "num_comments": 3, "text": "Issue: SPARK-864\nSummary: DAGScheduler Exception if A Node is Added then Deleted\nDescription: According to [~markhamstra], if you run the UI tester locally and remove a slave, then add another slave, everything freezes. UPDATE: This appears to be caused by the DAGScheduler:  Exception in thread \"DAGScheduler\" java.util.NoSuchElementException: key not found: 2 at scala.collection.MapLike$class.default(MapLike.scala:225) at scala.collection.mutable.HashMap.default(HashMap.scala:45) at scala.collection.MapLike$class.apply(MapLike.scala:135) at scala.collection.mutable.HashMap.apply(HashMap.scala:45) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:515) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:481) at spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:383) at spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:382) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) at spark.scheduler.DAGScheduler.resubmitFailedStages(DAGScheduler.scala:382) at spark.scheduler.DAGScheduler.spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:433) at spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:135)  This code is related to the FairScheduler change. Hey [~andrew xia] - could you take a look at this?\n\nComments (3):\n1. Mark Hamstra: Right, so the scenario is: ./run spark.deploy.master.Master ./run spark.deploy.worker.Worker spark://... ./run spark.ui.UIWorkloadGenerator spark://... ...everything looks lovely in the WebUI... ...kill the worker ...everything is dead in the WebUI (obviously, since there is no worker available) ./run spark.deploy.worker.Worker spark://... ...looks like UIWorkloadGenerator is trying to launch new jobs with the new Worker... ...but the application details page of the WebUI doesn't change from when the first Worker was killed I haven't worked out just how serious a problem this really is (i.e. whether something similar happens with real jobs, or whether this is just an artifact of how the UIWorkloadGenerator works); but regardless, I don't think it really shows what you want it to right now.\n2. xiajunluan: Sure, I will look into this issue.\n3. Sean R. Owen: Closing as it looks exceptionally stale at this point and haven't seen similar reports", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "37eee6ba1afbbfb9e2a4cdf80421166c", "issue_key": "SPARK-865", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add the equivalent of ADD_JARS to PySpark", "description": "There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.", "reporter": "Matei Alexandru Zaharia", "assignee": "Andre Schumacher", "created": "2013-08-06T23:42:49.000+0000", "updated": "2013-08-12T21:46:19.000+0000", "resolved": "2013-08-12T21:03:43.000+0000", "labels": ["Starter"], "components": ["PySpark"], "comments": [{"author": "Andre Schumacher", "body": "PySpark's SparkContext has a pyFiles constructor parameter which seems to do exactly what is required. How about naming this environment variable then PY_FILES ? Or PYSPARK_FILES to be less generic?", "created": "2013-08-12T16:50:26.449+0000"}, {"author": "Holden Karau", "body": "Created a pull request which uses PYSPAKR_FILES", "created": "2013-08-12T21:46:19.786+0000"}], "num_comments": 2, "text": "Issue: SPARK-865\nSummary: Add the equivalent of ADD_JARS to PySpark\nDescription: There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.\n\nComments (2):\n1. Andre Schumacher: PySpark's SparkContext has a pyFiles constructor parameter which seems to do exactly what is required. How about naming this environment variable then PY_FILES ? Or PYSPARK_FILES to be less generic?\n2. Holden Karau: Created a pull request which uses PYSPAKR_FILES", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "fc6563da2966842275b7a42b0c863b91", "issue_key": "SPARK-866", "issue_type": "New Feature", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Add the equivalent of ADD_JARS to PySpark", "description": "There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-06T23:43:01.000+0000", "updated": "2013-08-12T17:25:38.000+0000", "resolved": "2013-08-06T23:43:17.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Andre Schumacher", "body": "PySpark's SparkContext has a pyFiles constructor parameter which seems to do exactly what is required. How about naming this environment variable then PY_FILES ? Or PYSPARK_FILES to be less generic?", "created": "2013-08-12T16:38:08.249+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I'd actually call it ADD_FILES to be similar to the Scala one. It's only a way of passing a parameter to the ./pyspark binary, so I don't want it to have a name that makes it sound like it would work for an arbitrary PySpark program.", "created": "2013-08-12T17:25:38.227+0000"}], "num_comments": 2, "text": "Issue: SPARK-866\nSummary: Add the equivalent of ADD_JARS to PySpark\nDescription: There should be an environment variable for the user to specify zip files / eggs to be passed to SparkContext and sent to workers.\n\nComments (2):\n1. Andre Schumacher: PySpark's SparkContext has a pyFiles constructor parameter which seems to do exactly what is required. How about naming this environment variable then PY_FILES ? Or PYSPARK_FILES to be less generic?\n2. Matei Alexandru Zaharia: I'd actually call it ADD_FILES to be similar to the Scala one. It's only a way of passing a parameter to the ./pyspark binary, so I don't want it to have a name that makes it sound like it would work for an arbitrary PySpark program.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "dc5cf07d6bb1aeb5dfc5cef0aed655e2", "issue_key": "SPARK-867", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add a native Python way to create input RDDs in PySpark", "description": "While PySpark can easily read RDDs of Strings from Java/Scala, it would be nice to create your own subclass that implements the partitions(), preferredLocations() and compute() methods as in Java, to plug in new input sources. It shouldn't be too hard to turn this into a special RDD on the Java side by, say, parallelizing an array of partition objects and piping them through Python.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-07T16:49:32.000+0000", "updated": "2015-05-31T20:59:08.000+0000", "resolved": "2015-05-31T20:59:08.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "I'm going to close this as \"Won't Fix\" in order to help clear out the Python issue backlog.", "created": "2015-05-31T20:59:08.481+0000"}], "num_comments": 1, "text": "Issue: SPARK-867\nSummary: Add a native Python way to create input RDDs in PySpark\nDescription: While PySpark can easily read RDDs of Strings from Java/Scala, it would be nice to create your own subclass that implements the partitions(), preferredLocations() and compute() methods as in Java, to plug in new input sources. It shouldn't be too hard to turn this into a special RDD on the Java side by, say, parallelizing an array of partition objects and piping them through Python.\n\nComments (1):\n1. Josh Rosen: I'm going to close this as \"Won't Fix\" in order to help clear out the Python issue backlog.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "06d0dcdb9e803b1afbfe6e46a14502a6", "issue_key": "SPARK-868", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Document fair scheduler", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-12T16:31:15.000+0000", "updated": "2013-09-09T16:18:13.000+0000", "resolved": "2013-09-09T16:17:58.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-868\nSummary: Document fair scheduler", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "526eb68a3b2918951b4b6a595a95bc90", "issue_key": "SPARK-869", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Retrofit rest of RDD api to use proper serializer type", "description": "SPARK-826 and SPARK-827 resolved proper serialization support for some RDD method parameters, but not all. This issue is to address the rest of RDD api and operations. Most of the time this is due to wrapping RDD parameters into a closure which can only use a closure serializer to communicate to the backend.", "reporter": "Dmitriy Lyubimov", "assignee": null, "created": "2013-08-12T18:31:28.000+0000", "updated": "2015-09-16T18:30:56.000+0000", "resolved": "2015-09-16T18:30:56.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Dmitriy Lyubimov", "body": "Sorry. the correct issues to reference are SPARK-826 and SPARK-835", "created": "2013-08-12T18:33:52.972+0000"}, {"author": "Sean R. Owen", "body": "Is this still live -- what are other methods that need the treatment? I know there's an issue about cloning the zero value in aggregate with the right serializer.", "created": "2015-02-28T03:51:06.774+0000"}, {"author": "Josh Rosen", "body": "Going to resolve this as Done; please open a new JIRA if you find specific examples where we're using the wrong serializer.", "created": "2015-09-16T18:30:56.688+0000"}], "num_comments": 3, "text": "Issue: SPARK-869\nSummary: Retrofit rest of RDD api to use proper serializer type\nDescription: SPARK-826 and SPARK-827 resolved proper serialization support for some RDD method parameters, but not all. This issue is to address the rest of RDD api and operations. Most of the time this is due to wrapping RDD parameters into a closure which can only use a closure serializer to communicate to the backend.\n\nComments (3):\n1. Dmitriy Lyubimov: Sorry. the correct issues to reference are SPARK-826 and SPARK-835\n2. Sean R. Owen: Is this still live -- what are other methods that need the treatment? I know there's an issue about cloning the zero value in aggregate with the right serializer.\n3. Josh Rosen: Going to resolve this as Done; please open a new JIRA if you find specific examples where we're using the wrong serializer.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.070531"}}
{"id": "cb2d5923e1b9022cfe06040dac564653", "issue_key": "SPARK-870", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Jobs UI shows incorrect task count if #tasks is not #partitions", "description": "val rdd = sc.textFile(\"/tpch10g/lineitem\") sc.runJob(rdd, (it: Iterator[String]) => it.take(10).toArray, Array(0), false)", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-08-12T22:15:46.000+0000", "updated": "2013-10-25T10:20:18.000+0000", "resolved": "2013-10-25T09:54:27.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "I'm bumping this to 0.8.1. It only affects things in a few cases and it's not super simple to fix.", "created": "2013-09-09T16:17:37.184+0000"}, {"author": "Reynold Xin", "body": "Which PR fixed this?", "created": "2013-10-25T10:15:29.921+0000"}, {"author": "Patrick McFadin", "body": "https://github.com/apache/incubator-spark/commit/fa9a0e40b2d76b85918958cf7d57ec95f766e785#diff-6ddec7f06d0cf5392943ecdb80fcea24R55", "created": "2013-10-25T10:20:18.143+0000"}], "num_comments": 3, "text": "Issue: SPARK-870\nSummary: Jobs UI shows incorrect task count if #tasks is not #partitions\nDescription: val rdd = sc.textFile(\"/tpch10g/lineitem\") sc.runJob(rdd, (it: Iterator[String]) => it.take(10).toArray, Array(0), false)\n\nComments (3):\n1. Patrick McFadin: I'm bumping this to 0.8.1. It only affects things in a few cases and it's not super simple to fix.\n2. Reynold Xin: Which PR fixed this?\n3. Patrick McFadin: https://github.com/apache/incubator-spark/commit/fa9a0e40b2d76b85918958cf7d57ec95f766e785#diff-6ddec7f06d0cf5392943ecdb80fcea24R55", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "601175cf790c2c335945798e5b6616ad", "issue_key": "SPARK-871", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add a link to the stdout/stderr log on the job level web ui", "description": "", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-08-12T22:49:16.000+0000", "updated": "2014-03-25T15:11:31.000+0000", "resolved": "2014-03-25T15:11:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "This was fixed a long time ago.", "created": "2014-03-25T15:11:31.981+0000"}], "num_comments": 1, "text": "Issue: SPARK-871\nSummary: Add a link to the stdout/stderr log on the job level web ui\n\nComments (1):\n1. Patrick McFadin: This was fixed a long time ago.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "1011737cbe559badf564869ac813b9b9", "issue_key": "SPARK-872", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Should revive offer after tasks finish in Mesos fine-grained mode", "description": "when running spark on latest Mesos release, I notice that spark on mesos fine-grained could not schedule spark tasks effectively, for example, if slave has 4 cpu cores resource, mesos master will call resourceOffer function of spark until 4 cpu cores are all free. but In my points like standalone scheduler mode, if one task finished and one cpus core is free, Mesos master should call spark resourceOffer to allocate resource to tasks.", "reporter": "xiajunluan", "assignee": null, "created": "2013-08-13T05:33:03.000+0000", "updated": "2015-05-19T14:03:17.000+0000", "resolved": "2015-05-19T14:03:17.000+0000", "labels": [], "components": ["Mesos"], "comments": [{"author": "Timothy Chen", "body": "I'm not quite understanding your statement where Mesos master will call resourceOffer until 4 cores are free? Can you elaborate what that means?", "created": "2014-07-17T22:42:18.581+0000"}, {"author": "Dragos Dascalita Haut", "body": "I think we should close this due to inactivity. What is the policy w.r.t. to the status to use?", "created": "2015-05-19T13:58:26.672+0000"}], "num_comments": 2, "text": "Issue: SPARK-872\nSummary: Should revive offer after tasks finish in Mesos fine-grained mode\nDescription: when running spark on latest Mesos release, I notice that spark on mesos fine-grained could not schedule spark tasks effectively, for example, if slave has 4 cpu cores resource, mesos master will call resourceOffer function of spark until 4 cpu cores are all free. but In my points like standalone scheduler mode, if one task finished and one cpus core is free, Mesos master should call spark resourceOffer to allocate resource to tasks.\n\nComments (2):\n1. Timothy Chen: I'm not quite understanding your statement where Mesos master will call resourceOffer until 4 cores are free? Can you elaborate what that means?\n2. Dragos Dascalita Haut: I think we should close this due to inactivity. What is the policy w.r.t. to the status to use?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "5d9af1a9a49546b1daa57ed786c9d11b", "issue_key": "SPARK-873", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add a way to specify rack topology in Mesos and standalone modes", "description": "Right now the YARN mode can look up rack information from YARN, but the standalone and Mesos modes don't have any way of specifying rack topology. We should have a pluggable script or config file that allows this. For the standalone mode, we'd probably want the rack info to be known by the Master rather than driver apps, and maybe the apps can get a cluster map when they register.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-13T10:53:42.000+0000", "updated": "2016-01-12T13:42:54.000+0000", "resolved": "2016-01-12T13:42:54.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-873\nSummary: Add a way to specify rack topology in Mesos and standalone modes\nDescription: Right now the YARN mode can look up rack information from YARN, but the standalone and Mesos modes don't have any way of specifying rack topology. We should have a pluggable script or config file that allows this. For the standalone mode, we'd probably want the rack info to be known by the Master rather than driver apps, and maybe the apps can get a cluster map when they register.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "378020a2da8d3aad839f123b53909a76", "issue_key": "SPARK-874", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Have a --wait flag in ./sbin/stop-all.sh that polls until Worker's are finished", "description": "When running benchmarking jobs, sometimes the cluster takes a long time to shut down. We should add a feature where it will ssh into all the workers every few seconds and check that the processes are dead, and won't return until they are all dead. This would help a lot with automating benchmarking scripts. There is some equivalent logic here written in python, we just need to add it to the shell script: https://github.com/pwendell/spark-perf/blob/master/bin/run#L117", "reporter": "Patrick Wendell", "assignee": "Ben Cook", "created": "2013-08-13T15:39:52.000+0000", "updated": "2014-12-09T20:17:27.000+0000", "resolved": "2014-12-09T20:17:27.000+0000", "labels": ["starter"], "components": ["Deploy"], "comments": [{"author": "Archit Thakur", "body": "I am interested in taking it up, please do assign. Thanks :) .", "created": "2014-05-29T06:45:17.270+0000"}, {"author": "Hari Dattada", "body": "I am not sure if this is still being worked on. If it is not, could you please assign it to me.", "created": "2014-11-13T21:00:50.689+0000"}, {"author": "Apache Spark", "body": "User 'jbencook' has created a pull request for this issue: https://github.com/apache/spark/pull/3567", "created": "2014-12-03T03:11:14.876+0000"}], "num_comments": 3, "text": "Issue: SPARK-874\nSummary: Have a --wait flag in ./sbin/stop-all.sh that polls until Worker's are finished\nDescription: When running benchmarking jobs, sometimes the cluster takes a long time to shut down. We should add a feature where it will ssh into all the workers every few seconds and check that the processes are dead, and won't return until they are all dead. This would help a lot with automating benchmarking scripts. There is some equivalent logic here written in python, we just need to add it to the shell script: https://github.com/pwendell/spark-perf/blob/master/bin/run#L117\n\nComments (3):\n1. Archit Thakur: I am interested in taking it up, please do assign. Thanks :) .\n2. Hari Dattada: I am not sure if this is still being worked on. If it is not, could you please assign it to me.\n3. Apache Spark: User 'jbencook' has created a pull request for this issue: https://github.com/apache/spark/pull/3567", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "fea77b1a651bbe6cc2f3902163ed5554", "issue_key": "SPARK-875", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Disk mounts can be wonky on EC2", "description": "", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-08-13T15:40:44.000+0000", "updated": "2013-08-26T10:08:13.000+0000", "resolved": "2013-08-26T10:08:13.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Here's what I saw:  root@ip-10-141-150-79 ~]$ mount /dev/xvda1 on / type ext4 (rw,noatime) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw) /dev/xvdb on /mnt type ext3 (rw,noatime) /dev/xvdc on /mnt2 type ext3 (rw,noatime) /dev/xvdd on /mnt3 type ext3 (rw,noatime) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) /dev/xvdf on /mnt2 type ext3 (rw,noatime) /dev/xvdc on /mnt3 type ext3 (rw,noatime) /dev/xvdd on /mnt4 type ext3 (rw,noatime) none on /cgroup type cgroup (rw) root@ip-10-141-150-79 ~]$ df -h Filesystem Size Used Avail Use% Mounted on /dev/xvda1 7.9G 3.0G 4.9G 39% / tmpfs 7.4G 0 7.4G 0% /dev/shm /dev/xvdb 414G 3.2G 390G 1% /mnt /dev/xvdc 414G 199M 393G 1% /mnt2 /dev/xvdd 414G 199M 393G 1% /mnt3 /dev/xvdf 414G 199M 393G 1% /mnt2 /dev/xvdc 414G 199M 393G 1% /mnt3 /dev/xvdd 414G 199M 393G 1% /mnt4", "created": "2013-08-13T15:42:07.646+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This was on m1.4xlarge nodes with no other arguments.", "created": "2013-08-13T15:42:43.297+0000"}], "num_comments": 2, "text": "Issue: SPARK-875\nSummary: Disk mounts can be wonky on EC2\n\nComments (2):\n1. Matei Alexandru Zaharia: Here's what I saw:  root@ip-10-141-150-79 ~]$ mount /dev/xvda1 on / type ext4 (rw,noatime) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw) /dev/xvdb on /mnt type ext3 (rw,noatime) /dev/xvdc on /mnt2 type ext3 (rw,noatime) /dev/xvdd on /mnt3 type ext3 (rw,noatime) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw) /dev/xvdf on /mnt2 type ext3 (rw,noatime) /dev/xvdc on /mnt3 type ext3 (rw,noatime) /dev/xvdd on /mnt4 type ext3 (rw,noatime) none on /cgroup type cgroup (rw) root@ip-10-141-150-79 ~]$ df -h Filesystem Size Used Avail Use% Mounted on /dev/xvda1 7.9G 3.0G 4.9G 39% / tmpfs 7.4G 0 7.4G 0% /dev/shm /dev/xvdb 414G 3.2G 390G 1% /mnt /dev/xvdc 414G 199M 393G 1% /mnt2 /dev/xvdd 414G 199M 393G 1% /mnt3 /dev/xvdf 414G 199M 393G 1% /mnt2 /dev/xvdc 414G 199M 393G 1% /mnt3 /dev/xvdd 414G 199M 393G 1% /mnt4\n2. Matei Alexandru Zaharia: This was on m1.4xlarge nodes with no other arguments.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "491cd49584974fdf9b883ac7d679c686", "issue_key": "SPARK-876", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Install Missing Features on AMI", "description": "We need to copy the ~/.vim directory from the old ami. We also need to install some python packages. Notes from [~matei] below: === yum update yum install python-matplotlib scipy python-tornado gcc-c++ easy_install pyzmq Note that the latter takes a while to decide to build its own ZMQ, and prints an error at the end, but it does work. You should be able to run ipython notebook --ip=* --port=33000 and see it. Note that if you do this, you should delete the *.ipynb file it creates before you make a new AMI.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-08-13T17:50:03.000+0000", "updated": "2013-08-26T10:08:25.000+0000", "resolved": "2013-08-26T10:08:25.000+0000", "labels": [], "components": ["EC2"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-876\nSummary: Install Missing Features on AMI\nDescription: We need to copy the ~/.vim directory from the old ami. We also need to install some python packages. Notes from [~matei] below: === yum update yum install python-matplotlib scipy python-tornado gcc-c++ easy_install pyzmq Note that the latter takes a while to decide to build its own ZMQ, and prints an error at the end, but it does work. You should be able to run ipython notebook --ip=* --port=33000 and see it. Note that if you do this, you should delete the *.ipynb file it creates before you make a new AMI.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "f911e8c449773caf79b2dd284d7d0145", "issue_key": "SPARK-877", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "java.lang.UnsupportedOperationException: empty.reduceLeft in UI", "description": "I opened stage's job progress UI page which had no active tasks and saw the following exception:  java.lang.UnsupportedOperationException: empty.reduceLeft at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:152) at scala.collection.mutable.ArrayOps.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayOps.scala:38) at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:69) at scala.collection.mutable.ArrayOps.reduceLeft(ArrayOps.scala:38) at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:180) at scala.collection.mutable.ArrayOps.reduce(ArrayOps.scala:38) at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35) at spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) at org.eclipse.jetty.server.Server.handle(Server.java:363) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483) at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982 ) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628) at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) at java.lang.Thread.run(Thread.java:722)", "reporter": "Reynold Xin", "assignee": "Kay Ousterhout", "created": "2013-08-13T23:00:17.000+0000", "updated": "2013-08-14T08:33:39.000+0000", "resolved": "2013-08-14T08:33:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Hey this stack trace suggests you were accessing the Executors page rather than the job progress page. Is that right?  at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)", "created": "2013-08-13T23:09:34.708+0000"}, {"author": "Reynold Xin", "body": "That was probably right (otherwise the stack wouldn't make sense). I had multiple pages open at the time.", "created": "2013-08-13T23:12:44.936+0000"}], "num_comments": 2, "text": "Issue: SPARK-877\nSummary: java.lang.UnsupportedOperationException: empty.reduceLeft in UI\nDescription: I opened stage's job progress UI page which had no active tasks and saw the following exception:  java.lang.UnsupportedOperationException: empty.reduceLeft at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:152) at scala.collection.mutable.ArrayOps.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayOps.scala:38) at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:69) at scala.collection.mutable.ArrayOps.reduceLeft(ArrayOps.scala:38) at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:180) at scala.collection.mutable.ArrayOps.reduce(ArrayOps.scala:38) at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35) at spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) at org.eclipse.jetty.server.Server.handle(Server.java:363) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483) at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982 ) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628) at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) at java.lang.Thread.run(Thread.java:722)\n\nComments (2):\n1. Patrick McFadin: Hey this stack trace suggests you were accessing the Executors page rather than the job progress page. Is that right?  at spark.ui.exec.ExecutorsUI.render(ExecutorsUI.scala:41) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35) at spark.ui.exec.ExecutorsUI$$anonfun$getHandlers$1.apply(ExecutorsUI.scala:35)\n2. Reynold Xin: That was probably right (otherwise the stack wouldn't make sense). I had multiple pages open at the time.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "34e9324d3ccd8482ceeec67eefcc072e", "issue_key": "SPARK-878", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "PySpark does not add Python *.zip and *.egg files to PYTHONPATH", "description": "When a list of *.zip or *.egg files is passed to SparkContext via pyFiles these do not get added to PYTHONPATH on the worker. The situation is different for *.py files since for these it is sufficient to add the working directory to PYTHONPATH (via sys.path). For the original discussion see: https://groups.google.com/forum/#!searchin/spark-users/pyfiles/spark-users/jG8VC17vTe4/z_DhBoRWuAMJ", "reporter": "Andre Schumacher", "assignee": "Andre Schumacher", "created": "2013-08-15T11:45:18.000+0000", "updated": "2013-08-19T15:31:51.000+0000", "resolved": "2013-08-19T15:31:51.000+0000", "labels": ["Starter"], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Does Jey's patch fix this? Want to test that and submit it as a pull request?", "created": "2013-08-15T14:00:28.692+0000"}, {"author": "Andre Schumacher", "body": "It does but it does also add all other *.zip files and such to the path. I actually talked to him and he suggested that it could be a nice case for learning PySpark internals. I now have an alternative solution (adding an include list to PythonRDD which is serialized into the worker input stream) which I still need to test though. Then it would be great to hear your comments on that, Josh.", "created": "2013-08-15T14:07:17.739+0000"}, {"author": "Andre Schumacher", "body": "Added pull request. Thanks Jey for discussing various approaches and helpful comments.", "created": "2013-08-15T16:42:06.710+0000"}, {"author": "Andre Schumacher", "body": "Issue was fixed in pull request #840. I tried to mark it as \"resolved\" but possibly my access right don't allow me to(?!)", "created": "2013-08-19T14:58:47.461+0000"}], "num_comments": 4, "text": "Issue: SPARK-878\nSummary: PySpark does not add Python *.zip and *.egg files to PYTHONPATH\nDescription: When a list of *.zip or *.egg files is passed to SparkContext via pyFiles these do not get added to PYTHONPATH on the worker. The situation is different for *.py files since for these it is sufficient to add the working directory to PYTHONPATH (via sys.path). For the original discussion see: https://groups.google.com/forum/#!searchin/spark-users/pyfiles/spark-users/jG8VC17vTe4/z_DhBoRWuAMJ\n\nComments (4):\n1. Josh Rosen: Does Jey's patch fix this? Want to test that and submit it as a pull request?\n2. Andre Schumacher: It does but it does also add all other *.zip files and such to the path. I actually talked to him and he suggested that it could be a nice case for learning PySpark internals. I now have an alternative solution (adding an include list to PythonRDD which is serialized into the worker input stream) which I still need to test though. Then it would be great to hear your comments on that, Josh.\n3. Andre Schumacher: Added pull request. Thanks Jey for discussing various approaches and helpful comments.\n4. Andre Schumacher: Issue was fixed in pull request #840. I tried to mark it as \"resolved\" but possibly my access right don't allow me to(?!)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "4a9ef0ea3668b21b8908244320cd36e8", "issue_key": "SPARK-879", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Typo in slaves file: \"listes\" instead of \"listed\"", "description": "The comment at the top of the conf/slaves file reads \"A Spark Worker will be started on each of the machines listes below.\" It should say \"listed\" instead of \"listes\".", "reporter": "William McNeill", "assignee": "SeanM", "created": "2013-08-15T14:54:21.000+0000", "updated": "2013-09-01T15:35:03.000+0000", "resolved": "2013-09-01T15:35:03.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "SeanM", "body": "resolved by: https://github.com/mesos/spark/pull/843", "created": "2013-08-15T20:12:49.951+0000"}], "num_comments": 1, "text": "Issue: SPARK-879\nSummary: Typo in slaves file: \"listes\" instead of \"listed\"\nDescription: The comment at the top of the conf/slaves file reads \"A Spark Worker will be started on each of the machines listes below.\" It should say \"listed\" instead of \"listes\".\n\nComments (1):\n1. SeanM: resolved by: https://github.com/mesos/spark/pull/843", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "d639699c4c8f8e7d1038c37aca7d8e6b", "issue_key": "SPARK-880", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "When built with Hadoop2, spark-shell and examples don't initialize log4j properly", "description": "They print this:  log4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jEventHandler). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.  It might have to do with not finding a log4j.properties file. I believe hadoop1 had one in its own JARs (or depended on an older log4j that came with a default)? but hadoop2 doesn't. We should probably have our own default one in conf.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-19T11:56:49.000+0000", "updated": "2014-11-06T07:06:14.000+0000", "resolved": "2014-11-06T07:06:14.000+0000", "labels": [], "components": [], "comments": [{"author": "Sean R. Owen", "body": "This should be resolved/obsoleted by subsequent updates to SLF4J and log4j integration, and the props file.", "created": "2014-09-11T09:02:51.929+0000"}], "num_comments": 1, "text": "Issue: SPARK-880\nSummary: When built with Hadoop2, spark-shell and examples don't initialize log4j properly\nDescription: They print this:  log4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jEventHandler). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.  It might have to do with not finding a log4j.properties file. I believe hadoop1 had one in its own JARs (or depended on an older log4j that came with a default)? but hadoop2 doesn't. We should probably have our own default one in conf.\n\nComments (1):\n1. Sean R. Owen: This should be resolved/obsoleted by subsequent updates to SLF4J and log4j integration, and the props file.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "6dfbab013b91a0f54493881c72612192", "issue_key": "SPARK-881", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add documentation for new monitoring capabilities", "description": "Should include, e.g. the JSON protocol.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-08-20T16:46:03.000+0000", "updated": "2013-09-15T23:07:42.000+0000", "resolved": "2013-09-15T23:07:42.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-881\nSummary: Add documentation for new monitoring capabilities\nDescription: Should include, e.g. the JSON protocol.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "93a112e59704c5e2e774796939bb1598", "issue_key": "SPARK-882", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Have link for feedback/suggestions in docs", "description": "It would be cool to have a link at the top of the docs for feedback/suggestions/errors. I bet we'd get a lot of interesting stuff from that and it could be a good way to crowdsource correctness checking, since a lot of us that write them never have to use them. Something to the right of the main top nav might be good. [~andyk] [~matei] - what do you guys think?", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2013-08-21T00:40:50.000+0000", "updated": "2016-10-31T09:38:42.000+0000", "resolved": "2016-10-31T09:38:40.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Nicholas Chammas", "body": "Is the intended use here that users could submit corrections easily without having to open a JIRA/PR? I think that's a great idea; it lowers the barrier to providing feedback on a high visibility item like the docs. Couple of questions: 1. Is integration with 3rd party tools like UserVoice or Disqus allowed? Actually, it might be really sweet if some simple, in-page feedback form automatically submitted a JIRA issue with the appropriate tags and info. 2. I assume the docs proper are the priority, right? Do we want to do this for the main site as well?", "created": "2015-03-03T04:07:32.161+0000"}, {"author": "Sean R. Owen", "body": "I think it should just prompt people to send to the user@ mailing list or if somehow possible, a link to open a minor doc JIRA. I would not want to add a third-party forum to monitor just for this, as it's likely to be a /dev/null. We already have an issue tracking and discussion system.", "created": "2015-03-03T09:16:41.785+0000"}, {"author": "Jon Deron Eriksson", "body": "I don't see any activity, so mind if I take a crack at this for the Spark documentation (link to open a pre-populated minor doc JIRA)? cc [~pwendell] [~srowen]", "created": "2016-10-21T01:07:50.934+0000"}, {"author": "Sean R. Owen", "body": "On second thought I'd just link to the https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark wiki as I'd rather funnel any contributions through that first.", "created": "2016-10-21T09:27:49.070+0000"}, {"author": "Jon Deron Eriksson", "body": "It looks like the More menu in the docs (http://spark.apache.org/docs/2.0.1/) already contains a \"Contributing to Spark\" link which takes the user to https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark. Since the link already exists, perhaps this JIRA should be resolved and closed?", "created": "2016-10-28T17:10:01.163+0000"}, {"author": "Sean R. Owen", "body": "You're right, I was thinking of the API docs rather than the general documentation. If there's an easy way to do that, OK, otherwise yeah this is effectively already done.", "created": "2016-10-28T18:24:19.927+0000"}], "num_comments": 6, "text": "Issue: SPARK-882\nSummary: Have link for feedback/suggestions in docs\nDescription: It would be cool to have a link at the top of the docs for feedback/suggestions/errors. I bet we'd get a lot of interesting stuff from that and it could be a good way to crowdsource correctness checking, since a lot of us that write them never have to use them. Something to the right of the main top nav might be good. [~andyk] [~matei] - what do you guys think?\n\nComments (6):\n1. Nicholas Chammas: Is the intended use here that users could submit corrections easily without having to open a JIRA/PR? I think that's a great idea; it lowers the barrier to providing feedback on a high visibility item like the docs. Couple of questions: 1. Is integration with 3rd party tools like UserVoice or Disqus allowed? Actually, it might be really sweet if some simple, in-page feedback form automatically submitted a JIRA issue with the appropriate tags and info. 2. I assume the docs proper are the priority, right? Do we want to do this for the main site as well?\n2. Sean R. Owen: I think it should just prompt people to send to the user@ mailing list or if somehow possible, a link to open a minor doc JIRA. I would not want to add a third-party forum to monitor just for this, as it's likely to be a /dev/null. We already have an issue tracking and discussion system.\n3. Jon Deron Eriksson: I don't see any activity, so mind if I take a crack at this for the Spark documentation (link to open a pre-populated minor doc JIRA)? cc [~pwendell] [~srowen]\n4. Sean R. Owen: On second thought I'd just link to the https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark wiki as I'd rather funnel any contributions through that first.\n5. Jon Deron Eriksson: It looks like the More menu in the docs (http://spark.apache.org/docs/2.0.1/) already contains a \"Contributing to Spark\" link which takes the user to https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark. Since the link already exists, perhaps this JIRA should be resolved and closed?\n6. Sean R. Owen: You're right, I was thinking of the API docs rather than the general documentation. If there's an easy way to do that, OK, otherwise yeah this is effectively already done.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.072538"}}
{"id": "e1eec9519df389e219a3d9d1be68aae8", "issue_key": "SPARK-883", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Remove dependency on Scala json library", "description": "Scala is going to deprecate its own json library. We should remove the uses of scala.util.parsing.json in Spark and move to a different library. One candidate choice is the lift-json library.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-08-22T13:59:25.000+0000", "updated": "2013-10-17T18:41:24.000+0000", "resolved": "2013-10-17T18:41:24.000+0000", "labels": ["Starter"], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-883\nSummary: Remove dependency on Scala json library\nDescription: Scala is going to deprecate its own json library. We should remove the uses of scala.util.parsing.json in Spark and move to a different library. One candidate choice is the lift-json library.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "38b35c537bb4345c208d2589712e0b37", "issue_key": "SPARK-1217", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add proximal gradient updater.", "description": "Add proximal gradient updater, in particular for L1 regularization.", "reporter": "Ameet Talwalkar", "assignee": null, "created": "2013-08-25T18:21:22.000+0000", "updated": "2014-04-07T17:46:33.000+0000", "resolved": "2014-04-07T17:46:33.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Martin Jaggi", "body": "The L1 updater is already proximal, as in the current code. Since it has no effect for L2, we could mark the issue as resolved for now.", "created": "2014-04-07T16:48:27.296+0000"}], "num_comments": 1, "text": "Issue: SPARK-1217\nSummary: Add proximal gradient updater.\nDescription: Add proximal gradient updater, in particular for L1 regularization.\n\nComments (1):\n1. Martin Jaggi: The L1 updater is already proximal, as in the current code. Since it has no effect for L2, we could mark the issue as resolved for now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "5e8acc17a5f677acc6af550876f09989", "issue_key": "SPARK-1214", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "0-1 labels", "description": "Use \\{0,1\\} labels for binary classification instead of {-1,1}. Advantages include: (+) Consistency across algorithms (+) Naturally extends to multi-class classification", "reporter": "Ameet Talwalkar", "assignee": "Xiangrui Meng", "created": "2013-08-25T18:25:21.000+0000", "updated": "2014-04-07T17:45:23.000+0000", "resolved": "2014-04-07T17:45:23.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "Fixed in 0.9.0 or an earlier version.", "created": "2014-04-07T17:45:23.507+0000"}], "num_comments": 1, "text": "Issue: SPARK-1214\nSummary: 0-1 labels\nDescription: Use \\{0,1\\} labels for binary classification instead of {-1,1}. Advantages include: (+) Consistency across algorithms (+) Naturally extends to multi-class classification\n\nComments (1):\n1. Xiangrui Meng: Fixed in 0.9.0 or an earlier version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "bb34c2ae51998054800b010f213ff4ca", "issue_key": "SPARK-884", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "unit test to validate various Spark json output", "description": "commit 5c7494d fixes this issue", "reporter": "Reynold Xin", "assignee": "Aaron Davidson", "created": "2013-08-26T12:00:15.000+0000", "updated": "2013-09-06T15:30:35.000+0000", "resolved": "2013-09-06T15:30:35.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Patrick McFadin", "body": "See some notes here: https://github.com/mesos/spark/pull/864#issuecomment-23285662", "created": "2013-08-26T12:05:14.082+0000"}], "num_comments": 1, "text": "Issue: SPARK-884\nSummary: unit test to validate various Spark json output\nDescription: commit 5c7494d fixes this issue\n\nComments (1):\n1. Patrick McFadin: See some notes here: https://github.com/mesos/spark/pull/864#issuecomment-23285662", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "e2ad03138249b2e84591798c79bfbee4", "issue_key": "SPARK-885", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark shell should capture ctrl-c to prevent users from accidentally killing the Java gateway", "description": "If users hit 'ctrl-c' to discard a line of input (or interrupt a command) in the `pyspark` shell, the resulting KeyboardInterrupt kills the Py4J Java Gateway. Maybe we could capture these interrupts to prevent this behavior. We'd still want to allow users to interrupt regular Python commands and jump to new input lines, but we'd throw a warning when attempting to interrupt a running PySpark job. I think the interrupt may also be forwarded to the Py4J java gateway; we may need to do something else to suppress this behavior.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-08-27T10:33:37.000+0000", "updated": "2013-09-01T15:10:12.000+0000", "resolved": "2013-09-01T15:10:12.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/870. We may want to backport this to branch-0.7.", "created": "2013-09-01T15:10:12.348+0000"}], "num_comments": 1, "text": "Issue: SPARK-885\nSummary: PySpark shell should capture ctrl-c to prevent users from accidentally killing the Java gateway\nDescription: If users hit 'ctrl-c' to discard a line of input (or interrupt a command) in the `pyspark` shell, the resulting KeyboardInterrupt kills the Py4J Java Gateway. Maybe we could capture these interrupts to prevent this behavior. We'd still want to allow users to interrupt regular Python commands and jump to new input lines, but we'd throw a warning when attempting to interrupt a running PySpark job. I think the interrupt may also be forwarded to the Py4J java gateway; we may need to do something else to suppress this behavior.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/870. We may want to backport this to branch-0.7.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "cf668a6352d69c6d1391b85e5adfb31f", "issue_key": "SPARK-886", "issue_type": "Bug", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "NullpointerExceptions in InputFormatInfo.computePreferredLocations/SparkHDFSLR", "description": "Since the changes from pr838 to change to hadoop agnostic builds, the SparkHdfsLR example throws a null pointer exception on Spark on Yarn (not sure if it fails on others). It also has the same issue if you call InputFormatInfo.computePreferredLocations before the SparkContext is created. The reason is that both of those call SparkEnv.get.hadoop which hasn't been created yet if you haven't created the SparkContext. This is being used in the SparkHdfsLR example to pass the preferred locations into the SparkContext: // This is used only by yarn for now, but should be relevant to other cluster types (mesos, etc) too. // This is typically generated from InputFormatInfo.computePreferredLocations .. host, set of data-local splits on host val preferredNodeLocationData: scala.collection.Map[String, scala.collection.Set[SplitInfo]] = scala.collection.immutable.Map()", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2013-08-27T13:05:33.000+0000", "updated": "2013-11-04T09:52:10.000+0000", "resolved": "2013-11-04T09:51:57.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Reynold Xin", "body": "Should we close this one now https://github.com/apache/incubator-spark/pull/124 is merged?", "created": "2013-11-01T18:00:11.267+0000"}, {"author": "Thomas Graves", "body": "Yes we can close it. I don't seem to have permission to assign to myself or close.", "created": "2013-11-04T06:47:48.765+0000"}, {"author": "Reynold Xin", "body": "Thanks. I closed it. I will look into the permission issues ...", "created": "2013-11-04T09:52:10.312+0000"}], "num_comments": 3, "text": "Issue: SPARK-886\nSummary: NullpointerExceptions in InputFormatInfo.computePreferredLocations/SparkHDFSLR\nDescription: Since the changes from pr838 to change to hadoop agnostic builds, the SparkHdfsLR example throws a null pointer exception on Spark on Yarn (not sure if it fails on others). It also has the same issue if you call InputFormatInfo.computePreferredLocations before the SparkContext is created. The reason is that both of those call SparkEnv.get.hadoop which hasn't been created yet if you haven't created the SparkContext. This is being used in the SparkHdfsLR example to pass the preferred locations into the SparkContext: // This is used only by yarn for now, but should be relevant to other cluster types (mesos, etc) too. // This is typically generated from InputFormatInfo.computePreferredLocations .. host, set of data-local splits on host val preferredNodeLocationData: scala.collection.Map[String, scala.collection.Set[SplitInfo]] = scala.collection.immutable.Map()\n\nComments (3):\n1. Reynold Xin: Should we close this one now https://github.com/apache/incubator-spark/pull/124 is merged?\n2. Thomas Graves: Yes we can close it. I don't seem to have permission to assign to myself or close.\n3. Reynold Xin: Thanks. I closed it. I will look into the permission issues ...", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "aaf291dc833feb3487934d5c7b15d208", "issue_key": "SPARK-887", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "mvn package doesn't include yarn in the repl-bin shaded jar", "description": "If you build with sbt (SPARK_WITH_YARN=true sbt/sbt package assembly) it will add the Yarn classes (YarnClientImpl) into the repl assembled jar ./repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar , but if you build with mvn -Phadoop2-yarn package it doesn't add the Yarn class in the repl shaded jar (./repl-bin/target/spark-repl-bin-0.8.0-SNAPSHOT-shaded.jar) We should keep these consistent. This matters for https://github.com/mesos/spark/pull/868 as it is relying on the YarnClientImpl being in the jar.", "reporter": "Thomas Graves", "assignee": "Matei Alexandru Zaharia", "created": "2013-08-29T15:29:13.000+0000", "updated": "2013-09-03T12:33:10.000+0000", "resolved": "2013-08-31T13:11:17.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Thomas Graves", "body": "I think this might be fixed by https://github.com/mesos/spark/pull/857", "created": "2013-08-30T06:37:27.592+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yup, it is.", "created": "2013-08-31T13:11:17.924+0000"}, {"author": "Thomas Graves", "body": "So it looks like the new assembly jar does not include the YarnClientImpl. The only way for me to run the example is to include the yarn jar in the export SPARK_CLASSPATH variable.", "created": "2013-09-03T12:33:10.071+0000"}], "num_comments": 3, "text": "Issue: SPARK-887\nSummary: mvn package doesn't include yarn in the repl-bin shaded jar\nDescription: If you build with sbt (SPARK_WITH_YARN=true sbt/sbt package assembly) it will add the Yarn classes (YarnClientImpl) into the repl assembled jar ./repl/target/spark-repl-assembly-0.8.0-SNAPSHOT.jar , but if you build with mvn -Phadoop2-yarn package it doesn't add the Yarn class in the repl shaded jar (./repl-bin/target/spark-repl-bin-0.8.0-SNAPSHOT-shaded.jar) We should keep these consistent. This matters for https://github.com/mesos/spark/pull/868 as it is relying on the YarnClientImpl being in the jar.\n\nComments (3):\n1. Thomas Graves: I think this might be fixed by https://github.com/mesos/spark/pull/857\n2. Matei Alexandru Zaharia: Yup, it is.\n3. Thomas Graves: So it looks like the new assembly jar does not include the YarnClientImpl. The only way for me to run the example is to include the yarn jar in the export SPARK_CLASSPATH variable.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "32c66b09511c7892a29ef6c0b9eeac48", "issue_key": "SPARK-888", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Killing jobs on standalone cluster", "description": "Need an admin interface for this.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-29T15:47:01.000+0000", "updated": "2015-03-02T15:16:33.000+0000", "resolved": "2015-03-02T15:16:33.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-888\nSummary: Killing jobs on standalone cluster\nDescription: Need an admin interface for this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "2e39214354eba63baee521419c244ba7", "issue_key": "SPARK-889", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Bring back DFS broadcast", "description": "DFS broadcast was a simple way to get better-than-single-master performance for broadcast, so we should add it back for people who have HDFS.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-30T10:48:29.000+0000", "updated": "2014-09-24T23:03:09.000+0000", "resolved": "2014-09-24T23:03:09.000+0000", "labels": [], "components": [], "comments": [{"author": "Colin McCabe", "body": "Hi Matei, Sounds interesting. I can take a look at this one.", "created": "2014-06-12T21:22:52.590+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This is a really old JIRA and actually I wouldn't implement this now. HTTPBroadcast works as well as DFS did, and TorrentBroadcast is even better. We just need to make TorrentBroadcast the default -- would be great if you can spend some time testing that.", "created": "2014-06-13T00:13:58.138+0000"}, {"author": "Andrew Ash", "body": "[~matei] should we close ticket this as Won't Fix then, since effort is better spent making TorrentBroadcast better?", "created": "2014-09-24T19:57:05.740+0000"}, {"author": "Josh Rosen", "body": "In fact, I think [~rxin] has some JIRAs and PRs to make TorrentBroadcast _even_ better than it is now (it was greatly improved from 1.0.2 to 1.1.0), so it's probably safe to close this.", "created": "2014-09-24T22:56:36.575+0000"}, {"author": "Reynold Xin", "body": "Yea I think we should close this as won't fix for now.", "created": "2014-09-24T23:03:03.969+0000"}], "num_comments": 5, "text": "Issue: SPARK-889\nSummary: Bring back DFS broadcast\nDescription: DFS broadcast was a simple way to get better-than-single-master performance for broadcast, so we should add it back for people who have HDFS.\n\nComments (5):\n1. Colin McCabe: Hi Matei, Sounds interesting. I can take a look at this one.\n2. Matei Alexandru Zaharia: This is a really old JIRA and actually I wouldn't implement this now. HTTPBroadcast works as well as DFS did, and TorrentBroadcast is even better. We just need to make TorrentBroadcast the default -- would be great if you can spend some time testing that.\n3. Andrew Ash: [~matei] should we close ticket this as Won't Fix then, since effort is better spent making TorrentBroadcast better?\n4. Josh Rosen: In fact, I think [~rxin] has some JIRAs and PRs to make TorrentBroadcast _even_ better than it is now (it was greatly improved from 1.0.2 to 1.1.0), so it's probably safe to close this.\n5. Reynold Xin: Yea I think we should close this as won't fix for now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "50361e89c7af159570d826bcc0409cd6", "issue_key": "SPARK-890", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Allow multiple parallel commands in spark-shell", "description": "While a long command is running, it would be cool to still query the dataset from another window. This might be possible with a UI in front of spark-shell (there's no reason why the REPL environment can't be shared).", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-30T12:34:47.000+0000", "updated": "2014-10-21T07:42:05.000+0000", "resolved": "2014-10-21T07:42:05.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick Wendell", "body": "Spark context was made thread-safe a long time ago.", "created": "2014-10-21T07:42:05.952+0000"}], "num_comments": 1, "text": "Issue: SPARK-890\nSummary: Allow multiple parallel commands in spark-shell\nDescription: While a long command is running, it would be cool to still query the dataset from another window. This might be possible with a UI in front of spark-shell (there's no reason why the REPL environment can't be shared).\n\nComments (1):\n1. Patrick Wendell: Spark context was made thread-safe a long time ago.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "3d34edc8beccc98efd4b14b4dabcec7f", "issue_key": "SPARK-891", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Update Windows launch scripts to use assembly", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-08-31T13:51:13.000+0000", "updated": "2014-05-19T00:24:04.000+0000", "resolved": "2014-05-19T00:24:04.000+0000", "labels": [], "components": ["Deploy"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-891\nSummary: Update Windows launch scripts to use assembly", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.074784"}}
{"id": "e621b3899dd1373ba4b1bdec8bdc4e86", "issue_key": "SPARK-892", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add docs page for fair scheduler", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2013-08-31T18:02:36.000+0000", "updated": "2013-09-09T16:18:13.000+0000", "resolved": "2013-09-09T16:16:38.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-892\nSummary: Add docs page for fair scheduler", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "9a78de27fb5980dbc56aef18f6202f01", "issue_key": "SPARK-893", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Create a docs page on monitoring and metrics", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Patrick McFadin", "created": "2013-08-31T23:12:12.000+0000", "updated": "2013-09-09T16:16:27.000+0000", "resolved": "2013-09-09T16:16:27.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-893\nSummary: Create a docs page on monitoring and metrics", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "22b20c0b9356b5de104e5ada7c6ee26f", "issue_key": "SPARK-894", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Not all WebUI fields delivered VIA JSON", "description": "The following fields which are displayed in the Master WebUI for both the are not delivered via /json url. 'state' within the Worker and Application list 'duration' within the Application list", "reporter": "David McCauley", "assignee": "David McCauley", "created": "2013-09-05T07:26:06.000+0000", "updated": "2013-09-16T08:10:50.000+0000", "resolved": "2013-09-16T08:10:50.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Reynold Xin", "body": "I added worker state in this PR: https://github.com/mesos/spark/pull/864/files", "created": "2013-09-05T18:23:31.330+0000"}, {"author": "David McCauley", "body": "Created pull request for this change: https://github.com/mesos/spark/pull/925", "created": "2013-09-11T03:27:22.293+0000"}], "num_comments": 2, "text": "Issue: SPARK-894\nSummary: Not all WebUI fields delivered VIA JSON\nDescription: The following fields which are displayed in the Master WebUI for both the are not delivered via /json url. 'state' within the Worker and Application list 'duration' within the Application list\n\nComments (2):\n1. Reynold Xin: I added worker state in this PR: https://github.com/mesos/spark/pull/864/files\n2. David McCauley: Created pull request for this change: https://github.com/mesos/spark/pull/925", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "ecb104be61650800cb38fed16bc3cf27", "issue_key": "SPARK-895", "issue_type": "Improvement", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Add a Ganglia sink to Spark Metrics", "description": "", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2013-09-05T23:33:10.000+0000", "updated": "2014-03-30T04:14:04.000+0000", "resolved": "2013-09-08T22:44:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Jie Huang", "body": "Great idea. Here is the similar work for my test environment. see the comment in SPARK-788 and [github link| https://github.com/GraceH/spark/commit/c9717adfb97dacb97c34ad1fe94c8076843e5bf9]. Meanwhile, it would be better if we can improve the metric name to group the metrics according to their source or category. This really helps to present a better view in the new ganglia web page.", "created": "2013-09-08T06:05:08.327+0000"}, {"author": "Patrick McFadin", "body": "Hey [~Grace Huang], [~andrew xia], I'm so sorry that I missed this! I spent time today duplicating this effort and actually just merged our own Ganglia sink (it looks almost identical to the code that Grace wrote). I didn't see the comment on this JIRA until just now. [~Grace Huang] if you want to take a look at what I merged in today, it would be great to have a second person look at it and propose any changes necessary as a pull request. I noticed that you also had an example in the config file that I didn't have, it would be nice to get that merged as well.", "created": "2013-09-08T22:44:00.339+0000"}, {"author": "Jie Huang", "body": "Hi Patrick, that is quite considerate to involve the app info which differentiates the metrics among different apps. BTW, is there any further plan to improve the naming mechanism for those metrics, so that we can organize certain types of metrics under the same group-view? As you know, the Codahale-Metrics chooses those characters before the last dot from its whole metric name as the group name. If we can make use of this implementation, it will present those metrics in a better way for new ganglia web frontend. For example, it would be nice to gruop the metrics of executor.threadpool.{activeTask.count, completeTask.count, currentPool.size, maxPool.size} altogether, instead of letting them isolated into different groups. Any comment?", "created": "2013-09-09T03:58:42.541+0000"}, {"author": "Jie Huang", "body": "Oops. Since this issue has been closed already, we can discuss the naming mechanism in another place. Thanks.", "created": "2013-09-09T04:03:40.997+0000"}, {"author": "Patrick McFadin", "body": "We can discuss here (we'll make a new JIRA if needed). Just so I understand, could you explain how the names would be different with the proposed change? For instance, how would the following be different:  <executor>.<appid>.threadpool.activeTasks.count", "created": "2013-09-09T09:30:13.521+0000"}, {"author": "Jie Huang", "body": "We can divide the metric name into two parts, one is before the last dot (which is the _GroupName_), and the other is after that last dot (which is the _CounterName_ without any dot mark ). For example, we can name it as  <executor>.<appid>.threadpool.{activeTask_count | completeTask_count | currentPool_size | maxPool_size}  Here the _GroupName_ is \"*<executor>.<appid>.threadpool*\", and the rest parts are connected by \"\\_\" mark representing the _CounterName_, like \"*activeTasks_count*\". What do you think?", "created": "2013-09-09T17:22:24.238+0000"}, {"author": "Patrick McFadin", "body": "Just wondering - is this exclusively for the benefit of Ganglia? Or are there other reasons for this as well...", "created": "2013-09-09T17:25:17.898+0000"}, {"author": "Jie Huang", "body": "Yes. Ganglia has the concept of metrics group, which is not involved by other reporters. This feature helps to cluster several metrics under the same group view. Otherwise, it looks fragmented in the web page. According to my understanding, the current metrics name is formatted as  XXX.YYY.ZZZ.*.COUNTER.UNIT  I wonder if it is possible to concatenate the COUNTER and UNIT parts, since it is quite clear or readable for the user to identify the counter name at a glance, like  XXX.YYY.ZZZ.COUNTER_UNIT or XXX.YYY.ZZZ.CounterUnit  Actually, the metrics of _FileSystemStat_ already demonstrates such kind of format to some extent. For example  *.filesystem.{hdfs|file}.{bytesRead| bytesWritten| readOps| largeReadOps| writeOps}  It shows two groups for two different file systems respectively. And each group has 5 counters. Is it possible to unify the formats for all the metrics alike? Any comment?", "created": "2013-09-10T06:27:28.751+0000"}, {"author": "Patrick McFadin", "body": "This is a good idea - I've created SPARK 900 and assigned it to you Clare, feel free to submit a PR when you have done some work.", "created": "2013-09-10T10:11:47.241+0000"}, {"author": "Jie Huang", "body": "OK. I see. Thanks. Pls help to comment more after the PR is ready. Hi Patrick, one more question on the JVM metrics source. Is it possible to differentiate the sources of JVM metrics according to their metric names? Currently, all the JVM metrics are started with the _sourceName_ of “jvm”. It is hard to distinguish which process the jvm metrics belong to. This is the common issue for all the Reporters. We may need the role or proc information here, just like what you have done in the ExecutorSource? For example  {master|worker}.jvm.* OR {driver|executor}.{executorID|app}.jvm.*.  It is preferable to refine the _sourceName_ in the _JVMSource_ class. One proposal is to extract the information from the JVM’s main ClassName and the application’s arguments, or to simply obtain the current processID as the prefix name of _SourceName_? What do you think?", "created": "2013-09-11T06:26:43.287+0000"}, {"author": "Jie Huang", "body": "BTW, there is one trivial point which is also ignored in my patch file. Since we have installed the latest Ganglia, I did not enable the Ganglia30 protocol. In case someone else need the previous Ganglia version to be compatible, we may choose \"*public GMetric(String group, int port, UDPAddressingMode mode, int ttl, {color:red}boolean ganglia311{color})*\" to create the ganglia instance. And add one more option for either 311(as default value) or Ganglia30 protocol. If it is unnecessary, just ignore it. Thanks.", "created": "2013-09-11T06:29:27.345+0000"}], "num_comments": 11, "text": "Issue: SPARK-895\nSummary: Add a Ganglia sink to Spark Metrics\n\nComments (11):\n1. Jie Huang: Great idea. Here is the similar work for my test environment. see the comment in SPARK-788 and [github link| https://github.com/GraceH/spark/commit/c9717adfb97dacb97c34ad1fe94c8076843e5bf9]. Meanwhile, it would be better if we can improve the metric name to group the metrics according to their source or category. This really helps to present a better view in the new ganglia web page.\n2. Patrick McFadin: Hey [~Grace Huang], [~andrew xia], I'm so sorry that I missed this! I spent time today duplicating this effort and actually just merged our own Ganglia sink (it looks almost identical to the code that Grace wrote). I didn't see the comment on this JIRA until just now. [~Grace Huang] if you want to take a look at what I merged in today, it would be great to have a second person look at it and propose any changes necessary as a pull request. I noticed that you also had an example in the config file that I didn't have, it would be nice to get that merged as well.\n3. Jie Huang: Hi Patrick, that is quite considerate to involve the app info which differentiates the metrics among different apps. BTW, is there any further plan to improve the naming mechanism for those metrics, so that we can organize certain types of metrics under the same group-view? As you know, the Codahale-Metrics chooses those characters before the last dot from its whole metric name as the group name. If we can make use of this implementation, it will present those metrics in a better way for new ganglia web frontend. For example, it would be nice to gruop the metrics of executor.threadpool.{activeTask.count, completeTask.count, currentPool.size, maxPool.size} altogether, instead of letting them isolated into different groups. Any comment?\n4. Jie Huang: Oops. Since this issue has been closed already, we can discuss the naming mechanism in another place. Thanks.\n5. Patrick McFadin: We can discuss here (we'll make a new JIRA if needed). Just so I understand, could you explain how the names would be different with the proposed change? For instance, how would the following be different:  <executor>.<appid>.threadpool.activeTasks.count\n6. Jie Huang: We can divide the metric name into two parts, one is before the last dot (which is the _GroupName_), and the other is after that last dot (which is the _CounterName_ without any dot mark ). For example, we can name it as  <executor>.<appid>.threadpool.{activeTask_count | completeTask_count | currentPool_size | maxPool_size}  Here the _GroupName_ is \"*<executor>.<appid>.threadpool*\", and the rest parts are connected by \"\\_\" mark representing the _CounterName_, like \"*activeTasks_count*\". What do you think?\n7. Patrick McFadin: Just wondering - is this exclusively for the benefit of Ganglia? Or are there other reasons for this as well...\n8. Jie Huang: Yes. Ganglia has the concept of metrics group, which is not involved by other reporters. This feature helps to cluster several metrics under the same group view. Otherwise, it looks fragmented in the web page. According to my understanding, the current metrics name is formatted as  XXX.YYY.ZZZ.*.COUNTER.UNIT  I wonder if it is possible to concatenate the COUNTER and UNIT parts, since it is quite clear or readable for the user to identify the counter name at a glance, like  XXX.YYY.ZZZ.COUNTER_UNIT or XXX.YYY.ZZZ.CounterUnit  Actually, the metrics of _FileSystemStat_ already demonstrates such kind of format to some extent. For example  *.filesystem.{hdfs|file}.{bytesRead| bytesWritten| readOps| largeReadOps| writeOps}  It shows two groups for two different file systems respectively. And each group has 5 counters. Is it possible to unify the formats for all the metrics alike? Any comment?\n9. Patrick McFadin: This is a good idea - I've created SPARK 900 and assigned it to you Clare, feel free to submit a PR when you have done some work.\n10. Jie Huang: OK. I see. Thanks. Pls help to comment more after the PR is ready. Hi Patrick, one more question on the JVM metrics source. Is it possible to differentiate the sources of JVM metrics according to their metric names? Currently, all the JVM metrics are started with the _sourceName_ of “jvm”. It is hard to distinguish which process the jvm metrics belong to. This is the common issue for all the Reporters. We may need the role or proc information here, just like what you have done in the ExecutorSource? For example  {master|worker}.jvm.* OR {driver|executor}.{executorID|app}.jvm.*.  It is preferable to refine the _sourceName_ in the _JVMSource_ class. One proposal is to extract the information from the JVM’s main ClassName and the application’s arguments, or to simply obtain the current processID as the prefix name of _SourceName_? What do you think?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "e4ed87626320dac6a430526c75b447ef", "issue_key": "SPARK-896", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ADD_JARS does not add all classes to classpath in the spark-shell for cluster on Mesos.", "description": "I do not believe the issue is limited to scheduler/executors running on Mesos but added the information for debugging purposes. h3. Reproducing the issue: # Implement some custom functionalities and package them into a 'monster jar' with something like sbt assembly. # Drop this jar onto the Spark master box and specify the path to it in the ADD_JARS variable. # Start up the spark shell on same box as the master. You should be able to import packages/classes specified in the jar without any compilation trouble. # In a map function on an RDD, trying to call a class from within this jar (with fully qualified name) fails on a ClassNotFoundException. h3. Workaround Matei Zaharia suggested adding this jar to the SPARK_CLASSPATH environment variable - that resolved the issue. My understanding however is that the functionality should work using solely the ADD_JARS variable - the documentation does not capture this.", "reporter": "Gary Malouf", "assignee": null, "created": "2013-09-09T11:18:27.000+0000", "updated": "2015-03-08T13:52:46.000+0000", "resolved": "2015-03-08T13:52:46.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Sean R. Owen", "body": "I'm gonna call this WontFix as ADD_JARS has been deprecated for a while.", "created": "2015-03-08T13:52:46.158+0000"}], "num_comments": 1, "text": "Issue: SPARK-896\nSummary: ADD_JARS does not add all classes to classpath in the spark-shell for cluster on Mesos.\nDescription: I do not believe the issue is limited to scheduler/executors running on Mesos but added the information for debugging purposes. h3. Reproducing the issue: # Implement some custom functionalities and package them into a 'monster jar' with something like sbt assembly. # Drop this jar onto the Spark master box and specify the path to it in the ADD_JARS variable. # Start up the spark shell on same box as the master. You should be able to import packages/classes specified in the jar without any compilation trouble. # In a map function on an RDD, trying to call a class from within this jar (with fully qualified name) fails on a ClassNotFoundException. h3. Workaround Matei Zaharia suggested adding this jar to the SPARK_CLASSPATH environment variable - that resolved the issue. My understanding however is that the functionality should work using solely the ADD_JARS variable - the documentation does not capture this.\n\nComments (1):\n1. Sean R. Owen: I'm gonna call this WontFix as ADD_JARS has been deprecated for a while.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "6afedf663dd4f13b7a6fff78d9449f31", "issue_key": "SPARK-897", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Preemptively serialize closures to help users identify non-serializable errors early on", "description": "We can do a preemptive serialization of the closures in map, mapParititons and all other RDD functions that take closures as arguments to help users catch non-serializable errors early on. Right now those exceptions are triggered when an action is triggered, which makes it harder to debug.", "reporter": "Reynold Xin", "assignee": "William Benton", "created": "2013-09-09T20:34:24.000+0000", "updated": "2014-06-30T06:27:59.000+0000", "resolved": "2014-06-30T06:27:59.000+0000", "labels": [], "components": [], "comments": [{"author": "William Benton", "body": "I'd be interested in taking this issue if no one is working on it yet. Can someone assign it to me?", "created": "2014-03-10T15:09:22.993+0000"}], "num_comments": 1, "text": "Issue: SPARK-897\nSummary: Preemptively serialize closures to help users identify non-serializable errors early on\nDescription: We can do a preemptive serialization of the closures in map, mapParititons and all other RDD functions that take closures as arguments to help users catch non-serializable errors early on. Right now those exceptions are triggered when an action is triggered, which makes it harder to debug.\n\nComments (1):\n1. William Benton: I'd be interested in taking this issue if no one is working on it yet. Can someone assign it to me?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "6166f81af6a1bb0c939bd6356350e295", "issue_key": "SPARK-898", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add jets3t dependency to Spark Build", "description": "This is necessary for s3 reads and writes to work correctly with some hadoop versions.", "reporter": "Patrick McFadin", "assignee": "Matei Alexandru Zaharia", "created": "2013-09-09T23:10:06.000+0000", "updated": "2013-09-15T23:07:56.000+0000", "resolved": "2013-09-15T23:07:56.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-898\nSummary: Add jets3t dependency to Spark Build\nDescription: This is necessary for s3 reads and writes to work correctly with some hadoop versions.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "9560b863632b5e047afc251761fdd282", "issue_key": "SPARK-899", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Outdated Bagel documentation", "description": "The documentation for Bagel at http://spark.incubator.apache.org/docs/latest/bagel-programming-guide.html seems to be outdated. In the code example it refers to an Edge class that does not exist in Bagel.", "reporter": "Matteo Ceccarello", "assignee": null, "created": "2013-09-10T09:04:37.000+0000", "updated": "2014-11-08T09:35:15.000+0000", "resolved": "2014-11-08T09:35:15.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Sean R. Owen", "body": "Trawling old issues again... I assume this is a \"WontFix\" because GraphX has superseded Bagel.", "created": "2014-11-08T09:35:15.638+0000"}], "num_comments": 1, "text": "Issue: SPARK-899\nSummary: Outdated Bagel documentation\nDescription: The documentation for Bagel at http://spark.incubator.apache.org/docs/latest/bagel-programming-guide.html seems to be outdated. In the code example it refers to an Edge class that does not exist in Bagel.\n\nComments (1):\n1. Sean R. Owen: Trawling old issues again... I assume this is a \"WontFix\" because GraphX has superseded Bagel.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "cdf30e7b2ef65272f69926e78a30c1df", "issue_key": "SPARK-900", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Use coarser grained naming for metrics", "description": "See discussio in SPARK-895", "reporter": "Patrick McFadin", "assignee": "Jie Huang", "created": "2013-09-10T10:11:23.000+0000", "updated": "2015-07-14T17:39:07.000+0000", "resolved": "2013-10-09T11:10:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Jie Huang", "body": "see https://github.com/apache/incubator-spark/pull/22", "created": "2013-09-27T00:06:20.082+0000"}, {"author": "Apache Spark", "body": "User 'rikima' has created a pull request for this issue: https://github.com/apache/spark/pull/7400", "created": "2015-07-14T17:39:07.193+0000"}], "num_comments": 2, "text": "Issue: SPARK-900\nSummary: Use coarser grained naming for metrics\nDescription: See discussio in SPARK-895\n\nComments (2):\n1. Jie Huang: see https://github.com/apache/incubator-spark/pull/22\n2. Apache Spark: User 'rikima' has created a pull request for this issue: https://github.com/apache/spark/pull/7400", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "f2c1b61b74e1e94dc404ce992f3e28c9", "issue_key": "SPARK-901", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "UISuite \"jetty port increases under contention\" fails if startPort is in use", "description": "Recent change of startPort to 3030 conflicts with IANA assignment for arepa-cas. If 3030 is already in use, the UISuite fails.", "reporter": "Mark Hamstra", "assignee": null, "created": "2013-09-10T15:58:58.000+0000", "updated": "2014-09-17T22:17:07.000+0000", "resolved": "2014-09-17T22:17:07.000+0000", "labels": [], "components": ["Build", "Spark Core", "Web UI"], "comments": [{"author": "Mark Hamstra", "body": "Ah ha! It's actually Typesafe's Zinc server that is the other half of the conflict. Zinc declares 3030 to be the default Nailgun port.", "created": "2013-09-10T16:06:54.933+0000"}, {"author": "Konstantin I Boudnik", "body": "the question that Mark raised on the JIRA is essentially about finding which number is greater out of the following two: - 'how many people out there are relying on well-known 3030 port' vs - 'how many Zinc server devs are using Spark'", "created": "2013-09-10T16:51:17.963+0000"}, {"author": "Mark Hamstra", "body": "Doing some more googling, it's pretty hard to find out just what arepa-cas is, much less how many people are actually using it. I think that it is far more likely that Spark developers trying to use Zinc will run into this conflict than do arepa-cas users. Zinc is a pretty useful tool that more Scala developers should use, so we should at least warn them of the port conflict if we choose to leave the SparkUI.DEFAULT_PORT at 3030. Incidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important?", "created": "2013-09-10T16:59:48.056+0000"}, {"author": "Konstantin I Boudnik", "body": "bq. Incidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important? won't it cause the port conflict when server and a worker run on the same node?", "created": "2013-09-10T17:14:48.737+0000"}, {"author": "Sundeep Narravula", "body": "Leaving the current test as it is, still causes transient issues. Shouldn't we protect the test setup call server.start() ? Even though the chance of the port being used is low, it will be good to avoid the setup of a test being dependent on a fixed port.", "created": "2013-10-03T17:49:39.375+0000"}, {"author": "Patrick Wendell", "body": "This is fixed by SPARK-3555 since we no longer chose a specific starting port.", "created": "2014-09-17T22:17:07.343+0000"}], "num_comments": 6, "text": "Issue: SPARK-901\nSummary: UISuite \"jetty port increases under contention\" fails if startPort is in use\nDescription: Recent change of startPort to 3030 conflicts with IANA assignment for arepa-cas. If 3030 is already in use, the UISuite fails.\n\nComments (6):\n1. Mark Hamstra: Ah ha! It's actually Typesafe's Zinc server that is the other half of the conflict. Zinc declares 3030 to be the default Nailgun port.\n2. Konstantin I Boudnik: the question that Mark raised on the JIRA is essentially about finding which number is greater out of the following two: - 'how many people out there are relying on well-known 3030 port' vs - 'how many Zinc server devs are using Spark'\n3. Mark Hamstra: Doing some more googling, it's pretty hard to find out just what arepa-cas is, much less how many people are actually using it. I think that it is far more likely that Spark developers trying to use Zinc will run into this conflict than do arepa-cas users. Zinc is a pretty useful tool that more Scala developers should use, so we should at least warn them of the port conflict if we choose to leave the SparkUI.DEFAULT_PORT at 3030. Incidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important?\n4. Konstantin I Boudnik: bq. Incidentally, shouldn't UISuite be using SparkUI.DEFAULT_PORT instead of 3030 so that it is obvious where this magic number is coming from and why it is important? won't it cause the port conflict when server and a worker run on the same node?\n5. Sundeep Narravula: Leaving the current test as it is, still causes transient issues. Shouldn't we protect the test setup call server.start() ? Even though the chance of the port being used is low, it will be good to avoid the setup of a test being dependent on a fixed port.\n6. Patrick Wendell: This is fixed by SPARK-3555 since we no longer chose a specific starting port.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.076524"}}
{"id": "ae63108e4ffda95801fd6006099fd371", "issue_key": "SPARK-902", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "java.lang.AbstractMethodError when using FlatMapFunction from Java", "description": "*Important Note*: a patch for _FlatMapFunction.scala_ fixing the problem is attached! Defining a Java function class based on org.apache.spark.api.java.function.FlatMapFunction compiles without problems, but on execution results in  java.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object; at org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31)  I have investigated the problem in detail. Using the following small Java class, the issue can easily be reproduced.  import java.lang.reflect.Method; import java.util.Arrays; import org.apache.spark.api.java.function.FlatMapFunction; public class X { public static class MyFunction extends FlatMapFunction<String, Long> { @Override public Iterable<Long> call(String s) throws Exception { return Arrays.asList(Long.parseLong(s)); } } public static void main(String[] args) { //printMethods(WrappedFunction1.class); printMethods(FlatMapFunction.class); MyFunction f = new MyFunction(); final Iterable<Long> result = f.apply(\"1\"); System.out.println(result); } private static void printMethods(Class<?> cls) { System.out.println(cls.getName() + \" --------------\"); for (Method m: cls.getDeclaredMethods()) { System.out.println(m); } final Class<?> superCls = cls.getSuperclass(); if (superCls != null && superCls != Object.class) { printMethods(cls.getSuperclass()); } } }  When you run this code, you get following output:  org.apache.spark.api.java.function.FlatMapFunction -------------- public abstract java.lang.Iterable org.apache.spark.api.java.function.FlatMapFunction.call(java.lang.Object) throws java.lang.Exception public scala.reflect.ClassManifest org.apache.spark.api.java.function.FlatMapFunction.elementType() org.apache.spark.api.java.function.Function -------------- public scala.reflect.ClassManifest org.apache.spark.api.java.function.Function.returnType() public abstract java.lang.Object org.apache.spark.api.java.function.Function.call(java.lang.Object) throws java.lang.Exception org.apache.spark.api.java.function.WrappedFunction1 -------------- public final java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.apply(java.lang.Object) public abstract java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.call(java.lang.Object) throws java.lang.Exception ... Exception in thread \"main\" java.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object; at org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31) at X.main(X.java:21)  So the problem seems to be that the _call_ method is defined twice. # returning Object # returning Iterable Solution: just delete the definition of the _call_ method in _FlatMapFunction_ (see attached file)", "reporter": "Martin Weindel", "assignee": "Josh Rosen", "created": "2013-09-11T04:54:45.000+0000", "updated": "2013-10-22T16:33:12.000+0000", "resolved": "2013-10-22T16:33:12.000+0000", "labels": [], "components": ["Java API", "Spark Core"], "comments": [{"author": "Josh Rosen", "body": "I tried running your code example using both Maven and sbt, with both Spark 0.7.3 and 0.8.0-incubating, but I wasn't able to reproduce the AbstractMethodError. I'm running this Java version on OSX:  java version \"1.7.0_21\" Java(TM) SE Runtime Environment (build 1.7.0_21-b12) Java HotSpot(TM) 64-Bit Server VM (build 23.21-b01, mixed mode)  How are you compiling and running this example code?", "created": "2013-10-20T14:21:12.345+0000"}, {"author": "Martin Weindel", "body": "The problem only occurs with the Eclipse compiler! If I'm using javac, the sample runs without errors. I never expected that the Eclipse compiler is the cause. So this problem happens if someone builds a JAR package in Eclipse and then deploys it to Spark. In fact, this is my use case. To reproduce the problem, either compile the sample code in Eclipse or by running the following command line after downloading the Eclipse compiler from e.g. http://repo1.maven.org/maven2/org/eclipse/tycho/org.eclipse.jdt.core/3.9.0.v20130604-1421/org.eclipse.jdt.core-3.9.0.v20130604-1421.jar: java -jar org.eclipse.jdt.core-3.9.0.v20130604-1421.jar -cp spark-assembly_2.9.3-0.8.0-incubating-hadoop2.0.0-mr1-cdh4.2.0.jar -source 1.7 -target 1.7 Main.java", "created": "2013-10-21T13:13:27.989+0000"}, {"author": "Josh Rosen", "body": "Yep, it looks like the Eclipse compiler is the culprit. I was able to reproduce this bug by compiling my code through Maven, using the Eclipse compiler plugin. For my own future reference, here's the POM that I used:  <project> <groupId>edu.berkeley</groupId> <artifactId>simple-project</artifactId> <modelVersion>4.0.0</modelVersion> <name>Simple Project</name> <packaging>jar</packaging> <version>1.0</version> <repositories> <!-- Repositories that Spark needs --> <repository> <id>Spray Repository</id> <url>http://repo.spray.cc</url> </repository> <repository> <id>Akka Repository</id> <url>http://repo.akka.io/releases</url> </repository> </repositories> <dependencies> <dependency> <!-- Spark dependency --> <groupId>org.apache.spark</groupId> <artifactId>spark-core_2.9.3</artifactId> <version>0.8.0-incubating</version> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.1</version> <configuration> <compilerId>eclipse</compilerId> <source>1.7</source> <target>1.7</target> </configuration> <dependencies> <dependency> <groupId>org.codehaus.plexus</groupId> <artifactId>plexus-compiler-eclipse</artifactId> <version>2.3</version> </dependency> </dependencies> </plugin> </plugins> </build> </project>  I'm going to take a a closer look at the fix in your pull request (https://github.com/apache/incubator-spark/pull/30) to see whether we need to apply it anywhere else, or whether the change breaks the ability to throw exceptions from JavaFunctions.", "created": "2013-10-22T13:26:50.926+0000"}, {"author": "Josh Rosen", "body": "Fixed by https://github.com/apache/incubator-spark/pull/100", "created": "2013-10-22T16:33:12.875+0000"}], "num_comments": 4, "text": "Issue: SPARK-902\nSummary: java.lang.AbstractMethodError when using FlatMapFunction from Java\nDescription: *Important Note*: a patch for _FlatMapFunction.scala_ fixing the problem is attached! Defining a Java function class based on org.apache.spark.api.java.function.FlatMapFunction compiles without problems, but on execution results in  java.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object; at org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31)  I have investigated the problem in detail. Using the following small Java class, the issue can easily be reproduced.  import java.lang.reflect.Method; import java.util.Arrays; import org.apache.spark.api.java.function.FlatMapFunction; public class X { public static class MyFunction extends FlatMapFunction<String, Long> { @Override public Iterable<Long> call(String s) throws Exception { return Arrays.asList(Long.parseLong(s)); } } public static void main(String[] args) { //printMethods(WrappedFunction1.class); printMethods(FlatMapFunction.class); MyFunction f = new MyFunction(); final Iterable<Long> result = f.apply(\"1\"); System.out.println(result); } private static void printMethods(Class<?> cls) { System.out.println(cls.getName() + \" --------------\"); for (Method m: cls.getDeclaredMethods()) { System.out.println(m); } final Class<?> superCls = cls.getSuperclass(); if (superCls != null && superCls != Object.class) { printMethods(cls.getSuperclass()); } } }  When you run this code, you get following output:  org.apache.spark.api.java.function.FlatMapFunction -------------- public abstract java.lang.Iterable org.apache.spark.api.java.function.FlatMapFunction.call(java.lang.Object) throws java.lang.Exception public scala.reflect.ClassManifest org.apache.spark.api.java.function.FlatMapFunction.elementType() org.apache.spark.api.java.function.Function -------------- public scala.reflect.ClassManifest org.apache.spark.api.java.function.Function.returnType() public abstract java.lang.Object org.apache.spark.api.java.function.Function.call(java.lang.Object) throws java.lang.Exception org.apache.spark.api.java.function.WrappedFunction1 -------------- public final java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.apply(java.lang.Object) public abstract java.lang.Object org.apache.spark.api.java.function.WrappedFunction1.call(java.lang.Object) throws java.lang.Exception ... Exception in thread \"main\" java.lang.AbstractMethodError: org.apache.spark.api.java.function.WrappedFunction1.call(Ljava/lang/Object;)Ljava/lang/Object; at org.apache.spark.api.java.function.WrappedFunction1.apply(WrappedFunction1.scala:31) at X.main(X.java:21)  So the problem seems to be that the _call_ method is defined twice. # returning Object # returning Iterable Solution: just delete the definition of the _call_ method in _FlatMapFunction_ (see attached file)\n\nComments (4):\n1. Josh Rosen: I tried running your code example using both Maven and sbt, with both Spark 0.7.3 and 0.8.0-incubating, but I wasn't able to reproduce the AbstractMethodError. I'm running this Java version on OSX:  java version \"1.7.0_21\" Java(TM) SE Runtime Environment (build 1.7.0_21-b12) Java HotSpot(TM) 64-Bit Server VM (build 23.21-b01, mixed mode)  How are you compiling and running this example code?\n2. Martin Weindel: The problem only occurs with the Eclipse compiler! If I'm using javac, the sample runs without errors. I never expected that the Eclipse compiler is the cause. So this problem happens if someone builds a JAR package in Eclipse and then deploys it to Spark. In fact, this is my use case. To reproduce the problem, either compile the sample code in Eclipse or by running the following command line after downloading the Eclipse compiler from e.g. http://repo1.maven.org/maven2/org/eclipse/tycho/org.eclipse.jdt.core/3.9.0.v20130604-1421/org.eclipse.jdt.core-3.9.0.v20130604-1421.jar: java -jar org.eclipse.jdt.core-3.9.0.v20130604-1421.jar -cp spark-assembly_2.9.3-0.8.0-incubating-hadoop2.0.0-mr1-cdh4.2.0.jar -source 1.7 -target 1.7 Main.java\n3. Josh Rosen: Yep, it looks like the Eclipse compiler is the culprit. I was able to reproduce this bug by compiling my code through Maven, using the Eclipse compiler plugin. For my own future reference, here's the POM that I used:  <project> <groupId>edu.berkeley</groupId> <artifactId>simple-project</artifactId> <modelVersion>4.0.0</modelVersion> <name>Simple Project</name> <packaging>jar</packaging> <version>1.0</version> <repositories> <!-- Repositories that Spark needs --> <repository> <id>Spray Repository</id> <url>http://repo.spray.cc</url> </repository> <repository> <id>Akka Repository</id> <url>http://repo.akka.io/releases</url> </repository> </repositories> <dependencies> <dependency> <!-- Spark dependency --> <groupId>org.apache.spark</groupId> <artifactId>spark-core_2.9.3</artifactId> <version>0.8.0-incubating</version> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.1</version> <configuration> <compilerId>eclipse</compilerId> <source>1.7</source> <target>1.7</target> </configuration> <dependencies> <dependency> <groupId>org.codehaus.plexus</groupId> <artifactId>plexus-compiler-eclipse</artifactId> <version>2.3</version> </dependency> </dependencies> </plugin> </plugins> </build> </project>  I'm going to take a a closer look at the fix in your pull request (https://github.com/apache/incubator-spark/pull/30) to see whether we need to apply it anywhere else, or whether the change breaks the ability to throw exceptions from JavaFunctions.\n4. Josh Rosen: Fixed by https://github.com/apache/incubator-spark/pull/100", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.078529"}}
{"id": "106f35ca9cd93b177aa9234762104666", "issue_key": "SPARK-903", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Inconsistent spark assembly", "description": "I have two problems when using the spark assembly (build via sbt\\sbt assembly) 1. HDFS file system is not available  java.io.IOException: No FileSystem for scheme: hdfs  This seems to be caused by overriding the file {{META-INF/services/org.apache.hadoop.fs.FileSystem}} Merging the original files manually resolves this problem: - hadoop-hdfs-2.0.0-cdh4.3.0.jar and - hadoop-common-2.0.0-cdh4.3.0.jar 2. Illegal jar file on windows The assembly jar contains - a directory META-INF/license - a file META-INF/LICENSE As Windows file system is case-insensitive, this can cause trouble.", "reporter": "Martin Weindel", "assignee": "Patrick McFadin", "created": "2013-09-11T05:04:16.000+0000", "updated": "2013-09-15T14:39:31.000+0000", "resolved": "2013-09-15T14:39:31.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Martin Weindel", "body": "HDFS file system problem has already been fixed by Patrick Wendell (commit 0c1985b153a2dc2c891ae61c1ee67506926384ae)", "created": "2013-09-14T06:52:35.323+0000"}], "num_comments": 1, "text": "Issue: SPARK-903\nSummary: Inconsistent spark assembly\nDescription: I have two problems when using the spark assembly (build via sbt\\sbt assembly) 1. HDFS file system is not available  java.io.IOException: No FileSystem for scheme: hdfs  This seems to be caused by overriding the file {{META-INF/services/org.apache.hadoop.fs.FileSystem}} Merging the original files manually resolves this problem: - hadoop-hdfs-2.0.0-cdh4.3.0.jar and - hadoop-common-2.0.0-cdh4.3.0.jar 2. Illegal jar file on windows The assembly jar contains - a directory META-INF/license - a file META-INF/LICENSE As Windows file system is case-insensitive, this can cause trouble.\n\nComments (1):\n1. Martin Weindel: HDFS file system problem has already been fixed by Patrick Wendell (commit 0c1985b153a2dc2c891ae61c1ee67506926384ae)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.078529"}}
{"id": "0916a8ea6bdd9d2c5b751f0b3c2aaae5", "issue_key": "SPARK-904", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Not able to Start/Stop Spark Worker from Remote Machine", "description": "I have two machines A and B. I am trying to run Spark Master on machine A and Spark Worker on machine B. I have set machine B'host name in conf/slaves in my Spark directory. When I am executing start-all.sh to start master and workers, I am getting below message on console: abc@abc-vostro:~/spark-scala-2.10$ sudo sh bin/start-all.sh sudo: /etc/sudoers.d is world writable starting spark.deploy.master.Master, logging to /home/abc/spark-scala-2.10/bin/../logs/spark-root-spark.deploy.master.Master-1-abc-vostro.out 13/09/11 14:54:29 WARN spark.Utils: Your hostname, abc-vostro resolves to a loopback address: 127.0.1.1; using 1XY.1XY.Y.Y instead (on interface wlan2) 13/09/11 14:54:29 WARN spark.Utils: Set SPARK_LOCAL_IP if you need to bind to another address Master IP: abc-vostro cd /home/abc/spark-scala-2.10/bin/.. ; /home/abc/spark-scala-2.10/bin/start-slave.sh 1 spark://abc-vostro:7077 xyz@1XX.1XX.X.X's password: xyz@1XX.1XX.X.X: bash: line 0: cd: /home/abc/spark-scala-2.10/bin/..: No such file or directory xyz@1XX.1XX.X.X: bash: /home/abc/spark-scala-2.10/bin/start-slave.sh: No such file or directory Master is started but worker is failed to start. I have set xyz@1XX.1XX.X.X in conf/slaves in my Spark directory. Can anyone help me to resolve this? This is probably something I'm missing any configuration on my end. However When I create Spark Master and Worker on same machine, It is working fine.", "reporter": "Ayush", "assignee": null, "created": "2013-09-11T05:06:44.000+0000", "updated": "2014-11-14T09:59:30.000+0000", "resolved": "2014-11-14T09:59:30.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "[~ayushmishra2005] I suspect you don't have Spark installed on the remote machine -- the {{start-all.sh}} script won't install it for you on remote machines. If you're still having trouble, please reach out to the spark users list from http://spark.apache.org/community.html which is a better place for these kinds of requests anyway. I'm closing this issue for now but let me know here if you aren't able to get a resolution on the mailing lists. Thanks, and good luck with Spark! Andrew", "created": "2014-11-14T09:59:18.444+0000"}], "num_comments": 1, "text": "Issue: SPARK-904\nSummary: Not able to Start/Stop Spark Worker from Remote Machine\nDescription: I have two machines A and B. I am trying to run Spark Master on machine A and Spark Worker on machine B. I have set machine B'host name in conf/slaves in my Spark directory. When I am executing start-all.sh to start master and workers, I am getting below message on console: abc@abc-vostro:~/spark-scala-2.10$ sudo sh bin/start-all.sh sudo: /etc/sudoers.d is world writable starting spark.deploy.master.Master, logging to /home/abc/spark-scala-2.10/bin/../logs/spark-root-spark.deploy.master.Master-1-abc-vostro.out 13/09/11 14:54:29 WARN spark.Utils: Your hostname, abc-vostro resolves to a loopback address: 127.0.1.1; using 1XY.1XY.Y.Y instead (on interface wlan2) 13/09/11 14:54:29 WARN spark.Utils: Set SPARK_LOCAL_IP if you need to bind to another address Master IP: abc-vostro cd /home/abc/spark-scala-2.10/bin/.. ; /home/abc/spark-scala-2.10/bin/start-slave.sh 1 spark://abc-vostro:7077 xyz@1XX.1XX.X.X's password: xyz@1XX.1XX.X.X: bash: line 0: cd: /home/abc/spark-scala-2.10/bin/..: No such file or directory xyz@1XX.1XX.X.X: bash: /home/abc/spark-scala-2.10/bin/start-slave.sh: No such file or directory Master is started but worker is failed to start. I have set xyz@1XX.1XX.X.X in conf/slaves in my Spark directory. Can anyone help me to resolve this? This is probably something I'm missing any configuration on my end. However When I create Spark Master and Worker on same machine, It is working fine.\n\nComments (1):\n1. Andrew Ash: [~ayushmishra2005] I suspect you don't have Spark installed on the remote machine -- the {{start-all.sh}} script won't install it for you on remote machines. If you're still having trouble, please reach out to the spark users list from http://spark.apache.org/community.html which is a better place for these kinds of requests anyway. I'm closing this issue for now but let me know here if you aren't able to get a resolution on the mailing lists. Thanks, and good luck with Spark! Andrew", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.078529"}}
{"id": "3d43da321e5bb411162fe1cd6f996264", "issue_key": "SPARK-905", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Not able to run Job on remote machine", "description": "I have two machines A and B. On machine A, I run ./run spark.deploy.master.Master to start Master. Master URL is spark://abc-vostro.local:7077. Now on machine B, I run ./run spark.deploy.worker.Worker spark://abc-vostro.local:7077 Now worker has been registered to master. Now I want to run a simple job on cluster. Here is SimpleJob.scala package spark.examples import spark.SparkContext import SparkContext._ object SimpleJob { def main(args: Array[String]) { val logFile = \"s3n://<AWS_ACCESS_KEY_ID>:<AWS_SECRET_ACCESS_KEY>/<File Name>\" val sc = new SparkContext(\"spark://abc-vostro.local:7077\", \"Simple Job\", System.getenv(\"SPARK_HOME\"), Seq(\"/home/abc/spark-scala-2.10/examples/target/scala-2.10/spark-examples_2.10-0.8.0-SNAPSHOT.jar\")) val logData = sc.textFile(logFile) val numsa = logData.filter(line => line.contains(\"a\")).count val numsb = logData.filter(line => line.contains(\"b\")).count println(\"total a : %s, total b : %s\".format(numsa, numsb)) } } This file is located at \"/home/abc/spark-scala-2.10/examples/src/main/scala/spark/examples\" on machine A. Now on machine A, I run sbt/sbt package. When I run MASTER=spark://abc-vostro.local:7077 ./run spark.examples.SimpleJob to run my job, I am getting below exception on both machines A and B, (class java.io.IOException: Cannot run program \"/home/abc/spark-scala-2.10/bin/compute-classpath.sh\" (in directory \".\"): error=2, No such file or directory) Could you please help me to resolve this? This is probably something I'm missing any configuration on my end. Thanks in advance.", "reporter": "Ayush", "assignee": null, "created": "2013-09-11T05:08:42.000+0000", "updated": "2014-11-08T09:54:58.000+0000", "resolved": "2014-11-08T09:54:58.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Harrison Brundage", "body": "Ayush, did you manage to figure out how to fix this?", "created": "2014-01-09T08:29:38.056+0000"}, {"author": "Ayush", "body": "Not yet.", "created": "2014-01-10T02:15:49.647+0000"}, {"author": "Sean R. Owen", "body": "This looks like something that was either long since fixed, or just a matter of not having the Spark installation set up on each machine. The compute-classpath.sh script does exist in bin/ in the tree and distro.", "created": "2014-11-08T09:54:58.550+0000"}], "num_comments": 3, "text": "Issue: SPARK-905\nSummary: Not able to run Job on remote machine\nDescription: I have two machines A and B. On machine A, I run ./run spark.deploy.master.Master to start Master. Master URL is spark://abc-vostro.local:7077. Now on machine B, I run ./run spark.deploy.worker.Worker spark://abc-vostro.local:7077 Now worker has been registered to master. Now I want to run a simple job on cluster. Here is SimpleJob.scala package spark.examples import spark.SparkContext import SparkContext._ object SimpleJob { def main(args: Array[String]) { val logFile = \"s3n://<AWS_ACCESS_KEY_ID>:<AWS_SECRET_ACCESS_KEY>/<File Name>\" val sc = new SparkContext(\"spark://abc-vostro.local:7077\", \"Simple Job\", System.getenv(\"SPARK_HOME\"), Seq(\"/home/abc/spark-scala-2.10/examples/target/scala-2.10/spark-examples_2.10-0.8.0-SNAPSHOT.jar\")) val logData = sc.textFile(logFile) val numsa = logData.filter(line => line.contains(\"a\")).count val numsb = logData.filter(line => line.contains(\"b\")).count println(\"total a : %s, total b : %s\".format(numsa, numsb)) } } This file is located at \"/home/abc/spark-scala-2.10/examples/src/main/scala/spark/examples\" on machine A. Now on machine A, I run sbt/sbt package. When I run MASTER=spark://abc-vostro.local:7077 ./run spark.examples.SimpleJob to run my job, I am getting below exception on both machines A and B, (class java.io.IOException: Cannot run program \"/home/abc/spark-scala-2.10/bin/compute-classpath.sh\" (in directory \".\"): error=2, No such file or directory) Could you please help me to resolve this? This is probably something I'm missing any configuration on my end. Thanks in advance.\n\nComments (3):\n1. Harrison Brundage: Ayush, did you manage to figure out how to fix this?\n2. Ayush: Not yet.\n3. Sean R. Owen: This looks like something that was either long since fixed, or just a matter of not having the Spark installation set up on each machine. The compute-classpath.sh script does exist in bin/ in the tree and distro.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.078529"}}
{"id": "c2bcf0bb4ceed2ea2ddbfc92d1d9c939", "issue_key": "SPARK-906", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "How to recover Spark Master in case of machine failure, where Spark Master was running", "description": "I have one Spark master on machine A and two Spark workers on another machines B and C. If machine A is failed for any reason, Spark master would die in this case. Is there any way to recover Spark Master or to create a new Spark Master on another machine automatically? Can anyone help me to resolve this? Thanks in advance.", "reporter": "Ayush", "assignee": "Aaron Davidson", "created": "2013-09-11T05:10:29.000+0000", "updated": "2013-11-14T18:15:34.000+0000", "resolved": "2013-11-14T18:15:34.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Aaron Davidson", "body": "In Spark 0.8.1 and onward, there are two solutions for this (one uses ZooKeeper for full featured fault-tolerance, and the other uses the file system to be able to recover on the same machine). 0.8.1 is not yet released, but if you're interested in reading more about this, you can check out the unpublished docs at https://github.com/apache/incubator-spark/blob/master/docs/spark-standalone.md. (Even further down the pipeline, we hope to remove the Master entirely...)", "created": "2013-11-14T18:15:34.204+0000"}], "num_comments": 1, "text": "Issue: SPARK-906\nSummary: How to recover Spark Master in case of machine failure, where Spark Master was running\nDescription: I have one Spark master on machine A and two Spark workers on another machines B and C. If machine A is failed for any reason, Spark master would die in this case. Is there any way to recover Spark Master or to create a new Spark Master on another machine automatically? Can anyone help me to resolve this? Thanks in advance.\n\nComments (1):\n1. Aaron Davidson: In Spark 0.8.1 and onward, there are two solutions for this (one uses ZooKeeper for full featured fault-tolerance, and the other uses the file system to be able to recover on the same machine). 0.8.1 is not yet released, but if you're interested in reading more about this, you can check out the unpublished docs at https://github.com/apache/incubator-spark/blob/master/docs/spark-standalone.md. (Even further down the pipeline, we hope to remove the Master entirely...)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.078529"}}
{"id": "c87cbcb078a244483c293aa6c04c0219", "issue_key": "SPARK-907", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Add JSON endpoints to SparkUI", "description": "At the moment there does not appear to be any JSON endpoints for the main SparkUI like there is in place for the standalone Master and Worker web UIs. Adding these endpoints will allow for easier custom UI development.", "reporter": "David McCauley", "assignee": "David McCauley", "created": "2013-09-11T07:58:16.000+0000", "updated": "2014-11-25T17:00:17.000+0000", "resolved": "2014-11-25T17:00:17.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Andrew Ash", "body": "[~dmccauley] this report looks like a duplicate of SPARK-3644, not just related. Do you feel like this ticket is encapsulated in the design work for a Spark REST API happening there? If so I think we can close this one as a duplicate for the other, more fleshed-out ticket. Thanks! Andrew", "created": "2014-11-25T16:52:39.380+0000"}, {"author": "David McCauley", "body": "[~aash] yes, SPARK-3644 seems to be a much more comprehensive description of the work needed for the task. I'll close this ticket out. Thanks, Dave", "created": "2014-11-25T16:59:59.718+0000"}], "num_comments": 2, "text": "Issue: SPARK-907\nSummary: Add JSON endpoints to SparkUI\nDescription: At the moment there does not appear to be any JSON endpoints for the main SparkUI like there is in place for the standalone Master and Worker web UIs. Adding these endpoints will allow for easier custom UI development.\n\nComments (2):\n1. Andrew Ash: [~dmccauley] this report looks like a duplicate of SPARK-3644, not just related. Do you feel like this ticket is encapsulated in the design work for a Spark REST API happening there? If so I think we can close this one as a duplicate for the other, more fleshed-out ticket. Thanks! Andrew\n2. David McCauley: [~aash] yes, SPARK-3644 seems to be a much more comprehensive description of the work needed for the task. I'll close this ticket out. Thanks, Dave", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.078529"}}
{"id": "3d0501b4437e987965bfc4aea73b4bec", "issue_key": "SPARK-908", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "local metrics test has race condition due to new SparkListener architecture", "description": "This test assumes that the listener will synchronously process an event once an action is called. Kay's changes made this asynchronous, so this test can now fail sometimes depending on the order in which events occur. https://github.com/mesos/spark/blob/master/core/src/test/scala/org/apache/spark/scheduler/SparkListenerSuite.scala", "reporter": "Patrick McFadin", "assignee": "Kay Ousterhout", "created": "2013-09-15T14:41:36.000+0000", "updated": "2013-11-25T14:44:04.000+0000", "resolved": "2013-11-25T14:44:04.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-908\nSummary: local metrics test has race condition due to new SparkListener architecture\nDescription: This test assumes that the listener will synchronously process an event once an action is called. Kay's changes made this asynchronous, so this test can now fail sometimes depending on the order in which events occur. https://github.com/mesos/spark/blob/master/core/src/test/scala/org/apache/spark/scheduler/SparkListenerSuite.scala", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.078529"}}
{"id": "52c73943f5edab500ab05f761c9fad51", "issue_key": "SPARK-909", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "add task serialization footprint (time and size) into TaskMetrics", "description": "currently, task serialization taken time and size are only saved as log. To better using these information to tune the spark programming. I would suggest put them in TaskMetrics so that it could be accessed by adding a custom spark listener.", "reporter": "Colorsmart", "assignee": null, "created": "2013-09-16T06:36:24.000+0000", "updated": "2014-10-21T07:50:37.000+0000", "resolved": "2014-10-21T07:50:37.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Reynold Xin", "body": "Do you know what the overhead might be with this turned on? If we are going to add this, we need to make sure the overhead is low because this is on the critical path.", "created": "2013-09-16T08:24:38.063+0000"}, {"author": "Colorsmart", "body": "I could do some tests on that, but I think it won't be bigger than the deserialization part.", "created": "2013-09-16T17:21:51.977+0000"}, {"author": "Reynold Xin", "body": "Thanks that would be great. I think it would be fine as long as it brings minimal overhead.", "created": "2013-09-16T17:26:26.727+0000"}, {"author": "OuyangJin", "body": "Hi , I read revlevant code , and I wonder how we put task serialization time and size into TaskMetrics Task serialization is done in TaskSetManager.resourceOffer before it is passed as a ByteBuffer to TaskDescription, and finally to launchTask as a message to actor of ExectorBackEnd. And this ByteBuffer is finally fetched and deserialized in Executor's side ,in TaskRunner. One method I can think to put these footprint into TaskMetrics is that we do task serialization twice . First time we get a serializedTask ByteBuffer and serialization takenTime. And we put them into task.metrics , and then we serialize this task local var with task serizlization footprint in its merics again and get a new serializedTask ByteBuffer, whers its metrics member have serializtion footprint. But it may bring overhead because we serialize the task object twice, and the footprint in task.metrics is not that percise because first time we serialize it , task.metrics is empty, so serialized size may be smaller. Anyone has a better idea, I'd like to imporve this and make a PR", "created": "2014-02-04T23:21:34.575+0000"}, {"author": "Patrick Wendell", "body": "This has already been fixed.", "created": "2014-10-21T07:50:37.046+0000"}], "num_comments": 5, "text": "Issue: SPARK-909\nSummary: add task serialization footprint (time and size) into TaskMetrics\nDescription: currently, task serialization taken time and size are only saved as log. To better using these information to tune the spark programming. I would suggest put them in TaskMetrics so that it could be accessed by adding a custom spark listener.\n\nComments (5):\n1. Reynold Xin: Do you know what the overhead might be with this turned on? If we are going to add this, we need to make sure the overhead is low because this is on the critical path.\n2. Colorsmart: I could do some tests on that, but I think it won't be bigger than the deserialization part.\n3. Reynold Xin: Thanks that would be great. I think it would be fine as long as it brings minimal overhead.\n4. OuyangJin: Hi , I read revlevant code , and I wonder how we put task serialization time and size into TaskMetrics Task serialization is done in TaskSetManager.resourceOffer before it is passed as a ByteBuffer to TaskDescription, and finally to launchTask as a message to actor of ExectorBackEnd. And this ByteBuffer is finally fetched and deserialized in Executor's side ,in TaskRunner. One method I can think to put these footprint into TaskMetrics is that we do task serialization twice . First time we get a serializedTask ByteBuffer and serialization takenTime. And we put them into task.metrics , and then we serialize this task local var with task serizlization footprint in its merics again and get a new serializedTask ByteBuffer, whers its metrics member have serializtion footprint. But it may bring overhead because we serialize the task object twice, and the footprint in task.metrics is not that percise because first time we serialize it , task.metrics is empty, so serialized size may be smaller. Anyone has a better idea, I'd like to imporve this and make a PR\n5. Patrick Wendell: This has already been fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "fdf9d816aabf5f3bb693ea709db9865f", "issue_key": "SPARK-910", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "hadoopFile creates RecordReader key and value at the wrong scope", "description": "I'm not a scala or hadoop expert so forgive me if I'm in the wrong but it seems to me that SparkContext.hadoopFile is broken. hf = sc.hadoopFile(\"hdfs://namenod.local/something.xml\", XmlInputFormat.class, LongWritable.class, Text.class); hf.take(5); produces the same record over and over instead of iterating. Here is a pull request for a proposed fix: https://github.com/mesos/spark/pull/934", "reporter": "aaron babcock", "assignee": null, "created": "2013-09-16T21:36:26.000+0000", "updated": "2014-11-25T09:50:52.000+0000", "resolved": "2014-11-25T09:50:52.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Diana Carroll", "body": "I suspect this problem is similar related to SPARK-1018. The problem isn't the input format (in my case I used TextInputFormat), which correctly reads the file, but with the take method, which isn't correctly handling the Pair(LongWritable,Text) being output.", "created": "2014-01-09T10:47:28.337+0000"}, {"author": "Sean R. Owen", "body": "Given the PR discussion, it looks like this was resolved as NotAProblem. Either the InputFormat has to create new key/value objects, or the caller in Spark does.", "created": "2014-11-25T09:50:52.112+0000"}], "num_comments": 2, "text": "Issue: SPARK-910\nSummary: hadoopFile creates RecordReader key and value at the wrong scope\nDescription: I'm not a scala or hadoop expert so forgive me if I'm in the wrong but it seems to me that SparkContext.hadoopFile is broken. hf = sc.hadoopFile(\"hdfs://namenod.local/something.xml\", XmlInputFormat.class, LongWritable.class, Text.class); hf.take(5); produces the same record over and over instead of iterating. Here is a pull request for a proposed fix: https://github.com/mesos/spark/pull/934\n\nComments (2):\n1. Diana Carroll: I suspect this problem is similar related to SPARK-1018. The problem isn't the input format (in my case I used TextInputFormat), which correctly reads the file, but with the take method, which isn't correctly handling the Pair(LongWritable,Text) being output.\n2. Sean R. Owen: Given the PR discussion, it looks like this was resolved as NotAProblem. Either the InputFormat has to create new key/value objects, or the caller in Spark does.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "012658bf511dc505a66191c570453fd2", "issue_key": "SPARK-911", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support map pruning on sorted (K, V) RDD's", "description": "If someone has sorted a (K, V) rdd, we should offer them a way to filter a range of the partitions that employs map pruning. This would be simple using a small range index within the rdd itself. A good example is I sort my dataset by time and then I want to serve queries that are restricted to a certain time range.", "reporter": "Patrick Wendell", "assignee": "Aaron", "created": "2013-09-17T13:01:43.000+0000", "updated": "2015-04-25T21:58:57.000+0000", "resolved": "2015-02-23T06:09:26.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Aaron", "body": "I was just wondering about this when answering http://stackoverflow.com/questions/24677180/how-do-i-select-a-range-of-elements-in-spark-rdd, I noticed that RangePartitioner stores an array of upper bounds, but I was unsure from the docs if that meant that the each partition had no overlap in range. If they don't this seems like it would be a pretty easy feature to implement", "created": "2014-07-10T14:21:07.681+0000"}, {"author": "Aaron", "body": "I decided to take a look at this myself (disclaimer haven't worked on sparks source code before). A couple things I noticed were: 1. since compute is called per split for all RDD's, the best I could figure out using that approach is to just return an empty iterator, not sure if that creates a weird partition situation 2. you could more efficiently check the partitions that could contain correct numbers, but with only an iterator you have to linearly search", "created": "2014-07-11T03:50:31.923+0000"}, {"author": "Aaron", "body": "pull request https://github.com/apache/spark/pull/1381/files", "created": "2014-07-12T00:33:14.893+0000"}, {"author": "Apache Spark", "body": "User 'aaronjosephs' has created a pull request for this issue: https://github.com/apache/spark/pull/1381", "created": "2014-11-25T06:47:28.033+0000"}, {"author": "Josh Rosen", "body": "Issue resolved by pull request 1381 [https://github.com/apache/spark/pull/1381]", "created": "2015-02-23T06:09:26.750+0000"}], "num_comments": 5, "text": "Issue: SPARK-911\nSummary: Support map pruning on sorted (K, V) RDD's\nDescription: If someone has sorted a (K, V) rdd, we should offer them a way to filter a range of the partitions that employs map pruning. This would be simple using a small range index within the rdd itself. A good example is I sort my dataset by time and then I want to serve queries that are restricted to a certain time range.\n\nComments (5):\n1. Aaron: I was just wondering about this when answering http://stackoverflow.com/questions/24677180/how-do-i-select-a-range-of-elements-in-spark-rdd, I noticed that RangePartitioner stores an array of upper bounds, but I was unsure from the docs if that meant that the each partition had no overlap in range. If they don't this seems like it would be a pretty easy feature to implement\n2. Aaron: I decided to take a look at this myself (disclaimer haven't worked on sparks source code before). A couple things I noticed were: 1. since compute is called per split for all RDD's, the best I could figure out using that approach is to just return an empty iterator, not sure if that creates a weird partition situation 2. you could more efficiently check the partitions that could contain correct numbers, but with only an iterator you have to linearly search\n3. Aaron: pull request https://github.com/apache/spark/pull/1381/files\n4. Apache Spark: User 'aaronjosephs' has created a pull request for this issue: https://github.com/apache/spark/pull/1381\n5. Josh Rosen: Issue resolved by pull request 1381 [https://github.com/apache/spark/pull/1381]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "55e3774be62bb39d2c02ab6168791e0c", "issue_key": "SPARK-912", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Take stage breakdown functionality and runLocally out of the main event loop in DAGScheduler", "description": "This can reduce the complexity of the main event loop and improve performance (since the main event loop is single threaded). We can also take the result task deserialization code out of the main loop (maybe Kay is already working on this?).", "reporter": "Reynold Xin", "assignee": null, "created": "2013-09-18T04:09:23.000+0000", "updated": "2020-05-17T17:47:56.000+0000", "resolved": "2015-05-24T04:43:57.000+0000", "labels": [], "components": ["Scheduler", "Spark Core"], "comments": [{"author": "Josh Rosen", "body": "It looks like the task result deserialization code was moved out of the main loop in SPARK-7655. It also looks like the {{runLocally}} was moved into a separate thread a long time ago (pre-1.0?). Given this, I'm going to resolve this as fixed; I'm not quite sure what the \"stage breakdown\" in the title is referring to but I think that code has been significantly optimized since 0.8, so it's probably no longer relevant.", "created": "2015-05-24T04:43:57.985+0000"}], "num_comments": 1, "text": "Issue: SPARK-912\nSummary: Take stage breakdown functionality and runLocally out of the main event loop in DAGScheduler\nDescription: This can reduce the complexity of the main event loop and improve performance (since the main event loop is single threaded). We can also take the result task deserialization code out of the main loop (maybe Kay is already working on this?).\n\nComments (1):\n1. Josh Rosen: It looks like the task result deserialization code was moved out of the main loop in SPARK-7655. It also looks like the {{runLocally}} was moved into a separate thread a long time ago (pre-1.0?). Given this, I'm going to resolve this as fixed; I'm not quite sure what the \"stage breakdown\" in the title is referring to but I think that code has been significantly optimized since 0.8, so it's probably no longer relevant.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "125caa96b6a93d51cdc2277e30352fe6", "issue_key": "SPARK-913", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "log the size of each shuffle block in block manager", "description": "", "reporter": "Reynold Xin", "assignee": null, "created": "2013-09-20T14:52:33.000+0000", "updated": "2020-05-17T18:21:33.000+0000", "resolved": "2016-03-18T16:46:00.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Apache Spark", "body": "User 'devaraj-kavali' has created a pull request for this issue: https://github.com/apache/spark/pull/11819", "created": "2016-03-18T10:23:04.648+0000"}], "num_comments": 1, "text": "Issue: SPARK-913\nSummary: log the size of each shuffle block in block manager\n\nComments (1):\n1. Apache Spark: User 'devaraj-kavali' has created a pull request for this issue: https://github.com/apache/spark/pull/11819", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "daab2c5c490536d45ac3727e34f16f67", "issue_key": "SPARK-914", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Make RDD implement Scala and Java Iterable interfaces", "description": "It would benefit developers if RDD implemented the Java and Scala Iterable interfaces so RDDs could be used in contexts requiring those types. A good source of inspiration might be Kafka's KafkaStream, which implements both interfaces.", "reporter": "Paul Snively", "assignee": null, "created": "2013-09-21T12:10:40.000+0000", "updated": "2014-11-06T07:06:49.000+0000", "resolved": "2014-11-06T07:06:49.000+0000", "labels": [], "components": ["Java API", "Spark Core"], "comments": [{"author": "Egor Pakhomov", "body": "https://github.com/apache/spark/pull/156", "created": "2014-03-16T05:18:06.427+0000"}, {"author": "Andrew Ash", "body": "Please close this ticket -- it is a duplicate of SPARK-1259 which was completed for 1.0", "created": "2014-06-18T22:30:23.001+0000"}], "num_comments": 2, "text": "Issue: SPARK-914\nSummary: Make RDD implement Scala and Java Iterable interfaces\nDescription: It would benefit developers if RDD implemented the Java and Scala Iterable interfaces so RDDs could be used in contexts requiring those types. A good source of inspiration might be Kafka's KafkaStream, which implements both interfaces.\n\nComments (2):\n1. Egor Pakhomov: https://github.com/apache/spark/pull/156\n2. Andrew Ash: Please close this ticket -- it is a duplicate of SPARK-1259 which was completed for 1.0", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "4f98ac6ba2a1149b43a4f47da016df62", "issue_key": "SPARK-915", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Tidy up the scripts", "description": "Having worked with a few customers, we found the current organization of scripts looked a bit confusing and inconvenient. For most of our customer, the application developers/users and platform administrators belong to two teams. So it's more convenient to separate the scripts used by administrators and application users. Specifically, we'd propose to employ a methodology similar to hadoop's. 1, make an \"sbin\" folder containing all the scripts for administrators, specifically, 1) all service administration scripts, i.e. start-*, stop-*, slaves.sh, *-daemons, *-daemon scripts 2) low-level or internally used utility scripts, i.e. compute-classpath, spark-config, spark-class, spark-executor 2. make a \"bin\" folder containing all the scripts for application developers/users, specifically, 1) user level app running scripts, i.e. pyspark, spark-shell, and we propose to add a script \"spark\" for users to run applications (very much like spark-class but may add some more control or convenient utilities) scripts for status checking, e.g. spark and hadoop version checking, 2) running applications checking, etc. We can make this a separate script or add functionality to \"spark\" script. 3. No wandering scripts outside the sbin and bin folders", "reporter": "Shane Huang", "assignee": null, "created": "2013-09-21T21:05:40.000+0000", "updated": "2014-07-25T23:27:09.000+0000", "resolved": "2014-07-25T23:27:09.000+0000", "labels": [], "components": ["Deploy", "Spark Core"], "comments": [{"author": "Shane Huang", "body": "The link to the original discussion on mailing list is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html", "created": "2013-09-21T21:12:50.709+0000"}, {"author": "Shane Huang", "body": "A pull request was submitted for separating admin and user scripts. https://github.com/apache/incubator-spark/pull/21", "created": "2013-09-26T22:03:04.925+0000"}], "num_comments": 2, "text": "Issue: SPARK-915\nSummary: Tidy up the scripts\nDescription: Having worked with a few customers, we found the current organization of scripts looked a bit confusing and inconvenient. For most of our customer, the application developers/users and platform administrators belong to two teams. So it's more convenient to separate the scripts used by administrators and application users. Specifically, we'd propose to employ a methodology similar to hadoop's. 1, make an \"sbin\" folder containing all the scripts for administrators, specifically, 1) all service administration scripts, i.e. start-*, stop-*, slaves.sh, *-daemons, *-daemon scripts 2) low-level or internally used utility scripts, i.e. compute-classpath, spark-config, spark-class, spark-executor 2. make a \"bin\" folder containing all the scripts for application developers/users, specifically, 1) user level app running scripts, i.e. pyspark, spark-shell, and we propose to add a script \"spark\" for users to run applications (very much like spark-class but may add some more control or convenient utilities) scripts for status checking, e.g. spark and hadoop version checking, 2) running applications checking, etc. We can make this a separate script or add functionality to \"spark\" script. 3. No wandering scripts outside the sbin and bin folders\n\nComments (2):\n1. Shane Huang: The link to the original discussion on mailing list is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html\n2. Shane Huang: A pull request was submitted for separating admin and user scripts. https://github.com/apache/incubator-spark/pull/21", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "3ad4df48815c791150787c9c99b521ac", "issue_key": "SPARK-916", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Better Support for Flat/Tabular RDD's", "description": "Many people use Spark to run analysis on flat datasets, where the RDD is composed records with a single set of non-nested fields. We could have better support for this use case in a variety of areas. Two of which are: 1. Allowing people to name individual fields and access them by name, rather than using tuple indices (see Scalding[1]). This avoids the mess that is {x => (x._3(), x._4())} 2. Support columnar in-memory storage. This is just an umbrella/brainstorming JIRA to see if other people have thoughts about this. Curious to hear feedback. [1] https://dev.twitter.com/blog/scalding", "reporter": "Patrick McFadin", "assignee": null, "created": "2013-09-23T12:43:49.000+0000", "updated": "2014-10-21T07:45:54.000+0000", "resolved": "2014-10-21T07:45:54.000+0000", "labels": [], "components": [], "comments": [{"author": "Stephen Haberman", "body": "Just kibitzing, but you can kinda/sorta solve this by have per-table case classes, e.g. case class FooTableLine(line: String) { def columnA() = ... def columnB() = ... } So, then with an RDD[FooTableLine], stuff like rdd.filter { _.columnA == \"whatever\" } is using the nice aliases. So, fine, this can be solved with user convention. The hard part is as soon as soon as you change the shape, e.g.: val rdd1: RDD[FooTableLine] = ... val rdd2 = rdd.map { l => (l.columnA, l.columnB) } Now you've lost your case class goodness and are stuck with a tuple. So, right, I'm sure this is all obvious/review, but just making sure I'm on the same page. Personally, I think the only way to get non-tuples is with C#-style anonymous types, where the compiler can basically synthesize (l.columnA, l.columnB) into a new unnamed type that has \"columnA\" and \"columnB\" fields. With scala, this would require compiler/macro magic. Is that where you're thinking of heading, Patrick? I.e. is that the nut you're trying to crack? FWIW I think Scalding's approach of using strings to keys is a cop-out, as you lose type-safety, which is one of the points of using Scala in the first place. (Disclaimer: I'm a macro/compiler/etc. newbie, so will defer to others to correct any wrong assumptions I have. This is just my understanding.)", "created": "2013-10-01T10:11:01.398+0000"}, {"author": "Patrick Wendell", "body": "Turned out [~marmbrus] did all of this and more in SparkSQL (which btw also works for nested types). So I'm gonna close this very old issue.", "created": "2014-10-21T07:45:54.211+0000"}], "num_comments": 2, "text": "Issue: SPARK-916\nSummary: Better Support for Flat/Tabular RDD's\nDescription: Many people use Spark to run analysis on flat datasets, where the RDD is composed records with a single set of non-nested fields. We could have better support for this use case in a variety of areas. Two of which are: 1. Allowing people to name individual fields and access them by name, rather than using tuple indices (see Scalding[1]). This avoids the mess that is {x => (x._3(), x._4())} 2. Support columnar in-memory storage. This is just an umbrella/brainstorming JIRA to see if other people have thoughts about this. Curious to hear feedback. [1] https://dev.twitter.com/blog/scalding\n\nComments (2):\n1. Stephen Haberman: Just kibitzing, but you can kinda/sorta solve this by have per-table case classes, e.g. case class FooTableLine(line: String) { def columnA() = ... def columnB() = ... } So, then with an RDD[FooTableLine], stuff like rdd.filter { _.columnA == \"whatever\" } is using the nice aliases. So, fine, this can be solved with user convention. The hard part is as soon as soon as you change the shape, e.g.: val rdd1: RDD[FooTableLine] = ... val rdd2 = rdd.map { l => (l.columnA, l.columnB) } Now you've lost your case class goodness and are stuck with a tuple. So, right, I'm sure this is all obvious/review, but just making sure I'm on the same page. Personally, I think the only way to get non-tuples is with C#-style anonymous types, where the compiler can basically synthesize (l.columnA, l.columnB) into a new unnamed type that has \"columnA\" and \"columnB\" fields. With scala, this would require compiler/macro magic. Is that where you're thinking of heading, Patrick? I.e. is that the nut you're trying to crack? FWIW I think Scalding's approach of using strings to keys is a cop-out, as you lose type-safety, which is one of the points of using Scala in the first place. (Disclaimer: I'm a macro/compiler/etc. newbie, so will defer to others to correct any wrong assumptions I have. This is just my understanding.)\n2. Patrick Wendell: Turned out [~marmbrus] did all of this and more in SparkSQL (which btw also works for nested types). So I'm gonna close this very old issue.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "5f5e3bc56752c74ab70fa7de3987871b", "issue_key": "SPARK-917", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "API docs for Spark/MLLib/Streaming should point to package URL", "description": "Now they point to a base page from which you need to click \"org->apache->spark\" etc.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-09-24T19:12:49.000+0000", "updated": "2014-03-17T10:22:27.000+0000", "resolved": "2014-03-17T10:22:27.000+0000", "labels": ["Starter"], "components": ["Documentation"], "comments": [{"author": "Patrick McFadin", "body": "This was fixed a while back", "created": "2014-03-17T10:22:27.504+0000"}], "num_comments": 1, "text": "Issue: SPARK-917\nSummary: API docs for Spark/MLLib/Streaming should point to package URL\nDescription: Now they point to a base page from which you need to click \"org->apache->spark\" etc.\n\nComments (1):\n1. Patrick McFadin: This was fixed a while back", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "11e7476000078d6c7c19132236c7e383", "issue_key": "SPARK-918", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "hadoop-client dependency should be explained for Scala in addition to Java in quickstart", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "2013-09-24T20:50:41.000+0000", "updated": "2014-04-21T17:56:16.000+0000", "resolved": "2014-04-21T17:56:16.000+0000", "labels": ["starter"], "components": ["Documentation"], "comments": [{"author": "Patrick Wendell", "body": "This was fixed as a result of a separate re-factoring of the docs.", "created": "2014-04-21T17:56:16.132+0000"}], "num_comments": 1, "text": "Issue: SPARK-918\nSummary: hadoop-client dependency should be explained for Scala in addition to Java in quickstart\n\nComments (1):\n1. Patrick Wendell: This was fixed as a result of a separate re-factoring of the docs.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "fbe3c5a3b48b95a81ff7bee89a2ab6ea", "issue_key": "SPARK-919", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark-ec2 launch --resume doesn't re-initialize all modules", "description": "I launched a Spark cluster using the new EC2 scripts, stopped it with {{stop}}, then restarted it with {{start}} and ran {{launch --resume}} to re-deploy the Spark configurations. It looks like the script exits after initializing Spark if it's already installed:  Initializing spark ~ ~/spark-ec2 Spark seems to be installed. Exiting. Connection to ec2-*-.amazonaws.com closed. Spark standalone cluster started at http://ec2-*.compute-1.amazonaws.com:8080 Ganglia started at http://ec2-*-1.amazonaws.com:5080/ganglia Done!  It looks like the problem is that the module init scripts are run by {{sourcing}} them into the top-level {{setup.sh}} script rather than running them in subshells, so the module script's own exit command kills the top-level setup script:  # Install / Init module for module in $MODULES; do echo \"Initializing $module\" if [[ -e $module/init.sh ]]; then source $module/init.sh fi done", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-09-26T13:51:54.000+0000", "updated": "2013-10-04T21:28:27.000+0000", "resolved": "2013-10-04T21:28:27.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Josh Rosen", "body": "Pull request here: https://github.com/mesos/spark-ec2/pull/22", "created": "2013-09-26T14:13:06.624+0000"}], "num_comments": 1, "text": "Issue: SPARK-919\nSummary: spark-ec2 launch --resume doesn't re-initialize all modules\nDescription: I launched a Spark cluster using the new EC2 scripts, stopped it with {{stop}}, then restarted it with {{start}} and ran {{launch --resume}} to re-deploy the Spark configurations. It looks like the script exits after initializing Spark if it's already installed:  Initializing spark ~ ~/spark-ec2 Spark seems to be installed. Exiting. Connection to ec2-*-.amazonaws.com closed. Spark standalone cluster started at http://ec2-*.compute-1.amazonaws.com:8080 Ganglia started at http://ec2-*-1.amazonaws.com:5080/ganglia Done!  It looks like the problem is that the module init scripts are run by {{sourcing}} them into the top-level {{setup.sh}} script rather than running them in subshells, so the module script's own exit command kills the top-level setup script:  # Install / Init module for module in $MODULES; do echo \"Initializing $module\" if [[ -e $module/init.sh ]]; then source $module/init.sh fi done\n\nComments (1):\n1. Josh Rosen: Pull request here: https://github.com/mesos/spark-ec2/pull/22", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "7c465f370f08a6eb42f816e11bbae4b1", "issue_key": "SPARK-1226", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Import breeze to Spark mllib", "description": "Breeze' math module includes many common linear algebra and optimization routines. It is going to be very handy for developers if it is imported into mllib. I am implementing ADMM (which is mentioned by the speaker in the Spark meetup) in mllib, but there are few utility functions in current mllib and it is error-prone to code it from scratch. Many large scale machine learning algorithms need to solve several smaller scale algorithms and combine the results (paralleled SGD) or to convert a disk resident problem into a memory resident problem (linear regression). Breeze should be very useful in these cases.", "reporter": "Kun Yang", "assignee": "Xiangrui Meng", "created": "2013-09-27T11:38:42.000+0000", "updated": "2014-04-01T05:33:08.000+0000", "resolved": "2014-04-01T05:33:08.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Kun Yang", "body": "if we are planning to expand the scope of ML algorithms, I think it is necessary. Any feedback?", "created": "2013-09-27T11:46:58.323+0000"}, {"author": "Xiangrui Meng", "body": "We use breeze to support underlying linear algebra operations in version 1.0.0.", "created": "2014-04-01T05:33:08.266+0000"}], "num_comments": 2, "text": "Issue: SPARK-1226\nSummary: Import breeze to Spark mllib\nDescription: Breeze' math module includes many common linear algebra and optimization routines. It is going to be very handy for developers if it is imported into mllib. I am implementing ADMM (which is mentioned by the speaker in the Spark meetup) in mllib, but there are few utility functions in current mllib and it is error-prone to code it from scratch. Many large scale machine learning algorithms need to solve several smaller scale algorithms and combine the results (paralleled SGD) or to convert a disk resident problem into a memory resident problem (linear regression). Breeze should be very useful in these cases.\n\nComments (2):\n1. Kun Yang: if we are planning to expand the scope of ML algorithms, I think it is necessary. Any feedback?\n2. Xiangrui Meng: We use breeze to support underlying linear algebra operations in version 1.0.0.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.080532"}}
{"id": "9a2888f8ea16a0e24bfc6a3f11e112dc", "issue_key": "SPARK-920", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "JSON endpoint URI scheme part (spark://) duplicated", "description": "Within JsonProtocol the MasterStateResponse URI field is prepended with \"spark://\" but the URI is always already prepended with this string within DeployMessages. Example output: spark://spark://host:port", "reporter": "David McCauley", "assignee": "David McCauley", "created": "2013-10-02T05:00:22.000+0000", "updated": "2013-10-09T02:25:39.000+0000", "resolved": "2013-10-09T02:25:39.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "David McCauley", "body": "Merged, PR: https://github.com/apache/incubator-spark/pull/27", "created": "2013-10-09T02:25:39.221+0000"}], "num_comments": 1, "text": "Issue: SPARK-920\nSummary: JSON endpoint URI scheme part (spark://) duplicated\nDescription: Within JsonProtocol the MasterStateResponse URI field is prepended with \"spark://\" but the URI is always already prepended with this string within DeployMessages. Example output: spark://spark://host:port\n\nComments (1):\n1. David McCauley: Merged, PR: https://github.com/apache/incubator-spark/pull/27", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.082535"}}
{"id": "64bd49da56c8ab8ec983bcf222134e29", "issue_key": "SPARK-921", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add Application UI URL to ApplicationInfo Json output", "description": "Adding the URL of the Main Application UI will allow custom interfaces (that use the JSON output) to redirect from the standalone UI. The idea is that eventually the Main Application UI will also have JSON output allowing for complete custom UIs (or many other types of app) to plug in and gather information (SPARK-907).", "reporter": "David McCauley", "assignee": "David McCauley", "created": "2013-10-02T06:41:20.000+0000", "updated": "2013-10-09T02:24:57.000+0000", "resolved": "2013-10-09T02:24:57.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "David McCauley", "body": "Merged, PR: https://github.com/apache/incubator-spark/pull/27", "created": "2013-10-09T02:24:57.624+0000"}], "num_comments": 1, "text": "Issue: SPARK-921\nSummary: Add Application UI URL to ApplicationInfo Json output\nDescription: Adding the URL of the Main Application UI will allow custom interfaces (that use the JSON output) to redirect from the standalone UI. The idea is that eventually the Main Application UI will also have JSON output allowing for complete custom UIs (or many other types of app) to plug in and gather information (SPARK-907).\n\nComments (1):\n1. David McCauley: Merged, PR: https://github.com/apache/incubator-spark/pull/27", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.082535"}}
{"id": "aea5a70586f651207eadf3ab05506278", "issue_key": "SPARK-922", "issue_type": "Task", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "Update Spark AMI to Python 2.7", "description": "Many Python libraries only support Python 2.7+, so we should make Python 2.7 the default Python on the Spark AMIs.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-10-04T14:45:55.000+0000", "updated": "2016-04-22T16:42:48.000+0000", "resolved": "2016-04-22T16:42:48.000+0000", "labels": [], "components": ["EC2", "PySpark"], "comments": [{"author": "Josh Rosen", "body": "As a short-term workaround, here's a quick technique to update an existing cluster to use Python 2.7:  yum install -y pssh yum install -y python27 python27-devel pssh -h /root/spark-ec2/slaves yum install -y python27 wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27 easy_install-2.7 pip pip-2.7 install ipython[all]  Then, edit {{spark-env.sh}} to {{export PYSPARK_PYTHON=python2.7}}. If you're using IPython notebook, make sure sure to {{source spark-env.sh}} before launching the notebook server so that this environment variable is picked up (the {{pyspark}} script normally handles this).", "created": "2013-10-04T17:00:01.892+0000"}, {"author": "Nicholas Chammas", "body": "This little script is so cool! I didn't know there was such a thing as {{pssh}}. By the way, I think there is a typo at the end of the code block. The command that worked for me was:  pip2.7 install ipython[all]  That is, minus the dash. Also, you could script the rest of the required upgrade steps as follows:  printf \"\\n# Set Spark Python version\\nexport PYSPARK_PYTHON=python2.7\\n\" >> /root/spark/conf/spark-env.sh source /root/spark/conf/spark-env.sh", "created": "2014-02-28T15:13:40.340+0000"}, {"author": "Patrick Wendell", "body": "This is no longer a blocker now that we've downgraded the python dependency, but would still be nice to have.", "created": "2014-04-30T06:27:48.662+0000"}, {"author": "Josh Rosen", "body": "Updated script, which also updates numpy:  yum install -y pssh yum install -y python27 python27-devel pssh -h /root/spark-ec2/slaves yum install -y python27 python27-devel wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27 pssh -h /root/spark-ec2/slaves \"wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\" easy_install-2.7 pip pssh -h /root/spark-ec2/slaves easy_install-2.7 pip pip2.7 install numpy pssh -h /root/spark-ec2/slaves pip2.7 -t0 install numpy  And to check that numpy is successfully installed:  pssh -h /root/spark-ec2/slaves --inline-stdout 'python2.7 -c \"import numpy; print numpy\"'", "created": "2014-08-15T17:53:18.797+0000"}, {"author": "Nicholas Chammas", "body": "Josh, at the end of your updated script do we still also need the step to edit {{spark-env.sh}}?", "created": "2014-08-15T20:16:50.776+0000"}, {"author": "Josh Rosen", "body": "Yeah, you still need to set PYSPARK_PYTHON since this doesn't overwrite the system Python. I was updating this to brain-dump the script I'm using for a Python 2.6 vs Python 2.7 benchmark.", "created": "2014-08-15T20:36:35.618+0000"}, {"author": "Nicholas Chammas", "body": "FYI, I believe the line to install numpy on the slaves should read:  pssh -t0 -h /root/spark-ec2/slaves pip2.7 install numpy  i.e. Change the position of the {{-t0}}.", "created": "2014-08-29T18:41:06.819+0000"}, {"author": "Nicholas Chammas", "body": "[~joshrosen] By the way, as part of this work to update the AMIs, can we also have them include the latest security patches and updates? It's a good practice, and we also suspect that it would [improve our EC2 startup time|https://github.com/apache/spark/pull/2339#issuecomment-55483793].", "created": "2014-09-16T01:14:44.511+0000"}, {"author": "Josh Rosen", "body": "[~nchammas] In the long run, it might be nice to automate the AMI creation / upgrade process so that changes like this can be done in build configuration files. We might have some internal tooling for this; I'll ask around and find out.", "created": "2014-09-16T18:10:45.849+0000"}, {"author": "Andrew Davidson", "body": "here is how I am launching iPython notebook. I am running as the ec2-user IPYTHON_OPTS=\"notebook --pylab inline --no-browser --port=7000\" $SPARK_HOME/bin/pyspark Bellow are all the upgrade commands I ran I ran into a small problem the ipython magic %matplotlib inline creates an error, you can work around this by commenting it out. Andy yum install -y pssh yum install -y python27 python27-devel pssh -h /root/spark-ec2/slaves yum install -y python27 python27-devel wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27 pssh -h /root/spark-ec2/slaves \"wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\" easy_install-2.7 pip pssh -h /root/spark-ec2/slaves easy_install-2.7 pip pip2.7 install numpy pssh -t0 -h /root/spark-ec2/slaves pip2.7 install numpy pip2.7 install ipython[all] printf \"\\n# Set Spark Python version\\nexport PYSPARK_PYTHON=/usr/bin/python2.7\\n\" >> /root/spark/conf/spark-env.sh source /root/spark/conf/spark-env.sh", "created": "2014-09-27T18:38:10.508+0000"}, {"author": "Josh Rosen", "body": "I chatted with someone who had a job that ran ~20x slower on Python 2.6 than on 2.7, likely due to changes to the json library (in 2.7+, json is implemented as a C extension rather than in pure-Python). Maybe we should add a note on this to the docs.", "created": "2014-10-04T21:05:11.533+0000"}, {"author": "Davies Liu", "body": "We did not use json heavily in pyspark, also user have several choice of json library in Python, this should not be a issue, i think. We definitely need to upgrade to Python2.7 (as default), if some user need python2.6, it's easy to use it by PYSPARK_PYTHON.", "created": "2014-10-11T05:56:03.290+0000"}, {"author": "Nicholas Chammas", "body": "[~joshrosen] Are you open to having this resolved as part of [SPARK-3821]?", "created": "2014-10-12T01:39:31.249+0000"}, {"author": "Andrew Davidson", "body": "Wow upgrading matplotlib was a bear. The following worked for me. The trick was getting the correct version of the source code. The recipe bellow is not 100% correct. I have not figured out how to use pssh with yum. yum prompts you y/n before downloading pip2.7 install six pssh -t0 -h /root/spark-ec2/slaves pip2.7 install six pip2.7 install python-dateutil pssh -t0 -h /root/spark-ec2/slaves pip2.7 install python-dateutil pip2.7 install pyparsing pssh -t0 -h /root/spark-ec2/slaves pip2.7 install pyparsing yum install yum-utils wget https://github.com/matplotlib/matplotlib/archive/master.tar.gz tar -zxvf master.tar.gz cd matplotlib-master/ yum install freetype-devel yum install libpng-devel python2.7 setup.py build python2.7 setup.py install", "created": "2014-10-12T21:06:35.046+0000"}, {"author": "Andrew Davidson", "body": "also forgot the mention there are a couple of steps on http://nbviewer.ipython.org/gist/JoshRosen/6856670 that are important in the upgrade process # # restart spark # /root/spark/sbin/stop-all.sh /root/spark/sbin/start-all.sh", "created": "2014-10-12T21:08:44.906+0000"}, {"author": "Josh Rosen", "body": "[~nchammas]: It would be great to include Python 2.7 in the next AMI; I think our current AMI shell script has it, though. [~aedwip]:  I have not figured out how to use pssh with yum. yum prompts you y/n before downloading  Try {{yum install -y}}.", "created": "2014-10-13T04:15:32.086+0000"}, {"author": "Nicholas Chammas", "body": "[~joshrosen] - Do you mean [this script|https://github.com/mesos/spark-ec2/blob/v4/create_image.sh]? It doesn't seem to have anything related to Python 2.7. Anyway, what I meant was if you were open to holding off on updating the Spark AMIs until we had also figured out how to automate that process per [SPARK-3821]. I should have something for that as soon as this week or next.", "created": "2014-10-13T14:18:56.396+0000"}, {"author": "Josh Rosen", "body": "[~nchammas] - I don't think that there's an urgent rush to update the AMIs before the next round of releases, so I'm fine with waiting to incorporate this into SPARK-3821.", "created": "2014-10-13T15:13:40.323+0000"}, {"author": "Reynold Xin", "body": "cc [~shivaram]", "created": "2016-01-09T06:26:08.816+0000"}, {"author": "Reynold Xin", "body": "This is now outside the scope of Spark.", "created": "2016-04-22T16:42:48.699+0000"}], "num_comments": 20, "text": "Issue: SPARK-922\nSummary: Update Spark AMI to Python 2.7\nDescription: Many Python libraries only support Python 2.7+, so we should make Python 2.7 the default Python on the Spark AMIs.\n\nComments (20):\n1. Josh Rosen: As a short-term workaround, here's a quick technique to update an existing cluster to use Python 2.7:  yum install -y pssh yum install -y python27 python27-devel pssh -h /root/spark-ec2/slaves yum install -y python27 wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27 easy_install-2.7 pip pip-2.7 install ipython[all]  Then, edit {{spark-env.sh}} to {{export PYSPARK_PYTHON=python2.7}}. If you're using IPython notebook, make sure sure to {{source spark-env.sh}} before launching the notebook server so that this environment variable is picked up (the {{pyspark}} script normally handles this).\n2. Nicholas Chammas: This little script is so cool! I didn't know there was such a thing as {{pssh}}. By the way, I think there is a typo at the end of the code block. The command that worked for me was:  pip2.7 install ipython[all]  That is, minus the dash. Also, you could script the rest of the required upgrade steps as follows:  printf \"\\n# Set Spark Python version\\nexport PYSPARK_PYTHON=python2.7\\n\" >> /root/spark/conf/spark-env.sh source /root/spark/conf/spark-env.sh\n3. Patrick Wendell: This is no longer a blocker now that we've downgraded the python dependency, but would still be nice to have.\n4. Josh Rosen: Updated script, which also updates numpy:  yum install -y pssh yum install -y python27 python27-devel pssh -h /root/spark-ec2/slaves yum install -y python27 python27-devel wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27 pssh -h /root/spark-ec2/slaves \"wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\" easy_install-2.7 pip pssh -h /root/spark-ec2/slaves easy_install-2.7 pip pip2.7 install numpy pssh -h /root/spark-ec2/slaves pip2.7 -t0 install numpy  And to check that numpy is successfully installed:  pssh -h /root/spark-ec2/slaves --inline-stdout 'python2.7 -c \"import numpy; print numpy\"'\n5. Nicholas Chammas: Josh, at the end of your updated script do we still also need the step to edit {{spark-env.sh}}?\n6. Josh Rosen: Yeah, you still need to set PYSPARK_PYTHON since this doesn't overwrite the system Python. I was updating this to brain-dump the script I'm using for a Python 2.6 vs Python 2.7 benchmark.\n7. Nicholas Chammas: FYI, I believe the line to install numpy on the slaves should read:  pssh -t0 -h /root/spark-ec2/slaves pip2.7 install numpy  i.e. Change the position of the {{-t0}}.\n8. Nicholas Chammas: [~joshrosen] By the way, as part of this work to update the AMIs, can we also have them include the latest security patches and updates? It's a good practice, and we also suspect that it would [improve our EC2 startup time|https://github.com/apache/spark/pull/2339#issuecomment-55483793].\n9. Josh Rosen: [~nchammas] In the long run, it might be nice to automate the AMI creation / upgrade process so that changes like this can be done in build configuration files. We might have some internal tooling for this; I'll ask around and find out.\n10. Andrew Davidson: here is how I am launching iPython notebook. I am running as the ec2-user IPYTHON_OPTS=\"notebook --pylab inline --no-browser --port=7000\" $SPARK_HOME/bin/pyspark Bellow are all the upgrade commands I ran I ran into a small problem the ipython magic %matplotlib inline creates an error, you can work around this by commenting it out. Andy yum install -y pssh yum install -y python27 python27-devel pssh -h /root/spark-ec2/slaves yum install -y python27 python27-devel wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27 pssh -h /root/spark-ec2/slaves \"wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python27\" easy_install-2.7 pip pssh -h /root/spark-ec2/slaves easy_install-2.7 pip pip2.7 install numpy pssh -t0 -h /root/spark-ec2/slaves pip2.7 install numpy pip2.7 install ipython[all] printf \"\\n# Set Spark Python version\\nexport PYSPARK_PYTHON=/usr/bin/python2.7\\n\" >> /root/spark/conf/spark-env.sh source /root/spark/conf/spark-env.sh", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.082535"}}
{"id": "1d3e51241c5df09e783a0c1764a263a7", "issue_key": "SPARK-923", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Bytes columns in web UI tables don't sort properly", "description": "In the Web UI, columns displaying sizes in bytes do not sort correctly; they are sorted by the string value instead of the number of bytes, megabytes, etc. It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by. I observed this issue on the \"Shuffle Write\" column in a detailed stage view in the application Web UI, but it might also affect other column like \"Shuffle Read\". I've attached a screenshot illustrating the problem. SPARK-801 was a similar issue, except with Time columns.", "reporter": "Josh Rosen", "assignee": "xiajunluan", "created": "2013-10-04T23:22:05.000+0000", "updated": "2013-11-30T15:59:44.000+0000", "resolved": "2013-11-30T15:59:44.000+0000", "labels": ["Starter"], "components": ["Web UI"], "comments": [{"author": "Josh Rosen", "body": "Fixed by https://github.com/apache/incubator-spark/pull/160", "created": "2013-11-30T15:59:44.335+0000"}], "num_comments": 1, "text": "Issue: SPARK-923\nSummary: Bytes columns in web UI tables don't sort properly\nDescription: In the Web UI, columns displaying sizes in bytes do not sort correctly; they are sorted by the string value instead of the number of bytes, megabytes, etc. It's possible to add a sort key in the HTML that the table sorter plugin will recognize and sort by. I observed this issue on the \"Shuffle Write\" column in a detailed stage view in the application Web UI, but it might also affect other column like \"Shuffle Read\". I've attached a screenshot illustrating the problem. SPARK-801 was a similar issue, except with Time columns.\n\nComments (1):\n1. Josh Rosen: Fixed by https://github.com/apache/incubator-spark/pull/160", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.082535"}}
{"id": "3465de6aa86c0ae80b6c22019ab52e44", "issue_key": "SPARK-924", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Nested RDD", "description": "Is it possible to work with nested RDD (ie RDD[RDD])? This code is failed (scala) val q1 = sc.makeRDD(Array(1,2,3,4,5)) val q2 = sc.makeRDD(Array(1,2,3,4,5)) val qq = sc.makeRDD(Array(q1, q2)) val ar = qq.collect val z = ar(0) z.collect with java.lang.NullPointerException at org.apache.spark.rdd.RDD.collect(RDD.scala:560) at <init>(<console>:23) at <init>(<console>:28) at <init>(<console>:30) at <init>(<console>:32) at <init>(<console>:34) at .<init>(<console>:38) at .<clinit>(<console>) at .<init>(<console>:11) at .<clinit>(<console>) at $export(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:629) at org.apache.spark.repl.SparkIMain$Request$$anonfun$10.apply(SparkIMain.scala:890) at scala.tools.nsc.interpreter.Line$$anonfun$1.apply$mcV$sp(Line.scala:43) at scala.tools.nsc.io.package$$anon$2.run(package.scala:25) at java.lang.Thread.run(Thread.java:724)", "reporter": "Pavel Ajtkulov", "assignee": null, "created": "2013-10-06T23:24:21.000+0000", "updated": "2013-10-07T14:59:39.000+0000", "resolved": "2013-10-07T14:59:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Spark doesn't support nested RDDs; see https://groups.google.com/d/msg/spark-users/_Efj40upvx4/DbHCixW7W7kJ Maybe you can express your program using RDDs of collections (such as {{RDD[Array[[T]]])}}, or with RDD.union()/SparkContext.union().", "created": "2013-10-07T14:59:39.302+0000"}], "num_comments": 1, "text": "Issue: SPARK-924\nSummary: Nested RDD\nDescription: Is it possible to work with nested RDD (ie RDD[RDD])? This code is failed (scala) val q1 = sc.makeRDD(Array(1,2,3,4,5)) val q2 = sc.makeRDD(Array(1,2,3,4,5)) val qq = sc.makeRDD(Array(q1, q2)) val ar = qq.collect val z = ar(0) z.collect with java.lang.NullPointerException at org.apache.spark.rdd.RDD.collect(RDD.scala:560) at <init>(<console>:23) at <init>(<console>:28) at <init>(<console>:30) at <init>(<console>:32) at <init>(<console>:34) at .<init>(<console>:38) at .<clinit>(<console>) at .<init>(<console>:11) at .<clinit>(<console>) at $export(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:629) at org.apache.spark.repl.SparkIMain$Request$$anonfun$10.apply(SparkIMain.scala:890) at scala.tools.nsc.interpreter.Line$$anonfun$1.apply$mcV$sp(Line.scala:43) at scala.tools.nsc.io.package$$anon$2.run(package.scala:25) at java.lang.Thread.run(Thread.java:724)\n\nComments (1):\n1. Josh Rosen: Spark doesn't support nested RDDs; see https://groups.google.com/d/msg/spark-users/_Efj40upvx4/DbHCixW7W7kJ Maybe you can express your program using RDDs of collections (such as {{RDD[Array[[T]]])}}, or with RDD.union()/SparkContext.union().", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.082535"}}
{"id": "eda02d73ed30812da5eaedab50b222c7", "issue_key": "SPARK-925", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Allow ec2 scripts to load default options from a json file", "description": "The option list for ec2 script can be a little irritating to type in, especially things like path to identity-file, region , zone, ami etc. It would be nice if ec2 script looks for an options.json file in the following order: (1) CWD, (2) ~/spark-ec2, (3) same dir as spark_ec2.py Something like: def get_defaults_from_options(): # Check to see if a options.json file exists, if so load it. # However, values in the options.json file can only overide values in opts # if the Opt values are None or \"\" # i.e. commandline options take presidence defaults = {'aws-access-key-id':'','aws-secret-access-key':'','key-pair':'', 'identity-file':'', 'region':'ap-southeast-1', 'zone':'', 'ami':'','slaves':1, 'instance-type':'m1.large'} # Look for options.json in directory cluster was called from # Had to modify the spark_ec2 wrapper script since it mangles the pwd startwd = os.environ['STARTWD'] if os.path.exists(os.path.join(startwd,\"options.json\")): optionspath = os.path.join(startwd,\"options.json\") else: optionspath = os.path.join(os.getcwd(),\"options.json\") try: print \"Loading options file: \", optionspath with open (optionspath) as json_data: jdata = json.load(json_data) for k in jdata: defaults[k]=jdata[k] except IOError: print 'Warning: options.json file not loaded' # Check permissions on identity-file, if defined, otherwise launch will fail late and will be irritating if defaults['identity-file']!='': st = os.stat(defaults['identity-file']) user_can_read = bool(st.st_mode & stat.S_IRUSR) grp_perms = bool(st.st_mode & stat.S_IRWXG) others_perm = bool(st.st_mode & stat.S_IRWXO) if (not user_can_read): print \"No read permission to read \", defaults['identify-file'] sys.exit(1) if (grp_perms or others_perm): print \"Permissions are too open, please chmod 600 file \", defaults['identify-file'] sys.exit(1) # if defaults contain AWS access id or private key, set it to environment. # required for use with boto to access the AWS console if defaults['aws-access-key-id'] != '': os.environ['AWS_ACCESS_KEY_ID']=defaults['aws-access-key-id'] if defaults['aws-secret-access-key'] != '': os.environ['AWS_SECRET_ACCESS_KEY'] = defaults['aws-secret-access-key'] return defaults", "reporter": "Shay Seng", "assignee": null, "created": "2013-10-08T10:58:31.000+0000", "updated": "2016-01-16T13:00:31.000+0000", "resolved": "2016-01-16T13:00:31.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Nicholas Chammas", "body": "Loading config from a file seems like a good thing to have and matches what comparable tools like Ubuntu Juju and MIT StarCluster do. However, I would favor a format other than JSON since JSON doesn't allow you to comment stuff out. I've used tools with JSON-backed config files, and they are super annoying to deal with if you try to alternate invocations between version A with this line uncommented and version B with the same line commented out. YAML seems like a better choice for this task. What do you think [~shayping]/[~shay]? cc [~shivaram]", "created": "2015-02-15T04:59:14.395+0000"}, {"author": "Shay Seng", "body": "JSON config files are indeed irritating to work with. Unfortunately the spark conf files are in JSON. AFAIK. I would rather stick to one form rather than have to bother with more than one format. With JSON we deal with internally, we have started to nest definitions so that it is easy for some one to modify one small setting without having to specify all the other settings -- and as a work around to comments. e.g. { _default: { settingA : settingB : settingC : }, mainOption: { default : _default, settingB : xyz }, myModifiedOption: { default: mainOption, settingC: abc }", "created": "2015-02-17T19:03:44.719+0000"}, {"author": "Nicholas Chammas", "body": "Formatting side comment: You can surround your code block with {} tags and it'll look nice. e.g.   { \"ayyo\": \"wassup\" }", "created": "2015-02-17T19:15:08.532+0000"}, {"author": "Nicholas Chammas", "body": "I would prefer a format that is more human friendly and that supports comments directly. To me, JSON is better for data exchange, and YAML is better for config files and other things that humans are going to be dealing with directly. It's true that there are other config formats used in Spark. The ones under [conf/|https://github.com/apache/spark/tree/master/conf], however, are not JSON. Which ones were you thinking of? As long as the config format is consistent within a sub-project, I think it's OK. Since spark-ec2 doesn't have any config files yet, I don't think it's bad to go with YAML.  With JSON we deal with internally, we have started to nest definitions so that it is easy for some one to modify one small setting without having to specify all the other settings – and as a work around to comments.  As discussed before, YAML supports comments directly, which IMO is essential for a config format. With regards to modifying a setting without specifying everything, I'm not sure I understand the use case. If we define some config file resolution order (first check /first/config, then check /second/config, etc.), is it that bad if people just copied the default config from /second/config to /first/config and modified what they wanted? I believe that's how it generally works in tools that check multiple places for configuration. A better way to do this would probably be to allow people to specify a sub-set of options in any given file, and option sets just get merged on top of the options specified in the preceding file. That seems more complexity than is worth it at this time, though.", "created": "2015-02-18T02:13:59.152+0000"}, {"author": "Nicholas Chammas", "body": "Here's an example of what a spark-ec2 {{config.yml}} file could look like:  region: us-east-1 aws_auth: key_pair: mykey identity_file: /path/to/file.pem # spark_version: 1.2.1 slaves: 5 instance_type: m3.large use_existing_master: no  It's dead simple and there's not much to learn, really.", "created": "2015-02-18T02:27:51.288+0000"}, {"author": "Apache Spark", "body": "User 'marmbrus' has created a pull request for this issue: https://github.com/apache/spark/pull/7906", "created": "2015-08-03T21:54:09.526+0000"}], "num_comments": 6, "text": "Issue: SPARK-925\nSummary: Allow ec2 scripts to load default options from a json file\nDescription: The option list for ec2 script can be a little irritating to type in, especially things like path to identity-file, region , zone, ami etc. It would be nice if ec2 script looks for an options.json file in the following order: (1) CWD, (2) ~/spark-ec2, (3) same dir as spark_ec2.py Something like: def get_defaults_from_options(): # Check to see if a options.json file exists, if so load it. # However, values in the options.json file can only overide values in opts # if the Opt values are None or \"\" # i.e. commandline options take presidence defaults = {'aws-access-key-id':'','aws-secret-access-key':'','key-pair':'', 'identity-file':'', 'region':'ap-southeast-1', 'zone':'', 'ami':'','slaves':1, 'instance-type':'m1.large'} # Look for options.json in directory cluster was called from # Had to modify the spark_ec2 wrapper script since it mangles the pwd startwd = os.environ['STARTWD'] if os.path.exists(os.path.join(startwd,\"options.json\")): optionspath = os.path.join(startwd,\"options.json\") else: optionspath = os.path.join(os.getcwd(),\"options.json\") try: print \"Loading options file: \", optionspath with open (optionspath) as json_data: jdata = json.load(json_data) for k in jdata: defaults[k]=jdata[k] except IOError: print 'Warning: options.json file not loaded' # Check permissions on identity-file, if defined, otherwise launch will fail late and will be irritating if defaults['identity-file']!='': st = os.stat(defaults['identity-file']) user_can_read = bool(st.st_mode & stat.S_IRUSR) grp_perms = bool(st.st_mode & stat.S_IRWXG) others_perm = bool(st.st_mode & stat.S_IRWXO) if (not user_can_read): print \"No read permission to read \", defaults['identify-file'] sys.exit(1) if (grp_perms or others_perm): print \"Permissions are too open, please chmod 600 file \", defaults['identify-file'] sys.exit(1) # if defaults contain AWS access id or private key, set it to environment. # required for use with boto to access the AWS console if defaults['aws-access-key-id'] != '': os.environ['AWS_ACCESS_KEY_ID']=defaults['aws-access-key-id'] if defaults['aws-secret-access-key'] != '': os.environ['AWS_SECRET_ACCESS_KEY'] = defaults['aws-secret-access-key'] return defaults\n\nComments (6):\n1. Nicholas Chammas: Loading config from a file seems like a good thing to have and matches what comparable tools like Ubuntu Juju and MIT StarCluster do. However, I would favor a format other than JSON since JSON doesn't allow you to comment stuff out. I've used tools with JSON-backed config files, and they are super annoying to deal with if you try to alternate invocations between version A with this line uncommented and version B with the same line commented out. YAML seems like a better choice for this task. What do you think [~shayping]/[~shay]? cc [~shivaram]\n2. Shay Seng: JSON config files are indeed irritating to work with. Unfortunately the spark conf files are in JSON. AFAIK. I would rather stick to one form rather than have to bother with more than one format. With JSON we deal with internally, we have started to nest definitions so that it is easy for some one to modify one small setting without having to specify all the other settings -- and as a work around to comments. e.g. { _default: { settingA : settingB : settingC : }, mainOption: { default : _default, settingB : xyz }, myModifiedOption: { default: mainOption, settingC: abc }\n3. Nicholas Chammas: Formatting side comment: You can surround your code block with {} tags and it'll look nice. e.g.   { \"ayyo\": \"wassup\" }\n4. Nicholas Chammas: I would prefer a format that is more human friendly and that supports comments directly. To me, JSON is better for data exchange, and YAML is better for config files and other things that humans are going to be dealing with directly. It's true that there are other config formats used in Spark. The ones under [conf/|https://github.com/apache/spark/tree/master/conf], however, are not JSON. Which ones were you thinking of? As long as the config format is consistent within a sub-project, I think it's OK. Since spark-ec2 doesn't have any config files yet, I don't think it's bad to go with YAML.  With JSON we deal with internally, we have started to nest definitions so that it is easy for some one to modify one small setting without having to specify all the other settings – and as a work around to comments.  As discussed before, YAML supports comments directly, which IMO is essential for a config format. With regards to modifying a setting without specifying everything, I'm not sure I understand the use case. If we define some config file resolution order (first check /first/config, then check /second/config, etc.), is it that bad if people just copied the default config from /second/config to /first/config and modified what they wanted? I believe that's how it generally works in tools that check multiple places for configuration. A better way to do this would probably be to allow people to specify a sub-set of options in any given file, and option sets just get merged on top of the options specified in the preceding file. That seems more complexity than is worth it at this time, though.\n5. Nicholas Chammas: Here's an example of what a spark-ec2 {{config.yml}} file could look like:  region: us-east-1 aws_auth: key_pair: mykey identity_file: /path/to/file.pem # spark_version: 1.2.1 slaves: 5 instance_type: m3.large use_existing_master: no  It's dead simple and there's not much to learn, really.\n6. Apache Spark: User 'marmbrus' has created a pull request for this issue: https://github.com/apache/spark/pull/7906", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.082535"}}
{"id": "4bab014987c1e6ff937d1391064a52d4", "issue_key": "SPARK-926", "issue_type": "New Feature", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "spark_ec2 script when ssh/scp-ing should pipe UserknowHostFile to /dev/null", "description": "The know host file in the local machine gets all kinds of crap after a few cluster launches. When SSHing, or SCPing, please add \"-o UserKnowHostFile=/dev/null\" Also remove the -t option from SSH, and only add in when necessary - to reduce chatter on console. e.g. # Copy a file to a given host through scp, throwing an exception if scp fails def scp(host, opts, local_file, dest_file): subprocess.check_call( \"scp -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i %s '%s' '%s@%s:%s'\" % (opts.identity_file, local_file, opts.user, host, dest_file), shell=True) # Run a command on a host through ssh, retrying up to two times # and then throwing an exception if ssh continues to fail. def ssh(host, opts, command, sshopts=\"\"): tries = 0 while True: try: # removed -t option from ssh command, not sure why it is required all the time. return subprocess.check_call( \"ssh %s -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i %s %s@%s '%s'\" % (sshopts, opts.identity_file, opts.user, host, command), shell=True) except subprocess.CalledProcessError as e: if (tries > 2): raise e print \"Couldn't connect to host {0}, waiting 30 seconds\".format(e) time.sleep(30) tries = tries + 1", "reporter": "Shay Seng", "assignee": null, "created": "2013-10-08T11:03:09.000+0000", "updated": "2015-02-06T07:34:18.000+0000", "resolved": "2015-02-06T07:34:18.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Nicholas Chammas", "body": "[~shay] / [~shayping] (pinging both names registered): Is this still an issue as of 1.2.0? I just stumbled across this issue and am curious.", "created": "2015-01-07T06:41:53.338+0000"}, {"author": "Josh Rosen", "body": "I think it is; there's now a PR to fix this, since it's also the cause of SPARK-5403: https://github.com/apache/spark/pull/4196", "created": "2015-01-26T22:41:55.765+0000"}, {"author": "Sean R. Owen", "body": "Going to make this one the duplicate since SPARK-5403 has an active PR.", "created": "2015-02-06T07:34:18.498+0000"}], "num_comments": 3, "text": "Issue: SPARK-926\nSummary: spark_ec2 script when ssh/scp-ing should pipe UserknowHostFile to /dev/null\nDescription: The know host file in the local machine gets all kinds of crap after a few cluster launches. When SSHing, or SCPing, please add \"-o UserKnowHostFile=/dev/null\" Also remove the -t option from SSH, and only add in when necessary - to reduce chatter on console. e.g. # Copy a file to a given host through scp, throwing an exception if scp fails def scp(host, opts, local_file, dest_file): subprocess.check_call( \"scp -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i %s '%s' '%s@%s:%s'\" % (opts.identity_file, local_file, opts.user, host, dest_file), shell=True) # Run a command on a host through ssh, retrying up to two times # and then throwing an exception if ssh continues to fail. def ssh(host, opts, command, sshopts=\"\"): tries = 0 while True: try: # removed -t option from ssh command, not sure why it is required all the time. return subprocess.check_call( \"ssh %s -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i %s %s@%s '%s'\" % (sshopts, opts.identity_file, opts.user, host, command), shell=True) except subprocess.CalledProcessError as e: if (tries > 2): raise e print \"Couldn't connect to host {0}, waiting 30 seconds\".format(e) time.sleep(30) tries = tries + 1\n\nComments (3):\n1. Nicholas Chammas: [~shay] / [~shayping] (pinging both names registered): Is this still an issue as of 1.2.0? I just stumbled across this issue and am curious.\n2. Josh Rosen: I think it is; there's now a PR to fix this, since it's also the cause of SPARK-5403: https://github.com/apache/spark/pull/4196\n3. Sean R. Owen: Going to make this one the duplicate since SPARK-5403 has an active PR.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "c97c9a167baa6cb1287dd02b01abab28", "issue_key": "SPARK-927", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "PySpark sample() doesn't work if numpy is installed on master but not on workers", "description": "PySpark's sample() method crashes with ImportErrors on the workers if numpy is installed on the driver machine but not on the workers. I'm not sure what's the best way to fix this. A general mechanism for automatically shipping libraries from the master to the workers would address this, but that could be complicated to implement.", "reporter": "Josh Rosen", "assignee": "Matthew Farrellee", "created": "2013-10-08T11:03:56.000+0000", "updated": "2015-01-05T23:05:46.000+0000", "resolved": "2015-01-05T23:05:46.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Matthew Farrellee", "body": "it looks like the issue is rddsampler checks for numpy in its constructor instead of when initializing the random number generator", "created": "2014-09-07T15:25:34.900+0000"}, {"author": "Matthew Farrellee", "body": "PR #2313 was subsumed by PR #3351, which resolved SPARK-4477 and this issue the resolution was to remove the use of numpy altogether", "created": "2015-01-05T23:05:05.450+0000"}], "num_comments": 2, "text": "Issue: SPARK-927\nSummary: PySpark sample() doesn't work if numpy is installed on master but not on workers\nDescription: PySpark's sample() method crashes with ImportErrors on the workers if numpy is installed on the driver machine but not on the workers. I'm not sure what's the best way to fix this. A general mechanism for automatically shipping libraries from the master to the workers would address this, but that could be complicated to implement.\n\nComments (2):\n1. Matthew Farrellee: it looks like the issue is rddsampler checks for numpy in its constructor instead of when initializing the random number generator\n2. Matthew Farrellee: PR #2313 was subsumed by PR #3351, which resolved SPARK-4477 and this issue the resolution was to remove the use of numpy altogether", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "03ffd71a2f405849f4b5c3c53fe271be", "issue_key": "SPARK-928", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add support for Unsafe-based serializer in Kryo 2.22", "description": "This can reportedly be quite a bit faster, but it also requires Chill to update its Kryo dependency. Once that happens we should add a spark.kryo.useUnsafe flag.", "reporter": "Matei Alexandru Zaharia", "assignee": "Sandeep Singh", "created": "2013-10-09T16:59:39.000+0000", "updated": "2016-10-22T19:03:12.000+0000", "resolved": "2016-10-22T19:03:12.000+0000", "labels": ["starter"], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "This probably can't be fixed in 1.0.0 because no Chill release uses Kryo 2.22 yet, and as far as I can tell we can't build the current Chill with Kryo 2.22 (I get some nasty Scala compiler errors when I try that). We can bump fix version to 1.1.0 once we do the final pass through 1.0.0 issues.", "created": "2014-04-24T23:22:01.110+0000"}, {"author": "Andrew Ash", "body": "Latest Chill (0.5.0) is still using Kryo 2.21 so this is still waiting on a Chill update https://github.com/twitter/chill/blob/0.5.0/project/Build.scala#L13", "created": "2014-11-14T11:13:24.938+0000"}, {"author": "Josh Rosen", "body": "It looks like we'll _finally_ be able to do this after SPARK-11416 goes in.", "created": "2016-03-30T22:31:54.594+0000"}, {"author": "Josh Rosen", "body": "We've now upgraded to Kryo 3.0.0 (in SPARK-11416), so it would be awesome if someone wants to try this out and report back with benchmark results.", "created": "2016-04-08T23:40:17.125+0000"}, {"author": "Kai Jiang", "body": "Since it is labeled as Starter, I would like to take a try on this one.", "created": "2016-04-19T12:42:15.271+0000"}, {"author": "Sandeep Singh", "body": "[~joshrosen] I would like to work on this. I tried benchmarking the difference between unsafe kryo and our current impl. and then we can have a spark.kryo.useUnsafe flag as Matei has mentioned.  JBenchmark Kryo Unsafe vs safe Serialization: Best/Avg Time(ms) Rate(M/s) Per Row(ns) Relative ------------------------------------------------------------------------------------------------ basicTypes: Int unsafe:true 160 / 178 98.5 10.1 1.0X basicTypes: Long unsafe:true 210 / 218 74.9 13.4 0.8X basicTypes: Float unsafe:true 203 / 213 77.5 12.9 0.8X basicTypes: Double unsafe:true 226 / 235 69.5 14.4 0.7X Array: Int unsafe:true 1087 / 1101 14.5 69.1 0.1X Array: Long unsafe:true 2758 / 2844 5.7 175.4 0.1X Array: Float unsafe:true 1511 / 1552 10.4 96.1 0.1X Array: Double unsafe:true 2942 / 2972 5.3 187.0 0.1X Map of string->Double unsafe:true 2645 / 2739 5.9 168.2 0.1X basicTypes: Int unsafe:false 211 / 218 74.7 13.4 0.8X basicTypes: Long unsafe:false 247 / 253 63.6 15.7 0.6X basicTypes: Float unsafe:false 211 / 216 74.5 13.4 0.8X basicTypes: Double unsafe:false 227 / 233 69.2 14.4 0.7X Array: Int unsafe:false 3012 / 3032 5.2 191.5 0.1X Array: Long unsafe:false 4463 / 4515 3.5 283.8 0.0X Array: Float unsafe:false 2788 / 2868 5.6 177.2 0.1X Array: Double unsafe:false 3558 / 3752 4.4 226.2 0.0X Map of string->Double unsafe:false 2806 / 2933 5.6 178.4 0.1X  You can find the code for benchmarking here (https://github.com/techaddict/spark/commit/46fa44141c849ca15bbd6136cea2fa52bd927da2), very ugly right now but will improve it(add more benchmarks) before creating a PR.", "created": "2016-05-01T16:14:52.572+0000"}, {"author": "Apache Spark", "body": "User 'techaddict' has created a pull request for this issue: https://github.com/apache/spark/pull/12913", "created": "2016-05-04T22:43:04.468+0000"}], "num_comments": 7, "text": "Issue: SPARK-928\nSummary: Add support for Unsafe-based serializer in Kryo 2.22\nDescription: This can reportedly be quite a bit faster, but it also requires Chill to update its Kryo dependency. Once that happens we should add a spark.kryo.useUnsafe flag.\n\nComments (7):\n1. Matei Alexandru Zaharia: This probably can't be fixed in 1.0.0 because no Chill release uses Kryo 2.22 yet, and as far as I can tell we can't build the current Chill with Kryo 2.22 (I get some nasty Scala compiler errors when I try that). We can bump fix version to 1.1.0 once we do the final pass through 1.0.0 issues.\n2. Andrew Ash: Latest Chill (0.5.0) is still using Kryo 2.21 so this is still waiting on a Chill update https://github.com/twitter/chill/blob/0.5.0/project/Build.scala#L13\n3. Josh Rosen: It looks like we'll _finally_ be able to do this after SPARK-11416 goes in.\n4. Josh Rosen: We've now upgraded to Kryo 3.0.0 (in SPARK-11416), so it would be awesome if someone wants to try this out and report back with benchmark results.\n5. Kai Jiang: Since it is labeled as Starter, I would like to take a try on this one.\n6. Sandeep Singh: [~joshrosen] I would like to work on this. I tried benchmarking the difference between unsafe kryo and our current impl. and then we can have a spark.kryo.useUnsafe flag as Matei has mentioned.  JBenchmark Kryo Unsafe vs safe Serialization: Best/Avg Time(ms) Rate(M/s) Per Row(ns) Relative ------------------------------------------------------------------------------------------------ basicTypes: Int unsafe:true 160 / 178 98.5 10.1 1.0X basicTypes: Long unsafe:true 210 / 218 74.9 13.4 0.8X basicTypes: Float unsafe:true 203 / 213 77.5 12.9 0.8X basicTypes: Double unsafe:true 226 / 235 69.5 14.4 0.7X Array: Int unsafe:true 1087 / 1101 14.5 69.1 0.1X Array: Long unsafe:true 2758 / 2844 5.7 175.4 0.1X Array: Float unsafe:true 1511 / 1552 10.4 96.1 0.1X Array: Double unsafe:true 2942 / 2972 5.3 187.0 0.1X Map of string->Double unsafe:true 2645 / 2739 5.9 168.2 0.1X basicTypes: Int unsafe:false 211 / 218 74.7 13.4 0.8X basicTypes: Long unsafe:false 247 / 253 63.6 15.7 0.6X basicTypes: Float unsafe:false 211 / 216 74.5 13.4 0.8X basicTypes: Double unsafe:false 227 / 233 69.2 14.4 0.7X Array: Int unsafe:false 3012 / 3032 5.2 191.5 0.1X Array: Long unsafe:false 4463 / 4515 3.5 283.8 0.0X Array: Float unsafe:false 2788 / 2868 5.6 177.2 0.1X Array: Double unsafe:false 3558 / 3752 4.4 226.2 0.0X Map of string->Double unsafe:false 2806 / 2933 5.6 178.4 0.1X  You can find the code for benchmarking here (https://github.com/techaddict/spark/commit/46fa44141c849ca15bbd6136cea2fa52bd927da2), very ugly right now but will improve it(add more benchmarks) before creating a PR.\n7. Apache Spark: User 'techaddict' has created a pull request for this issue: https://github.com/apache/spark/pull/12913", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "3dae23bc52bc6a4485a392483a2ccdcc", "issue_key": "SPARK-929", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Deprecate SPARK_MEM", "description": "Now that executor memory can be set through the spark.executor.memory system property, there's no reason to have SPARK_MEM. In general as we update the configuration system we should reduce reliance on environment variables.", "reporter": "Matei Alexandru Zaharia", "assignee": "Aaron Davidson", "created": "2013-10-09T19:34:49.000+0000", "updated": "2014-03-17T10:21:54.000+0000", "resolved": "2014-03-17T10:21:54.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Mark Grover", "body": "I will work on this. I should assign this to myself, can someone give me karma to do so, please?", "created": "2013-10-09T20:00:35.082+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Alright, I've assigned it to you. I'll also add you to the developers group so you can do that.", "created": "2013-10-09T21:26:03.021+0000"}, {"author": "Mark Grover", "body": "Thanks Matei! I posted this question on the original pull request (https://github.com/apache/incubator-spark/pull/48) but I am posting it here in case you missed the notification there: I see SPARK_MEM's usage in 2 places. 1. In populating of the Spark context (see https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L1011) 2. In passing this info to the executors so they can set the memory size. (see https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L145) Are you referring to deprecation at both 1 and 2 above? Or just 1? Thanks again!", "created": "2013-10-11T06:10:23.740+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I think doing it in just the first one is fine, but we may want to rename the parameter in the second one. For that second one, it's hard to set the memory without an environment var because it needs to be seen in a shell script *before* the JVM launches in order to set parameters on the `java` command.", "created": "2013-10-11T18:31:45.272+0000"}, {"author": "Mark Grover", "body": "Thanks and agreed!", "created": "2013-10-13T10:36:13.918+0000"}, {"author": "Mark Grover", "body": "Issued a pull request for feedback. I haven't done much testing but plan to do it soon.", "created": "2013-10-23T16:26:39.896+0000"}, {"author": "Sanford Ryza", "body": "After this, what will be the preferred way for setting the driver memory?", "created": "2014-01-26T13:32:56.962+0000"}, {"author": "Aaron Davidson", "body": "As far as I know, SPARK_MEM is still the way to do this, though it is no longer documented in Spark Configuration. Is this a problem, or am I missing something?", "created": "2014-02-14T11:54:30.719+0000"}, {"author": "Aaron Davidson", "body": "I've confirmed with @[~patrick] that this is indeed a problem. Here are some thoughts from our discussion: The current state of the code is not great -- SPARK_MEM is no longer documented, but it is still the only way to specify the memory to be used in spark-shell and run-examples. (Additionally, it is still the default for executors if spark.executor.mem is not specified.) One proposal for going forward is to create a new environment variable SPARK_DRIVER_MEM which is intended to be used for both spark-shell and run-examples, and nowhere else. This allows us to keep SPARK_MEM deprecated and to avoid introducing another super-general option. It also means that users who intend to set the spark-shell memory do not accidentally set executor memory as well. Since environment variables are so passé, we can make spark-shell take an additional parameter for the driver memory (which uses SPARK_DRIVER_MEM to pass to spark-class). It's harder to add such an option to run-examples, since it's unprecedented to have parameters that run-examples parses before running the actual example (so it would complicate the interface), and since it's unlikely that changing the memory is needed. For this case, users can still use SPARK_DRIVER_MEM directly. Complications of this approach: (1) The name \"SPARK_DRIVER_MEM\" may confuse users into thinking it actually sets the driver memory, rather than just setting the memory for the host process for spark-shell and run-examples. If a user runs their own driver, the driver will still use the memory of the host JVM, totally independent of SPARK_DRIVER_MEM. (2) Configuring the spark-shell memory can be done through both SPARK_DRIVER_MEM and a spark-shell command-line option, which can be confusing to users. This is true of the other spark-shell configuration options as well. Perhaps we can a warning if the user attempts to specify both.", "created": "2014-02-14T14:28:31.786+0000"}, {"author": "Aaron Davidson", "body": "On reflection, I think the following may be simpler: Two environment variables, SPARK_SHELL_MEM (private, not documented) and SPARK_EXAMPLES_MEM (documented somewhere, but probably not on the Configuration page). The main way to configure the spark-shell's memory will be through a command-line option like \"spark-shell --driver-mem 4g\", which simply sets SPARK_SHELL_MEM (which is only used by spark-class when executing the repl class, similar to SPARK_DAEMON_MEM for master and worker classes). SPARK_EXAMPLES_MEM can be documented in the same way as SPARK_EXAMPLES_JAR, since it is should be infrequently used, but does need to be configurable. This avoids both complications listed above, and still keeps us from using SPARK_MEM internally so we don't accidentally set the executor memory. Any thoughts?", "created": "2014-02-14T14:38:37.680+0000"}, {"author": "Patrick McFadin", "body": "[~ilikerps] This does seem better. Just to be clear though - we still have the issue of `spark-class` needing to check what class is being run in order to determine which XX_MEM to us - correct? Also, how about SPARK_EXAMPLE_MEM (no -S) since it will apply at any given time to one example (similar to SPARK_SHELL_MEM).", "created": "2014-02-14T15:15:44.996+0000"}, {"author": "Aaron Davidson", "body": "Right, same issue of needing to check the class that's being run to decide which env variable to use. I'm fine with SPARK_EXAMPLE_MEM, but if someone types SPARK_EXAMPLES_MEM, following the SPARK_EXAMPLES_JAR style, they'll be in for a fun debugging session. (Could actually specifically check for the \"S\" as a little usability improvement.)", "created": "2014-02-14T15:21:06.028+0000"}, {"author": "Sanford Ryza", "body": "How would one set the memory when running a job with spark-class?", "created": "2014-02-17T22:49:49.941+0000"}, {"author": "Aaron Davidson", "body": "Great question, one I had just hit as I was writing up the commit message for my patch :) I think a good solution is to add a SPARK_DRIVER_MEMORY which is publicly documented. By also introducing an explicit SPARK_EXECUTOR_MEMORY which is set for the Mesos schedulers (standalone and YARN do not use spark-class) using spark.executor.memory, we can keep backwards compatibility of SPARK_MEM while ensuring SPARK_DRIVER_MEMORY does not affect executors. I should have a patch ready for review tomorrow in case that made not so much sense.", "created": "2014-02-18T00:21:42.657+0000"}, {"author": "Sanford Ryza", "body": "That makes sense to me. Thanks Aaron. One more thing: how will this function in yarn-standalone mode, where the driver runs in the YARN application master? Will SPARK_DRIVER_MEM get passed there? Do we consider the client that is submitting the job, but not driving it in terms of scheduling, a DRIVER?", "created": "2014-02-18T13:01:12.394+0000"}], "num_comments": 15, "text": "Issue: SPARK-929\nSummary: Deprecate SPARK_MEM\nDescription: Now that executor memory can be set through the spark.executor.memory system property, there's no reason to have SPARK_MEM. In general as we update the configuration system we should reduce reliance on environment variables.\n\nComments (15):\n1. Mark Grover: I will work on this. I should assign this to myself, can someone give me karma to do so, please?\n2. Matei Alexandru Zaharia: Alright, I've assigned it to you. I'll also add you to the developers group so you can do that.\n3. Mark Grover: Thanks Matei! I posted this question on the original pull request (https://github.com/apache/incubator-spark/pull/48) but I am posting it here in case you missed the notification there: I see SPARK_MEM's usage in 2 places. 1. In populating of the Spark context (see https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L1011) 2. In passing this info to the executors so they can set the memory size. (see https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L145) Are you referring to deprecation at both 1 and 2 above? Or just 1? Thanks again!\n4. Matei Alexandru Zaharia: I think doing it in just the first one is fine, but we may want to rename the parameter in the second one. For that second one, it's hard to set the memory without an environment var because it needs to be seen in a shell script *before* the JVM launches in order to set parameters on the `java` command.\n5. Mark Grover: Thanks and agreed!\n6. Mark Grover: Issued a pull request for feedback. I haven't done much testing but plan to do it soon.\n7. Sanford Ryza: After this, what will be the preferred way for setting the driver memory?\n8. Aaron Davidson: As far as I know, SPARK_MEM is still the way to do this, though it is no longer documented in Spark Configuration. Is this a problem, or am I missing something?\n9. Aaron Davidson: I've confirmed with @[~patrick] that this is indeed a problem. Here are some thoughts from our discussion: The current state of the code is not great -- SPARK_MEM is no longer documented, but it is still the only way to specify the memory to be used in spark-shell and run-examples. (Additionally, it is still the default for executors if spark.executor.mem is not specified.) One proposal for going forward is to create a new environment variable SPARK_DRIVER_MEM which is intended to be used for both spark-shell and run-examples, and nowhere else. This allows us to keep SPARK_MEM deprecated and to avoid introducing another super-general option. It also means that users who intend to set the spark-shell memory do not accidentally set executor memory as well. Since environment variables are so passé, we can make spark-shell take an additional parameter for the driver memory (which uses SPARK_DRIVER_MEM to pass to spark-class). It's harder to add such an option to run-examples, since it's unprecedented to have parameters that run-examples parses before running the actual example (so it would complicate the interface), and since it's unlikely that changing the memory is needed. For this case, users can still use SPARK_DRIVER_MEM directly. Complications of this approach: (1) The name \"SPARK_DRIVER_MEM\" may confuse users into thinking it actually sets the driver memory, rather than just setting the memory for the host process for spark-shell and run-examples. If a user runs their own driver, the driver will still use the memory of the host JVM, totally independent of SPARK_DRIVER_MEM. (2) Configuring the spark-shell memory can be done through both SPARK_DRIVER_MEM and a spark-shell command-line option, which can be confusing to users. This is true of the other spark-shell configuration options as well. Perhaps we can a warning if the user attempts to specify both.\n10. Aaron Davidson: On reflection, I think the following may be simpler: Two environment variables, SPARK_SHELL_MEM (private, not documented) and SPARK_EXAMPLES_MEM (documented somewhere, but probably not on the Configuration page). The main way to configure the spark-shell's memory will be through a command-line option like \"spark-shell --driver-mem 4g\", which simply sets SPARK_SHELL_MEM (which is only used by spark-class when executing the repl class, similar to SPARK_DAEMON_MEM for master and worker classes). SPARK_EXAMPLES_MEM can be documented in the same way as SPARK_EXAMPLES_JAR, since it is should be infrequently used, but does need to be configurable. This avoids both complications listed above, and still keeps us from using SPARK_MEM internally so we don't accidentally set the executor memory. Any thoughts?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "82f93147d21ca948b578f6930cf8c3fc", "issue_key": "SPARK-930", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Unicode failing in pyspark - UnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 440: ordinal not in range(128)", "description": "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.takePartition. : org.apache.spark.api.python.PythonException: Traceback (most recent call last): File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/worker.py\", line 82, in main for obj in func(split_index, iterator): File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/serializers.py\", line 41, in batched for item in iterator: File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/rdd.py\", line 521, in takeUpToNum yield next(iterator) File \"/usr/lib64/python2.6/csv.py\", line 104, in next row = self.reader.next() UnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 440: ordinal not in range(128) at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:151) at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:173) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:116) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) at org.apache.spark.rdd.RDD.iterator(RDD.scala:226) at org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:484) at org.apache.spark.scheduler.DAGScheduler$$anon$2.run(DAGScheduler.scala:470)", "reporter": "Ben Duffield", "assignee": null, "created": "2013-10-10T00:13:29.000+0000", "updated": "2013-12-15T14:18:44.000+0000", "resolved": "2013-12-15T14:18:44.000+0000", "labels": [], "components": [], "comments": [{"author": "Ben Duffield", "body": "This is me being a total idiot. It's happening outside of spark - it's me. Please close.", "created": "2013-10-10T00:17:06.842+0000"}], "num_comments": 1, "text": "Issue: SPARK-930\nSummary: Unicode failing in pyspark - UnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 440: ordinal not in range(128)\nDescription: Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.takePartition. : org.apache.spark.api.python.PythonException: Traceback (most recent call last): File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/worker.py\", line 82, in main for obj in func(split_index, iterator): File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/serializers.py\", line 41, in batched for item in iterator: File \"/opt/palantir/spark/spark-0.8.0/python/pyspark/rdd.py\", line 521, in takeUpToNum yield next(iterator) File \"/usr/lib64/python2.6/csv.py\", line 104, in next row = self.reader.next() UnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 440: ordinal not in range(128) at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:151) at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:173) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:116) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) at org.apache.spark.rdd.RDD.iterator(RDD.scala:226) at org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:484) at org.apache.spark.scheduler.DAGScheduler$$anon$2.run(DAGScheduler.scala:470)\n\nComments (1):\n1. Ben Duffield: This is me being a total idiot. It's happening outside of spark - it's me. Please close.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "82df8971f9abe263cdb525af0ce161c8", "issue_key": "SPARK-931", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Potential bug with Spark streaming example", "description": "See e-mail thread: https://groups.google.com/forum/#!topic/spark-users/GVPDZOCZMcw", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-10-10T20:03:58.000+0000", "updated": "2013-10-16T10:45:08.000+0000", "resolved": "2013-10-16T10:45:08.000+0000", "labels": ["starter"], "components": ["DStreams"], "comments": [{"author": "Patrick McFadin", "body": "Patch submitted: https://github.com/apache/incubator-spark/pull/63/files", "created": "2013-10-15T22:58:28.970+0000"}], "num_comments": 1, "text": "Issue: SPARK-931\nSummary: Potential bug with Spark streaming example\nDescription: See e-mail thread: https://groups.google.com/forum/#!topic/spark-users/GVPDZOCZMcw\n\nComments (1):\n1. Patrick McFadin: Patch submitted: https://github.com/apache/incubator-spark/pull/63/files", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "c57266c97eaf7d6f8b19ba417c0adaa6", "issue_key": "SPARK-932", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Consolidate local scheduler and cluster scheduler", "description": "We should consolidate LocalScheduler and ClusterScheduler, given most of the functionalities are duplicated in both. This can be done by removing the LocalScheduler, and create a LocalSchedulerBackend that connects directly to an Executor.", "reporter": "Reynold Xin", "assignee": "Kay Ousterhout", "created": "2013-10-10T23:18:53.000+0000", "updated": "2014-01-05T23:16:28.000+0000", "resolved": "2014-01-05T23:16:26.000+0000", "labels": ["starter"], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-932\nSummary: Consolidate local scheduler and cluster scheduler\nDescription: We should consolidate LocalScheduler and ClusterScheduler, given most of the functionalities are duplicated in both. This can be done by removing the LocalScheduler, and create a LocalSchedulerBackend that connects directly to an Executor.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "220e0ffa3a58ed0bd1727483c8ceb95c", "issue_key": "SPARK-933", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Doesn't compile", "description": "Calling from RHEL5 spark-0.8.0-incubating$ sbt/sbt assembly gives these warnings and errors [warn] /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:129: method cleanupJob in class OutputCommitter is deprecated: see corresponding Javadoc for more information. [warn] getOutputCommitter().cleanupJob(getJobContext()) [warn] ^ [warn] /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:592: method cleanupJob in class OutputCommitter is deprecated: see corresponding Javadoc for more information. [warn] jobCommitter.cleanupJob(jobTaskContext) [warn] ^ [warn] two warnings found [error] ---------- [error] 1. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileClient.java (at line 22) [error] import io.netty.channel.ChannelFuture; [error] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [error] The import io.netty.channel.ChannelFuture is never used [error] ---------- [error] 2. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileClient.java (at line 23) [error] import io.netty.channel.ChannelFutureListener; [error] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [error] The import io.netty.channel.ChannelFutureListener is never used [error] ---------- [error] ---------- [error] 3. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileServer.java (at line 23) [error] import io.netty.channel.Channel; [error] ^^^^^^^^^^^^^^^^^^^^^^^^ [error] The import io.netty.channel.Channel is never used [error] ---------- [error] ---------- [error] 4. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java (at line 20) [error] import java.util.Arrays; [error] ^^^^^^^^^^^^^^^^ [error] The import java.util.Arrays is never used [error] ---------- [error] ---------- [error] 5. ERROR in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/api/java/function/DoubleFlatMapFunction.java (at line 36) [error] public final Iterable<Double> apply(T t) { return call(t); } [error] ^^^^^^^^^^ [error] The method apply(T) of type DoubleFlatMapFunction<T> must override a superclass method [error] ---------- [error] 5 problems (1 error, 4 warnings) [error] (core/compile:compile) javac returned nonzero exit code", "reporter": "Teodor Sigaev", "assignee": null, "created": "2013-10-14T18:56:55.000+0000", "updated": "2013-10-15T14:38:45.000+0000", "resolved": "2013-10-15T14:38:45.000+0000", "labels": ["compile"], "components": ["Build"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Which Java version do you have? You may need to use Java 6, or maybe your Java is somehow configured to treat those as errors instead of warnings.", "created": "2013-10-15T00:05:54.954+0000"}, {"author": "Teodor Sigaev", "body": "Currently compiling with java 1.6.0_13, javac Eclipse Java Compiler v_677_R32x, 3.2.1 release. Will check another version and let you know", "created": "2013-10-15T09:23:04.646+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yeah try the javac from OpenJDK or Oracle. My guess is that the Eclipse one is configured to treat those as errors.", "created": "2013-10-15T10:00:53.939+0000"}, {"author": "Teodor Sigaev", "body": "javac 1.7.0_25 compiled, thank you for the suggestion", "created": "2013-10-15T10:14:19.391+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Marking this as \"won't fix\" since I don't think we want to support the Eclipse version of javac necessarily; maybe we'll revisit it later though so it's good to have this issue here for people to find.", "created": "2013-10-15T14:38:45.552+0000"}], "num_comments": 5, "text": "Issue: SPARK-933\nSummary: Doesn't compile\nDescription: Calling from RHEL5 spark-0.8.0-incubating$ sbt/sbt assembly gives these warnings and errors [warn] /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:129: method cleanupJob in class OutputCommitter is deprecated: see corresponding Javadoc for more information. [warn] getOutputCommitter().cleanupJob(getJobContext()) [warn] ^ [warn] /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:592: method cleanupJob in class OutputCommitter is deprecated: see corresponding Javadoc for more information. [warn] jobCommitter.cleanupJob(jobTaskContext) [warn] ^ [warn] two warnings found [error] ---------- [error] 1. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileClient.java (at line 22) [error] import io.netty.channel.ChannelFuture; [error] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [error] The import io.netty.channel.ChannelFuture is never used [error] ---------- [error] 2. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileClient.java (at line 23) [error] import io.netty.channel.ChannelFutureListener; [error] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [error] The import io.netty.channel.ChannelFutureListener is never used [error] ---------- [error] ---------- [error] 3. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/java/org/apache/spark/network/netty/FileServer.java (at line 23) [error] import io.netty.channel.Channel; [error] ^^^^^^^^^^^^^^^^^^^^^^^^ [error] The import io.netty.channel.Channel is never used [error] ---------- [error] ---------- [error] 4. WARNING in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java (at line 20) [error] import java.util.Arrays; [error] ^^^^^^^^^^^^^^^^ [error] The import java.util.Arrays is never used [error] ---------- [error] ---------- [error] 5. ERROR in /rhel5pdi/workplace/extprojects/spark-0.8.0-incubating/core/src/main/scala/org/apache/spark/api/java/function/DoubleFlatMapFunction.java (at line 36) [error] public final Iterable<Double> apply(T t) { return call(t); } [error] ^^^^^^^^^^ [error] The method apply(T) of type DoubleFlatMapFunction<T> must override a superclass method [error] ---------- [error] 5 problems (1 error, 4 warnings) [error] (core/compile:compile) javac returned nonzero exit code\n\nComments (5):\n1. Matei Alexandru Zaharia: Which Java version do you have? You may need to use Java 6, or maybe your Java is somehow configured to treat those as errors instead of warnings.\n2. Teodor Sigaev: Currently compiling with java 1.6.0_13, javac Eclipse Java Compiler v_677_R32x, 3.2.1 release. Will check another version and let you know\n3. Matei Alexandru Zaharia: Yeah try the javac from OpenJDK or Oracle. My guess is that the Eclipse one is configured to treat those as errors.\n4. Teodor Sigaev: javac 1.7.0_25 compiled, thank you for the suggestion\n5. Matei Alexandru Zaharia: Marking this as \"won't fix\" since I don't think we want to support the Eclipse version of javac necessarily; maybe we'll revisit it later though so it's good to have this issue here for people to find.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.084537"}}
{"id": "420ed4bf5acb7da5d2e0b514ec14f2f8", "issue_key": "SPARK-934", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark-mllib occasionally throw java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509)", "description": "java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509) com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:346) com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:192) com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254) com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129) java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2309) java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322) java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3012) java.io.ObjectInputStream.readArray(ObjectInputStream.java:1691) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342) java.io.ObjectInputStream.readArray(ObjectInputStream.java:1704) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348) java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39) org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:101) org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26) org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:40) org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:121) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:118) scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:118) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.scheduler.ResultTask.run(ResultTask.scala:99) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:158) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:724)", "reporter": "Guoqiang Li", "assignee": null, "created": "2013-10-15T02:30:22.000+0000", "updated": "2014-08-20T08:22:06.000+0000", "resolved": "2014-08-20T08:22:06.000+0000", "labels": [], "components": [], "comments": [{"author": "Shivaram Venkataraman", "body": "Could you provide some more details on what was the program being run and what was the input data etc. ?", "created": "2013-10-15T11:13:15.280+0000"}, {"author": "Guoqiang Li", "body": "code: System.setProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") System.setProperty(\"spark.kryo.registrator\", classOfALSRegistrator.getName) System.setProperty(\"spark.kryo.referenceTracking\", \"false\") System.setProperty(\"spark.kryoserializer.buffer.mb\", \"8\") System.setProperty(\"spark.locality.wait\", \"10000\") ALS.trainImplicit(ratings, rank, iterations) rank is 20 iterations is 20 ratings.count is 3609314 java.lang.ArrayIndexOutOfBoundsException (java.lang.ArrayIndexOutOfBoundsException: 344) org.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:369) scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78) org.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1.apply$mcVI$sp(ALS.scala:366) scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78) org.apache.spark.mllib.recommendation.ALS.updateBlock(ALS.scala:365) org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:337) org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:336) org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32) org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32) scala.collection.Iterator$$anon$19.next(Iterator.scala:401) scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) scala.collection.Iterator$class.foreach(Iterator.scala:772) scala.collection.Iterator$$anon$21.foreach(Iterator.scala:437) org.apache.spark.rdd.PairRDDFunctions.reducePartition$1(PairRDDFunctions.scala:176) org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191) org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107) org.apache.spark.scheduler.Task.run(Task.scala:53) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:210) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:724)", "created": "2013-10-16T03:30:26.511+0000"}, {"author": "Guoqiang Li", "body": "Seems org.apache.spark.serializer.KryoSerializer causing problems. Switch to org.apache.spark.serializer.JavaSerializer no problem.", "created": "2013-10-17T04:50:48.290+0000"}], "num_comments": 3, "text": "Issue: SPARK-934\nSummary: spark-mllib occasionally throw java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509)\nDescription: java.io.IOException (java.io.IOException: Corrupt data: overrun in decompress, input offset 51381, output offset 57509) com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:346) com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:192) com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254) com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129) java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2309) java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2322) java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:3012) java.io.ObjectInputStream.readArray(ObjectInputStream.java:1691) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342) java.io.ObjectInputStream.readArray(ObjectInputStream.java:1704) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1342) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348) java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39) org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:101) org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26) org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:40) org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:98) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:121) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:118) scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:38) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:118) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:32) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.scheduler.ResultTask.run(ResultTask.scala:99) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:158) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:724)\n\nComments (3):\n1. Shivaram Venkataraman: Could you provide some more details on what was the program being run and what was the input data etc. ?\n2. Guoqiang Li: code: System.setProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") System.setProperty(\"spark.kryo.registrator\", classOfALSRegistrator.getName) System.setProperty(\"spark.kryo.referenceTracking\", \"false\") System.setProperty(\"spark.kryoserializer.buffer.mb\", \"8\") System.setProperty(\"spark.locality.wait\", \"10000\") ALS.trainImplicit(ratings, rank, iterations) rank is 20 iterations is 20 ratings.count is 3609314 java.lang.ArrayIndexOutOfBoundsException (java.lang.ArrayIndexOutOfBoundsException: 344) org.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:369) scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78) org.apache.spark.mllib.recommendation.ALS$$anonfun$updateBlock$1.apply$mcVI$sp(ALS.scala:366) scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:78) org.apache.spark.mllib.recommendation.ALS.updateBlock(ALS.scala:365) org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:337) org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:336) org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32) org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:32) scala.collection.Iterator$$anon$19.next(Iterator.scala:401) scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) scala.collection.Iterator$class.foreach(Iterator.scala:772) scala.collection.Iterator$$anon$21.foreach(Iterator.scala:437) org.apache.spark.rdd.PairRDDFunctions.reducePartition$1(PairRDDFunctions.scala:176) org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191) org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1.apply(PairRDDFunctions.scala:191) org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:36) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) org.apache.spark.rdd.RDD.iterator(RDD.scala:226) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107) org.apache.spark.scheduler.Task.run(Task.scala:53) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:210) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:724)\n3. Guoqiang Li: Seems org.apache.spark.serializer.KryoSerializer causing problems. Switch to org.apache.spark.serializer.JavaSerializer no problem.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "ca11f21e93996cae7d7c6da7468d050a", "issue_key": "SPARK-935", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Typo in documentation", "description": "Small typo on: http://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html s/chd4/cdh4/ for cloudera artifacts", "reporter": "Rafal Kwasny", "assignee": null, "created": "2013-10-16T08:08:22.000+0000", "updated": "2013-12-07T14:40:16.000+0000", "resolved": "2013-12-07T14:40:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/apache/incubator-spark/pull/171", "created": "2013-12-07T14:40:16.995+0000"}], "num_comments": 1, "text": "Issue: SPARK-935\nSummary: Typo in documentation\nDescription: Small typo on: http://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html s/chd4/cdh4/ for cloudera artifacts\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/apache/incubator-spark/pull/171", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "d2e2a4df455ad177e1ebbf67aec79f32", "issue_key": "SPARK-936", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Please publish jars for Scala 2.10", "description": "Please publish jars for the 2.10 branch. Currently, only jars for 2.9.3 are available: http://mvnrepository.com/artifact/org.apache.spark", "reporter": "Eric Christiansen", "assignee": null, "created": "2013-10-16T17:40:26.000+0000", "updated": "2013-12-11T09:26:31.000+0000", "resolved": "2013-12-11T09:26:31.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Patrick McFadin", "body": "We'll do this once we have official support for 2.10. For this, follow SPARK-994.", "created": "2013-12-11T09:26:13.548+0000"}], "num_comments": 1, "text": "Issue: SPARK-936\nSummary: Please publish jars for Scala 2.10\nDescription: Please publish jars for the 2.10 branch. Currently, only jars for 2.9.3 are available: http://mvnrepository.com/artifact/org.apache.spark\n\nComments (1):\n1. Patrick McFadin: We'll do this once we have official support for 2.10. For this, follow SPARK-994.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "38ce34ff33c767686e3243bccce2f1be", "issue_key": "SPARK-937", "issue_type": "Improvement", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Executors that exit cleanly should not have KILLED status", "description": "This is an unintuitive and overloaded status message when Executors are killed during normal termination of an application.", "reporter": "Aaron Davidson", "assignee": "Kan Zhang", "created": "2013-10-18T09:09:13.000+0000", "updated": "2014-06-15T21:57:50.000+0000", "resolved": "2014-06-15T21:57:38.000+0000", "labels": [], "components": [], "comments": [{"author": "Kan Zhang", "body": "PR: https://github.com/apache/spark/pull/306", "created": "2014-06-05T21:34:30.199+0000"}], "num_comments": 1, "text": "Issue: SPARK-937\nSummary: Executors that exit cleanly should not have KILLED status\nDescription: This is an unintuitive and overloaded status message when Executors are killed during normal termination of an application.\n\nComments (1):\n1. Kan Zhang: PR: https://github.com/apache/spark/pull/306", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "e49be442224f9ac14c28e309cd74b88c", "issue_key": "SPARK-938", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "OpenStack Swift Storage Support", "description": "This issue is to track OpenStack Swift Storage support (development in progress) in addition to S3 for Spark.", "reporter": "Murali Raju", "assignee": "Gil Vernik", "created": "2013-10-18T19:42:15.000+0000", "updated": "2014-09-08T03:58:05.000+0000", "resolved": "2014-09-08T03:56:44.000+0000", "labels": [], "components": ["Documentation", "Examples", "Input/Output", "Spark Core"], "comments": [{"author": "Zhen Peng", "body": "hi, is there any follow-up on this issue?", "created": "2014-06-03T05:37:25.888+0000"}, {"author": "Gil Vernik", "body": "I am working on it. About to submit the patch. Almost done.", "created": "2014-06-08T06:10:35.193+0000"}, {"author": "Gil Vernik", "body": "Here we go: https://github.com/apache/spark/pull/1010 This is very initial documentation, describing how to integrate Spark and Swift. ( uses stand alone mode of Spark ) Will provide more patches with information how to integrate other cluster deployments of Spark with Swift and also how to use earlier versions of Hadoop.", "created": "2014-06-08T08:05:33.820+0000"}, {"author": "Patrick Wendell", "body": "Issue resolved by pull request 2298 [https://github.com/apache/spark/pull/2298]", "created": "2014-09-08T03:56:45.014+0000"}, {"author": "Patrick Wendell", "body": "This was fixed by [~gvernik] with [~rxin] authoring a slightly revised version of the patch.", "created": "2014-09-08T03:58:05.346+0000"}], "num_comments": 5, "text": "Issue: SPARK-938\nSummary: OpenStack Swift Storage Support\nDescription: This issue is to track OpenStack Swift Storage support (development in progress) in addition to S3 for Spark.\n\nComments (5):\n1. Zhen Peng: hi, is there any follow-up on this issue?\n2. Gil Vernik: I am working on it. About to submit the patch. Almost done.\n3. Gil Vernik: Here we go: https://github.com/apache/spark/pull/1010 This is very initial documentation, describing how to integrate Spark and Swift. ( uses stand alone mode of Spark ) Will provide more patches with information how to integrate other cluster deployments of Spark with Swift and also how to use earlier versions of Hadoop.\n4. Patrick Wendell: Issue resolved by pull request 2298 [https://github.com/apache/spark/pull/2298]\n5. Patrick Wendell: This was fixed by [~gvernik] with [~rxin] authoring a slightly revised version of the patch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "b3eee1e199be6c3866ad954d8b40f2d3", "issue_key": "SPARK-939", "issue_type": "New Feature", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Allow user jars to take precedence over Spark jars, if desired", "description": "Sometimes a user may want to include their own version of a jar that spark itself uses. For example, if their code requires a newer version of that jar than Spark offers. It would be good to have an option to give the users dependencies precedence over Spark. This options should be disabled by default, since it could lead to some odd behavior (e.g. parts of Spark not working). But I think we should have it. From an implementation perspective, this would require modifying the way we do class loading inside of an Executor. The default behavior of the URLClassLoader is to delegate to it's parent first and, if that fails, to find a class locally. We want to have the opposite behavior. This is sometimes referred to as \"parent-last\" (as opposed to \"parent-first\") class loading precedence. There is an example of how to do this here: http://stackoverflow.com/questions/5445511/how-do-i-create-a-parent-last-child-first-classloader-in-java-or-how-to-overr We should write a similar class which can encapsulate a URL classloader and change the delegation order. Or if possible, maybe we could find a more elegant way to do this. See relevant discussion on the user list here: https://groups.google.com/forum/#!topic/spark-users/b278DW3e38g Also see the corresponding option in Hadoop: https://issues.apache.org/jira/browse/MAPREDUCE-4521 Some other relevant Hadoop JIRA's: https://issues.apache.org/jira/browse/MAPREDUCE-1700 https://issues.apache.org/jira/browse/MAPREDUCE-1938", "reporter": "Patrick Wendell", "assignee": "Holden Karau", "created": "2013-10-19T14:54:38.000+0000", "updated": "2015-11-23T13:14:05.000+0000", "resolved": "2014-04-09T05:31:30.000+0000", "labels": ["starter"], "components": ["Spark Core"], "comments": [{"author": "Piyush Kansal", "body": "I am interested in working on this issue. Please assign it to me.", "created": "2014-03-05T00:02:24.971+0000"}, {"author": "Sanford Ryza", "body": "It's also worth considering something like the fancier classpath isolation stuff that was implemented for Hadoop later on: https://issues.apache.org/jira/browse/MAPREDUCE-1700", "created": "2014-03-05T23:06:02.962+0000"}, {"author": "Thomas Graves", "body": "Note that spark on yarn already supports putting user jars first - set config spark.yarn.user.classpath.first to true. The other parts mentioned are not supported.", "created": "2014-03-24T05:50:42.551+0000"}, {"author": "Patrick McFadin", "body": "Is this still relevant on YARN if someone uses sc.addJar()?", "created": "2014-03-24T10:16:27.248+0000"}, {"author": "Thomas Graves", "body": "No it probably doesn't work properly with addJars without the other changes Sandy mentioned.", "created": "2014-03-24T10:37:10.491+0000"}, {"author": "Sanford Ryza", "body": "jars added through addJars on YARN are distributed to executors in the same way they are on standalone mode. IIUC the changes proposed here are for how classes are loaded on the executor side, so they would still be relevant and likely work properly for Spark on YARN.", "created": "2014-03-24T12:30:33.049+0000"}, {"author": "Holden Karau", "body": "https://github.com/apache/spark/pull/217", "created": "2014-04-02T03:20:22.010+0000"}, {"author": "Jayson Minard", "body": "Ok, but how does this actually work? Any time we add Jackson 2.6.x to our classes then it crashes Spark when run on AWS EMR. Obviously our JAR is not isolated from Spark.", "created": "2015-11-23T13:13:03.323+0000"}, {"author": "Jayson Minard", "body": "I see, this now reverses the problem. We can crash Spark, but Spark can't crash us. Maybe Spark can shade/rename common libraries it uses that are likely to conflict (Jackson is one example).", "created": "2015-11-23T13:14:05.851+0000"}], "num_comments": 9, "text": "Issue: SPARK-939\nSummary: Allow user jars to take precedence over Spark jars, if desired\nDescription: Sometimes a user may want to include their own version of a jar that spark itself uses. For example, if their code requires a newer version of that jar than Spark offers. It would be good to have an option to give the users dependencies precedence over Spark. This options should be disabled by default, since it could lead to some odd behavior (e.g. parts of Spark not working). But I think we should have it. From an implementation perspective, this would require modifying the way we do class loading inside of an Executor. The default behavior of the URLClassLoader is to delegate to it's parent first and, if that fails, to find a class locally. We want to have the opposite behavior. This is sometimes referred to as \"parent-last\" (as opposed to \"parent-first\") class loading precedence. There is an example of how to do this here: http://stackoverflow.com/questions/5445511/how-do-i-create-a-parent-last-child-first-classloader-in-java-or-how-to-overr We should write a similar class which can encapsulate a URL classloader and change the delegation order. Or if possible, maybe we could find a more elegant way to do this. See relevant discussion on the user list here: https://groups.google.com/forum/#!topic/spark-users/b278DW3e38g Also see the corresponding option in Hadoop: https://issues.apache.org/jira/browse/MAPREDUCE-4521 Some other relevant Hadoop JIRA's: https://issues.apache.org/jira/browse/MAPREDUCE-1700 https://issues.apache.org/jira/browse/MAPREDUCE-1938\n\nComments (9):\n1. Piyush Kansal: I am interested in working on this issue. Please assign it to me.\n2. Sanford Ryza: It's also worth considering something like the fancier classpath isolation stuff that was implemented for Hadoop later on: https://issues.apache.org/jira/browse/MAPREDUCE-1700\n3. Thomas Graves: Note that spark on yarn already supports putting user jars first - set config spark.yarn.user.classpath.first to true. The other parts mentioned are not supported.\n4. Patrick McFadin: Is this still relevant on YARN if someone uses sc.addJar()?\n5. Thomas Graves: No it probably doesn't work properly with addJars without the other changes Sandy mentioned.\n6. Sanford Ryza: jars added through addJars on YARN are distributed to executors in the same way they are on standalone mode. IIUC the changes proposed here are for how classes are loaded on the executor side, so they would still be relevant and likely work properly for Spark on YARN.\n7. Holden Karau: https://github.com/apache/spark/pull/217\n8. Jayson Minard: Ok, but how does this actually work? Any time we add Jackson 2.6.x to our classes then it crashes Spark when run on AWS EMR. Obviously our JAR is not isolated from Spark.\n9. Jayson Minard: I see, this now reverses the problem. We can crash Spark, but Spark can't crash us. Maybe Spark can shade/rename common libraries it uses that are likely to conflict (Jackson is one example).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "9b72fc9ec17188bec30dfd390b1742dc", "issue_key": "SPARK-940", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Do not directly pass Stage objects to SparkListener", "description": "Right now the SparkListener interface directly passes `Stage` objects to listeners. This is a problem because it exposes internal objects to Listeners, which is a semi-public interface. Consumers could, e.g. mutate the stages and break Spark. I recently found an even bigger reason this is a problem. It causes a bunch of extra pointers to RDD's and other objects to remain live when downstream consumers like the Web UI keep references to them. Even though the UI does its own cleaning, it will retain 1000 stages by default, which can reference a large number of RDD's. In a long running Spark streaming program I was running, these references caused the JVM to run out of memory and begin GC thrashing. A heap analysis later revealed that this was the cause. To fix this, we should make `StageInfo` not just encapsulate a `Stage` (which it does now) and instead be similar to `TaskInfo` with its own distinct fields.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-10-20T13:05:39.000+0000", "updated": "2013-11-07T19:55:35.000+0000", "resolved": "2013-10-26T14:51:02.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "xiajunluan", "body": "Hi Patrick Have you started to involve in this issue? I would like manage to contribute this improvement.", "created": "2013-11-07T17:36:43.010+0000"}, {"author": "Patrick McFadin", "body": "Hey [~andrew xia]. This issue is fixed (see the status is \"Resolved\"). I forgot to close it though.", "created": "2013-11-07T19:55:30.583+0000"}], "num_comments": 2, "text": "Issue: SPARK-940\nSummary: Do not directly pass Stage objects to SparkListener\nDescription: Right now the SparkListener interface directly passes `Stage` objects to listeners. This is a problem because it exposes internal objects to Listeners, which is a semi-public interface. Consumers could, e.g. mutate the stages and break Spark. I recently found an even bigger reason this is a problem. It causes a bunch of extra pointers to RDD's and other objects to remain live when downstream consumers like the Web UI keep references to them. Even though the UI does its own cleaning, it will retain 1000 stages by default, which can reference a large number of RDD's. In a long running Spark streaming program I was running, these references caused the JVM to run out of memory and begin GC thrashing. A heap analysis later revealed that this was the cause. To fix this, we should make `StageInfo` not just encapsulate a `Stage` (which it does now) and instead be similar to `TaskInfo` with its own distinct fields.\n\nComments (2):\n1. xiajunluan: Hi Patrick Have you started to involve in this issue? I would like manage to contribute this improvement.\n2. Patrick McFadin: Hey [~andrew xia]. This issue is fixed (see the status is \"Resolved\"). I forgot to close it though.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "63dd9116153b9dad8ab3305f37d6cfd4", "issue_key": "SPARK-941", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add javadoc and user docs for JobLogger", "description": "", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-10-20T18:36:34.000+0000", "updated": "2014-03-21T14:59:00.000+0000", "resolved": "2014-03-21T14:59:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Given the UI patch we're deprecating this. So adding more docs probably doesn't make sense.", "created": "2014-03-21T14:58:53.769+0000"}], "num_comments": 1, "text": "Issue: SPARK-941\nSummary: Add javadoc and user docs for JobLogger\n\nComments (1):\n1. Patrick McFadin: Given the UI patch we're deprecating this. So adding more docs probably doesn't make sense.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "5b1b1005d9c538ebe8c2e4b7f1c69768", "issue_key": "SPARK-942", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Do not materialize partitions when DISK_ONLY storage level is used", "description": "If an operation returns a generating iterator (i.e. one that creates return values as the 'next' method is called), for example as the result of a 'flatMap' call on an RDD, the CacheManager first completely unrolls the iterator into an Array buffer before passing it to the blockManager (CacheManager.scala:74). Only after the entire iterator has been put into a buffer does it check if there is enough space in memory to store the data (BlockManager.scala:608). In the attached test, the code can complete the operation of 'saveAsTextFile' of text strings if it is called directly on the result RDD of a flatMap operation, this is because it is given an iterator result, and works on the map-then-save operation as the results are generated. In the other branch, a 'persist' is called, and the cacheManger first tries to un-roll the entire iterator before deciding to store it too disk, this will cause a Memory Error (on systems with -Xmx512m) In the cases where storing to disk is an option perhaps the CacheManager(or the BlockManager), can start to scan the iterator, calculating its size as the is pushed into a buffer as it goes (rather pushing everything into a buffer in a single operation), and if it determines that it will run out of memory, start pushing the already buffered portion of the iterator to disk, and then finish scanning the original iterator pushing that onto disk. Example Code (switch value of 'fail' variable to toggle behavior):  import org.apache.spark.SparkContext import org.apache.spark.storage.StorageLevel class Expander(base:String, count:Integer) extends Iterator[String] { var i = 0; def next() : String = { i += 1; return base + i.toString; } def hasNext() : Boolean = i < count; } object MemTest { def expand(s:String, i:Integer) : Iterator[String] = { return new Expander(s,i) } def main(args:Array[String]) = { val fail = false; val sc = new SparkContext(\"local\", \"mem_test\"); val seeds = sc.parallelize( Array( \"This is the first sentence that we will test:\", \"This is the second sentence that we will test:\", \"This is the third sentence that we will test:\" ) ); val out = seeds.flatMap(expand(_,10000000)); if (fail) { out.map(_ + \"...\").persist(StorageLevel.MEMORY_AND_DISK).saveAsTextFile(\"test.out\") } else { out.map(_ + \"...\").saveAsTextFile(\"test.out\") } } }", "reporter": "Kyle Ellrott", "assignee": "Kyle Ellrott", "created": "2013-10-20T21:36:09.000+0000", "updated": "2014-03-17T10:21:14.000+0000", "resolved": "2014-03-17T10:21:14.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Kyle Ellrott", "body": "This happens with persist set to StorageLevel.DISK_ONLY as well.", "created": "2013-11-02T15:55:02.385+0000"}, {"author": "Kyle Ellrott", "body": "Fix submitted: https://github.com/apache/incubator-spark/pull/180", "created": "2013-11-16T22:52:07.392+0000"}, {"author": "Patrick McFadin", "body": "I changed the title slightly here. This is a problem not only for generative iterators but even for normal `map` operations that happen to return large values. In all these cases we materialize the input partition even though, when writing to disk, we don't have to.", "created": "2014-03-06T14:47:36.778+0000"}], "num_comments": 3, "text": "Issue: SPARK-942\nSummary: Do not materialize partitions when DISK_ONLY storage level is used\nDescription: If an operation returns a generating iterator (i.e. one that creates return values as the 'next' method is called), for example as the result of a 'flatMap' call on an RDD, the CacheManager first completely unrolls the iterator into an Array buffer before passing it to the blockManager (CacheManager.scala:74). Only after the entire iterator has been put into a buffer does it check if there is enough space in memory to store the data (BlockManager.scala:608). In the attached test, the code can complete the operation of 'saveAsTextFile' of text strings if it is called directly on the result RDD of a flatMap operation, this is because it is given an iterator result, and works on the map-then-save operation as the results are generated. In the other branch, a 'persist' is called, and the cacheManger first tries to un-roll the entire iterator before deciding to store it too disk, this will cause a Memory Error (on systems with -Xmx512m) In the cases where storing to disk is an option perhaps the CacheManager(or the BlockManager), can start to scan the iterator, calculating its size as the is pushed into a buffer as it goes (rather pushing everything into a buffer in a single operation), and if it determines that it will run out of memory, start pushing the already buffered portion of the iterator to disk, and then finish scanning the original iterator pushing that onto disk. Example Code (switch value of 'fail' variable to toggle behavior):  import org.apache.spark.SparkContext import org.apache.spark.storage.StorageLevel class Expander(base:String, count:Integer) extends Iterator[String] { var i = 0; def next() : String = { i += 1; return base + i.toString; } def hasNext() : Boolean = i < count; } object MemTest { def expand(s:String, i:Integer) : Iterator[String] = { return new Expander(s,i) } def main(args:Array[String]) = { val fail = false; val sc = new SparkContext(\"local\", \"mem_test\"); val seeds = sc.parallelize( Array( \"This is the first sentence that we will test:\", \"This is the second sentence that we will test:\", \"This is the third sentence that we will test:\" ) ); val out = seeds.flatMap(expand(_,10000000)); if (fail) { out.map(_ + \"...\").persist(StorageLevel.MEMORY_AND_DISK).saveAsTextFile(\"test.out\") } else { out.map(_ + \"...\").saveAsTextFile(\"test.out\") } } }\n\nComments (3):\n1. Kyle Ellrott: This happens with persist set to StorageLevel.DISK_ONLY as well.\n2. Kyle Ellrott: Fix submitted: https://github.com/apache/incubator-spark/pull/180\n3. Patrick McFadin: I changed the title slightly here. This is a problem not only for generative iterators but even for normal `map` operations that happen to return large values. In all these cases we materialize the input partition even though, when writing to disk, we don't have to.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "f58497914b3f9b967fac86df7f88ddf2", "issue_key": "SPARK-943", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add `coalesce` and `repartition` to the streaming API", "description": "With streams, people often want to increase or decrease the level of parallelism. A good example is when the stream is ingested at a single node, but then fairly expensive operations happen at subsequent stages of the DAG. It would be good to add coalesce() and repartition() [this is just a new word I made up which is shorthand for coalesce with a larger # of partitions rather than smaller] directly to the DStream API. Once we have this, we should also document it.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-10-22T23:28:09.000+0000", "updated": "2013-10-25T10:08:20.000+0000", "resolved": "2013-10-25T10:08:20.000+0000", "labels": [], "components": ["DStreams"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-943\nSummary: Add `coalesce` and `repartition` to the streaming API\nDescription: With streams, people often want to increase or decrease the level of parallelism. A good example is when the stream is ingested at a single node, but then fairly expensive operations happen at subsequent stages of the DAG. It would be good to add coalesce() and repartition() [this is just a new word I made up which is shorthand for coalesce with a larger # of partitions rather than smaller] directly to the DStream API. Once we have this, we should also document it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.086539"}}
{"id": "318ca8624026093e4a1246d35f41a95b", "issue_key": "SPARK-944", "issue_type": "New Feature", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Give example of writing to HBase from Spark Streaming", "description": "", "reporter": "Patrick Wendell", "assignee": "Tathagata Das", "created": "2013-10-23T22:35:38.000+0000", "updated": "2015-01-22T00:32:39.000+0000", "resolved": "2015-01-22T00:32:39.000+0000", "labels": [], "components": ["DStreams"], "comments": [{"author": "Kanwaldeep", "body": "Hi Patrick. I do have working example of writing data to Spark Streaming but running into a into an issue with setting up checkpointing on the context as it is unable to serialize the org.apache.hadoop.mapred.JobConf object Exception in thread \"pool-6-thread-1\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf Any suggestion on how we can fix this? I've tried couple of workarounds so far but no luck. Thanks Kanwal", "created": "2014-01-10T12:38:19.796+0000"}, {"author": "Tathagata Das", "body": "Hi Kanwaldeep, Have you been able to make any progress on this?", "created": "2014-01-24T14:00:04.278+0000"}, {"author": "Kanwaldeep", "body": "Thanks for checking. Not so far. Any pointers to fix this would be very helpful.", "created": "2014-01-24T14:44:34.330+0000"}, {"author": "Tathagata Das", "body": "Can you explain the problem a little bit more. What is the spark streaming program doing? What is your cluster setup? Any logs and stacktraces, sample code for us reproduce the problem? Also, what version of Spark you are using?", "created": "2014-01-24T14:48:07.091+0000"}, {"author": "Kanwaldeep", "body": "Currently using Spark 0.8 version but in the process of building using 0.8.1 Cluster Setup - Local for now Code object KafkMetricStream { def main(args: Array[String]) { if (args.length < 5) { System.err.println(\"Usage: MetricStream <master> <zkQuorum> <group> <topics> <numThreads>\") System.exit(1) } val Array(master, zkQuorum, group, topics, numThreads, hbaseCluster) = args val ssc = new StreamingContext(master, \"KafkaWordCount\", Seconds(2), System.getenv(\"SPARK_HOME\"), Seq(System.getenv(\"SPARK_EXAMPLES_JAR\"))) ssc.checkpoint(\"checkpoint\") val topicpMap = topics.split(\",\").map((_,numThreads.toInt)).toMap val lines = ssc.kafkaStream(zkQuorum, group, topicpMap) val transformLines = lines.mapPartitions(ids => { ids.map(id => gsonconvert(id)) }) val appNodeMetrics = transformLines.flatMap(x => x.toList) // save data to HBase val conf = HBaseConfiguration.create() conf.set(\"hbase.zookeeper.quorum\", hbaseCluster) val jobConfig = new JobConf(conf) jobConfig.setOutputFormat(classOf[TableOutputFormat]) jobConfig.set(TableOutputFormat.OUTPUT_TABLE, \"sinet_test\") appNodeMetrics.foreach(rdd => new PairRDDFunctions(rdd.map(convert)).saveAsHadoopDataset( jobConfig)) ssc.start() } Exception Details Exception in thread \"Thread-35\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1359) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1155) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:422) at org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:152) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:950) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1482) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:329) at org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:112) at org.apache.spark.streaming.Scheduler.doCheckpoint(Scheduler.scala:127) at org.apache.spark.streaming.Scheduler.generateJobs(Scheduler.scala:110) at org.apache.spark.streaming.Scheduler$$anonfun$1.apply$mcVJ$sp(Scheduler.scala:41) at org.apache.spark.streaming.util.RecurringTimer.org$apache$spark$streaming$util$RecurringTimer$$loop(RecurringTimer.scala:66) at org.apache.spark.streaming.util.RecurringTimer$$anon$1.run(RecurringTimer.scala:34) Exception in thread \"pool-5-thread-1\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1359) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1155) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:422) at org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:152) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:950) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1482) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:329) at org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:112) at org.apache.spark.streaming.Scheduler.doCheckpoint(Scheduler.scala:127) at org.apache.spark.streaming.Scheduler.clearOldMetadata(Scheduler.scala:119) at org.apache.spark.streaming.JobManager.org$apache$spark$streaming$JobManager$$clearJob(JobManager.scala:79) at org.apache.spark.streaming.JobManager$JobHandler.run(JobManager.scala:41) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:695)", "created": "2014-01-24T15:14:03.478+0000"}, {"author": "Tathagata Das", "body": "Can you create the jobConfig object within the function in foreach(). So something like this.  appNodeMetrics.foreach(rdd => { val jobConfig = new JobConf(conf) jobConfig.setOutputFormat(classOfTableOutputFormat) jobConfig.set(TableOutputFormat.OUTPUT_TABLE, \"sinet_test\") new PairRDDFunctions(rdd.map(convert)).saveAsHadoopDataset( jobConfig)) }  That should most probably solve it.", "created": "2014-01-24T15:20:37.285+0000"}, {"author": "Kanwaldeep", "body": "Thanks. I had to move the HBaseConfiguration in the foreach as well. I would like to understand if there is any performance impact in doing since for each rdd we create a new config which needs to establish connection with zookeeper and Hbase servers. Will this be done for each task that is created for a batch of streaming data.", "created": "2014-01-24T16:26:35.424+0000"}, {"author": "Tathagata Das", "body": "Well, if I understand correctly, just creating the jobConf object should not be creating any connection. The connection is made in the worker only when data needs to saved. And I believe it is done once per partition, per call to saveAsHadoopDataset(). So here the only cost of moving jobConfig into the foreach is that a JobConf object every time the foreach function is called. I dont think that is too costly, probably takes a few extra milliseconds to create a JobConf object.", "created": "2014-01-24T19:07:43.066+0000"}, {"author": "Kanwaldeep", "body": "Thanks a lot. I'd test it further. Any suggestions on how to best handle failure scenarios writing to HBase? I would like to stop reading messages from Kafka till the problem is resolved i..e make sure all the data read is aggregated and then persisted to HBase.", "created": "2014-01-24T23:15:38.803+0000"}, {"author": "Tathagata Das", "body": "That is tricky to do. Well, what spark streaming would guarantee that the Spark pushes the data into HBase wont complete until the pushing is over. And Spark jobs of next batch wont start until the previous Spark jobs is over. The system would keep receiving data from Kafka, storing them in memory, generating Spark jobs on them and queueing up the Spark jobs from processing. Which may be fine under your circumstances. In future, we are thinking of adding flow control where the receiving will be slowed down if the queue of Spark jobs is too long.", "created": "2014-01-26T15:46:24.078+0000"}, {"author": "Kanwaldeep", "body": "Thanks for the explanation this was my expectation as well. In my local cluster setup using 0.8 version, I don't see such a behavior. The tasks fail writing to HBase and when I bring the HBase cluster back online the failed jobs are not being retried. Is this a change in 0.8.1 version?", "created": "2014-01-27T08:26:44.156+0000"}, {"author": "Tathagata Das", "body": "Aah, if the job is marked as failed then nothing can be done. And job is marked as failed if any task has failed the maximum number of times. See the property spark.task.maxFailures in the Spark configuration guide. Setting that to a very high number maybe worth while. Then the task will keep failing while the Hbase is down, but the job will not failed until max task failures is reached. However, this in general may not be good idea if you expect your HBase to be down for a long time (long relative to the batch interval). Why does HBase have to be down for a long time? On a related note, how are you writing to HBase, using saveAsHadoopFiles?", "created": "2014-01-27T13:24:45.132+0000"}, {"author": "Kanwaldeep", "body": "Well HBase doesn't have to down for long time but I'm just trying to understand the worse case scenarios. I'd check out the spark task properties. Writing to HBase using saveAsHadoopDataset.", "created": "2014-01-27T13:45:02.196+0000"}, {"author": "Tathagata Das", "body": "[~kanwal] Do you have a working example now, that you would like to contribute?", "created": "2014-04-25T22:10:35.771+0000"}, {"author": "Kanwaldeep", "body": "Sure. I'd share out an example using Spark Streaming and writing to HBase. Kanwal", "created": "2014-04-28T05:15:16.382+0000"}, {"author": "Tathagata Das", "body": "Would be great if you can submit an example soon, would be great if we can make it to Spark 1.0 ;)", "created": "2014-04-28T23:21:54.465+0000"}, {"author": "Kanwaldeep", "body": "Hi Tathagata I have the sample code ready and is attached. Let me know your feedback. Thanks Kanwal", "created": "2014-05-03T23:35:15.258+0000"}, {"author": "Tathagata Das", "body": "Hi Kanwal, the usual process of the contributing to Spark is through pull requests in Github. That way your name comes up in the change list as a contributor.", "created": "2014-05-15T20:36:41.403+0000"}, {"author": "Theodore michael Malaska", "body": "Thank you for the assign. I will start working on this tomorrow. We will review our approach to HBase and Spark and I will write the patch over the weekend. Thanks again.", "created": "2014-07-11T01:06:50.217+0000"}, {"author": "Tathagata Das", "body": "I am closing this JIRA because this is not relevant any more. For examples, any reader take a look at https://github.com/cloudera-labs/SparkOnHBase/blob/cdh5-0.0.1/src/main/java/com/cloudera/spark/hbase/example/JavaHBaseStreamingBulkPutExample.java", "created": "2015-01-22T00:32:09.481+0000"}], "num_comments": 20, "text": "Issue: SPARK-944\nSummary: Give example of writing to HBase from Spark Streaming\n\nComments (20):\n1. Kanwaldeep: Hi Patrick. I do have working example of writing data to Spark Streaming but running into a into an issue with setting up checkpointing on the context as it is unable to serialize the org.apache.hadoop.mapred.JobConf object Exception in thread \"pool-6-thread-1\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf Any suggestion on how we can fix this? I've tried couple of workarounds so far but no luck. Thanks Kanwal\n2. Tathagata Das: Hi Kanwaldeep, Have you been able to make any progress on this?\n3. Kanwaldeep: Thanks for checking. Not so far. Any pointers to fix this would be very helpful.\n4. Tathagata Das: Can you explain the problem a little bit more. What is the spark streaming program doing? What is your cluster setup? Any logs and stacktraces, sample code for us reproduce the problem? Also, what version of Spark you are using?\n5. Kanwaldeep: Currently using Spark 0.8 version but in the process of building using 0.8.1 Cluster Setup - Local for now Code object KafkMetricStream { def main(args: Array[String]) { if (args.length < 5) { System.err.println(\"Usage: MetricStream <master> <zkQuorum> <group> <topics> <numThreads>\") System.exit(1) } val Array(master, zkQuorum, group, topics, numThreads, hbaseCluster) = args val ssc = new StreamingContext(master, \"KafkaWordCount\", Seconds(2), System.getenv(\"SPARK_HOME\"), Seq(System.getenv(\"SPARK_EXAMPLES_JAR\"))) ssc.checkpoint(\"checkpoint\") val topicpMap = topics.split(\",\").map((_,numThreads.toInt)).toMap val lines = ssc.kafkaStream(zkQuorum, group, topicpMap) val transformLines = lines.mapPartitions(ids => { ids.map(id => gsonconvert(id)) }) val appNodeMetrics = transformLines.flatMap(x => x.toList) // save data to HBase val conf = HBaseConfiguration.create() conf.set(\"hbase.zookeeper.quorum\", hbaseCluster) val jobConfig = new JobConf(conf) jobConfig.setOutputFormat(classOf[TableOutputFormat]) jobConfig.set(TableOutputFormat.OUTPUT_TABLE, \"sinet_test\") appNodeMetrics.foreach(rdd => new PairRDDFunctions(rdd.map(convert)).saveAsHadoopDataset( jobConfig)) ssc.start() } Exception Details Exception in thread \"Thread-35\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1359) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1155) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:422) at org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:152) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:950) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1482) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:329) at org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:112) at org.apache.spark.streaming.Scheduler.doCheckpoint(Scheduler.scala:127) at org.apache.spark.streaming.Scheduler.generateJobs(Scheduler.scala:110) at org.apache.spark.streaming.Scheduler$$anonfun$1.apply$mcVJ$sp(Scheduler.scala:41) at org.apache.spark.streaming.util.RecurringTimer.org$apache$spark$streaming$util$RecurringTimer$$loop(RecurringTimer.scala:66) at org.apache.spark.streaming.util.RecurringTimer$$anon$1.run(RecurringTimer.scala:34) Exception in thread \"pool-5-thread-1\" java.io.NotSerializableException: org.apache.hadoop.mapred.JobConf at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1165) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1359) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1155) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:422) at org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:152) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:950) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1482) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1535) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1413) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1159) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:329) at org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:112) at org.apache.spark.streaming.Scheduler.doCheckpoint(Scheduler.scala:127) at org.apache.spark.streaming.Scheduler.clearOldMetadata(Scheduler.scala:119) at org.apache.spark.streaming.JobManager.org$apache$spark$streaming$JobManager$$clearJob(JobManager.scala:79) at org.apache.spark.streaming.JobManager$JobHandler.run(JobManager.scala:41) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:695)\n6. Tathagata Das: Can you create the jobConfig object within the function in foreach(). So something like this.  appNodeMetrics.foreach(rdd => { val jobConfig = new JobConf(conf) jobConfig.setOutputFormat(classOfTableOutputFormat) jobConfig.set(TableOutputFormat.OUTPUT_TABLE, \"sinet_test\") new PairRDDFunctions(rdd.map(convert)).saveAsHadoopDataset( jobConfig)) }  That should most probably solve it.\n7. Kanwaldeep: Thanks. I had to move the HBaseConfiguration in the foreach as well. I would like to understand if there is any performance impact in doing since for each rdd we create a new config which needs to establish connection with zookeeper and Hbase servers. Will this be done for each task that is created for a batch of streaming data.\n8. Tathagata Das: Well, if I understand correctly, just creating the jobConf object should not be creating any connection. The connection is made in the worker only when data needs to saved. And I believe it is done once per partition, per call to saveAsHadoopDataset(). So here the only cost of moving jobConfig into the foreach is that a JobConf object every time the foreach function is called. I dont think that is too costly, probably takes a few extra milliseconds to create a JobConf object.\n9. Kanwaldeep: Thanks a lot. I'd test it further. Any suggestions on how to best handle failure scenarios writing to HBase? I would like to stop reading messages from Kafka till the problem is resolved i..e make sure all the data read is aggregated and then persisted to HBase.\n10. Tathagata Das: That is tricky to do. Well, what spark streaming would guarantee that the Spark pushes the data into HBase wont complete until the pushing is over. And Spark jobs of next batch wont start until the previous Spark jobs is over. The system would keep receiving data from Kafka, storing them in memory, generating Spark jobs on them and queueing up the Spark jobs from processing. Which may be fine under your circumstances. In future, we are thinking of adding flow control where the receiving will be slowed down if the queue of Spark jobs is too long.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.088541"}}
{"id": "c64f39524ddb7b6e63069898ab37a60a", "issue_key": "SPARK-945", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fix confusing behavior when assembly jars already exist", "description": "If you have multiple assembly jars (e.g. from doing builds with different hadoop versions) `spark-class` gives a very confusing message. We should check for this explicitly. Also we should tell people do to `clean assembly` wherever we document this to avoid this happening.", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "created": "2013-10-25T14:59:33.000+0000", "updated": "2014-03-30T04:15:01.000+0000", "resolved": "2013-10-26T14:51:26.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-945\nSummary: Fix confusing behavior when assembly jars already exist\nDescription: If you have multiple assembly jars (e.g. from doing builds with different hadoop versions) `spark-class` gives a very confusing message. We should check for this explicitly. Also we should tell people do to `clean assembly` wherever we document this to avoid this happening.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.088541"}}
{"id": "051bd7fefa5cb882b447ae0144074ce3", "issue_key": "SPARK-946", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap", "description": "blockToFileSegmentMap right now is taking up ~400 bytes per FileSegment. In large shuffles (e.g., >1000 mappers/executor and >1000 reducers), this can lead to several GB used just for this map, which is leading to OOMs.", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2013-10-26T13:53:24.000+0000", "updated": "2013-11-05T08:13:58.000+0000", "resolved": "2013-11-05T08:13:58.000+0000", "labels": [], "components": [], "comments": [{"author": "Aaron Davidson", "body": "Due to this issue, it'd also be useful if turning off shuffle file consolidation avoided this map entirely, rather than just further exploding the map size.", "created": "2013-10-26T13:56:09.291+0000"}, {"author": "Aaron Davidson", "body": "Fixed by: https://github.com/apache/incubator-spark/pull/130 https://github.com/apache/incubator-spark/pull/139", "created": "2013-11-05T08:13:58.117+0000"}], "num_comments": 2, "text": "Issue: SPARK-946\nSummary: Reduce memory footprint of DiskBlockManager.blockToFileSegmentMap\nDescription: blockToFileSegmentMap right now is taking up ~400 bytes per FileSegment. In large shuffles (e.g., >1000 mappers/executor and >1000 reducers), this can lead to several GB used just for this map, which is leading to OOMs.\n\nComments (2):\n1. Aaron Davidson: Due to this issue, it'd also be useful if turning off shuffle file consolidation avoided this map entirely, rather than just further exploding the map size.\n2. Aaron Davidson: Fixed by: https://github.com/apache/incubator-spark/pull/130 https://github.com/apache/incubator-spark/pull/139", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.088541"}}
{"id": "1f299a204ec45a8eb082fb96ef613f30", "issue_key": "SPARK-947", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Class path issue in case assembled with specific hadoop version", "description": "assemble with hadoop version 1.2.1 using command SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly now we have two jars $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar and $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar compute class path script put both jars in classpath without adding \":\" between them and results in invalid jar name. classpath computed by script:: /data/installs/spark-0.8.0-incubating-bin-hadoop1/conf:/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar:/data/installs/hadoop-1.2.1/conf", "reporter": "shekhar", "assignee": null, "created": "2013-10-28T02:33:11.000+0000", "updated": "2013-11-01T00:32:18.000+0000", "resolved": "2013-11-01T00:32:18.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Reynold Xin", "body": "How did you generate multiple assembled jars? I just tried doing SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly and only see one assembled jar: rxin @ rxin-air : ~/Downloads/spark-0.8.0-incubating-bin-hadoop1 > find . -name \"spark-assembly*\" ./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3 ./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar", "created": "2013-10-31T18:39:12.705+0000"}, {"author": "shekhar", "body": "now I am not able to reproduce it with sbt clean assembly command but i see two jars when i run stb assembly command. steps to reproduce. download spark-0.8.0-incubating-bin-hadoop1.tgz extract it run find command ./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar run assembly command [admin@COL1 spark-0.8.0-incubating-bin-hadoop1]# SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly run find command again ./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar ./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar ./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3", "created": "2013-11-01T00:28:11.141+0000"}, {"author": "Reynold Xin", "body": "Yea I think if you don't include \"clean\" and you have a different hadoop version, the assembly would create another jar resulting in two jars in the target directory. I am going to close this ticket because we already added to the spark-class script to throw a more meaningful error message when this situation happens. https://github.com/apache/incubator-spark/commit/4ba32678e04dc687a9f574eeeb1450e4d291ae1f", "created": "2013-11-01T00:31:55.724+0000"}], "num_comments": 3, "text": "Issue: SPARK-947\nSummary: Class path issue in case assembled with specific hadoop version\nDescription: assemble with hadoop version 1.2.1 using command SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly now we have two jars $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar and $SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar compute class path script put both jars in classpath without adding \":\" between them and results in invalid jar name. classpath computed by script:: /data/installs/spark-0.8.0-incubating-bin-hadoop1/conf:/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.0.4.jar/data/installs/spark-0.8.0-incubating-bin-hadoop1/assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar:/data/installs/hadoop-1.2.1/conf\n\nComments (3):\n1. Reynold Xin: How did you generate multiple assembled jars? I just tried doing SPARK_HADOOP_VERSION=1.2.1 sbt/sbt clean assembly and only see one assembled jar: rxin @ rxin-air : ~/Downloads/spark-0.8.0-incubating-bin-hadoop1 > find . -name \"spark-assembly*\" ./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3 ./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar\n2. shekhar: now I am not able to reproduce it with sbt clean assembly command but i see two jars when i run stb assembly command. steps to reproduce. download spark-0.8.0-incubating-bin-hadoop1.tgz extract it run find command ./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar run assembly command [admin@COL1 spark-0.8.0-incubating-bin-hadoop1]# SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly run find command again ./assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar ./assembly/target/scala-2.9.3/spark-assembly-0.8.0-incubating-hadoop1.2.1.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating.jar ./assembly/target/spark-assembly_2.9.3-0.8.0-incubating-sources.jar ./assembly/target/resolution-cache/org.apache.spark/spark-assembly_2.9.3\n3. Reynold Xin: Yea I think if you don't include \"clean\" and you have a different hadoop version, the assembly would create another jar resulting in two jars in the target directory. I am going to close this ticket because we already added to the spark-class script to throw a more meaningful error message when this situation happens. https://github.com/apache/incubator-spark/commit/4ba32678e04dc687a9f574eeeb1450e4d291ae1f", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.088541"}}
{"id": "cdbca50614b04848f9639f40fb0c6f41", "issue_key": "SPARK-948", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Move \"Classpath Entries\" in WebUI", "description": "Currently the \"Classpath Entries\" section of the application environment page is way down at the bottom of the page, after the typically lengthy \"System Properties\" section. That means that this useful information is usually not visible (or its existence apparent) without scrolling down a fair amount. Since \"Classpath Entries\" is usually short, repositioning it ahead of \"System Properties\" would typically allow the classpaths to be seen along with the first few lines of \"System Properties\", and there would be no sections hidden off the bottom of the page. Just a little better UX.", "reporter": "Mark Hamstra", "assignee": null, "created": "2013-11-03T11:50:10.000+0000", "updated": "2014-11-21T15:11:34.000+0000", "resolved": "2014-11-21T15:11:34.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3170", "created": "2014-11-08T11:35:57.309+0000"}, {"author": "Sean R. Owen", "body": "I'll close as WontFix unless someone agrees with the move and commits the PR. It's quite minor and a question of taste.", "created": "2014-11-19T08:27:09.560+0000"}], "num_comments": 2, "text": "Issue: SPARK-948\nSummary: Move \"Classpath Entries\" in WebUI\nDescription: Currently the \"Classpath Entries\" section of the application environment page is way down at the bottom of the page, after the typically lengthy \"System Properties\" section. That means that this useful information is usually not visible (or its existence apparent) without scrolling down a fair amount. Since \"Classpath Entries\" is usually short, repositioning it ahead of \"System Properties\" would typically allow the classpaths to be seen along with the first few lines of \"System Properties\", and there would be no sections hidden off the bottom of the page. Just a little better UX.\n\nComments (2):\n1. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3170\n2. Sean R. Owen: I'll close as WontFix unless someone agrees with the move and commits the PR. It's quite minor and a question of taste.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.088541"}}
{"id": "3be7c00c541d3afa0d922dfad08a59d0", "issue_key": "SPARK-949", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Turn DAGScheduler into an Actor", "description": "At present {{DAGScheduler}} creates a thread to process events in the {{eventQueue}}, this is a typical situation where Actor is suitable. So how about turn {{DAGScheduler}} into an actor, and send events as messages to it? The new {{DAGScheduler}} will be more efficient and the code will be shorter. Maybe {{JobWaiter}} can also be replaced with {{Future}}.", "reporter": "Frank Dai", "assignee": "liancheng", "created": "2013-11-05T19:24:15.000+0000", "updated": "2013-11-14T18:03:11.000+0000", "resolved": "2013-11-14T18:03:11.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Aaron Davidson", "body": "Fixed in https://github.com/apache/incubator-spark/pull/159", "created": "2013-11-14T18:03:11.433+0000"}], "num_comments": 1, "text": "Issue: SPARK-949\nSummary: Turn DAGScheduler into an Actor\nDescription: At present {{DAGScheduler}} creates a thread to process events in the {{eventQueue}}, this is a typical situation where Actor is suitable. So how about turn {{DAGScheduler}} into an actor, and send events as messages to it? The new {{DAGScheduler}} will be more efficient and the code will be shorter. Maybe {{JobWaiter}} can also be replaced with {{Future}}.\n\nComments (1):\n1. Aaron Davidson: Fixed in https://github.com/apache/incubator-spark/pull/159", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.088541"}}
{"id": "ce121e3f0865adefbc95abdc720e83da", "issue_key": "SPARK-950", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Use faster random number generator for sampling in K-means", "description": "The k-means|| initialization algorithm in MLlib is fairly CPU-bound due to a lot of sampling across the datasets. It uses java.util.Random, but there exist faster RNGs that have equally good properties, like XORshift (http://www.javamex.com/tutorials/random_numbers/xorshift.shtml#.Unq1sZHnaMM). Implementing one of these instead might help. If it's successful, we can also consider using it for RDD.sample().", "reporter": "Matei Alexandru Zaharia", "assignee": "Marek Kolodziej", "created": "2013-11-06T13:34:46.000+0000", "updated": "2013-12-07T14:34:22.000+0000", "resolved": "2013-12-07T14:34:22.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Marek Kolodziej", "body": "I looked into it this morning and it turns out that XORShift is about 3.5x faster than java.util.Random - it generates 100 million random doubles in 1,254 ms instead of 3,928 ms for java.util.Random. This is based on subclassing java.util.Random and overriding next(). However, this is only true in a naive implementation like this one: http://www.javamex.com/tutorials/random_numbers/java_util_random_subclassing.shtml The above performance improvement disappears when trying to make XORShift as thread-safe as java.util.Random, e.g. by making it use an AtomicLong as its seed. When I did that, XORShift was actually a bit slower than java.util.Random - in this particular run, it ran for 4,303 ms as compared to 3,957 ms for java.util.Random. Apparently the speed issue has to do with thread safety rather than java.util.Random's LCG algorithm being slow. Would lack of thread rsafety only affect Spark in local mode since in distributed mode we'd be dealing with separate JVMs? Also, it would seem to me that if each thread had its own instance of the Random object (LCG or XORShift), we wouldn't need AtomicLong with its compare-and-swap. Lastly, even if we retain thread safety and the XORShift algorithm ends up not running faster, it could have substantially better statistical properties, which could be important for data science applications that heavily rely on random number generation (sampling, Monte Carlo simulations, bootstrapping, random weight matrix initialization of neural networks, more accurate draws from the normal distribution using nextGaussian(), etc.).", "created": "2013-11-07T07:48:40.369+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Hi Marek, We don't need it to be thread-safe -- no thread-safe implementation would scale well. If you look at our code, we're creating one Random per task. It's fine if this doesn't even extend Random and if it's is documented as not being thread-safe.", "created": "2013-11-07T10:52:51.988+0000"}, {"author": "Marek Kolodziej", "body": "Hi Matei, That's what I thought, but I wanted to double-check. This would mean that the speed-up would simply have to do with turning off the thread safety while subclassing Random, rather than with an algorithmic change. However, the XORShift algorithm would give better statistical results beyond that, so I think it would be worth it. Feel free to officially assign this Jira ticket to me, and I'll go ahead and start integrating the code and testing it. Thanks! Marek", "created": "2013-11-07T11:00:50.377+0000"}, {"author": "Marek Kolodziej", "body": "I'm in a bit of a predicament. Most of Spark's code is in Scala, and I would love to write the new random number generator in Scala. However, I wrote both a Scala and a Java version, and the Scala version performs half as fast. The Scala code is an exact port of the Java code (save for the uniquifier preincrement in Java and postincrement in Scala, which is immaterial) - so it's imperative and mutable, rather than functional and immutable - therefore, one can't talk about the pentalty having to do with immutable constructs, or the JVM not inlining lambdas efficiently (http://www.azulsystems.com/blog/cliff/2011-04-04-fixing-the-inlining-problem). Here's the sample code. On my laptop, the Java code took 426 ms to run, while the Scala code took 873 ms. Also note that java.util.Random took 1,385 ms. Also note that these estimates were based on 100 million calls to nextInt() - the Scala version performs much worse if nextDouble() or nextGaussian() is called - which may not be important for the use of the new random number generator for sampling, but it may be important for other data science applications such as Monte Carlo simulations. My recommendation is that for performance reasons, the random number generator be implemented in Java. Apparently there are cases in which Scala's compiler doesn't optimize as well as Java's. I would like to get feedback before proceeding. I'm pasting the code below. Note that the code formatting isn't necessarily the target formatting/style that would be committed - Jira is changing my original formating by introducing undesired inlining, etc.  // Java public class XORShiftRandom extends Random { private static final long serialVersionUID = -6177051610615407537L; // same as in java.util.Random - including here since it can't be inherited private static volatile long seedUniquifier = 8682522807148012L; private long seed = System.nanoTime(); public XORShiftRandom() { this(++seedUniquifier + System.nanoTime()); } public XORShiftRandom(final long seed) { super(); this.seed = seed; } @Override protected int next(final int bits) { long nextSeed = seed ^ (seed << 21); nextSeed ^= (nextSeed >>> 35); nextSeed ^= (nextSeed << 4); seed = nextSeed; return (int) (nextSeed & ((1L << bits) -1)); } }   // Scala import XORShiftRandom.seedUniquifier class XORShiftRandom(init: Long) extends java.util.Random(init) { def this() = { this(seedUniquifier + System.nanoTime()) seedUniquifier += 1 } var seed = init override protected def next(bits: Int): Int = { var nextSeed = seed ^ (seed << 21) nextSeed ^= (nextSeed >>> 35) nextSeed ^= (nextSeed << 4) seed = nextSeed (nextSeed & ((1L << bits) -1)).asInstanceOf[Int] } } object XORShiftRandom { val serialVersionUID = -6177051610615407537L var seedUniquifier = 8682522807148012L }", "created": "2013-11-09T10:20:36.127+0000"}, {"author": "Marek Kolodziej", "body": "BTW, the substantially varying benchmark execution times have to do with my switching machines, but for any given comparison, the execution was obviously done on the same machine - e.g. the Java/Scala comparisons from the post above were done on one machine, while the previous java.util.Random and XORShift results were obtained on another - but the comparison done at a given time was done on the same hardware.", "created": "2013-11-09T11:39:03.116+0000"}, {"author": "Reynold Xin", "body": "What if you change the definition of seed from var to  private[this] var", "created": "2013-11-09T11:42:15.119+0000"}, {"author": "Marek Kolodziej", "body": "Thanks Reynold! I tried it, and it doesn't confer an execution speed benefit. Of course it is nicer from an access control perspective, but the runtime bahavior remained the same. The testing was done with Scala 2.9.3 and OpenJDK/JRE 7 on Ubuntu, but I also tested with Sun JRE 7 on OS X. The irony is that the bytecode for the next() method is exactly the same for Scala as it is for Java. Java:  protected int next(int); Code: Stack=6, Locals=4, Args_size=2 0: aload_0 1: getfield #5; //Field seed:J 4: aload_0 5: getfield #5; //Field seed:J 8: bipush 21 10: lshl 11: lxor 12: lstore_2 13: lload_2 14: lload_2 15: bipush 35 17: lushr 18: lxor 19: lstore_2 20: lload_2 21: lload_2 22: iconst_4 23: lshl 24: lxor 25: lstore_2 26: aload_0 27: lload_2 28: putfield #5; //Field seed:J 31: lload_2 32: lconst_1 33: iload_1 34: lshl 35: lconst_1 36: lsub 37: land 38: l2i 39: ireturn  Scala:  public int next(int); Code: Stack=6, Locals=4, Args_size=2 0: aload_0 1: getfield #31; //Field seed:J 4: aload_0 5: getfield #31; //Field seed:J 8: bipush 21 10: lshl 11: lxor 12: lstore_2 13: lload_2 14: lload_2 15: bipush 35 17: lushr 18: lxor 19: lstore_2 20: lload_2 21: lload_2 22: iconst_4 23: lshl 24: lxor 25: lstore_2 26: aload_0 27: lload_2 28: putfield #31; //Field seed:J 31: lload_2 32: lconst_1 33: iload_1 34: lshl 35: lconst_1 36: lsub 37: lando 38: l2i 39: ireturn  If the bytecode is identical, then why is Scala half as fast as Java? Is there a way to get a hold of a profiling tool like YourKit? I'm not yet an Apache committer but I don't have a profiler in Eclipse. JVisualVM didn't show anytthing beyond the main() method, it didn't show the call to next() of the XORShiftRandom object. I know that a real profiler would show where the bottleneck is.", "created": "2013-11-09T13:05:56.915+0000"}, {"author": "Reynold Xin", "body": "How are you testing the runtime?", "created": "2013-11-09T13:22:54.016+0000"}, {"author": "Marek Kolodziej", "body": "So far I just ran the tests in Eclipse, using Scala and Java runtimes, respectively. Of course since the Scala runtime is actually just the Java runtime with scala-library.jar etc. on the classpath, the results should theoretically be the same.  object SpeedTest extends App { val r = new XORShiftRandom() val start = System.currentTimeMillis for (i <- 1 to 100000000) r.nextInt val end = System.currentTimeMillis println(end - start) }   public class RandomTest { public static void main(final String[] args) { final Random rand = new XORShiftRandom(); final long start = System.currentTimeMillis(); for (int i = 0; i <= 100000000; i++) { rand.nextInt(); } final long end = System.currentTimeMillis(); System.out.println(end - start); } }  At first I thought that it was an Eclipse config/env. var. issue, but I compiled the classes and ran them from the command line, with the same results. Like I mentioned, I couldn't learn anything interesting using JVisualVM, because it doesn't seem to show nested calls the way YourKit, JProfiler or even the Netbeans profiler would.", "created": "2013-11-09T13:33:49.838+0000"}, {"author": "Reynold Xin", "body": "Ah I see. Try this with Scala (basically replace the for loop with the while loop to avoid closure in this case)  object SpeedTest extends App { val r = new XORShiftRandom() var sum = 0 val start = System.currentTimeMillis var i = 0 while (i < 100000000) { sum |= r.nextInt i += 1 } val end = System.currentTimeMillis println(end - start) println(sum) }  You should also change the Java implementation to do the same thing with a sum variable to make sure JIT doesn't eliminate the nextInt call as dead code ...", "created": "2013-11-09T13:37:22.291+0000"}, {"author": "Marek Kolodziej", "body": "Interesting. In the case of Scala, I forgot that for is really just syntactic sugar for map/flatMap/filter, so that could be part of the problem. Would the JIT really not optimize a regular Java for loop? I mean yes, we're not holding state, but the code is heavily iterative.", "created": "2013-11-09T14:20:21.295+0000"}, {"author": "Matei Alexandru Zaharia", "body": "The JIT and Scala compiler does indeed not fully optimize a Range.foreach(). By the way, are you running this multiple times in the same process? Usually the JIT takes time to compile code, and you don't want it to be warming up for different amounts of time based on other stuff in the program. The best way to run such benchmarks is to do them 3-4 times in the same process and see if the running times stabilize. You could also try a tool like this: https://code.google.com/p/caliper/.", "created": "2013-11-09T23:22:23.384+0000"}, {"author": "Marek Kolodziej", "body": "Hi Matei, very good point. I should have indeed let it stabilize. I will check out Caliper. By the way, are there good references for what the standard Hotspot optimizes, and which features of Scala's core are not optimized?", "created": "2013-11-10T03:57:08.650+0000"}, {"author": "Marek Kolodziej", "body": "Also, while the XORShift algorithm had already been peer-reviewed and had been showed to pass more statistical tests than the LCG algorithm used by java.util.Random, I was wondering if you would require one or more regression tests for the test suite? If so, I was thinking that a low-hanging fruit case would be a chi-square test. Including a Kolmogorov-Smirnov, serial correlation and other tests would be useful for proving that a particular algorithm has good statistical properties overall, but we already know that with XORShift. I gues s a chi-square test would be good to prevent fatal code regression that would negatively affect sampling. Of course, given a fixed seed, we don't even need a statistical test, we could just test the bins for exact values rather than for passing at some particular significance level. Any preferences with regard to regression tests? Finally, is it OK to just run some benchmarks using actual datasets, or microbenchmarks for the RNG and trying out the execution speed with java.util.Random and XORShift, or would you like benchmarking tests as part of the test suite? In the latter case it might change the code base since it would require one to provide the particular RNG implementations for testing, so it could use implicit vals that are overridden for testing purposes. The question is though whether that would be necessary in the long run, or just important for the sake of proving the concept of the new RNG being faster.", "created": "2013-11-10T11:22:44.611+0000"}, {"author": "Aaron Davidson", "body": "As far as tests go, I think simpler is better, as ultimately the community has to be able to maintain them. As you said, we don't need to prove statistical soundness of the algorithm, just correctness of the implementation. Just a chi-squared test over a hundred million values would sound reasonable to me. I also think that just adding a test to compare the execution of generating N random numbers using XORShift versus Random would be good. Not only would this demonstrate in code that the new generator has a real benefit over Random and prevent regressions, but it would also allow us to be alerted if future Java implementations become faster.", "created": "2013-11-14T18:00:52.772+0000"}, {"author": "Marek Kolodziej", "body": "Thanks Aaron! In that case, I'll include a chi-squared test for regressions, and a microbenchmark for a comparison against java.util.Random's speed. In fact, it could all be done in one execution, but as two assertions.", "created": "2013-11-15T07:39:57.801+0000"}, {"author": "Marek Kolodziej", "body": "After the JIT warm-up time (> 10k iterations for both Random instance), the benchmarks that I got for XORShiftRandom and java.util.Random were ~460 ms and 1,200 ms, respectively. This means that XORShift (without Random's thread safety) took only 38% of the time it took java.util.Random to generate 100 million random numbers. I wrapped the while loop in a class and an implicit def to ease the pain of writing loops that optimize well yet are convenient to use. In Scala 2.10+, I would have used an implicit class, but in 2.9.3 I used a class and an implicit def.  class TimesInt(i: Int) { def times(f: => Unit) = { var x = 1 while (x <= i) { f x += 1 } } } implicit def intToTimesInt(i: Int) = new TimesInt(i) def timeIt(f: => Unit, iters: Int): Long = { val start = System.currentTimeMillis iters.times(f) System.currentTimeMillis - start }  Here's a sample result from the run after the JIT had a chance to work.  scala> timeIt(javaRand.nextInt, 100000000) res10: Long = 1199 scala> timeIt(xorShift.nextInt, 100000000) res11: Long = 379  Regarding applying unit tests to get specific performance improvements of XORShift over java.util.Random, it's clear that the exact numbers will not be deterministic, or even approximate if the build tool is doing a lot of things in a multithreaded way that can overwhelm the CPU. For instance, if we compare the above results to ones that show up by running the test in SBT, we will see that the SBT results are worse:  > test-only org.apache.spark.util.RandomNumberGeneratorSuite [info] RandomNumberGeneratorSuite: javaTime 2117.0 xorTime 1275.0 [info] - XORShift should be faster than java.util.Random [info] - XORShift generates valid random numbers [info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0  Thus, I think that for the simple timing test in the test suite, it would be enough to check that the XORShiftRandom RNG is faster than java.util.Random, but even that might not be enough in non-deterministic utilization of the JVM. So perhaps it would be enough to do the microbenchmarks and a real-world dataset timing with the old and new RNGs, and just keep the chi-squared test as a regression test. However, Aaron's point regarding the test that verifies that the new RNG is faster than java.util.Random would be helpful in case the JDK one improves in the future. I have the code and the unit tests, and will issue a pull request once I verify the execution performance against a reasonably-sized dataset. That is my plan for tomorrow.", "created": "2013-11-16T20:21:51.926+0000"}, {"author": "Reynold Xin", "body": "I wouldn't worry about putting performance tests in unit tests. If you want, you can make a short standalone program (i.e. a scala object with main) to evaluate performance, and check that into the codebase. But really no need to run that every time unit tests are run.", "created": "2013-11-16T21:46:10.028+0000"}, {"author": "Marek Kolodziej", "body": "Great! I also thought it would be more reasonable, from both the unit test execution time perspective and the deterministic aspect.", "created": "2013-11-17T07:26:06.639+0000"}, {"author": "Marek Kolodziej", "body": "I am glad to report that the savings for even a modest-sized dataset are considerable. I ran K-means in stand-alone mode over a single 386 MiB file on a 4-core, 16 GiB machine, with 4 threads. The file contained 5 million records. Each line looked as follows: 0002322919CBB308E9FCC984D438164CEE201F9D,0,19,0,0,0,0,0,0,0,0,0,0,0,0,0,0,27,0,0,0 where the first element was a user ID and the next 20 elements represented the number of times that a user watched a video from a given IAB (Internet Advertising Bureau) category. This dataset is private to my company so I can't share it, but I used it for the benchmark to test a real-world case. I ran Spark with both the java.util.Random and the XORShift RNG implementation 10 times, and the average execution times were as follows: java.util.Random: 614.5 seconds XORShift: 600.6 seconds Thus, for this particular test, the time savings were ~2.3%. Note that I didn't just change the RNG in K-means.scala for this, I applied it to RDD.scala as well. I was expecting the savings in the case of K-means to be relatively small. However, I would like to make the claim that changing the RNG in the RDD class might have a much larger benefit for certain cases that require more random numbers to be generated, e.g. for Monte Carlo simulations, bootstrap algorithms, etc. So, with K-means we probably see a lower bound of the benefit, and other algorithms could benefit even more from faster sampling. I will issue a pull request today.", "created": "2013-11-18T04:16:42.454+0000"}, {"author": "Marek Kolodziej", "body": "I just sent the pull request: https://github.com/apache/incubator-spark/pull/185 Note that in addition to implementing the XORShift RNG and adding a unit test, I also updated KMeans.scala and RDD.scala to use the new RNG. I mentioned the \"real-world\" dataset time savings in my previous JIRA comment. For a microbenchmark, try the one described in the Git commit message.", "created": "2013-11-18T12:30:09.838+0000"}], "num_comments": 21, "text": "Issue: SPARK-950\nSummary: Use faster random number generator for sampling in K-means\nDescription: The k-means|| initialization algorithm in MLlib is fairly CPU-bound due to a lot of sampling across the datasets. It uses java.util.Random, but there exist faster RNGs that have equally good properties, like XORshift (http://www.javamex.com/tutorials/random_numbers/xorshift.shtml#.Unq1sZHnaMM). Implementing one of these instead might help. If it's successful, we can also consider using it for RDD.sample().\n\nComments (21):\n1. Marek Kolodziej: I looked into it this morning and it turns out that XORShift is about 3.5x faster than java.util.Random - it generates 100 million random doubles in 1,254 ms instead of 3,928 ms for java.util.Random. This is based on subclassing java.util.Random and overriding next(). However, this is only true in a naive implementation like this one: http://www.javamex.com/tutorials/random_numbers/java_util_random_subclassing.shtml The above performance improvement disappears when trying to make XORShift as thread-safe as java.util.Random, e.g. by making it use an AtomicLong as its seed. When I did that, XORShift was actually a bit slower than java.util.Random - in this particular run, it ran for 4,303 ms as compared to 3,957 ms for java.util.Random. Apparently the speed issue has to do with thread safety rather than java.util.Random's LCG algorithm being slow. Would lack of thread rsafety only affect Spark in local mode since in distributed mode we'd be dealing with separate JVMs? Also, it would seem to me that if each thread had its own instance of the Random object (LCG or XORShift), we wouldn't need AtomicLong with its compare-and-swap. Lastly, even if we retain thread safety and the XORShift algorithm ends up not running faster, it could have substantially better statistical properties, which could be important for data science applications that heavily rely on random number generation (sampling, Monte Carlo simulations, bootstrapping, random weight matrix initialization of neural networks, more accurate draws from the normal distribution using nextGaussian(), etc.).\n2. Matei Alexandru Zaharia: Hi Marek, We don't need it to be thread-safe -- no thread-safe implementation would scale well. If you look at our code, we're creating one Random per task. It's fine if this doesn't even extend Random and if it's is documented as not being thread-safe.\n3. Marek Kolodziej: Hi Matei, That's what I thought, but I wanted to double-check. This would mean that the speed-up would simply have to do with turning off the thread safety while subclassing Random, rather than with an algorithmic change. However, the XORShift algorithm would give better statistical results beyond that, so I think it would be worth it. Feel free to officially assign this Jira ticket to me, and I'll go ahead and start integrating the code and testing it. Thanks! Marek\n4. Marek Kolodziej: I'm in a bit of a predicament. Most of Spark's code is in Scala, and I would love to write the new random number generator in Scala. However, I wrote both a Scala and a Java version, and the Scala version performs half as fast. The Scala code is an exact port of the Java code (save for the uniquifier preincrement in Java and postincrement in Scala, which is immaterial) - so it's imperative and mutable, rather than functional and immutable - therefore, one can't talk about the pentalty having to do with immutable constructs, or the JVM not inlining lambdas efficiently (http://www.azulsystems.com/blog/cliff/2011-04-04-fixing-the-inlining-problem). Here's the sample code. On my laptop, the Java code took 426 ms to run, while the Scala code took 873 ms. Also note that java.util.Random took 1,385 ms. Also note that these estimates were based on 100 million calls to nextInt() - the Scala version performs much worse if nextDouble() or nextGaussian() is called - which may not be important for the use of the new random number generator for sampling, but it may be important for other data science applications such as Monte Carlo simulations. My recommendation is that for performance reasons, the random number generator be implemented in Java. Apparently there are cases in which Scala's compiler doesn't optimize as well as Java's. I would like to get feedback before proceeding. I'm pasting the code below. Note that the code formatting isn't necessarily the target formatting/style that would be committed - Jira is changing my original formating by introducing undesired inlining, etc.  // Java public class XORShiftRandom extends Random { private static final long serialVersionUID = -6177051610615407537L; // same as in java.util.Random - including here since it can't be inherited private static volatile long seedUniquifier = 8682522807148012L; private long seed = System.nanoTime(); public XORShiftRandom() { this(++seedUniquifier + System.nanoTime()); } public XORShiftRandom(final long seed) { super(); this.seed = seed; } @Override protected int next(final int bits) { long nextSeed = seed ^ (seed << 21); nextSeed ^= (nextSeed >>> 35); nextSeed ^= (nextSeed << 4); seed = nextSeed; return (int) (nextSeed & ((1L << bits) -1)); } }   // Scala import XORShiftRandom.seedUniquifier class XORShiftRandom(init: Long) extends java.util.Random(init) { def this() = { this(seedUniquifier + System.nanoTime()) seedUniquifier += 1 } var seed = init override protected def next(bits: Int): Int = { var nextSeed = seed ^ (seed << 21) nextSeed ^= (nextSeed >>> 35) nextSeed ^= (nextSeed << 4) seed = nextSeed (nextSeed & ((1L << bits) -1)).asInstanceOf[Int] } } object XORShiftRandom { val serialVersionUID = -6177051610615407537L var seedUniquifier = 8682522807148012L }\n5. Marek Kolodziej: BTW, the substantially varying benchmark execution times have to do with my switching machines, but for any given comparison, the execution was obviously done on the same machine - e.g. the Java/Scala comparisons from the post above were done on one machine, while the previous java.util.Random and XORShift results were obtained on another - but the comparison done at a given time was done on the same hardware.\n6. Reynold Xin: What if you change the definition of seed from var to  private[this] var\n7. Marek Kolodziej: Thanks Reynold! I tried it, and it doesn't confer an execution speed benefit. Of course it is nicer from an access control perspective, but the runtime bahavior remained the same. The testing was done with Scala 2.9.3 and OpenJDK/JRE 7 on Ubuntu, but I also tested with Sun JRE 7 on OS X. The irony is that the bytecode for the next() method is exactly the same for Scala as it is for Java. Java:  protected int next(int); Code: Stack=6, Locals=4, Args_size=2 0: aload_0 1: getfield #5; //Field seed:J 4: aload_0 5: getfield #5; //Field seed:J 8: bipush 21 10: lshl 11: lxor 12: lstore_2 13: lload_2 14: lload_2 15: bipush 35 17: lushr 18: lxor 19: lstore_2 20: lload_2 21: lload_2 22: iconst_4 23: lshl 24: lxor 25: lstore_2 26: aload_0 27: lload_2 28: putfield #5; //Field seed:J 31: lload_2 32: lconst_1 33: iload_1 34: lshl 35: lconst_1 36: lsub 37: land 38: l2i 39: ireturn  Scala:  public int next(int); Code: Stack=6, Locals=4, Args_size=2 0: aload_0 1: getfield #31; //Field seed:J 4: aload_0 5: getfield #31; //Field seed:J 8: bipush 21 10: lshl 11: lxor 12: lstore_2 13: lload_2 14: lload_2 15: bipush 35 17: lushr 18: lxor 19: lstore_2 20: lload_2 21: lload_2 22: iconst_4 23: lshl 24: lxor 25: lstore_2 26: aload_0 27: lload_2 28: putfield #31; //Field seed:J 31: lload_2 32: lconst_1 33: iload_1 34: lshl 35: lconst_1 36: lsub 37: lando 38: l2i 39: ireturn  If the bytecode is identical, then why is Scala half as fast as Java? Is there a way to get a hold of a profiling tool like YourKit? I'm not yet an Apache committer but I don't have a profiler in Eclipse. JVisualVM didn't show anytthing beyond the main() method, it didn't show the call to next() of the XORShiftRandom object. I know that a real profiler would show where the bottleneck is.\n8. Reynold Xin: How are you testing the runtime?\n9. Marek Kolodziej: So far I just ran the tests in Eclipse, using Scala and Java runtimes, respectively. Of course since the Scala runtime is actually just the Java runtime with scala-library.jar etc. on the classpath, the results should theoretically be the same.  object SpeedTest extends App { val r = new XORShiftRandom() val start = System.currentTimeMillis for (i <- 1 to 100000000) r.nextInt val end = System.currentTimeMillis println(end - start) }   public class RandomTest { public static void main(final String[] args) { final Random rand = new XORShiftRandom(); final long start = System.currentTimeMillis(); for (int i = 0; i <= 100000000; i++) { rand.nextInt(); } final long end = System.currentTimeMillis(); System.out.println(end - start); } }  At first I thought that it was an Eclipse config/env. var. issue, but I compiled the classes and ran them from the command line, with the same results. Like I mentioned, I couldn't learn anything interesting using JVisualVM, because it doesn't seem to show nested calls the way YourKit, JProfiler or even the Netbeans profiler would.\n10. Reynold Xin: Ah I see. Try this with Scala (basically replace the for loop with the while loop to avoid closure in this case)  object SpeedTest extends App { val r = new XORShiftRandom() var sum = 0 val start = System.currentTimeMillis var i = 0 while (i < 100000000) { sum |= r.nextInt i += 1 } val end = System.currentTimeMillis println(end - start) println(sum) }  You should also change the Java implementation to do the same thing with a sum variable to make sure JIT doesn't eliminate the nextInt call as dead code ...", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.088541"}}
{"id": "abf65df372dbf28276ecead5ef6485be", "issue_key": "SPARK-951", "issue_type": "Story", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Gaussian Mixture Model", "description": "This code includes the code for Gaussian Mixture Model. The input file named Gmm_spark.tbl is the input for this program.", "reporter": "caizhua", "assignee": null, "created": "2013-11-09T21:30:13.000+0000", "updated": "2014-11-08T09:38:47.000+0000", "resolved": "2014-11-08T09:38:47.000+0000", "labels": ["Learning", "Machine", "Model"], "components": ["Examples"], "comments": [{"author": "Anant Daksh Asthana", "body": "caizhua Could you please elaborate a little more on the issue? right now 'This code' and 'input file named Gmm_spark.tbl' are unknown to me at the time of reading this", "created": "2014-09-19T19:18:21.977+0000"}, {"author": "Sean R. Owen", "body": "Duplicate of I assume this is superseded, if anything, by SPARK-3588", "created": "2014-11-08T09:38:47.381+0000"}], "num_comments": 2, "text": "Issue: SPARK-951\nSummary: Gaussian Mixture Model\nDescription: This code includes the code for Gaussian Mixture Model. The input file named Gmm_spark.tbl is the input for this program.\n\nComments (2):\n1. Anant Daksh Asthana: caizhua Could you please elaborate a little more on the issue? right now 'This code' and 'input file named Gmm_spark.tbl' are unknown to me at the time of reading this\n2. Sean R. Owen: Duplicate of I assume this is superseded, if anything, by SPARK-3588", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "8099ab436c6c62ed05c1ef98c3b30c9b", "issue_key": "SPARK-952", "issue_type": "Story", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Python version of Gaussian Mixture Model", "description": "This piece of code is written by Shangyu Luo at Rice University. The code is to learn the Gaussian Mixture Model.", "reporter": "caizhua", "assignee": null, "created": "2013-11-09T21:37:26.000+0000", "updated": "2014-11-08T09:38:22.000+0000", "resolved": "2014-11-08T09:38:22.000+0000", "labels": ["Learning"], "components": ["Examples"], "comments": [{"author": "Sean R. Owen", "body": "I assume this is superseded, if anything, by SPARK-3588", "created": "2014-11-08T09:38:22.708+0000"}], "num_comments": 1, "text": "Issue: SPARK-952\nSummary: Python version of Gaussian Mixture Model\nDescription: This piece of code is written by Shangyu Luo at Rice University. The code is to learn the Gaussian Mixture Model.\n\nComments (1):\n1. Sean R. Owen: I assume this is superseded, if anything, by SPARK-3588", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "1e5cc52178c99eded2c1c1e6d28d3de3", "issue_key": "SPARK-953", "issue_type": "Story", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Latent Dirichlet Association (LDA model)", "description": "This code is for learning the LDA model. However, if our input is 2.5 M documents per machine, a dictionary with 10000 words, running in EC2 m2.4xlarge instance with 68 G memory each machine. The time is really really slow. For five iterations, the time cost is 8145, 24725, 51688, 58674, 56850 seconds. The time for shuffling is quite slow. The LDA.tbl is the simulated data set for the program, and it is quite fast.", "reporter": "caizhua", "assignee": null, "created": "2013-11-09T21:44:44.000+0000", "updated": "2014-08-27T17:37:10.000+0000", "resolved": "2014-08-27T17:37:10.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Masaki Rikitoku", "body": "Latent Dirichlet Allocation?", "created": "2014-07-14T03:34:07.675+0000"}, {"author": "Masaki Rikitoku", "body": "parallel gibbs sampling for lda (plda) may be usable.", "created": "2014-07-16T13:02:20.663+0000"}], "num_comments": 2, "text": "Issue: SPARK-953\nSummary: Latent Dirichlet Association (LDA model)\nDescription: This code is for learning the LDA model. However, if our input is 2.5 M documents per machine, a dictionary with 10000 words, running in EC2 m2.4xlarge instance with 68 G memory each machine. The time is really really slow. For five iterations, the time cost is 8145, 24725, 51688, 58674, 56850 seconds. The time for shuffling is quite slow. The LDA.tbl is the simulated data set for the program, and it is quite fast.\n\nComments (2):\n1. Masaki Rikitoku: Latent Dirichlet Allocation?\n2. Masaki Rikitoku: parallel gibbs sampling for lda (plda) may be usable.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "6445d34762aeca8811776821245259a4", "issue_key": "SPARK-954", "issue_type": "Story", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "One repeated sampling, and I am not sure if it is correct.", "description": "This piece of code reads the dataset, and then has two operations on the dataset. If I consider the RDD as a view definition, I think the result is correct. However, since the first iteration does result_sample.count(), then I was wondering whether we should repeat the computation in the initialize_doc_topic_word_count(.) function, when we run the the second result_sample.map(lambda (block_id, doc_prob): doc_prob).count(). Since people write Spark as a program not as a database view, sometimes it is confusing. For example, considering there initialize_doc_topic_word_count(.) is a statistical function with runtime seeds, I am not sure if this have impact on the result.", "reporter": "caizhua", "assignee": null, "created": "2013-11-09T21:53:41.000+0000", "updated": "2014-11-08T09:42:12.000+0000", "resolved": "2014-11-08T09:42:12.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Spark's lineage-based fault-tolerance assumes that your RDD transformations are deterministic. Your code is seeding a shared RNG based on the time that the task is executed on the worker:  def initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size): # initialize the seed import pygsl.rng r = pygsl.rng.rng() st = datetime.now() seed = st.hour*60*60+st.minute*60+st.second seed = seed*1000 print \"--------------------------step 0: initialization-------------------------------\" alpha = np.zeros(topic_size, float) alpha.fill(1.0) doc_prob = {} for (doc_id, word_count_list) in doc_word_count_list: r.set(seed+doc_id) prob = r.dirichlet(alpha) doc_prob[doc_id] = prob return doc_prob  If a task that uses this function is recomputed, it won't produce the same result because the RNG seed will have changed. In your example, this is leading to incorrect results:  result_sample = block_doc_word_count.map(lambda (block_id, doc_word_count_list): (block_id, initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size))) print \"count before projection = \", result_sample.count() print \"count after projection = \", result_sample.map(lambda (block_id, doc_prob): doc_prob).count()  Here, the {{result_sample.map()}} is evaluated twice with different RNG seeds, leading to different results. To avoid these issues, you should ensure that your transformations are deterministic. In this case, I recommend generating a list of RNG seeds in the driver program and writing your map function using {{mapPartitionsWithIndex}}; this will allow you to index into the list of RNG seeds based on the current partition index, ensuring determinism.", "created": "2013-11-10T17:36:27.334+0000"}, {"author": "caizhua", "body": "That is correct! I agree with that, and it should be correct for keeping trace of the randomness. If RDD was consider as a view in database, it is correct as I said. The question I have is whether it should be considered as a view, since the data flow itself can be considered as a logical plan for database SQL queries. And like this case, block_doc_word_count is a vertex in a graph, and it has two leaves, whether the DAG should recognize it and save the computation. Anyway, I did not intend to report it as a bug, just as a story. :) I like Spark very much.", "created": "2013-11-10T20:49:18.516+0000"}, {"author": "Josh Rosen", "body": "The Spark scheduler doesn't get to see a logical plan up-front for your entire-program. At the time that you call the first {{count()}} action, Spark doesn't know that you're going to subsequently call another {{map()}} and {{count()}} on the same RDD; it only knows about the portion of the DAG that's been defined up to that point in the driver program. We can express fairly general DAGs, but the current scheduler doesn't allow us to run multiple actions/queries in parallel and share common parts of their query plans. In many cases, you can work around this by using accumulators (so you could perform the pre-projection {{count()}} inside the projection's {{map()}} transformation). (Aside: accumulators currently have slightly different semantics than running two actions in parallel; see SPARK-732) Even if we did have automatic caching (I think that it's still possible, even in the current model), it will be risky to have programs whose output changes depending on whether an RDD is cached or not.", "created": "2013-11-11T11:08:27.947+0000"}, {"author": "Sean R. Owen", "body": "From the discussion, and later ones about guarantees of determinism in RDDs, sounds like this is working as intended.", "created": "2014-11-08T09:42:12.061+0000"}], "num_comments": 4, "text": "Issue: SPARK-954\nSummary: One repeated sampling, and I am not sure if it is correct.\nDescription: This piece of code reads the dataset, and then has two operations on the dataset. If I consider the RDD as a view definition, I think the result is correct. However, since the first iteration does result_sample.count(), then I was wondering whether we should repeat the computation in the initialize_doc_topic_word_count(.) function, when we run the the second result_sample.map(lambda (block_id, doc_prob): doc_prob).count(). Since people write Spark as a program not as a database view, sometimes it is confusing. For example, considering there initialize_doc_topic_word_count(.) is a statistical function with runtime seeds, I am not sure if this have impact on the result.\n\nComments (4):\n1. Josh Rosen: Spark's lineage-based fault-tolerance assumes that your RDD transformations are deterministic. Your code is seeding a shared RNG based on the time that the task is executed on the worker:  def initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size): # initialize the seed import pygsl.rng r = pygsl.rng.rng() st = datetime.now() seed = st.hour*60*60+st.minute*60+st.second seed = seed*1000 print \"--------------------------step 0: initialization-------------------------------\" alpha = np.zeros(topic_size, float) alpha.fill(1.0) doc_prob = {} for (doc_id, word_count_list) in doc_word_count_list: r.set(seed+doc_id) prob = r.dirichlet(alpha) doc_prob[doc_id] = prob return doc_prob  If a task that uses this function is recomputed, it won't produce the same result because the RNG seed will have changed. In your example, this is leading to incorrect results:  result_sample = block_doc_word_count.map(lambda (block_id, doc_word_count_list): (block_id, initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size))) print \"count before projection = \", result_sample.count() print \"count after projection = \", result_sample.map(lambda (block_id, doc_prob): doc_prob).count()  Here, the {{result_sample.map()}} is evaluated twice with different RNG seeds, leading to different results. To avoid these issues, you should ensure that your transformations are deterministic. In this case, I recommend generating a list of RNG seeds in the driver program and writing your map function using {{mapPartitionsWithIndex}}; this will allow you to index into the list of RNG seeds based on the current partition index, ensuring determinism.\n2. caizhua: That is correct! I agree with that, and it should be correct for keeping trace of the randomness. If RDD was consider as a view in database, it is correct as I said. The question I have is whether it should be considered as a view, since the data flow itself can be considered as a logical plan for database SQL queries. And like this case, block_doc_word_count is a vertex in a graph, and it has two leaves, whether the DAG should recognize it and save the computation. Anyway, I did not intend to report it as a bug, just as a story. :) I like Spark very much.\n3. Josh Rosen: The Spark scheduler doesn't get to see a logical plan up-front for your entire-program. At the time that you call the first {{count()}} action, Spark doesn't know that you're going to subsequently call another {{map()}} and {{count()}} on the same RDD; it only knows about the portion of the DAG that's been defined up to that point in the driver program. We can express fairly general DAGs, but the current scheduler doesn't allow us to run multiple actions/queries in parallel and share common parts of their query plans. In many cases, you can work around this by using accumulators (so you could perform the pre-projection {{count()}} inside the projection's {{map()}} transformation). (Aside: accumulators currently have slightly different semantics than running two actions in parallel; see SPARK-732) Even if we did have automatic caching (I think that it's still possible, even in the current model), it will be risky to have programs whose output changes depending on whether an RDD is cached or not.\n4. Sean R. Owen: From the discussion, and later ones about guarantees of determinism in RDDs, sounds like this is working as intended.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "72744fc86d362290001eeab6b925723a", "issue_key": "SPARK-955", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Paritions increase expotentially when doing cartisian product in loop program", "description": "Paritions increase expotentially when doing cartisian product in loop program. An example program is as follows: block_doc_word_count = lines.map(parseVector).groupBy(lambda (doc_id, word_count): (doc_id % doc_block)) block_dwc_dtp_twc = block_doc_word_count.map(lambda (block_id, doc_word_count_list): (block_id, initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size))).persist(StorageLevel.MEMORY_ONLY) for loop in range(0, max_superstep): topic_word_count_map = block_dwc_dtp_twc.flatMap(lambda (block_id, (doc_word_count_list, doc_prob, topic_word_count)): topic_word_count).reduceByKey(lambda para1, para2: addWordCount(para1, para2)) topic_word_prob_block = topic_word_count_map.mapValues(lambda wordcount_map: sample_topic_word_prob(wordcount_map, dic_size)).groupBy(lambda (topic_id, topic_word_prob): topic_id % 1) block_dwc_dtp_twc = block_dwc_dtp_twc.cartesian(topic_word_prob_block).map(lambda (block_dwc_dtp_twc_temp, topic_word_prob_block_temp): sample_doc_word_topic(block_dwc_dtp_twc_temp, topic_word_prob_block_temp, topic_size, dic_size, loop)) The program and its input are as the affixed files.", "reporter": "caizhua", "assignee": null, "created": "2013-11-09T22:01:07.000+0000", "updated": "2013-11-14T17:48:11.000+0000", "resolved": "2013-11-14T17:48:11.000+0000", "labels": [], "components": [], "comments": [{"author": "Aaron Davidson", "body": "This is not a bug -- cartesian(rdd1, rdd2) does return an rdd with (rdd1.numPartitions * rdd2.numPartitions) partitions. In order to avoid having too many partitions, you can use the coalesce() function to reduce the number of partitions after doing the cartesian product.", "created": "2013-11-14T17:48:11.536+0000"}], "num_comments": 1, "text": "Issue: SPARK-955\nSummary: Paritions increase expotentially when doing cartisian product in loop program\nDescription: Paritions increase expotentially when doing cartisian product in loop program. An example program is as follows: block_doc_word_count = lines.map(parseVector).groupBy(lambda (doc_id, word_count): (doc_id % doc_block)) block_dwc_dtp_twc = block_doc_word_count.map(lambda (block_id, doc_word_count_list): (block_id, initialize_doc_topic_word_count(doc_word_count_list, topic_size, dic_size))).persist(StorageLevel.MEMORY_ONLY) for loop in range(0, max_superstep): topic_word_count_map = block_dwc_dtp_twc.flatMap(lambda (block_id, (doc_word_count_list, doc_prob, topic_word_count)): topic_word_count).reduceByKey(lambda para1, para2: addWordCount(para1, para2)) topic_word_prob_block = topic_word_count_map.mapValues(lambda wordcount_map: sample_topic_word_prob(wordcount_map, dic_size)).groupBy(lambda (topic_id, topic_word_prob): topic_id % 1) block_dwc_dtp_twc = block_dwc_dtp_twc.cartesian(topic_word_prob_block).map(lambda (block_dwc_dtp_twc_temp, topic_word_prob_block_temp): sample_doc_word_topic(block_dwc_dtp_twc_temp, topic_word_prob_block_temp, topic_size, dic_size, loop)) The program and its input are as the affixed files.\n\nComments (1):\n1. Aaron Davidson: This is not a bug -- cartesian(rdd1, rdd2) does return an rdd with (rdd1.numPartitions * rdd2.numPartitions) partitions. In order to avoid having too many partitions, you can use the coalesce() function to reduce the number of partitions after doing the cartesian product.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "76f1c1a53f32897a496cb59667c878d3", "issue_key": "SPARK-956", "issue_type": "Story", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "The Spark python program for Lasso", "description": "The code describes the Spark python implementation of Lasso", "reporter": "caizhua", "assignee": null, "created": "2013-11-09T22:03:42.000+0000", "updated": "2014-11-08T09:41:05.000+0000", "resolved": "2014-11-08T09:41:05.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Hi there, This is definitely great to have, but if you'd like to contribute the patch to Spark, please create a pull request on GitHub as described here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark. It makes it easier for us to comment on the code.", "created": "2013-11-09T23:23:59.285+0000"}, {"author": "Sean R. Owen", "body": "I assume this is WontFix as there was no followup. Spark has L1 regularization implemented already anyway.", "created": "2014-11-08T09:41:05.212+0000"}], "num_comments": 2, "text": "Issue: SPARK-956\nSummary: The Spark python program for Lasso\nDescription: The code describes the Spark python implementation of Lasso\n\nComments (2):\n1. Matei Alexandru Zaharia: Hi there, This is definitely great to have, but if you'd like to contribute the patch to Spark, please create a pull request on GitHub as described here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark. It makes it easier for us to comment on the code.\n2. Sean R. Owen: I assume this is WontFix as there was no followup. Spark has L1 regularization implemented already anyway.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "02cf8fa1adaf8beb2d43c2a629babf95", "issue_key": "SPARK-957", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "The problem that repeated computation among iterations", "description": "For LDA model, if we make each document as a single record of RDD, it is quite slow, so we try making the RDD as a set of blocks, where each block has a subset of documents. However, when we run the program, we find that a lot of computation among iterations are repeated. Basically, when we comes to the ith iteration, all the jobs that happened in 0 to (i-1)th iteration are repeated. Certainly, the jobs in the ith iteration will be repeated in the (i+1) iteration. In total, if you have m iterations, then the jobs in the ith iteration will be repeated. However, the result is still correct. :)", "reporter": "caizhua", "assignee": null, "created": "2013-11-09T22:14:23.000+0000", "updated": "2015-02-26T11:25:15.000+0000", "resolved": "2015-02-26T11:25:15.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "caizhua", "body": "The same problem happens to the Hidden Markov Model (HMM). See the affixed files.", "created": "2013-11-09T22:24:51.045+0000"}, {"author": "Josh Rosen", "body": "It looks like {{doc_word_topic_sequence}} (on line 76) is producing a long lineage chain that's not cached, causing it to be recomputed on each iteration:  doc_word_topic_sequence = doc_word_topic_sequence.map(lambda (doc_id, word_topic_sequence): (doc_id, sample_doc_topic(word_topic_sequence, topic_topic_prob, topic_word_prob, loop)))  I'd try adding cache() calls to {{doc_word_topic_sequence}}, which should improve performance. In Spark 0.8+, you could also add uncache() calls to the previous {{doc_word_topic_sequence}} after you've cached the new sequence. A minor style note: since your map function doesn't use the pairs' keys, you can use {{mapValues}} instead:  doc_word_topic_sequence = doc_word_topic_sequence.mapValues(lambda word_topic_sequence: sample_doc_topic(word_topic_sequence, topic_topic_prob, topic_word_prob, loop))).cache()", "created": "2013-11-10T17:18:15.847+0000"}, {"author": "caizhua", "body": "The code for repeated running.", "created": "2013-11-10T20:36:51.846+0000"}, {"author": "caizhua", "body": "Josh Rosen, you are correct! For HMM, if we add the cache(), it can address this problem, and we did our programming like that. I think it is my mistake to upload this code for HMM. However, the code for bloc_lda.py did have this problem. In the code, I use persist and unpersist together, however, the problems are stills there. See the affixed file block_lda_repeated_running.py", "created": "2013-11-10T20:40:26.937+0000"}, {"author": "Andrew Ash", "body": "Hi [~caizhua], are you still having issues with your implementation of the LDA algorithm? We try to only keep tickets open that have remaining work to be done. You can also reference SPARK-1405 for the LDA implementation being worked on for future inclusion into MLlib.", "created": "2014-11-14T10:05:27.955+0000"}], "num_comments": 5, "text": "Issue: SPARK-957\nSummary: The problem that repeated computation among iterations\nDescription: For LDA model, if we make each document as a single record of RDD, it is quite slow, so we try making the RDD as a set of blocks, where each block has a subset of documents. However, when we run the program, we find that a lot of computation among iterations are repeated. Basically, when we comes to the ith iteration, all the jobs that happened in 0 to (i-1)th iteration are repeated. Certainly, the jobs in the ith iteration will be repeated in the (i+1) iteration. In total, if you have m iterations, then the jobs in the ith iteration will be repeated. However, the result is still correct. :)\n\nComments (5):\n1. caizhua: The same problem happens to the Hidden Markov Model (HMM). See the affixed files.\n2. Josh Rosen: It looks like {{doc_word_topic_sequence}} (on line 76) is producing a long lineage chain that's not cached, causing it to be recomputed on each iteration:  doc_word_topic_sequence = doc_word_topic_sequence.map(lambda (doc_id, word_topic_sequence): (doc_id, sample_doc_topic(word_topic_sequence, topic_topic_prob, topic_word_prob, loop)))  I'd try adding cache() calls to {{doc_word_topic_sequence}}, which should improve performance. In Spark 0.8+, you could also add uncache() calls to the previous {{doc_word_topic_sequence}} after you've cached the new sequence. A minor style note: since your map function doesn't use the pairs' keys, you can use {{mapValues}} instead:  doc_word_topic_sequence = doc_word_topic_sequence.mapValues(lambda word_topic_sequence: sample_doc_topic(word_topic_sequence, topic_topic_prob, topic_word_prob, loop))).cache()\n3. caizhua: The code for repeated running.\n4. caizhua: Josh Rosen, you are correct! For HMM, if we add the cache(), it can address this problem, and we did our programming like that. I think it is my mistake to upload this code for HMM. However, the code for bloc_lda.py did have this problem. In the code, I use persist and unpersist together, however, the problems are stills there. See the affixed file block_lda_repeated_running.py\n5. Andrew Ash: Hi [~caizhua], are you still having issues with your implementation of the LDA algorithm? We try to only keep tickets open that have remaining work to be done. You can also reference SPARK-1405 for the LDA implementation being worked on for future inclusion into MLlib.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "b0f5ca3e639e7ca4c41c2d635b50bcfe", "issue_key": "SPARK-958", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "When iteration in ALS increases to 10 running in local mode, spark throws out error of StackOverflowError", "description": "I try to use ml-100k data to test ALS running in local mode in mllib project. If I specify iteration to be less than, it works well. However, when iteration is increased to more than 10 iterations, spark throws out error of StackOverflowError. Attached is the log file.", "reporter": "Qiuzhuang Lian", "assignee": null, "created": "2013-11-15T01:25:56.000+0000", "updated": "2014-04-01T06:28:25.000+0000", "resolved": "2014-04-01T06:28:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Qiuzhuang Lian", "body": "Is this issue related to Scala bug at https://issues.scala-lang.org/browse/SI-6961? Can somebody advise? Thanks.", "created": "2013-12-03T00:09:10.078+0000"}, {"author": "Qiuzhuang Lian", "body": "I tune JVM thread stack size to 512k via option -Xss512k and it works. This is deep level of object serialization issue known in JDK as here reported: http://bugs.sun.com/view_bug.do?bug_id=4152790, it is not related to Spark/Scala, closing this bug.", "created": "2013-12-04T02:12:39.517+0000"}, {"author": "Qiuzhuang Lian", "body": "This is duplicate of bug 1066: https://spark-project.atlassian.net/browse/SPARK-1006", "created": "2014-01-27T01:19:15.095+0000"}], "num_comments": 3, "text": "Issue: SPARK-958\nSummary: When iteration in ALS increases to 10 running in local mode, spark throws out error of StackOverflowError\nDescription: I try to use ml-100k data to test ALS running in local mode in mllib project. If I specify iteration to be less than, it works well. However, when iteration is increased to more than 10 iterations, spark throws out error of StackOverflowError. Attached is the log file.\n\nComments (3):\n1. Qiuzhuang Lian: Is this issue related to Scala bug at https://issues.scala-lang.org/browse/SI-6961? Can somebody advise? Thanks.\n2. Qiuzhuang Lian: I tune JVM thread stack size to 512k via option -Xss512k and it works. This is deep level of object serialization issue known in JDK as here reported: http://bugs.sun.com/view_bug.do?bug_id=4152790, it is not related to Spark/Scala, closing this bug.\n3. Qiuzhuang Lian: This is duplicate of bug 1066: https://spark-project.atlassian.net/browse/SPARK-1006", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "d9ea14ba7d6ba12cdcbb1b14635b3bc2", "issue_key": "SPARK-959", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Ivy fails to download javax.servlet.orbit dependency", "description": "Issue: Ivy attempts to download the \"javax.servlet.orbit\" dependency with an extension of \".orbit\" instead of \".jar\" and fails. (One) Solution: Can add the following to the libraryDependencies in SparkBuild.scala: \"org.eclipse.jetty.orbit\" % \"javax.servlet\" % \"2.5.0.v201103041518\" artifacts Artifact(\"javax.servlet\", \"jar\", \"jar\") Cause: I don't know. This does not occur for everyone, and does not seem directly related to the version of Ant. See the following for bug reports on the user lists: http://mail-archives.apache.org/mod_mbox/spark-user/201309.mbox/%3CCAJbo4neXyzQe6zGREQJTzZZ5ZrCoAvfEN+WmBYCed6N1EPftxA@mail.gmail.com%3E and http://mail-archives.apache.org/mod_mbox/spark-user/201311.mbox/%3CCANGvG8pXVhcKiGEpXnGHfQeAYTyUygA%3D1nxSe0%3D%2BfRfnKSq88w%40mail.gmail.com%3E", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2013-11-17T22:36:01.000+0000", "updated": "2014-03-24T22:42:56.000+0000", "resolved": "2013-12-19T00:10:29.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Aaron Davidson", "body": "This is the ivy bug report related to the issue: https://issues.apache.org/jira/browse/IVY-899 Looks like it was fixed in 2.3.x, but we're still using ivy 2.0, I believe.", "created": "2013-12-18T23:36:40.537+0000"}, {"author": "Aaron Davidson", "body": "https://github.com/apache/incubator-spark/pull/183", "created": "2013-12-19T00:10:29.992+0000"}, {"author": "liancheng", "body": "Seems that some transitive dependencies of javax.servlet also have the same issue. At least under my network environment, I have to add two more lines to get things done: \"org.eclipse.jetty.orbit\" % \"javax.activation\" % \"1.1.0.v201105071233\" artifacts Artifact(\"javax.activation\", \"jar\", \"jar\"), \"org.eclipse.jetty.orbit\" % \"javax.mail.glassfish\" % \"1.4.1.v201005082020\" artifacts Artifact(\"javax.mail.glassfish\", \"jar\", \"jar\"), But I'm not sure whether it's sufficient to workaround the problem...", "created": "2014-03-24T22:42:56.145+0000"}], "num_comments": 3, "text": "Issue: SPARK-959\nSummary: Ivy fails to download javax.servlet.orbit dependency\nDescription: Issue: Ivy attempts to download the \"javax.servlet.orbit\" dependency with an extension of \".orbit\" instead of \".jar\" and fails. (One) Solution: Can add the following to the libraryDependencies in SparkBuild.scala: \"org.eclipse.jetty.orbit\" % \"javax.servlet\" % \"2.5.0.v201103041518\" artifacts Artifact(\"javax.servlet\", \"jar\", \"jar\") Cause: I don't know. This does not occur for everyone, and does not seem directly related to the version of Ant. See the following for bug reports on the user lists: http://mail-archives.apache.org/mod_mbox/spark-user/201309.mbox/%3CCAJbo4neXyzQe6zGREQJTzZZ5ZrCoAvfEN+WmBYCed6N1EPftxA@mail.gmail.com%3E and http://mail-archives.apache.org/mod_mbox/spark-user/201311.mbox/%3CCANGvG8pXVhcKiGEpXnGHfQeAYTyUygA%3D1nxSe0%3D%2BfRfnKSq88w%40mail.gmail.com%3E\n\nComments (3):\n1. Aaron Davidson: This is the ivy bug report related to the issue: https://issues.apache.org/jira/browse/IVY-899 Looks like it was fixed in 2.3.x, but we're still using ivy 2.0, I believe.\n2. Aaron Davidson: https://github.com/apache/incubator-spark/pull/183\n3. liancheng: Seems that some transitive dependencies of javax.servlet also have the same issue. At least under my network environment, I have to add two more lines to get things done: \"org.eclipse.jetty.orbit\" % \"javax.activation\" % \"1.1.0.v201105071233\" artifacts Artifact(\"javax.activation\", \"jar\", \"jar\"), \"org.eclipse.jetty.orbit\" % \"javax.mail.glassfish\" % \"1.4.1.v201005082020\" artifacts Artifact(\"javax.mail.glassfish\", \"jar\", \"jar\"), But I'm not sure whether it's sufficient to workaround the problem...", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.090544"}}
{"id": "8d7f8a4c980768d36d94226ff2b04824", "issue_key": "SPARK-960", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "JobCancellationSuite \"two jobs sharing the same stage\" is broken", "description": "This test doesn't work as it appears to be intended since the map tasks can never acquire sem2. The simplest way to demonstrate this is to comment out f1.cancel() in the future. I believe the intention is that f1 and f2 would then complete normally; but they won't. Instead, both jobs block, waiting on sem2. It doesn't look like closing over Semaphores works even in a Local context, since sem2.hashCode() is different in each of f1, f2 and in the future containing f1.cancel, so the map jobs never see the sem2.release(10) in the future. Instead, the test only completes because all of the stages (the two final stages and the common dependent stage) get cancelled and aborted. When job <--> stage dependencies are fully accounted for and job cancellation changed so that f1.cancel does not abort the common stage, then this test can never finish since it then becomes hung waiting on sem2.", "reporter": "Mark Hamstra", "assignee": "Sean R. Owen", "created": "2013-11-18T13:59:35.000+0000", "updated": "2015-01-26T22:32:56.000+0000", "resolved": "2015-01-26T22:32:47.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/4180", "created": "2015-01-23T12:03:36.411+0000"}, {"author": "Josh Rosen", "body": "Issue resolved by pull request 4180 [https://github.com/apache/spark/pull/4180]", "created": "2015-01-26T22:32:47.067+0000"}], "num_comments": 2, "text": "Issue: SPARK-960\nSummary: JobCancellationSuite \"two jobs sharing the same stage\" is broken\nDescription: This test doesn't work as it appears to be intended since the map tasks can never acquire sem2. The simplest way to demonstrate this is to comment out f1.cancel() in the future. I believe the intention is that f1 and f2 would then complete normally; but they won't. Instead, both jobs block, waiting on sem2. It doesn't look like closing over Semaphores works even in a Local context, since sem2.hashCode() is different in each of f1, f2 and in the future containing f1.cancel, so the map jobs never see the sem2.release(10) in the future. Instead, the test only completes because all of the stages (the two final stages and the common dependent stage) get cancelled and aborted. When job <--> stage dependencies are fully accounted for and job cancellation changed so that f1.cancel does not abort the common stage, then this test can never finish since it then becomes hung waiting on sem2.\n\nComments (2):\n1. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/4180\n2. Josh Rosen: Issue resolved by pull request 4180 [https://github.com/apache/spark/pull/4180]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.092287"}}
{"id": "9449ff6d0c091a8c65ce2cfa8272166b", "issue_key": "SPARK-961", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add a Vector.random() method or make our website examples show something else", "description": "Pat pointed out this doesn't currently exist, although it's used in examples like the logistic regression on http://spark.incubator.apache.org/examples.html", "reporter": "Matei Alexandru Zaharia", "assignee": "Pillis", "created": "2013-11-19T11:56:15.000+0000", "updated": "2014-01-10T18:56:09.000+0000", "resolved": "2014-01-10T18:56:09.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Pillis", "body": "I have submitted a pull request. Requesting assignment of issue to me.", "created": "2014-01-09T01:27:19.661+0000"}, {"author": "Pillis", "body": "Fix pulled via https://github.com/apache/incubator-spark/pull/369. Do not have ability to resolve this issue.", "created": "2014-01-10T18:51:52.311+0000"}, {"author": "Reynold Xin", "body": "Closed it and also gave you dev access.", "created": "2014-01-10T18:56:09.894+0000"}], "num_comments": 3, "text": "Issue: SPARK-961\nSummary: Add a Vector.random() method or make our website examples show something else\nDescription: Pat pointed out this doesn't currently exist, although it's used in examples like the logistic regression on http://spark.incubator.apache.org/examples.html\n\nComments (3):\n1. Pillis: I have submitted a pull request. Requesting assignment of issue to me.\n2. Pillis: Fix pulled via https://github.com/apache/incubator-spark/pull/369. Do not have ability to resolve this issue.\n3. Reynold Xin: Closed it and also gave you dev access.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.092287"}}
{"id": "1d3f12f3995d2b8367f7bb0c7c6583fb", "issue_key": "SPARK-962", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "debian package contains old version of executable scripts", "description": "When building debian package with maven The spark-shell and spark-executor are not usable (packages are not org.apache.*) and seams outdated compare to ./spark-shell and ./spark-exector in repo. instead of having outdated repl-bin/src/deb/bin/ use a common directory for run spark-shell and spark-executor scripts that jdeb can refer to.", "reporter": "Diggory Hardy", "assignee": null, "created": "2013-11-21T07:40:58.000+0000", "updated": "2014-11-25T08:58:57.000+0000", "resolved": "2014-11-25T08:58:57.000+0000", "labels": ["deb", "debian", "jdeb", "package", "script"], "components": ["Build", "Deploy"], "comments": [{"author": "Diggory Hardy", "body": "https://github.com/clearstorydata/incubator-spark/pull/1 is a great way to do it I suppose.", "created": "2013-11-22T03:03:34.887+0000"}, {"author": "Mark Hamstra", "body": "So we've got a \"make it work\" hack in for 0.8.1, but Debian packaging should be handled better in 0.9. The most glaring issue in the current hack is that the packaged Spark doesn't use {{spark-class}} and the other new scripts that are described in all of the current how-to docs. Instead, it uses the equivalent of the old {{run}} script. The link above to the way I changed Debian packaging for ClearStory is adequate to build a package from the {{assembly}} sub-project and to deploy and use the standard scripts. With perhaps some changes in the package name and metadata, that may be enough for Apache Spark 0.9. However, this would still be a fair remove from Debian packaging adequate for inclusion in a Debian or Ubuntu distribution. It would still be just a minimalist wrapping of a fat Spark jar that can be used with a deployment tool such as Chef -- that's all that the Debian packaging of Spark has been intended for to date, and is how we use it at ClearStory. If we're ready to go further with Spark packaging (i.e. not just proper Debian packaging, but also RPMs and installation packages for Mac and Windows), then we should produce more refined Debian packaging. That would likely mean producing not just a spark-core package, but separate packages for examples, tools, Java API, Python API, MLlib, etc. as well as a proper source package. Building and maintaining packages for multiple OSes is a fair amount of work (and I'm not volunteering to do it, since what we've already got at ClearStory is adequate for my needs), so it is worth discussing whether that is something that Spark needs at this point or whether there are easier packaging/distribution targets to hit that are adequate for now.", "created": "2013-12-10T16:27:40.886+0000"}, {"author": "Sean R. Owen", "body": "This appears fixed. The referenced PR was apparently merged into 0.9, and the outdated scripts referenced in the description are no longer present. Spark uses spark-class from spark-submit et al as desired.", "created": "2014-11-25T08:58:57.191+0000"}], "num_comments": 3, "text": "Issue: SPARK-962\nSummary: debian package contains old version of executable scripts\nDescription: When building debian package with maven The spark-shell and spark-executor are not usable (packages are not org.apache.*) and seams outdated compare to ./spark-shell and ./spark-exector in repo. instead of having outdated repl-bin/src/deb/bin/ use a common directory for run spark-shell and spark-executor scripts that jdeb can refer to.\n\nComments (3):\n1. Diggory Hardy: https://github.com/clearstorydata/incubator-spark/pull/1 is a great way to do it I suppose.\n2. Mark Hamstra: So we've got a \"make it work\" hack in for 0.8.1, but Debian packaging should be handled better in 0.9. The most glaring issue in the current hack is that the packaged Spark doesn't use {{spark-class}} and the other new scripts that are described in all of the current how-to docs. Instead, it uses the equivalent of the old {{run}} script. The link above to the way I changed Debian packaging for ClearStory is adequate to build a package from the {{assembly}} sub-project and to deploy and use the standard scripts. With perhaps some changes in the package name and metadata, that may be enough for Apache Spark 0.9. However, this would still be a fair remove from Debian packaging adequate for inclusion in a Debian or Ubuntu distribution. It would still be just a minimalist wrapping of a fat Spark jar that can be used with a deployment tool such as Chef -- that's all that the Debian packaging of Spark has been intended for to date, and is how we use it at ClearStory. If we're ready to go further with Spark packaging (i.e. not just proper Debian packaging, but also RPMs and installation packages for Mac and Windows), then we should produce more refined Debian packaging. That would likely mean producing not just a spark-core package, but separate packages for examples, tools, Java API, Python API, MLlib, etc. as well as a proper source package. Building and maintaining packages for multiple OSes is a fair amount of work (and I'm not volunteering to do it, since what we've already got at ClearStory is adequate for my needs), so it is worth discussing whether that is something that Spark needs at this point or whether there are easier packaging/distribution targets to hit that are adequate for now.\n3. Sean R. Owen: This appears fixed. The referenced PR was apparently merged into 0.9, and the outdated scripts referenced in the description are no longer present. Spark uses spark-class from spark-submit et al as desired.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.092287"}}
{"id": "ebffa9fb73ac7c40d21689ffbe61aff3", "issue_key": "SPARK-1229", "issue_type": "Story", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "train on array (in addition to RDD)", "description": "since predict method accepts either RDD or Array for consistency so should train. (particularly since RDD.takeSample() returns Array)", "reporter": "Arshak Navruzyan", "assignee": null, "created": "2013-11-21T11:43:58.000+0000", "updated": "2014-11-08T11:55:30.000+0000", "resolved": "2014-11-08T11:55:30.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Aliaksei Litouka", "body": "May I start working on this issue? Please assign it to me.", "created": "2014-04-18T21:46:16.723+0000"}, {"author": "Aliaksei Litouka", "body": "Well... I'm new to Spark so please correct me if I'm wrong. _predict_ methods declared in ClassificationModel and in KMeansModel accept an RDD\\[Vector\\] or a single Vector (not an array!). Each item of the RDD\\[Vector\\] contains a single observation for which we want to calculate a prediction, and each item of the Vector contains a single feature of an observation. If we need to take a sample of an RDD and feed this sample to any of the train methods, we can use the _sample_ method instead of _takeSample_ . Actually, _takeSample_ calls _sample_ internally. However, these methods accept different parameters. If this is a problem, then I think it's possible to implement another version of the _sample_ method:  def sample(withReplacement: Boolean, num: Int, seed: Int): RDD[T]  Let me know if it makes sense.", "created": "2014-04-20T23:04:16.332+0000"}, {"author": "Sean R. Owen", "body": "If I may be so bold: generally, you train on lots of data, so an RDD makes more sense than an Array as training input. That said, you can always parallelize an Array as an RDD and train on it, which is most of the use case here. If you really mean you need a sample method that returns an RDD, yes that exists as you see.", "created": "2014-11-08T11:55:30.597+0000"}], "num_comments": 3, "text": "Issue: SPARK-1229\nSummary: train on array (in addition to RDD)\nDescription: since predict method accepts either RDD or Array for consistency so should train. (particularly since RDD.takeSample() returns Array)\n\nComments (3):\n1. Aliaksei Litouka: May I start working on this issue? Please assign it to me.\n2. Aliaksei Litouka: Well... I'm new to Spark so please correct me if I'm wrong. _predict_ methods declared in ClassificationModel and in KMeansModel accept an RDD\\[Vector\\] or a single Vector (not an array!). Each item of the RDD\\[Vector\\] contains a single observation for which we want to calculate a prediction, and each item of the Vector contains a single feature of an observation. If we need to take a sample of an RDD and feed this sample to any of the train methods, we can use the _sample_ method instead of _takeSample_ . Actually, _takeSample_ calls _sample_ internally. However, these methods accept different parameters. If this is a problem, then I think it's possible to implement another version of the _sample_ method:  def sample(withReplacement: Boolean, num: Int, seed: Int): RDD[T]  Let me know if it makes sense.\n3. Sean R. Owen: If I may be so bold: generally, you train on lots of data, so an RDD makes more sense than an Array as training input. That said, you can always parallelize an Array as an RDD and train on it, which is most of the use case here. If you really mean you need a sample method that returns an RDD, yes that exists as you see.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.092287"}}
{"id": "2db2b73666ebd18f6faa2ec7cb2824e7", "issue_key": "SPARK-1228", "issue_type": "Story", "status": "Closed", "priority": "Major", "resolution": null, "summary": "confusion matrix", "description": "utility that print confusion matrix for multi-class classification including precision and recall", "reporter": "Arshak Navruzyan", "assignee": "Xiangrui Meng", "created": "2013-11-21T17:41:33.000+0000", "updated": "2014-06-05T08:53:20.000+0000", "resolved": "2014-06-05T08:53:20.000+0000", "labels": ["classification"], "components": ["MLlib"], "comments": [{"author": "Jeff Zhang", "body": "Attach the implementation for ConfusionMatrix", "created": "2014-03-06T06:27:41.748+0000"}, {"author": "Jeff Zhang", "body": "In the attachment, I implement the ConfusionMatrix, but haven't add comment and unit test. After review, if you think the design is OK. I would add comments and unit test. Let me know your comment on my first patch to MLlib. :) Overall, I design 2 traits for the ConfusionMatrix, both of these 2 trait has the ability to get the precision, recall, f-value and formatted confusion matrix. The only difference is that trait ConfusionMatrix is based on double type label which correspond the label in LabeledPoint and ConfusionMatrixWithDict is based on the string type label. I also attach 2 screenshots of the confusion matrix copied to excel to allow you get an overview of what the confusion matrix looks like, one is double label, another is the string label.", "created": "2014-03-06T06:51:57.182+0000"}, {"author": "Xiangrui Meng", "body": "Confusion matrix was added in v1.0 as part of binary classification model evaluation.", "created": "2014-06-05T08:53:20.630+0000"}], "num_comments": 3, "text": "Issue: SPARK-1228\nSummary: confusion matrix\nDescription: utility that print confusion matrix for multi-class classification including precision and recall\n\nComments (3):\n1. Jeff Zhang: Attach the implementation for ConfusionMatrix\n2. Jeff Zhang: In the attachment, I implement the ConfusionMatrix, but haven't add comment and unit test. After review, if you think the design is OK. I would add comments and unit test. Let me know your comment on my first patch to MLlib. :) Overall, I design 2 traits for the ConfusionMatrix, both of these 2 trait has the ability to get the precision, recall, f-value and formatted confusion matrix. The only difference is that trait ConfusionMatrix is based on double type label which correspond the label in LabeledPoint and ConfusionMatrixWithDict is based on the string type label. I also attach 2 screenshots of the confusion matrix copied to excel to allow you get an overview of what the confusion matrix looks like, one is double label, another is the string label.\n3. Xiangrui Meng: Confusion matrix was added in v1.0 as part of binary classification model evaluation.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.092287"}}
{"id": "3cef4a0d28feca6c93d55b1baeaa6a6e", "issue_key": "SPARK-963", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Races in JobLoggerSuite", "description": "Since SparkListener events are now processed asynchronously from the SparkListenerBus eventQueue, the jobLogger conditions checked in the \"inner variables\" and \"interface functions\" tests of the JobLoggerSuite are not guaranteed to be satisfied just because rdd.reduceByKey(_+_).collect() has returned. This leads to infrequent failures of these tests when we lose the race.", "reporter": "Mark Hamstra", "assignee": "Patrick McFadin", "created": "2013-11-24T15:37:15.000+0000", "updated": "2013-12-08T18:24:29.000+0000", "resolved": "2013-12-08T18:24:29.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Mark Hamstra", "body": "How come I never see typos until after I have clicked a 'submit' button? This issue should be entitled \"Races in JobLoggerSuite\".", "created": "2013-11-24T15:41:10.296+0000"}, {"author": "Reynold Xin", "body": "I just changed it. Were you not able to edit the title yourself?", "created": "2013-11-24T15:49:52.892+0000"}, {"author": "Mark Hamstra", "body": "Some combination of not being permitted to edit the title and not knowing how to edit it.", "created": "2013-11-24T16:16:47.551+0000"}], "num_comments": 3, "text": "Issue: SPARK-963\nSummary: Races in JobLoggerSuite\nDescription: Since SparkListener events are now processed asynchronously from the SparkListenerBus eventQueue, the jobLogger conditions checked in the \"inner variables\" and \"interface functions\" tests of the JobLoggerSuite are not guaranteed to be satisfied just because rdd.reduceByKey(_+_).collect() has returned. This leads to infrequent failures of these tests when we lose the race.\n\nComments (3):\n1. Mark Hamstra: How come I never see typos until after I have clicked a 'submit' button? This issue should be entitled \"Races in JobLoggerSuite\".\n2. Reynold Xin: I just changed it. Were you not able to edit the title yourself?\n3. Mark Hamstra: Some combination of not being permitted to edit the title and not knowing how to edit it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.092287"}}
{"id": "1f39cc20dc49ce4304cf943fe985c747", "issue_key": "SPARK-964", "issue_type": "Story", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Investigate the potential for using JDK 8 lambda expressions for the Java/Scala APIs", "description": "JDK 8 (to be released soon) will have lambda expressions. The question is whether they can be leveraged for Java to use Scala's Spark API (perhaps with some modifications), or whether a new functional API would need to be developed for Java 8+.", "reporter": "Marek Kolodziej", "assignee": "Marek Kolodziej", "created": "2013-11-25T13:04:54.000+0000", "updated": "2015-12-10T15:06:41.000+0000", "resolved": "2014-03-03T22:32:18.000+0000", "labels": ["api-change"], "components": [], "comments": [{"author": "Marek Kolodziej", "body": "Here are some initial findings. Java's lambdas can be used with Scala, after a trivial conversion is applied. Scala's Function1<T, R> is essentially Java's Function<T, R>, but Java doesn't see scala.Function1 as honoring the JDK8 \"functional interface\" contract, with all non-apply() methods being [\"default\"|http://blog.sanaulla.info/2013/03/20/introduction-to-default-methods-defender-methods-in-java-8/]. It seems that Java's augmented \"interfaces with default methods\" aren't an exact match for Scala's traits. Therefore, one possibility is to do the following. To honor Scala's Function1 through Function22, one can create analogous Java interfaces, like so:  @FunctionalInterface public interface JavaFunction1<T1, R> { R apply(T1 v1); } @FunctionalInterface public interface JavaFunction2<T1, T2, R> { R apply(T1 v1, T2 v2); } // Function3-Function22 follow the same pattern  The FunctionalInterface annotation is solely there to issue compiler errors if the interface doesn't honor the contract required by lambdas, but it's not strictly necessary. Then, let's create a regular Scala class that will accept lambdas as method inputs. Here, I just created a dummy class  package foo class FakeRDD[T](in: List[T]) { def this(in: Array[T]) = this(in.toList) def map(f: T => T) = new FakeRDD(in.map(f)) /* don't let the user know that the internal representation is a List - hence I didn't use a case class with auto-generated toString() */ override def toString() = \"FakeRDD(\" + in.mkString(\",\") + \")\" }  We will also need a Java8-Scala lambda converter, like so (just for Function1 here, could be expanded to Function2-Function22:  package foo import foo.{JavaFunction1 => Function1} object Conv { def f[T1, R](fun: Function1[T1, R]) = (v1: T1) => fun(v1) }  Now, let's go and create our Java client - instead of map(javaLambda), we'll have the wrapper that converts from a Java 8 to a Scala lambda, e.g. map(f(javaLambda)) where f(javaLambda) will resolve to scalaLambda  package foo; import static foo.Conv.f; public class FooBar { public static void main(final String[] args) { final FakeRDD<Integer> rdd1 = new FakeRDD<Integer>(new Integer[] {1, 2, 3}); final FakeRDD<Integer> rdd2 = rdd1.map(f((Integer i) -> i * 2)); System.out.println(rdd1); System.out.println(rdd2); } }  So now, the output from the execution of the Java client will be:  FakeRDD(1,2,3) FakeRDD(2,4,6)  By the way, note that the f() wrapper is actually less boilerplate than using Java 8 collections with lambdas, since e.g. an ArrayList<T> would have to call a stream() method to get a Stream<T>, then apply the lambda, and then call collect() to get back a List, like so:  import java.util.ArrayList; import java.util.List; import java.util.Optional; import java.util.Random; public class Test { public static void main(final String[] args) { // seed = 1 final Random rand = new Random(1); final List<Double> list = new ArrayList<>(); for (int i = 1; i < 100; i++) { list.add(rand.nextGaussian()); } final Optional<Double> sumOfDoubledPositives = list.stream().filter(d -> d > 0).map(d -> d * 2).reduce((d1, d2) -> d1 + d2); if (sumOfDoubledPositives.isPresent()) { System.out.println(sumOfDoubledPositives.get()); } } }  So IMHO, that wrapper around the Scala Spark API would be more convenient for JDK 8 users than their own Java Collections Framework. However, note that the conversion from Java to Scala lambdas prevents the Java compiler from inferring types, because it can only do so for its own syntactic sugar, so instead of saying \"i -> i * 2\" you have to say \"(Integer i) -> i * 2.\" Nevertheless, I'm not sure if that inference is universal, and that's a small cost relative to the overall productivity boost stemming from a functional API. Maybe this inference issue will get resolved, after all JDK 8 is still in beta. Note also that I believe that for compatibility reasons, it may be necessary to keep the old Java Spark API for people with pre-Java 8 JDKs. However, it might be interesting to investigate whether moving from the existing Function interfaces in the Spark Java API to interfaces meeting the FunctionalInterface contract wouldn't allow JDK 8 users to benefit from lambdas directly, without the f() wrapper while using the Scala API. To the extent that the Scala API is more up to date though, it would be preferable for JDK 8 users to use the Scala API with that tiny f() wrapper in my opinion. Also, changing the Spark-specific Function interfaces to JDK 8 ones would break existing Java clientcode, so also for that reason it might be preferable to keep the pre-Java 8 API as-is while facilitating the use of the Scala API for JDK 8+ users. Of course, one could also get rid of the f() wrapper and have overloaded methods in the Spark classes, i.e. the original methods taking a Scala lambda, and analogous ones taking a Java lambda, that call the Java-to-Scala-lambda converter and redirect to the Scala lambda-based methods. Or one could accept Java lambdas in the existing methods instead of Scala ones, and import reusable implicit conversion defs for JavaFunction1-JavaFunction22 to Scala's Function1-Function22. The last option is way too invasive for the existing codebase and makes too many concessions for Java users, IMHO. The method overloading puts the burden on the Spark codebase too. On the other hand, a tiny client-side f() wrapper still permits the Java user to benefit from FP which is way better than creating anonymous classes everywhere, and lets the Java user access the more complete Scala API, but it doesn't affect all the existing Spark classes - one would merely need to include that wrapper and use it everywhere.", "created": "2013-11-27T07:46:29.830+0000"}, {"author": "Marek Kolodziej", "body": "Note that in the above case, I was able to use Java's lambda in place of Scala's lambda when calling map() on FakeRDD<Integer>. However, using Scala's collections with this approach would be a bit different than what we're used to in Scala itself. For instance, we don't think twice about calling map() on a List. However, this wouldn't work with Java's lambdas. Instead, one would need to call mapConserve, like so:  final List<String> scalaList = List.fromArray(new String[]{\"foo\", \"bar\"}); final List<String> scalaList2 = scalaList.mapConserve(f((String s) -> s.toUpperCase()));  This works whether the transformation results in the same or a different type, for instance:  final List<String> scalaList3 = List.fromArray(new String[]{\"1\", \"2\"}); final List<Integer> scalaList4 = scalaList3.mapConserve(f((String s) -> Integer.parseInt(s)*2));  The reason that map() doesn't work but mapConserve() does is that map() is a curried method with an implicit argument:  map[B, That](f: (A) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That  On the other hand, mapConserve() doesn't have currying or implicit parameters:  mapConserve[B >: A <: AnyRef](f: (A) ⇒ B): List[B]  Since in case of FakeRDD the Java-exposed public map() method didn't have currying or implicits, Java \"understood\" it after the Java-to-Scala lambda conversion. In case of Scala's List, that was not the case (the above declarations of map() and mapConserve() come from [there|http://www.scala-lang.org/api/current/scala/collection/immutable/List.html]). So, as long as the Spark Scala API sticks to single parameter lists and no implicits, Java can use it with that thin f() wrapper.", "created": "2013-11-27T17:50:25.907+0000"}, {"author": "Josh Rosen", "body": "The {{f()}} wrapper approach would require still Java 8 users to manually wrap RDD\\[(K, V)] into PairRDDFunctions\\[K, V] since the implicit conversion won't happen in Java (see https://cwiki.apache.org/confluence/display/SPARK/Java+API+Internals). We would probably want to continue to expose Java collections types instead of their Scala counterparts. Therefore, I think that we should continue maintain a separate Java wrapper. It's a bit annoying to maintain, but the JavaAPICompletenessChecker script should make that easier; most of this code shouldn't have to ever change once it's been written since the user-facing APIs should remain stable and most of the code is quite simple.", "created": "2013-11-28T22:37:26.655+0000"}, {"author": "Marek Kolodziej", "body": "Hi Josh, yes, no question about maintaining a separate Java API. First of all, not everyone will upgrade to Java 8 (in fact most people are on JDK 6), so one has to support a separate API. I also get your point regarding not using the Scala API with the f() wrapper so as to return Java instead of Scala collections, and so on. What I'm thinking of investigating next is whether Java 8 lambdas could be used with the Java API. Looking at Spark's [function|http://spark.incubator.apache.org/docs/latest/api/core/index.html#org.apache.spark.api.java.function.package] package, the various Functions are handled by abstract classes as opposed to interfaces. Java 8 lambdas are based on an API having methods that take lambdas accept interface types that are single abstract method (SAM) interfaces - the \"abstract\" no longer being redundant, since Java 8 interfaces can have concrete (\"default\") methods, much like Scala's traits. While abstract classes are similar to interfaces with default methods, they are not the same syntactic construct, and the compiler complained when I tried using a lambda with Spark's methods that take Function instances for that reason. This is the case even though they are SAM abstract classes, but lambdas were only built around SAM interfaces. I have a new idea. The key goals with any solution, I think, are to support pre-Java 8 users while providing Java 8+ users with the convenience of lambdas. With that in mind, here's what I've been thinking about. Since pre-Java 8 interfaces can't have concrete methods, we can't provide the same exact functional API to Java 8 users as we would get by using the [java.util.function.Function|http://download.java.net/jdk8/docs/api/java/util/function/Function.html] interface, with default compose()/andThen() methods. In other words, we couldn't replace the existing Spark abstract Function classes. However, since the only abstract method in org.apache.spark.api.java.function.Function is call(), then perhaps one could expose just interfaces requiring the implementation of the call() method, and these abstract classes could be instantiated with that implementation within the Spark code itself. For example, say we have this simple SAM Function interface:  package test; interface Function<T, R> { public R call(T t); }  This would be what the Java 6/7 user would have to implement as an anonymous class, while a Java 8 user could in-line as a lambda. Now, let's suppose that internally, we need an abstract class with concrete methods like Spark's Function. Let's call it AbstractFun here to distinguish it from the Function interface above.  package test; public abstract class AbstractFun<T, R> implements Function<T, R> { /* we're getting \"public R call(T t)\" from Function, but we're not implementing it since the class is abstract */ /* second abstract method (in addition to Function's call()) breaks Java 8 lambda SAM contract, and Java 8 requires an interface rather than an abstract class anyway for lambdas */ public abstract void breakFunctionalInterfaceSAMContract(); /* a concrete method is OK in an abstract class, but Java 8 requires an interface with default methods instead for lambdas, while pre-Java 8 there were no default methods */ public int theMeaningOfLife() { return 42; } }  So, how about having the API users only implement the Function interface, and having an internal conversion from the interface to the AbstractFun which has other concrete methods?  package test; public class Utils { public static <T, R> AbstractFun<T, R> convert(final Function<T, R> fun) { return new AbstractFun<T, R>() { @Override public R call(final T t) { return fun.call(t); } /* implement the other abstract method if need be - if there are no other abstract method, we can just implement call() and we're done */ @Override public void breakFunctionalInterfaceSAMContract() {} }; } }  Now in our API class (such as an RDD, we can just call the convert() method to instantiate the abstract class with the implementation of call() from the anonymous class passed by the user. Note the line  l2.add(convert(fun).call(t));  See the code below  package test; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import static test.Utils.convert; public class FakeRDD<T> { final private List<T> l; public FakeRDD(final List<T> l) { this.l = l; } public <R> FakeRDD<R> map(final Function<T, R> fun) { final List<R> l2 = new ArrayList<R>(); for (final T t : l) { l2.add(convert(fun).call(t)); } return new FakeRDD(l2); } @Override public String toString() { // don't show that we have a list inside - don't use List's toString() return Arrays.toString(l.toArray()); } }  Now let's run it.  package test; import java.util.Arrays; public class AbstractTest { public static void main(final String[] args) { final FakeRDD<Integer> foo1 = new FakeRDD<>(Arrays.asList(1, 2, 3, 4, 5)); final FakeRDD<Integer> foo2 = foo1.map(i -> i); System.out.println(foo1); System.out.println(foo2); } }  That way, if there are internal implementation dependencies on the abstract class, we can convert the interface to the existing class. This would allow pre-Java 8 to use the API as before (by implementing the call()) method, and would allow Java 8 users to use lambdas. Of course, switching from the abstract class to an interface would change the existing contract for current Java API users - so instead of calling the static convert() method in the implementation, the methods could be overloaded, with one taking the interface type (for Java 8 lambda users), and the other one taking the abstract class type. The interface type-taking overloaded method would then call convert() and would send the abstract class with implemented call() to the already existing API method. It seems that this might add some extra code to the already time-consuming effort of maintaining a separate Java API, but I thought I would share some ideas as to how to accommodate Java 8 users.", "created": "2013-11-30T08:22:27.828+0000"}, {"author": "Josh Rosen", "body": "Of course, switching from the abstract class to an interface would change the existing contract for current Java API users - so instead of calling the static convert() method in the implementation, the methods could be overloaded, with one taking the interface type (for Java 8 lambda users), and the other one taking the abstract class type.  Instead of overloading the methods, could we only implement methods that take the interface type and change the Java 6/7 abstract Function* classes to implement that interface instead of extending the Scala AbstractFunction classes? This shouldn't break user code as long as users haven't overridden {{andThen()}} or {{compose()}}. Most of the Java API wrapper is implemented in Scala, so we could use use implicit conversions to avoid adding explicit {{convert()}} calls. Hopefully we could make most of the changes via find-and-replace on the method signatures. Is there a clean way to handle the JavaDoubleRDD / JavaPairRDD conversions in Java 8? Right now, we require users to implement DoubleFunction / PairFunction classes because we can't overload methods like map() based on the function's return type. One approach would be to define constructors for {{DoubleFunction}} and {{PairFunction}} that accept a implementation of the new function interface; this would let Java 8 users write something like {{foo1.map(new PairFunction<Int, Int>(x -> new Tuple2<Int, Int>(x, x)))}} to get a JavaPairRDD. This is a little verbose, but it gives us static type safety.", "created": "2013-11-30T11:55:28.351+0000"}, {"author": "Marek Kolodziej", "body": "Yes, I totally agree. As long as users haven't overridden andThen() and compose(), we should be fine replacing the abstract classes with the interfaces. This would certainly be easier to maintain in the long run anyway. I'm thinking back to the \"revolution\" in the API that Akka went through in version 2.2 - some changes broke existing code at a pretty basic level, but after a headache, I appreciated the new changes. I think that swapping the interfaces in Spark's case would hardly affect any of the existing users, so in relative terms, that should be a small transition. And yes, I love implicit conversions. I'll look more into how the wrappers are implemented, so far I was focused on the Scala API. :) One thing that we have to consider to give Java 8 users lambdas while not breaking Java 6/7 users' access is that even if we replace the abstract Function classes with the Function interfaces, we would have to create a Spark package for those. There are two reasons for this: 1) JDK 6/7 users don't have default methods, so we couldn't have andThen() and compose() in the interfaces the way JDK 8 has them implemented. 2) JDK 6/7 users won't have the java.util.function package, which is why it would have to be a Spark package. This should be easy and quick to do. And this way, given one code base and no doubling up with overloaded methods, we could keep JDK 6/7 and JDK 8 users happy at minimum effort, I think. I can definitely look into the DoubleRDD/PairRDD aspect too. In Java 8, there are both non-generically typed and generic function interfaces ([see here|http://download.java.net/jdk8/docs/api/java/util/function/package-summary.html]. The specific ones are mostly for primitives, but I guess they could be useful in some other cases, given type erasure. So, in addition to Function<T, R>, there's also [DoubleFunction|http://download.java.net/jdk8/docs/api/java/util/function/DoubleFunction.html] (which is essentially Function<Double, R>), [DoubleUnaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleUnaryOperator.html] (which is Function<Double, Double> where the method applyAsDouble() (which could just be apply() for consistency) takes in a double primitive and returns a double primitive, after boxing/unboxing), etc. There's also [DoubleBinaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleBinaryOperator.html] and so on. It could either be a PairFunction wrapping the lambda, or just something like the above. I think the PairFunction wrapping the lambda wouldn't be too verbose for JDK 8 users since the lambda would reduce the verbosity coming from the wrapper, but it would be more anonymous class verbosity for JDK 6/7 users who are already dealing with one anonymous class. The nice thing for JDK 8 users is that they won't care if the interface is called DoubleBinaryOperator or FooBar, since all they have to do is insert the lambda, and the compiler will figure out whether the lambda honors the contract specified by the function interface in the map() or other method call. The JDK 6/7 users will of course have to know what the interface is.", "created": "2013-11-30T12:59:48.448+0000"}, {"author": "Marek Kolodziej", "body": "So are we all set regarding the approach for the implementation POC? Shall I try to get it working by replacing the Function abstract class with the Java 8-compatible functional interface for all classes, and once I get it working, demonstrate and see if it would be a good approach for all the other functional abstract classes?", "created": "2013-12-02T07:55:29.897+0000"}, {"author": "Josh Rosen", "body": "Sure, that sounds like a good plan. Thanks for helping with this! One comment:  I think the PairFunction wrapping the lambda wouldn't be too verbose for JDK 8 users since the lambda would reduce the verbosity coming from the wrapper, but it would be more anonymous class verbosity for JDK 6/7 users who are already dealing with one anonymous class.  I don't think that this has to introduce any additional verbosity for Java 6/7 users. PairFunction could be an abstract base class that Java 7 users subclass like they do now, but with a static method for constructing a concrete PairFunction sublcass from a Java 8 lambda:  public static PairFunction<T, K, V> of(Function<T, Tuple2<K, V>> function)  so Java 7 users would subclass this like they do now, but Java 8 users could write  PairFunction.of(x -> new Tuple2<Int, Int>(x, x))  I'm not sure if Java type inference would work here, so it might have to be something like  PairFunction.of<Int, Int, Int>(x -> new Tuple2<Int, Int>(x, x))", "created": "2013-12-03T11:14:02.960+0000"}, {"author": "Marek Kolodziej", "body": "This is a great idea! I'll get to work on a PoC right away. This week is looking pretty busy, but I'll keep you posted once I am able to provide a working example to get your feedback before applying it throughout the API. Thanks!", "created": "2013-12-10T08:44:35.827+0000"}, {"author": "Marek Kolodziej", "body": "I have one more concern before I go ahead and start implementing. Since pre-Java 8 users may wish to build Spark from source using their older JDKs, wouldn't it make sense to ignore java.util.function interfaces and create our own? Java 8 doesn't care which ones one is using, as long as there is a single abstract method and the right types (generic or not). On the other hand, pre-Java 8 users would care both because of the absence of the package, and because older Java compilers don't know about default methods that exist in the interfaces of that package.", "created": "2013-12-11T13:37:40.560+0000"}, {"author": "Marek Kolodziej", "body": "I just wanted to share an example to see if this is a desirable implementation so far. Note that Java 8 lambdas require those interfaces, rather than the current abstract Function and related classes, otherwise lambdas will not work. Per Josh's suggestions above, I created of() static methods in the DubleFunction/DoubleFlatMapFunction/Function2/Function3/PairFlatMapFunction/PairFunction classes that take instance of IDoubleFunction/IDoubleFlatMapFunction/IFunction/IFunction2/IFunction3/IPairFlatMapFunction/IPairFunction, and instantiate the enclosing abstract classes using the call() method implemented in the anonymous class passed by the user. I wanted to avoid the \"I\" prefix but the Function names were taken for the abstract classes. Unlike the abstract classes, these interfaces are compatible with Java 8 lambdas, but the abstract classes are kept for backward compatibility with the existing Java 6/7 API. The of() static methods can be used by Java 6/7 but that's pointless since the verbosity is the same - however these static methods can accept Java 8 lambdas. I have a working example - the Java 7-compatible code can be found [here|https://github.com/mkolod/incubator-spark/tree/java8-lambda]. While this should be clear from the commit, all the changes pertain to [org.apache.spark.api.java.function|https://github.com/mkolod/incubator-spark/tree/java8-lambda/core/src/main/scala/org/apache/spark/api/java/function]. JDK 8 can be downloaded from [here|https://jdk8.java.net/download.html]. IntelliJ 12 has excellent Java 8 support, and IntelliJ 13 has some additional improvements. Eclipse support is still considerably lagging, but Java 8-compatible Eclipse can be obtained from [here|http://downloads.efxclipse.org/eclipse-java8/2013-09-13/]. After building Spark itself (e.g. \"sbt assembly\"), it's possible to try the Java 8 lambda-based version of the Java example found [here|http://spark.incubator.apache.org/docs/latest/quick-start.html#a-standalone-app-in-java], by replacing the lines  long numAs = logData.filter(new Function<String, Boolean>() { public Boolean call(String s) { return s.contains(\"a\"); } }).count(); long numBs = logData.filter(new Function<String, Boolean>() { public Boolean call(String s) { return s.contains(\"b\"); } }).count();  with  long numAs = logData.filter(Function.of(s -> s.contains(\"a\"))).count(); long numBs = logData.filter(Function.of(s -> s.contains(\"b\"))).count();  Then, one can run it normally in local mode, e.g.  java8/jdk/path/java -cp .:spark-java8.jar Test  I'm not sure what kinds of unit tests would be applicable here. Since most users will be pre-Java 8, the arguments passed to Function.of(), and to similar static methods on DoubleFunction/DoubleFlatMapFunction/Function2/Function3/PairFlatMapFunction/PairFunction, would have to be in terms of anonymous class implementations of the new IDoubleFunction/IDoubleFlatMapFunction/IFunction/IFunction2/IFunction3/IPairFlatMapFunction/IPairFunction interfaces rather than lambdas. This would become a bit circular, but would ensure that the tests would run in all cases. I believe that it would simply suffice to verify that the behavior of the function objects created using anonymous classes and direct use of DoubleFunction/DoubleFlatMapFunction/Function/Function2/Function3/PairFlatMapFunction/PairFunction would be equivalent to the ones using anonymous class implementations of the IDoubleFunction/IDoubleFlatMapFunction/IFunction/IFunction2/IFunction3/IPairFlatMapFunction/IPairFunction, which would be used with lambdas as well. Please let me know what else should be done until it would be OK to issue a pull request. My only concern is that static methods on Function/Function2 etc. require the Java 8 user to know which function type they require. With lambdas, one could have overloaded static \"of\" methods in one place (e.g. AllFun class) that would take in the kind of lambda that the user supplies, and convert it to the instances of the existing abstract classes that the user provides. The lambda would be inferred to be of a particular interface type, which would call the required method from the list of available overloaded method options. That might make for an easier API for Java 8 developers - the point of lambdas was not to remember interfaces but to simply provide a lambda that would fit one of the legal interfaces, and to have Java pick up the right one. This might be a possible improvement over the current state.", "created": "2013-12-14T11:22:22.100+0000"}, {"author": "Josh Rosen", "body": "Sorry for the late reply; I've been really busy since it's the end of the semester here. This is a nice start. If we replaced occurrences of Function with IFunction in the API's methods (so we have {{RDD<T>.map: IFunction<T, R> -> RDD<R>}}), then we might be able to omit the {{Function.of()}} calls and only require {{DoubleFunction.of()}} and {{PairFunction.of()}}. On the other hand, that change would touch a lot more code and makes the API a little less uniform, since then we'd only _sometimes_ need to use the wrappers. As far as unit tests are concerned, the existing test suite should cover Java 6/7. It might be nice to include some examples and unit tests that use Java 8 lambdas, but I wouldn't want to break the ability for Java 6/7 users to run the tests. Maybe there's a way to conditionally run those tests if a Java 8 compiler is available, but that might be painful to configure in Maven and SBT. We should ensure that the interfaces are compatible with Java 8's lambda requirements, but this probably only requires simple test program to verify that everything compiles correctly. To start, I think the tests that you proposed sound reasonable. We should also update the [Java Programming Guide|https://spark.incubator.apache.org/docs/latest/java-programming-guide.html] to document this new feature.  With lambdas, one could have overloaded static \"of\" methods in one place (e.g. AllFun class) that would take in the kind of lambda that the user supplies, and convert it to the instances of the existing abstract classes that the user provides.  Is it possible to overload methods based on the lambda's type? I don't think we can have a class with overloaded methods like {{of(IFunction<T, Pair<K, V>>), of(IFunction<T, Double>)}} where we're overloading based on some generic type's type parameter. Even if those functions implemented different interfaces, wouldn't the resolution of the right method be ambiguous if I have methods like {{of(IFunction<T, R>)}} and {{of(IDoubleFunction<T>)}} that I call with a lambda {{of(x -> Double(2.0))}}? I'd love to have a cleaner interface with fewer wrappers, but my hunch is that it won't be possible due to type erasure.", "created": "2013-12-15T14:04:50.826+0000"}, {"author": "Marek Kolodziej", "body": "Sorry for the late reply on my end, I was really ill for a few days. I agree that having the direct calls in some cases and static method calls in others would lead to confusion. However, I believe that we could make direct calls in all cases. For example, if DoubleFunction is only used with DoubleRDDs, then DoubleRDDs could take a special interface that deals with Doubles. Also, the key thing that would allow Java 8 to distinguish between non-reified generics and specific types such as Double would be to have interfaces that either take generics or specific types such as Double. When the Java 8 compiler sees a lambda that uses a Double, it will try to match against the closest interface while performing type inference. Therefore, if there's a function that is Double => Double, or Double => T, the IDoubleFunction would be picked instead of IFunction. I will test that to be sure, but that is exactly why the [java.util.function|http://download.java.net/jdk8/docs/api/java/util/function/package-summary.html] package has both generic interfaces such as [Function<T,R>|http://download.java.net/jdk8/docs/api/java/util/function/Function.html] and specific ones such as [DoubleFunction<R>|http://download.java.net/jdk8/docs/api/java/util/function/DoubleFunction.html] and [DoubleBinaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleBinaryOperator.html], the latter being Double => Double. In case of IFunction<T, Pair<K,V>> maybe we could have IPairFunction<T, K, V> where the generic types would be unknown at runtime, but the fact that one of the types is Pair<?> would help with type inference for the lambda? Maybe the same could be the case for IFunction<T, Double> becoming IDoubleFunction<T>? I suppose it might be a problem that Double is the return type here, as opposed to a parameter type, but I would also assume that type inference has to be clever enough to determine that the returned type is consistent with whatever is consuming the returned value - so perhaps it would pick up the most specific case? I'll try to investigate that. I believe the above idea could resolve the issue of Java 8's inference of the special types that are not generic, such as Double. However, there are of course solutions that deal with the reification of generics. Most of the Java ones are hacky. The Scala ones, such as [scala.reflect.ClassTag|http://www.scala-lang.org/files/archive/nightly/docs/library/scala/reflect/ClassTag.html] are elegant but not suited for Java. Java 8 will have a similar solution using [type annotations|http://openjdk.java.net/projects/type-annotations/], but code using them wouldn't compile with earlier versions of Java, so the other above mentioned solutions (or maybe others not included here) would have to be considered. I would appreciate additional feedback given the above while I also investigate the possibility of using the above solution separating the interfaces for Double and generics, etc.", "created": "2013-12-26T09:11:27.576+0000"}, {"author": "Marek Kolodziej", "body": "I just did some further investigation and it seems that it would be possible to make certain distinctions while using the lambdas directly, without the static method conversions. I added the following in org.apache.spark.api.java.JavaRDDLike:  def map[T1, R](f: IFunction[T1, R]): JavaRDD[R] = new JavaRDD(rdd.map(x => f.apply(x))) def map(f: IDoubleFunction[T]): JavaDoubleRDD = new JavaDoubleRDD(rdd.map(x => f.apply(x)))  Then I updated the code not to use the Function.of() type wrappers, and just use the lambdas directly:  package foo.bar; import org.apache.spark.api.java.*; public class Test { public static void main(String[] args) { JavaSparkContext sc = new JavaSparkContext(\"master\", \"appName\"); JavaRDD<String> lines = sc.textFile(\"hdfs://...\"); JavaDoubleRDD doubleRDD = lines.map(Double::parseDouble); JavaRDD<String> genericDoubleRDD = lines.map(String::toLowerCase); } }  This seems to work, however Java 8 type inference can't distinguish between an IIFunction and an IPairFunction, the former returning generic R and the latter returning Tuple2<K, V>. Therefore, with overloaded map() methods, Java 8 would be confused as to whether create a JavaPairRDD<String, Integer> or a JavaRDD<Tuple2<String, Integer>>. So, perhaps the static wrappers we agreed on earlier are unavoidable. I would appreciate ideas to the contrary. Otherwise, if we agree on the static wrappers, shall I just add the unit tests and issue a pull request (I assume that updating the [Java Programming Guide|https://spark.incubator.apache.org/docs/latest/java-programming-guide.html] would only happen after the code is merged?).", "created": "2013-12-26T10:42:12.355+0000"}, {"author": "Josh Rosen", "body": "What happens when you call a method that's overloaded for IFunction and IPairFunction with a lambda that returns a Tuple2? Does it result in an ambiguity error or does it consider the IFunction interface to be more specific than IPairFunction? I'm curious why Double works while Tuple2 doesn't. Does the double example fail if you use valueOf instead of parseDouble? If you dump your current draft on GitHub, I'm happy to play around to see if there's any way to eliminate the wrapper classes. Otherwise, I guess we'll need to use the static wrappers. Feel free to submit a pull request, even if it's incomplete. GitHub is nice for discussing code, and you can always update the PR to add unit tests and new documentation once you've gotten community feedback on the basic design (I think a lot more people follow Spark's development on GitHub than on JIRA). I would like to make sure that the Java Programming Guide is updated before we merge this, but those changes don't need to be made before opening the PR.", "created": "2013-12-27T22:57:31.686+0000"}, {"author": "Matei Alexandru Zaharia", "body": "According to http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-types-and-method-arguments overloaded methods should resolve which method to call based on the return types, so it might be possible to make this work without wrappers even if you return a Tuple2 or Double. You guys should definitely try that.", "created": "2014-02-15T16:11:16.176+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/17", "created": "2015-12-10T15:06:40.066+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/71", "created": "2015-12-10T15:06:41.445+0000"}], "num_comments": 18, "text": "Issue: SPARK-964\nSummary: Investigate the potential for using JDK 8 lambda expressions for the Java/Scala APIs\nDescription: JDK 8 (to be released soon) will have lambda expressions. The question is whether they can be leveraged for Java to use Scala's Spark API (perhaps with some modifications), or whether a new functional API would need to be developed for Java 8+.\n\nComments (18):\n1. Marek Kolodziej: Here are some initial findings. Java's lambdas can be used with Scala, after a trivial conversion is applied. Scala's Function1<T, R> is essentially Java's Function<T, R>, but Java doesn't see scala.Function1 as honoring the JDK8 \"functional interface\" contract, with all non-apply() methods being [\"default\"|http://blog.sanaulla.info/2013/03/20/introduction-to-default-methods-defender-methods-in-java-8/]. It seems that Java's augmented \"interfaces with default methods\" aren't an exact match for Scala's traits. Therefore, one possibility is to do the following. To honor Scala's Function1 through Function22, one can create analogous Java interfaces, like so:  @FunctionalInterface public interface JavaFunction1<T1, R> { R apply(T1 v1); } @FunctionalInterface public interface JavaFunction2<T1, T2, R> { R apply(T1 v1, T2 v2); } // Function3-Function22 follow the same pattern  The FunctionalInterface annotation is solely there to issue compiler errors if the interface doesn't honor the contract required by lambdas, but it's not strictly necessary. Then, let's create a regular Scala class that will accept lambdas as method inputs. Here, I just created a dummy class  package foo class FakeRDD[T](in: List[T]) { def this(in: Array[T]) = this(in.toList) def map(f: T => T) = new FakeRDD(in.map(f)) /* don't let the user know that the internal representation is a List - hence I didn't use a case class with auto-generated toString() */ override def toString() = \"FakeRDD(\" + in.mkString(\",\") + \")\" }  We will also need a Java8-Scala lambda converter, like so (just for Function1 here, could be expanded to Function2-Function22:  package foo import foo.{JavaFunction1 => Function1} object Conv { def f[T1, R](fun: Function1[T1, R]) = (v1: T1) => fun(v1) }  Now, let's go and create our Java client - instead of map(javaLambda), we'll have the wrapper that converts from a Java 8 to a Scala lambda, e.g. map(f(javaLambda)) where f(javaLambda) will resolve to scalaLambda  package foo; import static foo.Conv.f; public class FooBar { public static void main(final String[] args) { final FakeRDD<Integer> rdd1 = new FakeRDD<Integer>(new Integer[] {1, 2, 3}); final FakeRDD<Integer> rdd2 = rdd1.map(f((Integer i) -> i * 2)); System.out.println(rdd1); System.out.println(rdd2); } }  So now, the output from the execution of the Java client will be:  FakeRDD(1,2,3) FakeRDD(2,4,6)  By the way, note that the f() wrapper is actually less boilerplate than using Java 8 collections with lambdas, since e.g. an ArrayList<T> would have to call a stream() method to get a Stream<T>, then apply the lambda, and then call collect() to get back a List, like so:  import java.util.ArrayList; import java.util.List; import java.util.Optional; import java.util.Random; public class Test { public static void main(final String[] args) { // seed = 1 final Random rand = new Random(1); final List<Double> list = new ArrayList<>(); for (int i = 1; i < 100; i++) { list.add(rand.nextGaussian()); } final Optional<Double> sumOfDoubledPositives = list.stream().filter(d -> d > 0).map(d -> d * 2).reduce((d1, d2) -> d1 + d2); if (sumOfDoubledPositives.isPresent()) { System.out.println(sumOfDoubledPositives.get()); } } }  So IMHO, that wrapper around the Scala Spark API would be more convenient for JDK 8 users than their own Java Collections Framework. However, note that the conversion from Java to Scala lambdas prevents the Java compiler from inferring types, because it can only do so for its own syntactic sugar, so instead of saying \"i -> i * 2\" you have to say \"(Integer i) -> i * 2.\" Nevertheless, I'm not sure if that inference is universal, and that's a small cost relative to the overall productivity boost stemming from a functional API. Maybe this inference issue will get resolved, after all JDK 8 is still in beta. Note also that I believe that for compatibility reasons, it may be necessary to keep the old Java Spark API for people with pre-Java 8 JDKs. However, it might be interesting to investigate whether moving from the existing Function interfaces in the Spark Java API to interfaces meeting the FunctionalInterface contract wouldn't allow JDK 8 users to benefit from lambdas directly, without the f() wrapper while using the Scala API. To the extent that the Scala API is more up to date though, it would be preferable for JDK 8 users to use the Scala API with that tiny f() wrapper in my opinion. Also, changing the Spark-specific Function interfaces to JDK 8 ones would break existing Java clientcode, so also for that reason it might be preferable to keep the pre-Java 8 API as-is while facilitating the use of the Scala API for JDK 8+ users. Of course, one could also get rid of the f() wrapper and have overloaded methods in the Spark classes, i.e. the original methods taking a Scala lambda, and analogous ones taking a Java lambda, that call the Java-to-Scala-lambda converter and redirect to the Scala lambda-based methods. Or one could accept Java lambdas in the existing methods instead of Scala ones, and import reusable implicit conversion defs for JavaFunction1-JavaFunction22 to Scala's Function1-Function22. The last option is way too invasive for the existing codebase and makes too many concessions for Java users, IMHO. The method overloading puts the burden on the Spark codebase too. On the other hand, a tiny client-side f() wrapper still permits the Java user to benefit from FP which is way better than creating anonymous classes everywhere, and lets the Java user access the more complete Scala API, but it doesn't affect all the existing Spark classes - one would merely need to include that wrapper and use it everywhere.\n2. Marek Kolodziej: Note that in the above case, I was able to use Java's lambda in place of Scala's lambda when calling map() on FakeRDD<Integer>. However, using Scala's collections with this approach would be a bit different than what we're used to in Scala itself. For instance, we don't think twice about calling map() on a List. However, this wouldn't work with Java's lambdas. Instead, one would need to call mapConserve, like so:  final List<String> scalaList = List.fromArray(new String[]{\"foo\", \"bar\"}); final List<String> scalaList2 = scalaList.mapConserve(f((String s) -> s.toUpperCase()));  This works whether the transformation results in the same or a different type, for instance:  final List<String> scalaList3 = List.fromArray(new String[]{\"1\", \"2\"}); final List<Integer> scalaList4 = scalaList3.mapConserve(f((String s) -> Integer.parseInt(s)*2));  The reason that map() doesn't work but mapConserve() does is that map() is a curried method with an implicit argument:  map[B, That](f: (A) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That  On the other hand, mapConserve() doesn't have currying or implicit parameters:  mapConserve[B >: A <: AnyRef](f: (A) ⇒ B): List[B]  Since in case of FakeRDD the Java-exposed public map() method didn't have currying or implicits, Java \"understood\" it after the Java-to-Scala lambda conversion. In case of Scala's List, that was not the case (the above declarations of map() and mapConserve() come from [there|http://www.scala-lang.org/api/current/scala/collection/immutable/List.html]). So, as long as the Spark Scala API sticks to single parameter lists and no implicits, Java can use it with that thin f() wrapper.\n3. Josh Rosen: The {{f()}} wrapper approach would require still Java 8 users to manually wrap RDD\\[(K, V)] into PairRDDFunctions\\[K, V] since the implicit conversion won't happen in Java (see https://cwiki.apache.org/confluence/display/SPARK/Java+API+Internals). We would probably want to continue to expose Java collections types instead of their Scala counterparts. Therefore, I think that we should continue maintain a separate Java wrapper. It's a bit annoying to maintain, but the JavaAPICompletenessChecker script should make that easier; most of this code shouldn't have to ever change once it's been written since the user-facing APIs should remain stable and most of the code is quite simple.\n4. Marek Kolodziej: Hi Josh, yes, no question about maintaining a separate Java API. First of all, not everyone will upgrade to Java 8 (in fact most people are on JDK 6), so one has to support a separate API. I also get your point regarding not using the Scala API with the f() wrapper so as to return Java instead of Scala collections, and so on. What I'm thinking of investigating next is whether Java 8 lambdas could be used with the Java API. Looking at Spark's [function|http://spark.incubator.apache.org/docs/latest/api/core/index.html#org.apache.spark.api.java.function.package] package, the various Functions are handled by abstract classes as opposed to interfaces. Java 8 lambdas are based on an API having methods that take lambdas accept interface types that are single abstract method (SAM) interfaces - the \"abstract\" no longer being redundant, since Java 8 interfaces can have concrete (\"default\") methods, much like Scala's traits. While abstract classes are similar to interfaces with default methods, they are not the same syntactic construct, and the compiler complained when I tried using a lambda with Spark's methods that take Function instances for that reason. This is the case even though they are SAM abstract classes, but lambdas were only built around SAM interfaces. I have a new idea. The key goals with any solution, I think, are to support pre-Java 8 users while providing Java 8+ users with the convenience of lambdas. With that in mind, here's what I've been thinking about. Since pre-Java 8 interfaces can't have concrete methods, we can't provide the same exact functional API to Java 8 users as we would get by using the [java.util.function.Function|http://download.java.net/jdk8/docs/api/java/util/function/Function.html] interface, with default compose()/andThen() methods. In other words, we couldn't replace the existing Spark abstract Function classes. However, since the only abstract method in org.apache.spark.api.java.function.Function is call(), then perhaps one could expose just interfaces requiring the implementation of the call() method, and these abstract classes could be instantiated with that implementation within the Spark code itself. For example, say we have this simple SAM Function interface:  package test; interface Function<T, R> { public R call(T t); }  This would be what the Java 6/7 user would have to implement as an anonymous class, while a Java 8 user could in-line as a lambda. Now, let's suppose that internally, we need an abstract class with concrete methods like Spark's Function. Let's call it AbstractFun here to distinguish it from the Function interface above.  package test; public abstract class AbstractFun<T, R> implements Function<T, R> { /* we're getting \"public R call(T t)\" from Function, but we're not implementing it since the class is abstract */ /* second abstract method (in addition to Function's call()) breaks Java 8 lambda SAM contract, and Java 8 requires an interface rather than an abstract class anyway for lambdas */ public abstract void breakFunctionalInterfaceSAMContract(); /* a concrete method is OK in an abstract class, but Java 8 requires an interface with default methods instead for lambdas, while pre-Java 8 there were no default methods */ public int theMeaningOfLife() { return 42; } }  So, how about having the API users only implement the Function interface, and having an internal conversion from the interface to the AbstractFun which has other concrete methods?  package test; public class Utils { public static <T, R> AbstractFun<T, R> convert(final Function<T, R> fun) { return new AbstractFun<T, R>() { @Override public R call(final T t) { return fun.call(t); } /* implement the other abstract method if need be - if there are no other abstract method, we can just implement call() and we're done */ @Override public void breakFunctionalInterfaceSAMContract() {} }; } }  Now in our API class (such as an RDD, we can just call the convert() method to instantiate the abstract class with the implementation of call() from the anonymous class passed by the user. Note the line  l2.add(convert(fun).call(t));  See the code below  package test; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import static test.Utils.convert; public class FakeRDD<T> { final private List<T> l; public FakeRDD(final List<T> l) { this.l = l; } public <R> FakeRDD<R> map(final Function<T, R> fun) { final List<R> l2 = new ArrayList<R>(); for (final T t : l) { l2.add(convert(fun).call(t)); } return new FakeRDD(l2); } @Override public String toString() { // don't show that we have a list inside - don't use List's toString() return Arrays.toString(l.toArray()); } }  Now let's run it.  package test; import java.util.Arrays; public class AbstractTest { public static void main(final String[] args) { final FakeRDD<Integer> foo1 = new FakeRDD<>(Arrays.asList(1, 2, 3, 4, 5)); final FakeRDD<Integer> foo2 = foo1.map(i -> i); System.out.println(foo1); System.out.println(foo2); } }  That way, if there are internal implementation dependencies on the abstract class, we can convert the interface to the existing class. This would allow pre-Java 8 to use the API as before (by implementing the call()) method, and would allow Java 8 users to use lambdas. Of course, switching from the abstract class to an interface would change the existing contract for current Java API users - so instead of calling the static convert() method in the implementation, the methods could be overloaded, with one taking the interface type (for Java 8 lambda users), and the other one taking the abstract class type. The interface type-taking overloaded method would then call convert() and would send the abstract class with implemented call() to the already existing API method. It seems that this might add some extra code to the already time-consuming effort of maintaining a separate Java API, but I thought I would share some ideas as to how to accommodate Java 8 users.\n5. Josh Rosen: Of course, switching from the abstract class to an interface would change the existing contract for current Java API users - so instead of calling the static convert() method in the implementation, the methods could be overloaded, with one taking the interface type (for Java 8 lambda users), and the other one taking the abstract class type.  Instead of overloading the methods, could we only implement methods that take the interface type and change the Java 6/7 abstract Function* classes to implement that interface instead of extending the Scala AbstractFunction classes? This shouldn't break user code as long as users haven't overridden {{andThen()}} or {{compose()}}. Most of the Java API wrapper is implemented in Scala, so we could use use implicit conversions to avoid adding explicit {{convert()}} calls. Hopefully we could make most of the changes via find-and-replace on the method signatures. Is there a clean way to handle the JavaDoubleRDD / JavaPairRDD conversions in Java 8? Right now, we require users to implement DoubleFunction / PairFunction classes because we can't overload methods like map() based on the function's return type. One approach would be to define constructors for {{DoubleFunction}} and {{PairFunction}} that accept a implementation of the new function interface; this would let Java 8 users write something like {{foo1.map(new PairFunction<Int, Int>(x -> new Tuple2<Int, Int>(x, x)))}} to get a JavaPairRDD. This is a little verbose, but it gives us static type safety.\n6. Marek Kolodziej: Yes, I totally agree. As long as users haven't overridden andThen() and compose(), we should be fine replacing the abstract classes with the interfaces. This would certainly be easier to maintain in the long run anyway. I'm thinking back to the \"revolution\" in the API that Akka went through in version 2.2 - some changes broke existing code at a pretty basic level, but after a headache, I appreciated the new changes. I think that swapping the interfaces in Spark's case would hardly affect any of the existing users, so in relative terms, that should be a small transition. And yes, I love implicit conversions. I'll look more into how the wrappers are implemented, so far I was focused on the Scala API. :) One thing that we have to consider to give Java 8 users lambdas while not breaking Java 6/7 users' access is that even if we replace the abstract Function classes with the Function interfaces, we would have to create a Spark package for those. There are two reasons for this: 1) JDK 6/7 users don't have default methods, so we couldn't have andThen() and compose() in the interfaces the way JDK 8 has them implemented. 2) JDK 6/7 users won't have the java.util.function package, which is why it would have to be a Spark package. This should be easy and quick to do. And this way, given one code base and no doubling up with overloaded methods, we could keep JDK 6/7 and JDK 8 users happy at minimum effort, I think. I can definitely look into the DoubleRDD/PairRDD aspect too. In Java 8, there are both non-generically typed and generic function interfaces ([see here|http://download.java.net/jdk8/docs/api/java/util/function/package-summary.html]. The specific ones are mostly for primitives, but I guess they could be useful in some other cases, given type erasure. So, in addition to Function<T, R>, there's also [DoubleFunction|http://download.java.net/jdk8/docs/api/java/util/function/DoubleFunction.html] (which is essentially Function<Double, R>), [DoubleUnaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleUnaryOperator.html] (which is Function<Double, Double> where the method applyAsDouble() (which could just be apply() for consistency) takes in a double primitive and returns a double primitive, after boxing/unboxing), etc. There's also [DoubleBinaryOperator|http://download.java.net/jdk8/docs/api/java/util/function/DoubleBinaryOperator.html] and so on. It could either be a PairFunction wrapping the lambda, or just something like the above. I think the PairFunction wrapping the lambda wouldn't be too verbose for JDK 8 users since the lambda would reduce the verbosity coming from the wrapper, but it would be more anonymous class verbosity for JDK 6/7 users who are already dealing with one anonymous class. The nice thing for JDK 8 users is that they won't care if the interface is called DoubleBinaryOperator or FooBar, since all they have to do is insert the lambda, and the compiler will figure out whether the lambda honors the contract specified by the function interface in the map() or other method call. The JDK 6/7 users will of course have to know what the interface is.\n7. Marek Kolodziej: So are we all set regarding the approach for the implementation POC? Shall I try to get it working by replacing the Function abstract class with the Java 8-compatible functional interface for all classes, and once I get it working, demonstrate and see if it would be a good approach for all the other functional abstract classes?\n8. Josh Rosen: Sure, that sounds like a good plan. Thanks for helping with this! One comment:  I think the PairFunction wrapping the lambda wouldn't be too verbose for JDK 8 users since the lambda would reduce the verbosity coming from the wrapper, but it would be more anonymous class verbosity for JDK 6/7 users who are already dealing with one anonymous class.  I don't think that this has to introduce any additional verbosity for Java 6/7 users. PairFunction could be an abstract base class that Java 7 users subclass like they do now, but with a static method for constructing a concrete PairFunction sublcass from a Java 8 lambda:  public static PairFunction<T, K, V> of(Function<T, Tuple2<K, V>> function)  so Java 7 users would subclass this like they do now, but Java 8 users could write  PairFunction.of(x -> new Tuple2<Int, Int>(x, x))  I'm not sure if Java type inference would work here, so it might have to be something like  PairFunction.of<Int, Int, Int>(x -> new Tuple2<Int, Int>(x, x))\n9. Marek Kolodziej: This is a great idea! I'll get to work on a PoC right away. This week is looking pretty busy, but I'll keep you posted once I am able to provide a working example to get your feedback before applying it throughout the API. Thanks!\n10. Marek Kolodziej: I have one more concern before I go ahead and start implementing. Since pre-Java 8 users may wish to build Spark from source using their older JDKs, wouldn't it make sense to ignore java.util.function interfaces and create our own? Java 8 doesn't care which ones one is using, as long as there is a single abstract method and the right types (generic or not). On the other hand, pre-Java 8 users would care both because of the absence of the package, and because older Java compilers don't know about default methods that exist in the interfaces of that package.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.094294"}}
{"id": "52e224fe2f46e18171bc989cdb6dcd58", "issue_key": "SPARK-965", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Race in DAGSchedulerSuite", "description": "After https://github.com/apache/incubator-spark/pull/159, resubmitFailedStages is scheduled in the eventProcessorActor preStart to run periodically in the actor's thread. But DAGSchedulerSuite calls resubmitFailedStages directly from its own thread, which means that it is possible for both threads to be concurrently mutating shared data structures (e.g. the set of failed stages and cacheLocs), leading to failure of the test calling resubmitFailedStages.", "reporter": "Mark Hamstra", "assignee": "liancheng", "created": "2013-11-25T14:19:29.000+0000", "updated": "2013-12-07T14:50:48.000+0000", "resolved": "2013-12-07T14:50:43.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/apache/incubator-spark/pull/216", "created": "2013-12-07T14:50:43.273+0000"}], "num_comments": 1, "text": "Issue: SPARK-965\nSummary: Race in DAGSchedulerSuite\nDescription: After https://github.com/apache/incubator-spark/pull/159, resubmitFailedStages is scheduled in the eventProcessorActor preStart to run periodically in the actor's thread. But DAGSchedulerSuite calls resubmitFailedStages directly from its own thread, which means that it is possible for both threads to be concurrently mutating shared data structures (e.g. the set of failed stages and cacheLocs), leading to failure of the test calling resubmitFailedStages.\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/apache/incubator-spark/pull/216", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.094294"}}
{"id": "3011e15e722c058f7bedbff203bd5e60", "issue_key": "SPARK-966", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Sometimes DAGScheduler throws NullPointerException", "description": "When running some spark examples, I run into NullPointerException thrown by DAGScheduler. Log is as follows, 13/11/27 17:26:40 ERROR dispatch.TaskInvocation: java.lang.NullPointerException at org.apache.spark.scheduler.DAGScheduler$$anonfun$3$$anon$1$$anonfun$preStart$1.apply$mcV$sp(DAGScheduler.scala:116) at akka.actor.DefaultScheduler$$anon$1.run(Scheduler.scala:142) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94) at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 13/11/27 17:26:42 WARN util.SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes", "reporter": "Qiuzhuang Lian", "assignee": "liancheng", "created": "2013-11-27T01:30:50.000+0000", "updated": "2013-12-07T14:46:03.000+0000", "resolved": "2013-12-07T14:45:43.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "What version of Spark is this? Can you paste the code near line 116 in DAGScheduler.scala?", "created": "2013-11-27T14:35:40.144+0000"}, {"author": "Qiuzhuang Lian", "body": "I am using git trunk version. The last git version is e2a43b3 on 11/11/2013 by Lian,Cheng. line 116 in that class is if (failed.size > 0) { and here is the code near that line, private val eventProcessActor: ActorRef = env.actorSystem.actorOf(Props(new Actor { override def preStart() { context.system.scheduler.schedule(RESUBMIT_TIMEOUT milliseconds, RESUBMIT_TIMEOUT milliseconds) { if (failed.size > 0) { resubmitFailedStages() } } }", "created": "2013-11-27T18:57:00.506+0000"}, {"author": "Reynold Xin", "body": "[~liancheng] can you take a look at this?", "created": "2013-11-27T19:13:37.738+0000"}, {"author": "liancheng", "body": "Hi [~Qiuzhuang], I think I've found the cause, but not fully verified yet. Would you please elaborate more on the Spark examples you were running? I need the command line arguments and possible input data files to verify this bug. Thanks! The problematic commit is actually [2539c06|https://github.com/liancheng/incubator-spark/commit/2539c0674501432fb62073577db6da52a26db850], an ancestor of [e2a43b3|https://github.com/liancheng/incubator-spark/commit/e2a43b3dcce81fc99098510d09095e1be4bf3e29]. In that commit, I replaced the daemon thread in {{DAGScheduler}} with an Akka actor {{eventProcessActor}}, and moved the stage re-submission logic into a scheduled task, which references {{DAGScheduler.failed}}. Furthermore, since {{DAGScheduler}} is always started right after creation, I removed the {{DAGScheduler.start()}} method and started the actor within the {{DAGScheduler}} constructor. Now comes the problem: when {{eventProcessActor}} is started, {{DAGScheduler}} is not fully constructed yet, but the stage re-submission task is already scheduled to run. Thus, sometimes, when the scheduled task is executed for the first time, {{DAGScheduler.failed}} may still be {{null}}, thus a {{NullPointerException}} is thrown. A possible solution would be: # Add back the {{DAGScheduler.start()}} # Create and start {{eventProcessActor}} within {{DAGScheduler.start()}} to ensure {{DAGScheduler}} is fully constructed when the scheduled task is executed. Will fix it ASAP.", "created": "2013-11-27T21:11:19.645+0000"}, {"author": "Qiuzhuang Lian", "body": "Code to simulate the bug.", "created": "2013-11-27T21:53:18.905+0000"}, {"author": "Qiuzhuang Lian", "body": "The example I write by myself to analyze my client's some log which contains all deployed tomcat's log files. I am attaching the code file for you. As for data, since it's from client I couldn't attach them here, but I think you could just copy some log text files in some folder to simulate this. Thanks.", "created": "2013-11-27T21:53:23.754+0000"}, {"author": "Mark Hamstra", "body": "[~liancheng], see also SPARK-965 and [PR215|https://github.com/apache/incubator-spark/pull/215].", "created": "2013-11-27T22:13:04.833+0000"}, {"author": "liancheng", "body": "Hi [~markhamstra], thanks for pointing this out. By adding back {{DAGScheduler.start()}}, [SPARK-965|https://spark-project.atlassian.net/browse/SPARK-965] can also be solved. Because {{DAGSchedulerSuite}} never calls {{DAGScheduler.start()}}, thus {{eventProcessActor}} won't be created, the stage re-submission task won't be scheduled, and the race won't exist. Nevertheless, while reviewing {{DAGScheduler}}, I found that fields like {{failed}}, {{running}} & {{waiting}} are defined to be {{HashSet\\[T\\]}}, which is not thread safe, but are accessed by multiple threads. This does introduce race conditions. They should be defined as something like {{HashSet\\[T\\] with SynchronizedSet\\[T\\]}}.", "created": "2013-11-27T23:29:39.461+0000"}, {"author": "Mark Hamstra", "body": "fields like failed, running & waiting are defined to be HashSet[T], which is not thread safe, but are accessed by multiple threads  I believe that you are mistaken. Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded. In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking. From DAGSchedulerSource, the sizes of those data structures are read in a way that isn't sensitive to concurrent mutation. It is only when scheduler package tests (like those in the DAGSchedulerSuite) manipulate the internals of the DAGScheduler directly that there should be any trouble with race conditions or other concurrency issues. If these and other DAGScheduler data structures are accessed by multiple threads, then those are errors that we would be very much interested in understanding and correcting.", "created": "2013-11-28T00:16:20.114+0000"}, {"author": "liancheng", "body": "bq. Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded. In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking. My fault... Just reviewed {{DAGScheduler}} in master HEAD and commit [bf4e613|https://github.com/liancheng/incubator-spark/blob/bf4e6131cceef4fe00fb5693117c0732f181dbd9/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala] (the commit before I replaced the daemon thread with Akka actor). In the former case, your statement is true. While in the latter case, I made a mistake that the scheduled stage re-submission task may run in another thread. I should add a new {{DAGSchedulerEvent}} message {{ResubmitFailedStages}} and send this message to {{eventProcessActor}}, which can then call {{resubmitFailedStages()}} in the same thread that runs {{processEvent()}}. So yes, embarrassingly, just spotted a bug introduced by myself :-(", "created": "2013-11-28T01:30:27.024+0000"}, {"author": "liancheng", "body": "Hi, [~markhamstra] & [~rxin], please help review [PR-216|https://github.com/apache/incubator-spark/pull/216] that solves both [SPARK-966|https://spark-project.atlassian.net/browse/SPARK-966] and [SPARK-965|https://spark-project.atlassian.net/browse/SPARK-965]. Thanks!", "created": "2013-11-28T01:56:37.174+0000"}, {"author": "Josh Rosen", "body": "Fixed in https://github.com/apache/incubator-spark/pull/216", "created": "2013-12-07T14:45:43.734+0000"}], "num_comments": 12, "text": "Issue: SPARK-966\nSummary: Sometimes DAGScheduler throws NullPointerException\nDescription: When running some spark examples, I run into NullPointerException thrown by DAGScheduler. Log is as follows, 13/11/27 17:26:40 ERROR dispatch.TaskInvocation: java.lang.NullPointerException at org.apache.spark.scheduler.DAGScheduler$$anonfun$3$$anon$1$$anonfun$preStart$1.apply$mcV$sp(DAGScheduler.scala:116) at akka.actor.DefaultScheduler$$anon$1.run(Scheduler.scala:142) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:94) at akka.jsr166y.ForkJoinTask$AdaptedRunnableAction.exec(ForkJoinTask.java:1381) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104) 13/11/27 17:26:42 WARN util.SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes\n\nComments (12):\n1. Reynold Xin: What version of Spark is this? Can you paste the code near line 116 in DAGScheduler.scala?\n2. Qiuzhuang Lian: I am using git trunk version. The last git version is e2a43b3 on 11/11/2013 by Lian,Cheng. line 116 in that class is if (failed.size > 0) { and here is the code near that line, private val eventProcessActor: ActorRef = env.actorSystem.actorOf(Props(new Actor { override def preStart() { context.system.scheduler.schedule(RESUBMIT_TIMEOUT milliseconds, RESUBMIT_TIMEOUT milliseconds) { if (failed.size > 0) { resubmitFailedStages() } } }\n3. Reynold Xin: [~liancheng] can you take a look at this?\n4. liancheng: Hi [~Qiuzhuang], I think I've found the cause, but not fully verified yet. Would you please elaborate more on the Spark examples you were running? I need the command line arguments and possible input data files to verify this bug. Thanks! The problematic commit is actually [2539c06|https://github.com/liancheng/incubator-spark/commit/2539c0674501432fb62073577db6da52a26db850], an ancestor of [e2a43b3|https://github.com/liancheng/incubator-spark/commit/e2a43b3dcce81fc99098510d09095e1be4bf3e29]. In that commit, I replaced the daemon thread in {{DAGScheduler}} with an Akka actor {{eventProcessActor}}, and moved the stage re-submission logic into a scheduled task, which references {{DAGScheduler.failed}}. Furthermore, since {{DAGScheduler}} is always started right after creation, I removed the {{DAGScheduler.start()}} method and started the actor within the {{DAGScheduler}} constructor. Now comes the problem: when {{eventProcessActor}} is started, {{DAGScheduler}} is not fully constructed yet, but the stage re-submission task is already scheduled to run. Thus, sometimes, when the scheduled task is executed for the first time, {{DAGScheduler.failed}} may still be {{null}}, thus a {{NullPointerException}} is thrown. A possible solution would be: # Add back the {{DAGScheduler.start()}} # Create and start {{eventProcessActor}} within {{DAGScheduler.start()}} to ensure {{DAGScheduler}} is fully constructed when the scheduled task is executed. Will fix it ASAP.\n5. Qiuzhuang Lian: Code to simulate the bug.\n6. Qiuzhuang Lian: The example I write by myself to analyze my client's some log which contains all deployed tomcat's log files. I am attaching the code file for you. As for data, since it's from client I couldn't attach them here, but I think you could just copy some log text files in some folder to simulate this. Thanks.\n7. Mark Hamstra: [~liancheng], see also SPARK-965 and [PR215|https://github.com/apache/incubator-spark/pull/215].\n8. liancheng: Hi [~markhamstra], thanks for pointing this out. By adding back {{DAGScheduler.start()}}, [SPARK-965|https://spark-project.atlassian.net/browse/SPARK-965] can also be solved. Because {{DAGSchedulerSuite}} never calls {{DAGScheduler.start()}}, thus {{eventProcessActor}} won't be created, the stage re-submission task won't be scheduled, and the race won't exist. Nevertheless, while reviewing {{DAGScheduler}}, I found that fields like {{failed}}, {{running}} & {{waiting}} are defined to be {{HashSet\\[T\\]}}, which is not thread safe, but are accessed by multiple threads. This does introduce race conditions. They should be defined as something like {{HashSet\\[T\\] with SynchronizedSet\\[T\\]}}.\n9. Mark Hamstra: fields like failed, running & waiting are defined to be HashSet[T], which is not thread safe, but are accessed by multiple threads  I believe that you are mistaken. Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded. In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking. From DAGSchedulerSource, the sizes of those data structures are read in a way that isn't sensitive to concurrent mutation. It is only when scheduler package tests (like those in the DAGSchedulerSuite) manipulate the internals of the DAGScheduler directly that there should be any trouble with race conditions or other concurrency issues. If these and other DAGScheduler data structures are accessed by multiple threads, then those are errors that we would be very much interested in understanding and correcting.\n10. liancheng: bq. Other than in DAGSchedulerSource, those data structures should only be accessed within the DAGScheduler itself and in a way that is carefully maintained to be single-threaded. In this way, we avoid synchronization overhead and other nasty possibilities such as dead locking. My fault... Just reviewed {{DAGScheduler}} in master HEAD and commit [bf4e613|https://github.com/liancheng/incubator-spark/blob/bf4e6131cceef4fe00fb5693117c0732f181dbd9/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala] (the commit before I replaced the daemon thread with Akka actor). In the former case, your statement is true. While in the latter case, I made a mistake that the scheduled stage re-submission task may run in another thread. I should add a new {{DAGSchedulerEvent}} message {{ResubmitFailedStages}} and send this message to {{eventProcessActor}}, which can then call {{resubmitFailedStages()}} in the same thread that runs {{processEvent()}}. So yes, embarrassingly, just spotted a bug introduced by myself :-(", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.094294"}}
{"id": "ede91f539c7b806a65aa1ca702df5737", "issue_key": "SPARK-967", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "start-slaves.sh uses local path from master on remote slave nodes", "description": "If a slave node has home path other than master, start-slave.sh fails to start a worker instance, for other nodes behaves as expected, in my case: $ ./bin/start-slaves.sh node05.dev.vega.ru: bash: line 0: cd: /usr/home/etsvigun/spark/bin/..: No such file or directory node04.dev.vega.ru: org.apache.spark.deploy.worker.Worker running as process 4796. Stop it first. node03.dev.vega.ru: org.apache.spark.deploy.worker.Worker running as process 61348. Stop it first. I don't mention /usr/home anywhere, the only environment variable I set is $SPARK_HOME, relative to $HOME on every node, which makes me think some script takes `pwd` on master and tries to use it on slaves. Spark version: fb6875dd5c9334802580155464cef9ac4d4cc1f0 OS: FreeBSD 8.4", "reporter": "Evgeniy Tsvigun", "assignee": null, "created": "2013-11-27T06:49:17.000+0000", "updated": "2016-01-16T13:29:12.000+0000", "resolved": "2016-01-16T13:29:12.000+0000", "labels": ["script", "starter"], "components": ["Deploy"], "comments": [{"author": "Theodore michael Malaska", "body": "The problem is related to ssh remote \"$SPARK_HOME\" I just tried this it will return the value from the local server and not the remote server. The solution here does work http://unix.stackexchange.com/questions/79723/why-do-ssh-host-echo-path-and-printing-the-path-after-sshing-into-the-machi So the implementation would be ssh remote \". $HOME/.bash_profile; echo \\$SPARK_HOME\" Let me know if you like the solution. I will make the patch.", "created": "2014-01-13T08:10:12.677+0000"}, {"author": "Sree Vaddi", "body": "Initiated conversation on dev list.", "created": "2015-04-14T21:35:27.997+0000"}, {"author": "David Chin", "body": "I won't create a pull request unless asked to, but I have a solution for this. I am running Spark in standalone mode within a Univa Grid Engine cluster. As such, configs and logs, etc should be specific to each UGE job, identified by an integer job ID. Currently, any environment variables on the master are not passed along by the sbin/start-slaves.sh invocation of ssh. I put in a fix on my local version, which works. However, this is still less than ideal in that UGE's job accounting cannot keep track of resource usage by jobs not under its process tree. Not sure, yet, what the correct solution is. I thought I saw a feature request to allow other remote shell programs besides ssh, but I can't find it now. Please see my version of sbin/start-slaves.sh here, forked from current master: https://github.com/prehensilecode/spark/blob/master/sbin/start-slaves.sh", "created": "2015-07-30T20:14:53.766+0000"}, {"author": "Sean R. Owen", "body": "I think this is obsolete, or no longer a problem; these scripts always respond to the local SPARK_HOME now.", "created": "2016-01-16T13:29:12.678+0000"}], "num_comments": 4, "text": "Issue: SPARK-967\nSummary: start-slaves.sh uses local path from master on remote slave nodes\nDescription: If a slave node has home path other than master, start-slave.sh fails to start a worker instance, for other nodes behaves as expected, in my case: $ ./bin/start-slaves.sh node05.dev.vega.ru: bash: line 0: cd: /usr/home/etsvigun/spark/bin/..: No such file or directory node04.dev.vega.ru: org.apache.spark.deploy.worker.Worker running as process 4796. Stop it first. node03.dev.vega.ru: org.apache.spark.deploy.worker.Worker running as process 61348. Stop it first. I don't mention /usr/home anywhere, the only environment variable I set is $SPARK_HOME, relative to $HOME on every node, which makes me think some script takes `pwd` on master and tries to use it on slaves. Spark version: fb6875dd5c9334802580155464cef9ac4d4cc1f0 OS: FreeBSD 8.4\n\nComments (4):\n1. Theodore michael Malaska: The problem is related to ssh remote \"$SPARK_HOME\" I just tried this it will return the value from the local server and not the remote server. The solution here does work http://unix.stackexchange.com/questions/79723/why-do-ssh-host-echo-path-and-printing-the-path-after-sshing-into-the-machi So the implementation would be ssh remote \". $HOME/.bash_profile; echo \\$SPARK_HOME\" Let me know if you like the solution. I will make the patch.\n2. Sree Vaddi: Initiated conversation on dev list.\n3. David Chin: I won't create a pull request unless asked to, but I have a solution for this. I am running Spark in standalone mode within a Univa Grid Engine cluster. As such, configs and logs, etc should be specific to each UGE job, identified by an integer job ID. Currently, any environment variables on the master are not passed along by the sbin/start-slaves.sh invocation of ssh. I put in a fix on my local version, which works. However, this is still less than ideal in that UGE's job accounting cannot keep track of resource usage by jobs not under its process tree. Not sure, yet, what the correct solution is. I thought I saw a feature request to allow other remote shell programs besides ssh, but I can't find it now. Please see my version of sbin/start-slaves.sh here, forked from current master: https://github.com/prehensilecode/spark/blob/master/sbin/start-slaves.sh\n4. Sean R. Owen: I think this is obsolete, or no longer a problem; these scripts always respond to the local SPARK_HOME now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.094294"}}
{"id": "165a32e7d5bb8f285ffb7915ba5df809", "issue_key": "SPARK-968", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "In stage UI, add an overview section that shows task stats grouped by executor id", "description": "It would be very useful in the stage ui to show aggregated task information similar in semantics to the following SQL:  SELECT sum(duration), count(tasks) count(failed tasks), count(succeeded tasks), sum(shuffle read), sum(shuffle write) FROM tasks GROUP BY executorId", "reporter": "Reynold Xin", "assignee": "Wangda Tan", "created": "2013-11-28T20:50:37.000+0000", "updated": "2014-03-02T17:49:51.000+0000", "resolved": "2014-03-02T17:49:51.000+0000", "labels": ["starter"], "components": [], "comments": [{"author": "Reynold Xin", "body": "To get started on this task, launch spark-shell and run some simple job such as  sc.parallelize(1 to 100, 2).map(x => (x, x)).reduceByKey(_ + _)  And then open localhost:4040 and go check out the stage web ui.", "created": "2013-11-28T20:53:08.169+0000"}, {"author": "Wangda Tan", "body": "I'll take a try on this, but it seems I cannot change assignee. Is this feature blocked for new registered users?", "created": "2013-12-03T00:04:40.669+0000"}, {"author": "Reynold Xin", "body": "[~leftnoteasy] I just added you to the developer group. You should be able to assign yourself now.", "created": "2013-12-03T00:17:56.579+0000"}, {"author": "Wangda Tan", "body": "Thanks :)", "created": "2013-12-03T00:22:41.280+0000"}, {"author": "Wangda Tan", "body": "Sent pull request in github for review. Thanks", "created": "2013-12-08T08:33:04.053+0000"}], "num_comments": 5, "text": "Issue: SPARK-968\nSummary: In stage UI, add an overview section that shows task stats grouped by executor id\nDescription: It would be very useful in the stage ui to show aggregated task information similar in semantics to the following SQL:  SELECT sum(duration), count(tasks) count(failed tasks), count(succeeded tasks), sum(shuffle read), sum(shuffle write) FROM tasks GROUP BY executorId\n\nComments (5):\n1. Reynold Xin: To get started on this task, launch spark-shell and run some simple job such as  sc.parallelize(1 to 100, 2).map(x => (x, x)).reduceByKey(_ + _)  And then open localhost:4040 and go check out the stage web ui.\n2. Wangda Tan: I'll take a try on this, but it seems I cannot change assignee. Is this feature blocked for new registered users?\n3. Reynold Xin: [~leftnoteasy] I just added you to the developer group. You should be able to assign yourself now.\n4. Wangda Tan: Thanks :)\n5. Wangda Tan: Sent pull request in github for review. Thanks", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "32d894326efd8fbfe4d9eb4cab3c54bb", "issue_key": "SPARK-969", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Persistent web ui", "description": "The Spark application web ui (at port 4040) is extremely helpful for debugging application correctness & performance. However, once the application completes (and thus SparkContext is stopped), the web ui is no longer accessible. It would be great to refactor the UI so stage informations (perhaps in JSON format or directly in HTML format) are stored persistently in the file system and can be viewed after the fact.", "reporter": "Reynold Xin", "assignee": null, "created": "2013-11-28T21:24:10.000+0000", "updated": "2016-10-01T08:23:09.000+0000", "resolved": "2014-03-17T10:20:45.000+0000", "labels": [], "components": [], "comments": [{"author": "Shivaram Venkataraman", "body": "As a workaround, I add the following lines to my programs to save the web ui right now  import scala.sys.process._ \"wget -r -k localhost:4040\"! val mvCmd = \"mv localhost:4040 <dirname>\" mvCmd!", "created": "2013-12-06T15:28:23.064+0000"}, {"author": "xiajunluan", "body": "Hi Reynold I have started to investigate into it, pls assign this improvement to me. Thanks.", "created": "2013-12-17T22:21:34.635+0000"}, {"author": "Reynold Xin", "body": "Thanks. I added you to the dev group so you should be able to assign tickets to yourself in the future. Once you come up with a design, do you mind sharing that among the Spark dev list for discussion?", "created": "2013-12-17T22:27:17.368+0000"}, {"author": "Patrick McFadin", "body": "Hey I actually un-marked this as \"starter\" since it's a fairly complicated issue. Also [~andrew xia] I think there are other people interested in this as well, including people on the core spark team, so for now I want to leave it unassigned.", "created": "2014-01-19T21:56:46.254+0000"}, {"author": "Thomas Graves", "body": "Any update on this as far as high level design? The yarn history server jira is https://issues.apache.org/jira/browse/YARN-1530. It would be nice to keep that in mind when designing this so spark on yarn could use that.", "created": "2014-01-29T06:05:08.177+0000"}, {"author": "Pillis", "body": "Attached proposal design document for discussion.", "created": "2014-01-31T07:31:36.934+0000"}, {"author": "Reynold Xin", "body": "Thanks for posting this. I did a pass over the design. It's a pretty good first pass. Here are some questions: 1. Some information are not append only. For example, executor information might change, and job/stage status change too (e.g. from running to error or success). Does your design intend to save all the snapshots of the inforrmation? 2. Have you looked into possible integration with YARN's job history server, as mentioned by [~tgraves]? 3. It seems to me the model (SparkContextData) and the view (SparkContextObserver) will have a lot in common (mostly about data representation and writing to external storage). It wouldn't be strictly MVC, but we should consider perhaps merging them.", "created": "2014-02-03T23:13:54.812+0000"}, {"author": "Pillis", "body": "1. SparkContextData is a simple key/value object stored into a single file (no snapshots spread into multiple files). If we want to store append only data (ex: values over time), then we have to make the value into an array of appended datum. If we want to store point-in-time data like job/status state, we have a value that is overwritten every time there is a change. The save() to disk saves the last set value/values. 2. Yes. SparkContext can be launched in various environments (YARN/Mesos/etc.). The environments which create the SparkContext should be responsible for persisting the SparkContextData into their environments (History servers, etc.). This keeps the core agnostic of clusters it runs in. So for YARN, the {{org.apache.spark.deploy.yarn.ApplicationMaster}} would be responsible for transforming SparkContextData.save() calls into YARN job history server REST calls. Maybe there we can override SparkContextData.save() to do YARN history server specific interactions. 3. SparkContextObserver is a glorified SparkListener/Sink hooked into a live SparkContext that does not hold onto any data. The only reason (other than pure MVC) I separated it out into another class is the standalone SparkUI use case, where there is no need for a SparkContextObserver at all. Then SparkContextData is a simple JSON deserialize. The class would look obvious as to its purpose. Hope that helps. Thanks for going through the design document.", "created": "2014-02-04T22:56:45.328+0000"}, {"author": "Thomas Graves", "body": "Thanks for the design doc. So your idea for allowing YARN integration is that YARN would extend and override the SparkContextData class? That should work fine just make sure it can be installed/plugged in easily. One way might be how the scheduler backend is installed in the SparkContext. If we get the config files working again it could just be a configurable class. Note, the applicationMaster can't do it as in yarn-client mode the application master does not contain the sparkContext. So in this model you have to either have all the SparkContext's and SparkUI run on a single machine or use a NFS mounted directory and the SparkUI can be started on separate machine? So just to be clear the save is saving the entire object with the current status to the file? The file is overwritten on each call to save. How long does this take for a long running applications with lots of executors and stages to write and load this file? How often is it saving and sync'ing this to disk? If this operation isn't atomic then the UI could end up in weird states between saves. It seems like this could potentially take a lot of time and thus have the user viewing the UI be out of sync with what is actually happening. This might be fine for historical data after the application has finished but if you are looking at a running application this doesn't seem ideal. The current model also rules out saving to HDFS without doing other modifications. In the doc you say \"Any update to SparkContextData schedules a delayed save (say 500ms). Further updates keep postponing that save\", what does this mean exactly? if you postpone the saves isn't the UI going to be out of sync with what is really happening. Can the sparkUI still be started by a single application just as it is today? This way if I didn't want to start a standalone sparkUI server I could still get the UI for my application. Perhaps the SparkUI that exists now should still be started with the application and read the data from memory from the SparkContextData in order to keep from having delays. The data can also be saved to disk to be read if the application crashes or finishes by a spark history UI which should just be the sparkUI with the added page to selecting which application to look at. It seems like having it append new events that came in might be a better approach, this way you have the entire history of the application rather then just the final snapshot, although the downside there is the file may not getting very big and take a long time for it to parse. Have you thought about security at all? How do you protect one user from seeing another users history files? This could be done by having the spark history ui run as a super user and making sure the file permissions are set restrictive enough. The user would have to authenticate with the history UI.", "created": "2014-02-10T07:14:57.269+0000"}, {"author": "Pillis", "body": ">> One way might be how the scheduler backend is installed in the SparkContext. Yes, just like how cluster specific (local, YARN, Mesos) schedulers are determined for SparkContext, we will need cluster specific SparkContextDatas which should only differ in the save() mechanism. The local SparkContextData stores into local disk, whereas YARN SparkContextData would interact with YARN Application Timeline Server to record application data. One approach is that each {{TaskSchedulerImpl}} itself sets the appropriate SparkContextData into the SparkContext, either during instance construction, or during its initialize(). {{TaskSchedulerImpl}} will by default set the local storing SparkContextData. {{YarnClusterScheduler}} will override to set YARN friendly SparkContextData. Another approach is TaskSchedulers can implement a trait (say CustomSparkContextData) which has a createSparkContextData() method to create a custom SparkContextData. >> So in this model you have to either have all the SparkContext's and SparkUI run on a single machine or use a NFS mounted directory and the SparkUI can be started on separate machine? I have always thought of SparkUI as showing local node SparkContexts (got via disk or in-memory). Requiring SparkUI to show SparkContexts in the entire cluster by aggregating data by some means (NFS, HDFS, long running service, etc.) might be duplication of effort being put in by cluster managers to have their own history services (ex: YARN Application Timeline Server). Rather than building a Spark History service it might better to write UI leveraging the various history services directly. >> ...read the data from memory from the SparkContextData in order to keep from having delays I am in agreement over this. Delays in save() for constantly updating SparkContextDatas is a good argument. >> It seems like having it append new events that came in might be a better approach... My concern was the unpredictability of file size, and the time it takes to reconstruct the data back - it was runtime dependent. When displaying UI it is better to have consistent response times. Also, as of now we do not have a usecase for showing historical changes within a SparkContext. Wanted to experiment first with periodic entire saves. If performance is an issue, maybe we can look into memory mapped IO implementation etc. >> Have you thought about security at all? ... The user would have to authenticate with the history UI. There is file-system permissions (depending on which OS user launched the Driver), and there is the SPARK_USER which could also vary independent of OS user. SparkUI will know SparkContextDatas it has file-permissions to read. Launch it as root - you will know all SparkContexts. Launch it as non-root user, you will know only a subset based on file-permissions. The UI itself will show SparkContexts based on logged in user. Logged in user should match SPARK_USER for the SparkContext. Logging in will be a single textfield entering a username. We can default to the username of the user launching SparkUI. To prevent loading all SparkContextDatas just to match SPARK_USER, we could put it in filename (Ex: sparkcontext_\\{start-timestamp\\}_\\{username\\}.json). Hope this helps. [~tgraves], thank you for your valuable feedback.", "created": "2014-02-17T21:03:29.463+0000"}, {"author": "gsemet", "body": "I agree that archiving this page for post mortem analysis is helpful.", "created": "2016-10-01T08:23:09.111+0000"}], "num_comments": 11, "text": "Issue: SPARK-969\nSummary: Persistent web ui\nDescription: The Spark application web ui (at port 4040) is extremely helpful for debugging application correctness & performance. However, once the application completes (and thus SparkContext is stopped), the web ui is no longer accessible. It would be great to refactor the UI so stage informations (perhaps in JSON format or directly in HTML format) are stored persistently in the file system and can be viewed after the fact.\n\nComments (11):\n1. Shivaram Venkataraman: As a workaround, I add the following lines to my programs to save the web ui right now  import scala.sys.process._ \"wget -r -k localhost:4040\"! val mvCmd = \"mv localhost:4040 <dirname>\" mvCmd!\n2. xiajunluan: Hi Reynold I have started to investigate into it, pls assign this improvement to me. Thanks.\n3. Reynold Xin: Thanks. I added you to the dev group so you should be able to assign tickets to yourself in the future. Once you come up with a design, do you mind sharing that among the Spark dev list for discussion?\n4. Patrick McFadin: Hey I actually un-marked this as \"starter\" since it's a fairly complicated issue. Also [~andrew xia] I think there are other people interested in this as well, including people on the core spark team, so for now I want to leave it unassigned.\n5. Thomas Graves: Any update on this as far as high level design? The yarn history server jira is https://issues.apache.org/jira/browse/YARN-1530. It would be nice to keep that in mind when designing this so spark on yarn could use that.\n6. Pillis: Attached proposal design document for discussion.\n7. Reynold Xin: Thanks for posting this. I did a pass over the design. It's a pretty good first pass. Here are some questions: 1. Some information are not append only. For example, executor information might change, and job/stage status change too (e.g. from running to error or success). Does your design intend to save all the snapshots of the inforrmation? 2. Have you looked into possible integration with YARN's job history server, as mentioned by [~tgraves]? 3. It seems to me the model (SparkContextData) and the view (SparkContextObserver) will have a lot in common (mostly about data representation and writing to external storage). It wouldn't be strictly MVC, but we should consider perhaps merging them.\n8. Pillis: 1. SparkContextData is a simple key/value object stored into a single file (no snapshots spread into multiple files). If we want to store append only data (ex: values over time), then we have to make the value into an array of appended datum. If we want to store point-in-time data like job/status state, we have a value that is overwritten every time there is a change. The save() to disk saves the last set value/values. 2. Yes. SparkContext can be launched in various environments (YARN/Mesos/etc.). The environments which create the SparkContext should be responsible for persisting the SparkContextData into their environments (History servers, etc.). This keeps the core agnostic of clusters it runs in. So for YARN, the {{org.apache.spark.deploy.yarn.ApplicationMaster}} would be responsible for transforming SparkContextData.save() calls into YARN job history server REST calls. Maybe there we can override SparkContextData.save() to do YARN history server specific interactions. 3. SparkContextObserver is a glorified SparkListener/Sink hooked into a live SparkContext that does not hold onto any data. The only reason (other than pure MVC) I separated it out into another class is the standalone SparkUI use case, where there is no need for a SparkContextObserver at all. Then SparkContextData is a simple JSON deserialize. The class would look obvious as to its purpose. Hope that helps. Thanks for going through the design document.\n9. Thomas Graves: Thanks for the design doc. So your idea for allowing YARN integration is that YARN would extend and override the SparkContextData class? That should work fine just make sure it can be installed/plugged in easily. One way might be how the scheduler backend is installed in the SparkContext. If we get the config files working again it could just be a configurable class. Note, the applicationMaster can't do it as in yarn-client mode the application master does not contain the sparkContext. So in this model you have to either have all the SparkContext's and SparkUI run on a single machine or use a NFS mounted directory and the SparkUI can be started on separate machine? So just to be clear the save is saving the entire object with the current status to the file? The file is overwritten on each call to save. How long does this take for a long running applications with lots of executors and stages to write and load this file? How often is it saving and sync'ing this to disk? If this operation isn't atomic then the UI could end up in weird states between saves. It seems like this could potentially take a lot of time and thus have the user viewing the UI be out of sync with what is actually happening. This might be fine for historical data after the application has finished but if you are looking at a running application this doesn't seem ideal. The current model also rules out saving to HDFS without doing other modifications. In the doc you say \"Any update to SparkContextData schedules a delayed save (say 500ms). Further updates keep postponing that save\", what does this mean exactly? if you postpone the saves isn't the UI going to be out of sync with what is really happening. Can the sparkUI still be started by a single application just as it is today? This way if I didn't want to start a standalone sparkUI server I could still get the UI for my application. Perhaps the SparkUI that exists now should still be started with the application and read the data from memory from the SparkContextData in order to keep from having delays. The data can also be saved to disk to be read if the application crashes or finishes by a spark history UI which should just be the sparkUI with the added page to selecting which application to look at. It seems like having it append new events that came in might be a better approach, this way you have the entire history of the application rather then just the final snapshot, although the downside there is the file may not getting very big and take a long time for it to parse. Have you thought about security at all? How do you protect one user from seeing another users history files? This could be done by having the spark history ui run as a super user and making sure the file permissions are set restrictive enough. The user would have to authenticate with the history UI.\n10. Pillis: >> One way might be how the scheduler backend is installed in the SparkContext. Yes, just like how cluster specific (local, YARN, Mesos) schedulers are determined for SparkContext, we will need cluster specific SparkContextDatas which should only differ in the save() mechanism. The local SparkContextData stores into local disk, whereas YARN SparkContextData would interact with YARN Application Timeline Server to record application data. One approach is that each {{TaskSchedulerImpl}} itself sets the appropriate SparkContextData into the SparkContext, either during instance construction, or during its initialize(). {{TaskSchedulerImpl}} will by default set the local storing SparkContextData. {{YarnClusterScheduler}} will override to set YARN friendly SparkContextData. Another approach is TaskSchedulers can implement a trait (say CustomSparkContextData) which has a createSparkContextData() method to create a custom SparkContextData. >> So in this model you have to either have all the SparkContext's and SparkUI run on a single machine or use a NFS mounted directory and the SparkUI can be started on separate machine? I have always thought of SparkUI as showing local node SparkContexts (got via disk or in-memory). Requiring SparkUI to show SparkContexts in the entire cluster by aggregating data by some means (NFS, HDFS, long running service, etc.) might be duplication of effort being put in by cluster managers to have their own history services (ex: YARN Application Timeline Server). Rather than building a Spark History service it might better to write UI leveraging the various history services directly. >> ...read the data from memory from the SparkContextData in order to keep from having delays I am in agreement over this. Delays in save() for constantly updating SparkContextDatas is a good argument. >> It seems like having it append new events that came in might be a better approach... My concern was the unpredictability of file size, and the time it takes to reconstruct the data back - it was runtime dependent. When displaying UI it is better to have consistent response times. Also, as of now we do not have a usecase for showing historical changes within a SparkContext. Wanted to experiment first with periodic entire saves. If performance is an issue, maybe we can look into memory mapped IO implementation etc. >> Have you thought about security at all? ... The user would have to authenticate with the history UI. There is file-system permissions (depending on which OS user launched the Driver), and there is the SPARK_USER which could also vary independent of OS user. SparkUI will know SparkContextDatas it has file-permissions to read. Launch it as root - you will know all SparkContexts. Launch it as non-root user, you will know only a subset based on file-permissions. The UI itself will show SparkContexts based on logged in user. Logged in user should match SPARK_USER for the SparkContext. Logging in will be a single textfield entering a username. We can default to the username of the user launching SparkUI. To prevent loading all SparkContextDatas just to match SPARK_USER, we could put it in filename (Ex: sparkcontext_\\{start-timestamp\\}_\\{username\\}.json). Hope this helps. [~tgraves], thank you for your valuable feedback.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "b97aa58b8aa1bbf5653e70cd873b0cd6", "issue_key": "SPARK-970", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark's saveAsTextFile() throws UnicodeEncodeError when saving unicode strings", "description": "PySpark throws a UnicodeEncodeError when trying to save unicode objects to text files. This is because saveAsTextFile() calls str() to get objects' string representations, when it should be calling unicode() instead. This is probably a one-line fix. This was originally reported on the mailing list at https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201311.mbox/%3CCAPS2vjrorZGbxt7Nyqb1ZLZABk2MZy1O1p-KfF%3DxGJzSN0oq9g%40mail.gmail.com%3E", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-11-28T22:49:38.000+0000", "updated": "2013-12-04T11:10:25.000+0000", "resolved": "2013-12-04T11:10:25.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/apache/incubator-spark/pull/218", "created": "2013-12-04T11:10:25.095+0000"}], "num_comments": 1, "text": "Issue: SPARK-970\nSummary: PySpark's saveAsTextFile() throws UnicodeEncodeError when saving unicode strings\nDescription: PySpark throws a UnicodeEncodeError when trying to save unicode objects to text files. This is because saveAsTextFile() calls str() to get objects' string representations, when it should be calling unicode() instead. This is probably a one-line fix. This was originally reported on the mailing list at https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201311.mbox/%3CCAPS2vjrorZGbxt7Nyqb1ZLZABk2MZy1O1p-KfF%3DxGJzSN0oq9g%40mail.gmail.com%3E\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/apache/incubator-spark/pull/218", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "6c9bd68766bad1313b8497e9a38b1ac5", "issue_key": "SPARK-971", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Link to Confluence wiki from project website / documentation", "description": "Spark's Confluence wiki (https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage) is really hard to find; try a Google search for \"apache spark wiki\", for example. We should link to the wiki from the Spark project website and documentation.", "reporter": "Josh Rosen", "assignee": "Sean R. Owen", "created": "2013-12-01T12:11:36.000+0000", "updated": "2014-11-10T01:41:22.000+0000", "resolved": "2014-11-10T01:41:22.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3169", "created": "2014-11-08T10:51:41.254+0000"}, {"author": "Patrick Wendell", "body": "https://cwiki.apache.org/confluence/display/SPARK", "created": "2014-11-10T01:41:22.474+0000"}], "num_comments": 2, "text": "Issue: SPARK-971\nSummary: Link to Confluence wiki from project website / documentation\nDescription: Spark's Confluence wiki (https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage) is really hard to find; try a Google search for \"apache spark wiki\", for example. We should link to the wiki from the Spark project website and documentation.\n\nComments (2):\n1. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3169\n2. Patrick Wendell: https://cwiki.apache.org/confluence/display/SPARK", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "f22e85c391c154bb2549fe06cde4a0dc", "issue_key": "SPARK-972", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "PySpark's \"cannot run multiple SparkContexts at once\" message should give source locations", "description": "It can be difficult to debug PySpark's \"Cannot run multiple SparkContexts at once\" error message if you're not sure where the first context is being created; it would be helpful if the SparkContext class remembered the linenumber/location where the active context was created and printed it in the error message.", "reporter": "Josh Rosen", "assignee": "Jyotiska NK", "created": "2013-12-01T17:37:28.000+0000", "updated": "2016-07-12T21:49:06.000+0000", "resolved": "2014-03-10T13:35:27.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Jyotiska NK", "body": "I have submitted PR #581 for this issue. Please see if it solves the problem.", "created": "2014-02-11T09:57:38.790+0000"}, {"author": "Josh Rosen", "body": "Your [pull request|https://github.com/apache/incubator-spark/pull/581] prints the master that the running SparkContext is connected to, but I was hoping to print the location of the call site where the running SparkContext was created in order to help debug the case in which a SparkContext is created deep within some piece of code and the user tries to create a new one elsewhere.", "created": "2014-02-11T11:24:38.734+0000"}, {"author": "Jyotiska NK", "body": "Pushed the updated commit. Now it also prints the location of the call site( filename and linenumber). Please check if this is what you had in mind.", "created": "2014-02-11T19:04:10.478+0000"}, {"author": "Jyotiska NK", "body": "Hi, I closed the old one and created a new pull request [https://github.com/apache/spark/pull/34], can you look into it?", "created": "2014-02-27T22:59:43.222+0000"}, {"author": "Apache Spark", "body": "User 'jyotiska' has created a pull request for this issue: https://github.com/apache/spark/pull/34", "created": "2016-07-12T21:49:06.359+0000"}], "num_comments": 5, "text": "Issue: SPARK-972\nSummary: PySpark's \"cannot run multiple SparkContexts at once\" message should give source locations\nDescription: It can be difficult to debug PySpark's \"Cannot run multiple SparkContexts at once\" error message if you're not sure where the first context is being created; it would be helpful if the SparkContext class remembered the linenumber/location where the active context was created and printed it in the error message.\n\nComments (5):\n1. Jyotiska NK: I have submitted PR #581 for this issue. Please see if it solves the problem.\n2. Josh Rosen: Your [pull request|https://github.com/apache/incubator-spark/pull/581] prints the master that the running SparkContext is connected to, but I was hoping to print the location of the call site where the running SparkContext was created in order to help debug the case in which a SparkContext is created deep within some piece of code and the user tries to create a new one elsewhere.\n3. Jyotiska NK: Pushed the updated commit. Now it also prints the location of the call site( filename and linenumber). Please check if this is what you had in mind.\n4. Jyotiska NK: Hi, I closed the old one and created a new pull request [https://github.com/apache/spark/pull/34], can you look into it?\n5. Apache Spark: User 'jyotiska' has created a pull request for this issue: https://github.com/apache/spark/pull/34", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "3e7e74cb061c0aa4e832f00411dbf2cb", "issue_key": "SPARK-973", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Mark fields in RDD class that are not used in workers as @transient to reduce task size", "description": "I can see a few candidates to mark as transient.  /** Optionally overridden by subclasses to specify how they are partitioned. */ val partitioner: Option[Partitioner] = None /** A friendly name for this RDD */ var name: String = null /** User-defined generator of this RDD*/ var generator = Utils.getCallSiteInfo.firstUserClass", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2013-12-02T16:04:34.000+0000", "updated": "2013-12-07T14:10:22.000+0000", "resolved": "2013-12-07T14:10:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/apache/incubator-spark/pull/223 and included in 0.8.1.", "created": "2013-12-07T14:10:22.225+0000"}], "num_comments": 1, "text": "Issue: SPARK-973\nSummary: Mark fields in RDD class that are not used in workers as @transient to reduce task size\nDescription: I can see a few candidates to mark as transient.  /** Optionally overridden by subclasses to specify how they are partitioned. */ val partitioner: Option[Partitioner] = None /** A friendly name for this RDD */ var name: String = null /** User-defined generator of this RDD*/ var generator = Utils.getCallSiteInfo.firstUserClass\n\nComments (1):\n1. Josh Rosen: Fixed in https://github.com/apache/incubator-spark/pull/223 and included in 0.8.1.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "b317938679c61a662e8d08addbd659e0", "issue_key": "SPARK-974", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "error in script spark-0.8.0-incubating/make-distribution.sh", "description": "file spark-0.8.0-incubating/make-distribution.sh, line 98: cp \"$FWDIR/conf/*.template\" \"$DISTDIR\"/conf should be cp \"$FWDIR\"/conf/*.template \"$DISTDIR\"/conf Otherwise report file not found error p.s.: The script file is included in this version: http://spark-project.org/download/spark-0.8.0-incubating.tgz", "reporter": "刘旭", "assignee": null, "created": "2013-12-02T18:59:55.000+0000", "updated": "2013-12-02T21:51:01.000+0000", "resolved": "2013-12-02T21:51:01.000+0000", "labels": ["script"], "components": ["Deploy"], "comments": [{"author": "Reynold Xin", "body": "Thanks for reporting. Do you mind submitting a pull request against the Apache github mirror to fix this? https://github.com/apache/incubator-spark", "created": "2013-12-02T21:27:32.366+0000"}, {"author": "刘旭", "body": "@Reynold Xin , never mind. I's already fixed with the #916 pull request. https://github.com/mesos/spark/pull/916", "created": "2013-12-02T21:48:19.145+0000"}, {"author": "Reynold Xin", "body": "Thanks. Closing this since it's been fixed.", "created": "2013-12-02T21:51:01.520+0000"}], "num_comments": 3, "text": "Issue: SPARK-974\nSummary: error in script spark-0.8.0-incubating/make-distribution.sh\nDescription: file spark-0.8.0-incubating/make-distribution.sh, line 98: cp \"$FWDIR/conf/*.template\" \"$DISTDIR\"/conf should be cp \"$FWDIR\"/conf/*.template \"$DISTDIR\"/conf Otherwise report file not found error p.s.: The script file is included in this version: http://spark-project.org/download/spark-0.8.0-incubating.tgz\n\nComments (3):\n1. Reynold Xin: Thanks for reporting. Do you mind submitting a pull request against the Apache github mirror to fix this? https://github.com/apache/incubator-spark\n2. 刘旭: @Reynold Xin , never mind. I's already fixed with the #916 pull request. https://github.com/mesos/spark/pull/916\n3. Reynold Xin: Thanks. Closing this since it's been fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "044f2cd90501ac1606f79d9f4f5f7eb9", "issue_key": "SPARK-975", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark Replay Debugger", "description": "The Spark debugger was first mentioned as {{rddbg}} in the [RDD technical report|http://www.cs.berkeley.edu/~matei/papers/2011/tr_spark.pdf]. [Arthur|https://github.com/mesos/spark/tree/arthur], authored by [Ankur Dave|https://github.com/ankurdave], is an old implementation of the Spark debugger, which demonstrated both the elegance and power behind the RDD abstraction. Unfortunately, the corresponding GitHub branch was not merged into the master branch and had stopped 2 years ago. For more information about Arthur, please refer to [the Spark Debugger Wiki page|https://github.com/mesos/spark/wiki/Spark-Debugger] in the old GitHub repository. As a useful tool for Spark application debugging and analysis, it would be nice to have a complete Spark debugger. In [PR-224|https://github.com/apache/incubator-spark/pull/224], I propose a new implementation of the Spark debugger, the Spark Replay Debugger (SRD). [PR-224|https://github.com/apache/incubator-spark/pull/224] is only a preview for discussion. In the current version, I only implemented features that can illustrate the basic mechanisms. There are still features appeared in Arthur but missing in SRD, such as checksum based nondeterminsm detection and single task debugging with conventional debugger (like {{jdb}}). However, these features can be easily built upon current SRD framework. To minimize code review effort, I didn't include them into the current version intentionally. Attached is the visualization of the MLlib ALS application (with 1 iteration) generated by SRD. For more information, please refer to [the SRD overview document|http://spark-replay-debugger-overview.readthedocs.org/en/latest/].", "reporter": "liancheng", "assignee": null, "created": "2013-12-03T05:28:36.000+0000", "updated": "2016-01-11T10:30:24.000+0000", "resolved": "2016-01-11T10:30:24.000+0000", "labels": ["arthur", "debugger"], "components": ["Spark Core"], "comments": [{"author": "Kousuke Saruta", "body": "Hi [~lian cheng]. Are there any updates on this issue?", "created": "2014-04-28T21:37:47.929+0000"}, {"author": "Cheng Lian", "body": "Hi [~sarutak], thanks for caring about this. Sorry that this issue hasn't been updated for a while. At the time SRD was developed, related interfaces exposed by Spark and used in SRD were not well chosen and exposed some implementation details to API users, so SRD was not merged yet. We do have plan to improve Spark debugging facilities. Before we settle on a final design, I would like to rebase the SRD branch to the current master so that people can use it to debug and analyze their applications, though I can't promise anything for now.", "created": "2014-04-29T02:39:09.552+0000"}, {"author": "Kousuke Saruta", "body": "Thank you for your reply [~lian cheng]. I understood the status. I think Debug tool like SRD is essentially needed, so may I help you?", "created": "2014-05-01T00:16:07.036+0000"}, {"author": "Cheng Lian", "body": "Drawback of GraphViz", "created": "2014-05-01T05:31:19.965+0000"}, {"author": "Cheng Lian", "body": "Thanks for being willing to help [~sarutak]! Actually I do need some help in seeking proper visualization tool. GraphViz is too primitive to draw complex RDD DAGs. Here is an over simplified case to illustrate the problem: !RDD DAG.png! Dashed boxes are stages and circles are RDDs. When two stages share some RDD(s), the left version is what we need, while the one on the right is what GraphViz outputs. I investigated a bit but didn't find any visualization tools / libraries that can easily meets this requirement. Ideally, a JS library would be perfect since it's easy to be integrated in the web UI.", "created": "2014-05-01T11:28:04.708+0000"}, {"author": "Kousuke Saruta", "body": "I understood the problem and I also investigate another tool / library for the requirement.", "created": "2014-05-01T22:26:55.933+0000"}, {"author": "Phuoc Do", "body": "Cheng Lian, some JS libraries that can draw flow diagrams: http://www.graphdracula.net/ http://www.daviddurman.com/automatic-graph-layout-with-jointjs-and-dagre.html Your diagram in 01/May/14 comment doesn't look like this one: http://spark-replay-debugger-overview.readthedocs.org/en/latest/_static/als-1-large.png Has the requirement changed?", "created": "2014-07-23T01:12:21.392+0000"}, {"author": "Cheng Lian", "body": "Hey [~phuocd], that image actually shows exactly the same issue as I commented. Take RDD #0 to #4 as an example: #0 to #3 form a stage, and #0, #1, #2 and #4 form another. These two stages share #0 to #2, and should overlap, and the generated dot file describes the topology correctly. But GraphVis gives wrong bounding boxes, just like the right part of the image I used in the comment.", "created": "2014-07-23T01:19:43.730+0000"}, {"author": "Phuoc Do", "body": "Cheng Lian, maybe something like this: !IMG_20140722_184149.jpg!", "created": "2014-07-23T02:05:05.844+0000"}, {"author": "Cheng Lian", "body": "[~phuocd] Yea, exactly :)", "created": "2014-07-23T02:15:59.872+0000"}, {"author": "Phuoc Do", "body": "Does the shape have any significance. I saw it was rectangle in the old screenshot and circle in new diagram.", "created": "2014-07-23T02:20:33.454+0000"}, {"author": "Cheng Lian", "body": "No it doesn't. I chose rectangles just because more text can be shown.", "created": "2014-07-23T02:22:14.863+0000"}, {"author": "Phuoc Do", "body": "In the old diagram, there are red links 5 - 3, 9 - 4. What's the meaning of red links?", "created": "2014-07-23T02:25:20.254+0000"}, {"author": "Cheng Lian", "body": "Red lines indicate wide dependencies (shuffles are introduced there).", "created": "2014-07-23T02:30:27.985+0000"}, {"author": "Phuoc Do", "body": "Cheng Lian, I saw that latest UI displays stack trace for each stage. Is there a way to filter out function calls that we don't display in debugger. There seems to be a lot of native code calls in there. See stack below. I did some work with d3 force layout. See here: https://github.com/dnprock/spark-debugger Stack: org.apache.spark.rdd.RDD.count(RDD.scala:904) $line9.$read$$iwC$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:15) $line9.$read$$iwC$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:20) $line9.$read$$iwC$$iwC.&lt;init&gt;(&lt;console&gt;:22) $line9.$read$$iwC.&lt;init&gt;(&lt;console&gt;:24) $line9.$read.&lt;init&gt;(&lt;console&gt;:26) $line9.$read$.&lt;init&gt;(&lt;console&gt;:30) $line9.$read$.&lt;clinit&gt;(&lt;console&gt;) $line9.$eval$.&lt;init&gt;(&lt;console&gt;:7) $line9.$eval$.&lt;clinit&gt;(&lt;console&gt;) $line9.$eval.$print(&lt;console&gt;) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:483) org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789) org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062) org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615) org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646) org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)", "created": "2014-08-15T17:01:16.596+0000"}, {"author": "Cheng Lian", "body": "Usually we just filter them out by checking package/class names . Similar trick is used in {{org.apache.spark.util.Utils.getCallSite}}.", "created": "2014-08-18T03:07:05.154+0000"}, {"author": "Phuoc Do", "body": "To make it consistent with existing stack details, I leave the calls in place. I submit a Pull Request: https://github.com/apache/spark/pull/2077 Can you review?", "created": "2014-08-21T05:18:05.571+0000"}, {"author": "Apache Spark", "body": "User 'dnprock' has created a pull request for this issue: https://github.com/apache/spark/pull/2077", "created": "2014-11-03T20:47:21.385+0000"}], "num_comments": 18, "text": "Issue: SPARK-975\nSummary: Spark Replay Debugger\nDescription: The Spark debugger was first mentioned as {{rddbg}} in the [RDD technical report|http://www.cs.berkeley.edu/~matei/papers/2011/tr_spark.pdf]. [Arthur|https://github.com/mesos/spark/tree/arthur], authored by [Ankur Dave|https://github.com/ankurdave], is an old implementation of the Spark debugger, which demonstrated both the elegance and power behind the RDD abstraction. Unfortunately, the corresponding GitHub branch was not merged into the master branch and had stopped 2 years ago. For more information about Arthur, please refer to [the Spark Debugger Wiki page|https://github.com/mesos/spark/wiki/Spark-Debugger] in the old GitHub repository. As a useful tool for Spark application debugging and analysis, it would be nice to have a complete Spark debugger. In [PR-224|https://github.com/apache/incubator-spark/pull/224], I propose a new implementation of the Spark debugger, the Spark Replay Debugger (SRD). [PR-224|https://github.com/apache/incubator-spark/pull/224] is only a preview for discussion. In the current version, I only implemented features that can illustrate the basic mechanisms. There are still features appeared in Arthur but missing in SRD, such as checksum based nondeterminsm detection and single task debugging with conventional debugger (like {{jdb}}). However, these features can be easily built upon current SRD framework. To minimize code review effort, I didn't include them into the current version intentionally. Attached is the visualization of the MLlib ALS application (with 1 iteration) generated by SRD. For more information, please refer to [the SRD overview document|http://spark-replay-debugger-overview.readthedocs.org/en/latest/].\n\nComments (18):\n1. Kousuke Saruta: Hi [~lian cheng]. Are there any updates on this issue?\n2. Cheng Lian: Hi [~sarutak], thanks for caring about this. Sorry that this issue hasn't been updated for a while. At the time SRD was developed, related interfaces exposed by Spark and used in SRD were not well chosen and exposed some implementation details to API users, so SRD was not merged yet. We do have plan to improve Spark debugging facilities. Before we settle on a final design, I would like to rebase the SRD branch to the current master so that people can use it to debug and analyze their applications, though I can't promise anything for now.\n3. Kousuke Saruta: Thank you for your reply [~lian cheng]. I understood the status. I think Debug tool like SRD is essentially needed, so may I help you?\n4. Cheng Lian: Drawback of GraphViz\n5. Cheng Lian: Thanks for being willing to help [~sarutak]! Actually I do need some help in seeking proper visualization tool. GraphViz is too primitive to draw complex RDD DAGs. Here is an over simplified case to illustrate the problem: !RDD DAG.png! Dashed boxes are stages and circles are RDDs. When two stages share some RDD(s), the left version is what we need, while the one on the right is what GraphViz outputs. I investigated a bit but didn't find any visualization tools / libraries that can easily meets this requirement. Ideally, a JS library would be perfect since it's easy to be integrated in the web UI.\n6. Kousuke Saruta: I understood the problem and I also investigate another tool / library for the requirement.\n7. Phuoc Do: Cheng Lian, some JS libraries that can draw flow diagrams: http://www.graphdracula.net/ http://www.daviddurman.com/automatic-graph-layout-with-jointjs-and-dagre.html Your diagram in 01/May/14 comment doesn't look like this one: http://spark-replay-debugger-overview.readthedocs.org/en/latest/_static/als-1-large.png Has the requirement changed?\n8. Cheng Lian: Hey [~phuocd], that image actually shows exactly the same issue as I commented. Take RDD #0 to #4 as an example: #0 to #3 form a stage, and #0, #1, #2 and #4 form another. These two stages share #0 to #2, and should overlap, and the generated dot file describes the topology correctly. But GraphVis gives wrong bounding boxes, just like the right part of the image I used in the comment.\n9. Phuoc Do: Cheng Lian, maybe something like this: !IMG_20140722_184149.jpg!\n10. Cheng Lian: [~phuocd] Yea, exactly :)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.096298"}}
{"id": "4a1c0d4eaccd0b1a2de74146d5d6dec9", "issue_key": "SPARK-976", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "WikipediaPageRand doesn't work anymore", "description": "Looks like wikipedia doesn't public the pages info in WEX format anymore, but instead is doing page dumps in XML format. Because of that the example is failing with IOOBE as it expects tab-separated input strings.", "reporter": "Konstantin I Boudnik", "assignee": null, "created": "2013-12-03T09:49:41.000+0000", "updated": "2014-11-08T09:47:28.000+0000", "resolved": "2014-11-08T09:47:28.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Sean R. Owen", "body": "I assume this is also WontFix as it is a Bagel example.", "created": "2014-11-08T09:47:28.096+0000"}], "num_comments": 1, "text": "Issue: SPARK-976\nSummary: WikipediaPageRand doesn't work anymore\nDescription: Looks like wikipedia doesn't public the pages info in WEX format anymore, but instead is doing page dumps in XML format. Because of that the example is failing with IOOBE as it expects tab-separated input strings.\n\nComments (1):\n1. Sean R. Owen: I assume this is also WontFix as it is a Bagel example.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.098302"}}
{"id": "b4630d51c0393d2101ec4c56c98e77b9", "issue_key": "SPARK-977", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add ZippedRDD / zip to PySpark", "description": "We should add an equivalent of ZippedRDD / zip() to PySpark.", "reporter": "Josh Rosen", "assignee": "Prabin Banka", "created": "2013-12-03T11:02:49.000+0000", "updated": "2014-03-16T22:30:08.000+0000", "resolved": "2014-03-10T13:35:44.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-977\nSummary: Add ZippedRDD / zip to PySpark\nDescription: We should add an equivalent of ZippedRDD / zip() to PySpark.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.098302"}}
{"id": "e43a367ea5fd854b340affe25735812c", "issue_key": "SPARK-978", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark's cartesian method throws ClassCastException exception", "description": "Try the following in PySpark:  a = sc.textFile(\"README.md\") a.cartesian(a).collect()  exception thrown  py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.writeToFile. : java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.spark.api.python.PythonRDD$.writeToStream(PythonRDD.scala:214) at org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:233) at org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:232) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:573) at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:232) at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:227) at org.apache.spark.api.python.PythonRDD.writeToFile(PythonRDD.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:228) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379) at py4j.Gateway.invoke(Gateway.java:259) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:695)  But if we convert the rdd to a list of strings it would work. i.e. a = a.map(lambda line: str(line))", "reporter": "Reynold Xin", "assignee": "Josh Rosen", "created": "2013-12-03T13:54:08.000+0000", "updated": "2014-01-23T19:48:05.000+0000", "resolved": "2014-01-23T19:48:05.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "What's happening here is cartesian() produces a JavaPairRDD<String, String> when called on untransformed RDDs that were created with sc.textFile(), but writeToStream() isn't prepared to handle this case. This fails with a ClassCastException rather than a MatchError because the type parameters of Tuple2 are eliminated by erasure; the compiler had actually warned about this:  [warn] /Users/joshrosen/Documents/spark/spark/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:214: non-variable type argument Array[Byte] in type pattern (Array[Byte], Array[Byte]) is unchecked since it is eliminated by erasure [warn] case pair: (Array[Byte], Array[Byte]) => [warn]  Based on the underlying JavaRDD's ClassTag, we should be able to figure out which Java -> Python serialization method to use, rather than attempting to determine it on a per-element basis (this should be more efficient, too). Unfortunately, this doesn't work since we need TypeTags rather than ClassTags to work around erasure of Tuple2's parameters. Rather than breaking a bunch of existing APIs by introducing TypeTags, I changed the scope of the getClass()-based pattern matching to determine types based on the first item in the iterator. I've submitted my fix as part of this pull request: https://github.com/apache/incubator-spark/pull/501", "created": "2014-01-23T15:18:53.576+0000"}], "num_comments": 1, "text": "Issue: SPARK-978\nSummary: PySpark's cartesian method throws ClassCastException exception\nDescription: Try the following in PySpark:  a = sc.textFile(\"README.md\") a.cartesian(a).collect()  exception thrown  py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.writeToFile. : java.lang.ClassCastException: java.lang.String cannot be cast to [B at org.apache.spark.api.python.PythonRDD$.writeToStream(PythonRDD.scala:214) at org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:233) at org.apache.spark.api.python.PythonRDD$$anonfun$writeToFile$1.apply(PythonRDD.scala:232) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:573) at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:232) at org.apache.spark.api.python.PythonRDD$.writeToFile(PythonRDD.scala:227) at org.apache.spark.api.python.PythonRDD.writeToFile(PythonRDD.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:228) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379) at py4j.Gateway.invoke(Gateway.java:259) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:695)  But if we convert the rdd to a list of strings it would work. i.e. a = a.map(lambda line: str(line))\n\nComments (1):\n1. Josh Rosen: What's happening here is cartesian() produces a JavaPairRDD<String, String> when called on untransformed RDDs that were created with sc.textFile(), but writeToStream() isn't prepared to handle this case. This fails with a ClassCastException rather than a MatchError because the type parameters of Tuple2 are eliminated by erasure; the compiler had actually warned about this:  [warn] /Users/joshrosen/Documents/spark/spark/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:214: non-variable type argument Array[Byte] in type pattern (Array[Byte], Array[Byte]) is unchecked since it is eliminated by erasure [warn] case pair: (Array[Byte], Array[Byte]) => [warn]  Based on the underlying JavaRDD's ClassTag, we should be able to figure out which Java -> Python serialization method to use, rather than attempting to determine it on a per-element basis (this should be more efficient, too). Unfortunately, this doesn't work since we need TypeTags rather than ClassTags to work around erasure of Tuple2's parameters. Rather than breaking a bunch of existing APIs by introducing TypeTags, I changed the scope of the getClass()-based pattern matching to determine types based on the first item in the iterator. I've submitted my fix as part of this pull request: https://github.com/apache/incubator-spark/pull/501", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.098302"}}
{"id": "4a34c70e85576cfc152098393f33773b", "issue_key": "SPARK-979", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add some randomization to scheduler to better balance in-memory partition distributions", "description": "The Spark scheduler is very deterministic, which causes problems for the following workload (in serial order on a cluster with a small number of nodes): cache rdd 1 with 1 partition cache rdd 2 with 1 partition cache rdd 3 with 1 partition .... After a while, only executor 1 will have data in memory, and eventually leading to evicting in-memory blocks to disk while all other executors are empty. We can solve this problem by adding some randomization to the cluster scheduling, or by adding memory aware scheduling (which is much harder to do).", "reporter": "Reynold Xin", "assignee": "Kay Ousterhout", "created": "2013-12-03T15:13:58.000+0000", "updated": "2015-08-24T03:55:04.000+0000", "resolved": "2014-03-01T11:26:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Huiming Li", "body": "For a production system, this is expected use case to have data coming in continuously. The problem will happen even with big cluster and big jobs. See, if I have a 100 nodes cluster and my partition # is 50, only the first 50 nodes will get data. The remaining 50 will never get a chance to get anything. It would be the same thing if number of partitions over than 100, (consider a case of 101) the memory usage will be most distributed onto the leading nodes (the workers with smaller ID). To use shark as a reporting database, we'll need to utilize as many memory as possible and the RDD will be staying in memory for quite long time. Hope this makes it clearer.", "created": "2013-12-04T16:47:56.073+0000"}, {"author": "Josh Rosen", "body": "Would picking free executors in an LRU order be better than simple randomization?", "created": "2013-12-04T17:26:31.803+0000"}, {"author": "Huiming Li", "body": "Simple randomization won't work well if you consider some nodes can be added in later. Unless there is some kind of memory rebalancer, the harder way sounds like the right choice.", "created": "2013-12-05T16:39:29.360+0000"}, {"author": "xiajunluan", "body": "Hi Reynold Could you assign this improvement to me? I would like to try to fix it.", "created": "2013-12-08T01:57:10.685+0000"}, {"author": "Nan Zhu", "body": "made a PR : https://github.com/apache/incubator-spark/pull/548", "created": "2014-02-06T10:18:19.481+0000"}, {"author": "Kay Ousterhout", "body": "Using LRU adds significant complexity to the scheduler and I'm not sure there's much benefit. For example, when a new node is added later, LRU will ensure that the next task is placed on the new node. But subsequent tasks will be placed in round-robin order, so memory use still won't be evenly balanced across nodes. I submitted a new PR that just uses randomization: https://github.com/apache/spark/pull/27", "created": "2014-02-27T00:51:51.165+0000"}, {"author": "Nan Zhu", "body": "Hi, Kay, I think here, the round-robin subsequent tasks should be the right thing to happen? because the following nodes are placed in a LRU order", "created": "2014-02-27T03:56:40.927+0000"}, {"author": "Kay Ousterhout", "body": "LRU will not always solve the problem. Consider the case when all nodes have N partitions on them, and then a new node is added: node 0: N partitions node 1: N partitions node 2: N partitions node 3: 0 partitions node 3 will, for the rest of time, always have N fewer partitions than the rest of the nodes. LRU is also imperfect because nodes may be selected out of order due to locality preferences, or when the cluster is busy. These problems don't make LRU a bad idea necessarily...I'm just not sure that the complexity of adding LRU is worth it. In general the scheduler code has gotten to be quite complicated, so I think we should be careful when adding more complexity there. Curious to hear what others think!", "created": "2014-02-27T08:58:18.997+0000"}, {"author": "Nan Zhu", "body": "Yeah, more feedbacks from others are better Generally, I think randomization provides better worst-case performance than LRU, for overall, LRU > random > existing (if we don't consider the OS buffer, and even more, new HDFS has the cache system now https://issues.apache.org/jira/browse/HDFS-4949 )? Just my idea, also expecting others' feedbacks", "created": "2014-02-27T09:51:56.811+0000"}, {"author": "Patrick McFadin", "body": "Given that this part of the code is getting really complex, I'd strongly favor a simple solution initially which we can refine based on feeedback over time. The randomized solution is simple and well tested so it seems preferable to the other patch.", "created": "2014-02-27T10:29:19.680+0000"}, {"author": "Nan Zhu", "body": "I agree, I don't mind closing my PR if you have decided to adopt Kay's solution", "created": "2014-02-27T10:38:03.225+0000"}, {"author": "Apache Spark", "body": "User 'tdas' has created a pull request for this issue: https://github.com/apache/spark/pull/8387", "created": "2015-08-24T03:55:04.754+0000"}], "num_comments": 12, "text": "Issue: SPARK-979\nSummary: Add some randomization to scheduler to better balance in-memory partition distributions\nDescription: The Spark scheduler is very deterministic, which causes problems for the following workload (in serial order on a cluster with a small number of nodes): cache rdd 1 with 1 partition cache rdd 2 with 1 partition cache rdd 3 with 1 partition .... After a while, only executor 1 will have data in memory, and eventually leading to evicting in-memory blocks to disk while all other executors are empty. We can solve this problem by adding some randomization to the cluster scheduling, or by adding memory aware scheduling (which is much harder to do).\n\nComments (12):\n1. Huiming Li: For a production system, this is expected use case to have data coming in continuously. The problem will happen even with big cluster and big jobs. See, if I have a 100 nodes cluster and my partition # is 50, only the first 50 nodes will get data. The remaining 50 will never get a chance to get anything. It would be the same thing if number of partitions over than 100, (consider a case of 101) the memory usage will be most distributed onto the leading nodes (the workers with smaller ID). To use shark as a reporting database, we'll need to utilize as many memory as possible and the RDD will be staying in memory for quite long time. Hope this makes it clearer.\n2. Josh Rosen: Would picking free executors in an LRU order be better than simple randomization?\n3. Huiming Li: Simple randomization won't work well if you consider some nodes can be added in later. Unless there is some kind of memory rebalancer, the harder way sounds like the right choice.\n4. xiajunluan: Hi Reynold Could you assign this improvement to me? I would like to try to fix it.\n5. Nan Zhu: made a PR : https://github.com/apache/incubator-spark/pull/548\n6. Kay Ousterhout: Using LRU adds significant complexity to the scheduler and I'm not sure there's much benefit. For example, when a new node is added later, LRU will ensure that the next task is placed on the new node. But subsequent tasks will be placed in round-robin order, so memory use still won't be evenly balanced across nodes. I submitted a new PR that just uses randomization: https://github.com/apache/spark/pull/27\n7. Nan Zhu: Hi, Kay, I think here, the round-robin subsequent tasks should be the right thing to happen? because the following nodes are placed in a LRU order\n8. Kay Ousterhout: LRU will not always solve the problem. Consider the case when all nodes have N partitions on them, and then a new node is added: node 0: N partitions node 1: N partitions node 2: N partitions node 3: 0 partitions node 3 will, for the rest of time, always have N fewer partitions than the rest of the nodes. LRU is also imperfect because nodes may be selected out of order due to locality preferences, or when the cluster is busy. These problems don't make LRU a bad idea necessarily...I'm just not sure that the complexity of adding LRU is worth it. In general the scheduler code has gotten to be quite complicated, so I think we should be careful when adding more complexity there. Curious to hear what others think!\n9. Nan Zhu: Yeah, more feedbacks from others are better Generally, I think randomization provides better worst-case performance than LRU, for overall, LRU > random > existing (if we don't consider the OS buffer, and even more, new HDFS has the cache system now https://issues.apache.org/jira/browse/HDFS-4949 )? Just my idea, also expecting others' feedbacks\n10. Patrick McFadin: Given that this part of the code is getting really complex, I'd strongly favor a simple solution initially which we can refine based on feeedback over time. The randomized solution is simple and well tested so it seems preferable to the other patch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.098302"}}
{"id": "bbb20ecc5e29afccd837c7d94b5db75d", "issue_key": "SPARK-980", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "NullPointerException for single-host setup with S3 URLs", "description": "Short version: * The use of {{execSparkHome_}} in [Worker.scala|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135] should be checked for {{null}} or that value should be defaulted or plumbed through. * If the {{sparkHome}} argument to {{new SparkContext(...)}} is non-optional, then it should not be marked as optional. Long version: Starting up with {{bin/start-all.sh}} and then connecting from a Scala program and attempting to read two S3 URLs results in the following trace in the worker log:  13/12/03 21:50:23 ERROR worker.Worker: java.lang.NullPointerException at java.io.File.<init>(File.java:277) at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:135) at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:120) at akka.actor.Actor$class.apply(Actor.scala:318) at org.apache.spark.deploy.worker.Worker.apply(Worker.scala:39) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)  This is on Mac OS X 10.9, Oracle Java 7u45, and the Hadoop 1 download from the incubator. Reading the code, this occurs because {{execSparkHome_}} is {{null}}; see [Worker.scala#L135|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135], and setting a value explicitly in the Scala driver allows the computation to complete.", "reporter": "Paul R. Brown", "assignee": null, "created": "2013-12-03T22:10:57.000+0000", "updated": "2015-01-23T12:06:43.000+0000", "resolved": "2015-01-23T12:06:41.000+0000", "labels": [], "components": ["Input/Output"], "comments": [{"author": "Reynold Xin", "body": "Thanks for reporting. What do you mean by S3 URLs?", "created": "2013-12-04T23:57:35.644+0000"}, {"author": "Nan Zhu", "body": "I think this has been resolved by PR 442 https://github.com/apache/incubator-spark/pull/442 and PR 447 https://github.com/apache/incubator-spark/pull/447 this is not related to S3, but just due to an empty SPARK_HOME in driver end", "created": "2014-01-16T05:35:50.369+0000"}, {"author": "Paul R. Brown", "body": "Those two PRs look like they would resolve the issue. Correct that it is not related to S3 URLs, per se, but it does list the line with the attempt to read the S3 objects as the root of the trace. (This is common for lazy invocations and stack traces, so c'est la vie.)", "created": "2014-01-16T10:34:19.273+0000"}, {"author": "Sean R. Owen", "body": "I believe Nan is correct that this was subsequently resolved. Although the references to source and PRs aren't live anymore, I see this in Worker.scala:  new File(sys.env.get(\"SPARK_HOME\").getOrElse(\".\"))  and this is the only reference to {{SPARK_HOME}}, so it seems to handle the case where this is missing.", "created": "2015-01-23T12:06:43.840+0000"}], "num_comments": 4, "text": "Issue: SPARK-980\nSummary: NullPointerException for single-host setup with S3 URLs\nDescription: Short version: * The use of {{execSparkHome_}} in [Worker.scala|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135] should be checked for {{null}} or that value should be defaulted or plumbed through. * If the {{sparkHome}} argument to {{new SparkContext(...)}} is non-optional, then it should not be marked as optional. Long version: Starting up with {{bin/start-all.sh}} and then connecting from a Scala program and attempting to read two S3 URLs results in the following trace in the worker log:  13/12/03 21:50:23 ERROR worker.Worker: java.lang.NullPointerException at java.io.File.<init>(File.java:277) at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:135) at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.apply(Worker.scala:120) at akka.actor.Actor$class.apply(Actor.scala:318) at org.apache.spark.deploy.worker.Worker.apply(Worker.scala:39) at akka.actor.ActorCell.invoke(ActorCell.scala:626) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:197) at akka.dispatch.Mailbox.run(Mailbox.scala:179) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)  This is on Mac OS X 10.9, Oracle Java 7u45, and the Hadoop 1 download from the incubator. Reading the code, this occurs because {{execSparkHome_}} is {{null}}; see [Worker.scala#L135|https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L135], and setting a value explicitly in the Scala driver allows the computation to complete.\n\nComments (4):\n1. Reynold Xin: Thanks for reporting. What do you mean by S3 URLs?\n2. Nan Zhu: I think this has been resolved by PR 442 https://github.com/apache/incubator-spark/pull/442 and PR 447 https://github.com/apache/incubator-spark/pull/447 this is not related to S3, but just due to an empty SPARK_HOME in driver end\n3. Paul R. Brown: Those two PRs look like they would resolve the issue. Correct that it is not related to S3 URLs, per se, but it does list the line with the attempt to read the S3 objects as the root of the trace. (This is common for lazy invocations and stack traces, so c'est la vie.)\n4. Sean R. Owen: I believe Nan is correct that this was subsequently resolved. Although the references to source and PRs aren't live anymore, I see this in Worker.scala:  new File(sys.env.get(\"SPARK_HOME\").getOrElse(\".\"))  and this is the only reference to {{SPARK_HOME}}, so it seems to handle the case where this is missing.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.098302"}}
{"id": "ce71b85c6c040b44091ef72fd0a19a46", "issue_key": "SPARK-981", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Seemingly spurious \"Duplicate worker ID\" error messages", "description": "I'm seeing {{Duplicate worker ID}} error messages after workers have crashed, and I'm presuming it comes from [Master.scala#L165|https://github.com/apache/incubator-spark/blob/c71499b7795564e1d16495c59273ecc027070fc5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L165]. Incomplete cleanup or ID collisions? This is on Mac OS X 10.9, Oracle Java 7u45, 0.8.0 download from the incubator.", "reporter": "Paul R. Brown", "assignee": null, "created": "2013-12-03T22:17:15.000+0000", "updated": "2014-11-25T09:54:14.000+0000", "resolved": "2014-11-25T09:54:14.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "I'm interested in this bug... what's the reason of the crash? if the worker actor is restarted, it should certainly have a new ID as def generateWorkerId(): String = { \"worker-%s-%s-%d\".format(DATE_FORMAT.format(new Date), host, port) } this function will be called", "created": "2014-01-16T11:41:50.779+0000"}, {"author": "Chen Chao", "body": "with the same port?", "created": "2014-01-16T18:38:23.071+0000"}, {"author": "Nan Zhu", "body": "even the port is the same, the DATE_FORMAT.formate(new Date) would make the ID distinct I once met register in the same address but not duplicate ID, (actually I'm thinking about what can trigger that if block(https://github.com/apache/incubator-spark/blob/c71499b7795564e1d16495c59273ecc027070fc5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L165))", "created": "2014-01-16T18:42:19.266+0000"}, {"author": "Sean R. Owen", "body": "It sounds like it may be the same issue reported, and being worked on, in https://issues.apache.org/jira/browse/SPARK-4592 . At least, both concern \"why does I see Duplicate worker ID\" messages? since the latter is active with a PR and this isn't, I suggest making this a duplicate.", "created": "2014-11-25T09:54:14.357+0000"}], "num_comments": 4, "text": "Issue: SPARK-981\nSummary: Seemingly spurious \"Duplicate worker ID\" error messages\nDescription: I'm seeing {{Duplicate worker ID}} error messages after workers have crashed, and I'm presuming it comes from [Master.scala#L165|https://github.com/apache/incubator-spark/blob/c71499b7795564e1d16495c59273ecc027070fc5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L165]. Incomplete cleanup or ID collisions? This is on Mac OS X 10.9, Oracle Java 7u45, 0.8.0 download from the incubator.\n\nComments (4):\n1. Nan Zhu: I'm interested in this bug... what's the reason of the crash? if the worker actor is restarted, it should certainly have a new ID as def generateWorkerId(): String = { \"worker-%s-%s-%d\".format(DATE_FORMAT.format(new Date), host, port) } this function will be called\n2. Chen Chao: with the same port?\n3. Nan Zhu: even the port is the same, the DATE_FORMAT.formate(new Date) would make the ID distinct I once met register in the same address but not duplicate ID, (actually I'm thinking about what can trigger that if block(https://github.com/apache/incubator-spark/blob/c71499b7795564e1d16495c59273ecc027070fc5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L165))\n4. Sean R. Owen: It sounds like it may be the same issue reported, and being worked on, in https://issues.apache.org/jira/browse/SPARK-4592 . At least, both concern \"why does I see Duplicate worker ID\" messages? since the latter is active with a PR and this isn't, I suggest making this a duplicate.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.098302"}}
{"id": "b2cd27de5fdf0b3a021bc45d760b37b3", "issue_key": "SPARK-982", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Typo on Hadoop third-party page", "description": "http://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html There's a subtle typo which prevents Spark users from compiling against CDH. It has \"chd4\" instead of \"cdh4\". Release Version code CDH 4.X.X (YARN mode) 2.0.0-chd4.X.X CDH 4.X.X 2.0.0-mr1-chd4.X.X CDH 3u6 0.20.2-cdh3u6 CDH 3u5 0.20.2-cdh3u5 CDH 3u4 0.20.2-cdh3u4 should be Release Version code CDH 4.X.X (YARN mode) 2.0.0-cdh4.X.X CDH 4.X.X 2.0.0-mr1-cdh4.X.X CDH 3u6 0.20.2-cdh3u6 CDH 3u5 0.20.2-cdh3u5 CDH 3u4 0.20.2-cdh3u4", "reporter": "Matt Massie", "assignee": null, "created": "2013-12-04T19:13:51.000+0000", "updated": "2013-12-04T20:31:50.000+0000", "resolved": "2013-12-04T20:31:50.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Patrick McFadin", "body": "This was fixed in bef398e5... closing!", "created": "2013-12-04T20:31:40.138+0000"}], "num_comments": 1, "text": "Issue: SPARK-982\nSummary: Typo on Hadoop third-party page\nDescription: http://spark.incubator.apache.org/docs/latest/hadoop-third-party-distributions.html There's a subtle typo which prevents Spark users from compiling against CDH. It has \"chd4\" instead of \"cdh4\". Release Version code CDH 4.X.X (YARN mode) 2.0.0-chd4.X.X CDH 4.X.X 2.0.0-mr1-chd4.X.X CDH 3u6 0.20.2-cdh3u6 CDH 3u5 0.20.2-cdh3u5 CDH 3u4 0.20.2-cdh3u4 should be Release Version code CDH 4.X.X (YARN mode) 2.0.0-cdh4.X.X CDH 4.X.X 2.0.0-mr1-cdh4.X.X CDH 3u6 0.20.2-cdh3u6 CDH 3u5 0.20.2-cdh3u5 CDH 3u4 0.20.2-cdh3u4\n\nComments (1):\n1. Patrick McFadin: This was fixed in bef398e5... closing!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.098302"}}
{"id": "1b2cb62af384813ca91b916946bc8c5a", "issue_key": "SPARK-983", "issue_type": "New Feature", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Support external sorting for RDD#sortByKey()", "description": "Currently, RDD#sortByKey() is implemented by a mapPartitions which creates a buffer to hold the entire partition, then sorts it. This will cause an OOM if an entire partition cannot fit in memory, which is especially problematic for skewed data. Rather than OOMing, the behavior should be similar to the [ExternalAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala], where we fallback to disk if we detect memory pressure.", "reporter": "Reynold Xin", "assignee": "Matei Alexandru Zaharia", "created": "2013-12-04T23:45:12.000+0000", "updated": "2014-08-01T07:17:11.000+0000", "resolved": "2014-08-01T07:17:11.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Madhu Siddalingaiah", "body": "Can someone comment on the scope of this feature? My specific interest is to sort each partition in an RDD given a comparison function. This can be done now if all partitions fit in memory, but I don't see an easy way to do it if partitions are larger than available memory.", "created": "2014-05-22T17:28:50.430+0000"}, {"author": "Andrew Ash", "body": "I think the main intention is to make the .sortByKey() call not require all data to fit into memory. If you just want to sort data within each partition, but not across partitions, that should already be possible using .mapPartitions()", "created": "2014-05-22T18:08:27.088+0000"}, {"author": "Aaron Davidson", "body": "This JIRA is pretty vague. We've already implemented an external hashing based solution for groupBys. We do not have a solution in the works for sorting, either for groupBys or RDD.sort(). We have deprioritized the former, since we have the hashing solution, but we don't have any way to do sort(), so that would be an excellent feature to have. Given what's been marked as a duplicate of this JIRA, let me update this one to just refer to supporting external RDD.sort(). I will also unassign it, as I am not currently working on this -- let me know if you'd like me to assign it to you.", "created": "2014-05-22T18:24:18.936+0000"}, {"author": "Aaron Davidson", "body": "[~aash] Note that the current implementation actually just sorts within each partition, using an initial pass from a RangePartitioner.", "created": "2014-05-22T18:29:25.209+0000"}, {"author": "Madhu Siddalingaiah", "body": "Looking at [OrderedRDDFunctions|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala], there's a shuffle step using RangePartitioner, then an in-memory sort of each partition by key. If we separate the partition sort and make that available as an independent API call, it could serve two purposes: sortByKey() and sortPartitions(). Then we could improve sortPartitions() to fall back to disk like [ExternalAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala]. The above approach would address this JIRA feature and support the equivalent of Hadoop secondary sort in a scalable way. There are plenty of time series-like use cases that could benefit from it. There's a lot more to it, but I'll code something up locally and see how it goes...", "created": "2014-05-23T00:50:38.282+0000"}, {"author": "Madhu Siddalingaiah", "body": "I have the beginnings of a SortedIterator working for data that will fit in memory. It does more or less the same thing as partition sort in OrderedRDDFunctions, but it's an iterator. If we know that a partition cannot fit in memory, it's possible to split it up into chunks, sort each chunk, write to disk, and merge the chunks on disk. To determine when to split, is it reasonable to use Runtime.freeMemory() / maxMemory() along with configuration limits? I could just keep adding to an in-memory sortable list until some memory threshold is reached, then sort/spill to disk, repeat until all data has been sorted and spilled. Then it's a basic merge operation. Any comments?", "created": "2014-05-23T18:05:44.681+0000"}, {"author": "Andrew Ash", "body": "Because ExternalAppendOnlyMap uses Runtime.getRuntime.maxMemory I think that's what you should use for this feature work also. See the maxMemoryThreshold value here: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L79 On Fri, May 23, 2014 at 2:07 PM, Madhu Siddalingaiah (JIRA) <jira@apache.org", "created": "2014-05-25T21:18:02.819+0000"}, {"author": "Aaron Davidson", "body": "Historically, we have not used Runtime.freeMemory(), instead favoring the use of \"memoryFractions\", such as spark.shuffle.memoryFraction and spark.storage.memoryFraction, which are fractions of Runtime.maxMemory(). One problem with Runtime.freeMemory() is that the JVM could very happily sit near 0 freeMemory, just waiting for significant new allocation to occur before freeing up some memory. If you just asked for some memory, though, it would be made available. The downside to using the memoryFractions, though, is twofold: (1) It can lead to OOMs if misconfigured, even if the sum of memoryFractions is < 1, because Java + Spark need some amount of working memory to keep running (and to garbage collect). (2) It can lead to underutilized memory, if, for instance, the user is not caching any RDDs or currently doing a shuffle, as all that memory will remain reserved nevertheless. In my opinion, the \"right\" solution is to have a memory manager for Spark, which allocates memory to various components (e.g., storage, shuffle, and sorting), which is willing to give one component more memory if other ones are not using it. However, this hasn't even been designed or agreed upon as a correct course, let alone implemented.", "created": "2014-05-26T03:48:41.172+0000"}, {"author": "Aaron Davidson", "body": "[~pwendell] or [~matei], any opinions on memory management best practices? Adding a new memoryFraction for sorting will only exacerbate the problems we see with them, but I'm not sure we can rely on Runtime.freeMemory() as even an intermediary solution. Perhaps this feature could draw from the same pool as shuffle.memoryFraction, as it's used for a similar purpose, and that pool already implements some notion of memory sharing.", "created": "2014-05-26T03:50:15.577+0000"}, {"author": "Madhu Siddalingaiah", "body": "I had similar concerns that RunTime.freeMemory() would not be reliable. What about using weak references? We could spill to disk in the finalize() method of a weak reference. From my reading of [finalize()|http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#finalize%28%29], it is acceptable to perform I/O operations (with sufficient rigor). The code might be a bit tricky as there is no guarantee on the thread that calls finalize(), but I think it could be done.", "created": "2014-05-26T15:16:26.752+0000"}, {"author": "Patrick Wendell", "body": "We are actually looking at this problem in a few different places in the code base (we did this already for the external aggregations, and we also have SPARK-1777). Relying on GC's to decide when to spill is an interesting approach, but I'd rather have control of the heuristics ourselves. I think you'd get this thrashing behavior where a GC occurred and suddenly a million threads start writing to disk. In the past we've used a different mechanism (the size estimator) which approximates memory usage. It might make sense to introduce a simple memory allocation mechanism that is shared between the external aggregation maps, partition unrolling, etc. This is something where a design doc would be helpful.", "created": "2014-05-26T23:40:39.799+0000"}, {"author": "Mark Hamstra", "body": "I'm hoping these can be kept orthogonal, but I think that it is worth noting the existence of SPARK-1021 and the fact that sortByKey as it currently exists breaks Spark's \"transformations of RDDs are lazy\" contract. I'm currently working on that issue, which is undoubtedly going to require at least some merge work to be compatible with the resolution of this issue.", "created": "2014-05-27T16:39:09.488+0000"}, {"author": "Madhu Siddalingaiah", "body": "Understood. Here are my thoughts: I can implement this feature using SizeEstimator, even though it's not ideal. Once there is greater consensus on how to manage memory more efficiently, I think it should not be hard to adapt the code. At the minimum, the spill/merge code will be in place. I'm watching [SPARK-1021|https://issues.apache.org/jira/browse/SPARK-1021], so if that's done first, I'll merge it in. Does that sound reasonable? [Aaron Davidson|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ilikerps], feel free to assign this feature to me.", "created": "2014-05-27T19:42:24.920+0000"}, {"author": "Aaron Davidson", "body": "Does sound reasonable. For some reason it does not allow me to assign the issue to you, though. Edit: Figured it out, thanks [~pwendell]!", "created": "2014-05-27T19:51:27.687+0000"}, {"author": "Madhu Siddalingaiah", "body": "I tested some additions locally that seem to work well so far. I created a SortedPartitionsRDD and a sortPartitions(...) method in [RDD|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala]:  /** * Return a new RDD containing sorted partitions in this RDD. */ def sortPartitions(lt: (T, T) => Boolean): RDD[T] = new SortedPartitionsRDD(this, sc.clean(lt))  I haven't added the spill/merge code to SortedPartitionsRDD yet. I wanted to get some buy in on this method as it's an addition to the API. It fits nicely with [OrderedRDDFunctions|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala] and passes all tests in [SortingSuite|https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/rdd/SortingSuite.scala]. I think this method can be used to address [SPARK-1021|https://issues.apache.org/jira/browse/SPARK-1021] as well as many use cases outside of sortByKey(). Does everyone agree? If so, I'll move forward with external sort in SortedPartitionsRDD and necessary tests.", "created": "2014-06-01T17:39:00.750+0000"}, {"author": "Mark Hamstra", "body": "Is that code visible someplace? In broad outline, it seems similar to the approach I'm anticipating taking to address SPARK-1021 -- and maybe I'll get a chance to actually do some work on that later this week.", "created": "2014-06-02T18:09:50.367+0000"}, {"author": "Madhu Siddalingaiah", "body": "Yes, the code is checked in here: [https://github.com/msiddalingaiah/spark] If you're happy with it, I can fill in the guts without affecting your work on [SPARK-1021|https://issues.apache.org/jira/browse/SPARK-1021]. BTW, I was able to get spark-core to build and run in Eclipse (Spark-IDE + Scala Test). There was a bit of fiddling, but it works quite well.", "created": "2014-06-02T21:17:54.466+0000"}, {"author": "Madhu Siddalingaiah", "body": "I made more progress over the weekend. I have the merge sort working reliably and passing all sorting tests. The only things left are to read/write blocks to disk (I'm developing all in-memory so far) and add spill conditions. I hope to get it finalized in the next few days and submit the PR.", "created": "2014-06-09T19:47:28.944+0000"}, {"author": "Madhu Siddalingaiah", "body": "[Aaron Davidson|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ilikerps], can you make a recommendation on how to fill in this [fitsInMemory|https://github.com/msiddalingaiah/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/SortedParitionsRDD.scala#L78] method? I have the disk spill/merge all working, I just need to complete the spill condition. I looked at [SizeTrackingAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala], but it's not completely clear to me how it's working. Thanks!", "created": "2014-06-12T01:46:42.524+0000"}, {"author": "Aaron Davidson", "body": "The idea for SizeTrackingAppendOnlyMap is that we can estimate the size of an object with SizeEstimator, but doing so can be relatively costly. Rather than running this estimation on every element added to the map, we amortize the cost by sampling exponentially less often (similar to amortization of adding to an ArrayList). If you expect your sublists to be relatively large, it may be perfectly fine to just call SizeEstimator on each one.", "created": "2014-06-12T17:52:29.379+0000"}, {"author": "Madhu Siddalingaiah", "body": "Thanks Aaron. PR submitted: https://github.com/apache/spark/pull/1090", "created": "2014-06-15T02:03:11.748+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Now that an ExternalSorter class from SPARK-2045 is in, I've submitted a much smaller PR that reuses that: https://github.com/apache/spark/pull/1677. Thanks both [~msiddalingaiah] and [~andrew xia] for your previous patches on this.", "created": "2014-07-31T01:45:48.317+0000"}], "num_comments": 22, "text": "Issue: SPARK-983\nSummary: Support external sorting for RDD#sortByKey()\nDescription: Currently, RDD#sortByKey() is implemented by a mapPartitions which creates a buffer to hold the entire partition, then sorts it. This will cause an OOM if an entire partition cannot fit in memory, which is especially problematic for skewed data. Rather than OOMing, the behavior should be similar to the [ExternalAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala], where we fallback to disk if we detect memory pressure.\n\nComments (22):\n1. Madhu Siddalingaiah: Can someone comment on the scope of this feature? My specific interest is to sort each partition in an RDD given a comparison function. This can be done now if all partitions fit in memory, but I don't see an easy way to do it if partitions are larger than available memory.\n2. Andrew Ash: I think the main intention is to make the .sortByKey() call not require all data to fit into memory. If you just want to sort data within each partition, but not across partitions, that should already be possible using .mapPartitions()\n3. Aaron Davidson: This JIRA is pretty vague. We've already implemented an external hashing based solution for groupBys. We do not have a solution in the works for sorting, either for groupBys or RDD.sort(). We have deprioritized the former, since we have the hashing solution, but we don't have any way to do sort(), so that would be an excellent feature to have. Given what's been marked as a duplicate of this JIRA, let me update this one to just refer to supporting external RDD.sort(). I will also unassign it, as I am not currently working on this -- let me know if you'd like me to assign it to you.\n4. Aaron Davidson: [~aash] Note that the current implementation actually just sorts within each partition, using an initial pass from a RangePartitioner.\n5. Madhu Siddalingaiah: Looking at [OrderedRDDFunctions|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala], there's a shuffle step using RangePartitioner, then an in-memory sort of each partition by key. If we separate the partition sort and make that available as an independent API call, it could serve two purposes: sortByKey() and sortPartitions(). Then we could improve sortPartitions() to fall back to disk like [ExternalAppendOnlyMap|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala]. The above approach would address this JIRA feature and support the equivalent of Hadoop secondary sort in a scalable way. There are plenty of time series-like use cases that could benefit from it. There's a lot more to it, but I'll code something up locally and see how it goes...\n6. Madhu Siddalingaiah: I have the beginnings of a SortedIterator working for data that will fit in memory. It does more or less the same thing as partition sort in OrderedRDDFunctions, but it's an iterator. If we know that a partition cannot fit in memory, it's possible to split it up into chunks, sort each chunk, write to disk, and merge the chunks on disk. To determine when to split, is it reasonable to use Runtime.freeMemory() / maxMemory() along with configuration limits? I could just keep adding to an in-memory sortable list until some memory threshold is reached, then sort/spill to disk, repeat until all data has been sorted and spilled. Then it's a basic merge operation. Any comments?\n7. Andrew Ash: Because ExternalAppendOnlyMap uses Runtime.getRuntime.maxMemory I think that's what you should use for this feature work also. See the maxMemoryThreshold value here: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L79 On Fri, May 23, 2014 at 2:07 PM, Madhu Siddalingaiah (JIRA) <jira@apache.org\n8. Aaron Davidson: Historically, we have not used Runtime.freeMemory(), instead favoring the use of \"memoryFractions\", such as spark.shuffle.memoryFraction and spark.storage.memoryFraction, which are fractions of Runtime.maxMemory(). One problem with Runtime.freeMemory() is that the JVM could very happily sit near 0 freeMemory, just waiting for significant new allocation to occur before freeing up some memory. If you just asked for some memory, though, it would be made available. The downside to using the memoryFractions, though, is twofold: (1) It can lead to OOMs if misconfigured, even if the sum of memoryFractions is < 1, because Java + Spark need some amount of working memory to keep running (and to garbage collect). (2) It can lead to underutilized memory, if, for instance, the user is not caching any RDDs or currently doing a shuffle, as all that memory will remain reserved nevertheless. In my opinion, the \"right\" solution is to have a memory manager for Spark, which allocates memory to various components (e.g., storage, shuffle, and sorting), which is willing to give one component more memory if other ones are not using it. However, this hasn't even been designed or agreed upon as a correct course, let alone implemented.\n9. Aaron Davidson: [~pwendell] or [~matei], any opinions on memory management best practices? Adding a new memoryFraction for sorting will only exacerbate the problems we see with them, but I'm not sure we can rely on Runtime.freeMemory() as even an intermediary solution. Perhaps this feature could draw from the same pool as shuffle.memoryFraction, as it's used for a similar purpose, and that pool already implements some notion of memory sharing.\n10. Madhu Siddalingaiah: I had similar concerns that RunTime.freeMemory() would not be reliable. What about using weak references? We could spill to disk in the finalize() method of a weak reference. From my reading of [finalize()|http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#finalize%28%29], it is acceptable to perform I/O operations (with sufficient rigor). The code might be a bit tricky as there is no guarantee on the thread that calls finalize(), but I think it could be done.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.100305"}}
{"id": "4df438f2f9672c6345fc7e5d7eb9e396", "issue_key": "SPARK-984", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "SPARK_TOOLS_JAR not set if multiple tools jars exists", "description": "If you have multiple tools assemblies (e.g., if you assembled on 0.8.1 and 0.9.0 before, for instance), then this error is thrown in spark-class: ./spark-class: line 115: [: /home/aaron/spark/tools/target/scala-2.9.3/spark-tools-assembly-0.8.1-incubating-SNAPSHOT.jar: binary operator expected This is because of a flaw in the bash script: if [ -e \"$TOOLS_DIR\"/target/scala-$SCALA_VERSION/*assembly*[0-9Tg].jar ]; then which does not parse correctly if the path resolves to multiple files. The error is non-fatal, but a nuisance and presumably breaks whatever SPARK_TOOLS_JAR is used for. Currently, we error if multiple Spark assemblies are found, so we could do something similar for tools assemblies. The only issue is that means that the user will always have to go through both errors (clean the assembly/ jars then tools/ jar) when it appears that the tools/ jar is not actually important for normal operation. The second possibility is to infer the correct tools jar using the single available assembly jar, but this is slightly complicated by the code path if $FWDIR/RELEASE exists. Since I'm not 100% on what SPARK_TOOLS_JAR is even for, I'm assigning this to Josh who wrote the code initially.", "reporter": "Aaron Davidson", "assignee": "Marcelo Masiero Vanzin", "created": "2013-12-07T01:33:07.000+0000", "updated": "2015-04-30T15:55:37.000+0000", "resolved": "2015-04-27T22:00:33.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Josh Rosen", "body": "SPARK_TOOLS_JAR is the assembly for the spark-tools subproject, which contains developer tools like JavaAPICompletenessChecker; it was added as an alternative to placing those tools in the examples subproject.", "created": "2013-12-07T09:19:47.876+0000"}, {"author": "Josh Rosen", "body": "Since the Spark Tools project contains tools for use by Spark developers and not ordinary users, we could probably just require developers to access those tools through sbt. In the sbt shell:  project tools run  I actually prefer to use JavaAPICompletenessChecker this was since it doesn't require me to go through a whole assembly cycle when I make changes. If nobody has any objections, I'll submit a PR to remove the assemblies for spark-tools and update the wiki to describe the sbt run method.", "created": "2014-01-23T19:52:52.808+0000"}, {"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/4181", "created": "2015-01-23T12:29:49.583+0000"}, {"author": "Josh Rosen", "body": "It looks like this setting was removed as part of SPARK-4924, Marcelo's Spark Launcher PR, so I'm going to mark this as resolved with 1.4.0 as the fix version.", "created": "2015-04-27T22:00:24.956+0000"}], "num_comments": 4, "text": "Issue: SPARK-984\nSummary: SPARK_TOOLS_JAR not set if multiple tools jars exists\nDescription: If you have multiple tools assemblies (e.g., if you assembled on 0.8.1 and 0.9.0 before, for instance), then this error is thrown in spark-class: ./spark-class: line 115: [: /home/aaron/spark/tools/target/scala-2.9.3/spark-tools-assembly-0.8.1-incubating-SNAPSHOT.jar: binary operator expected This is because of a flaw in the bash script: if [ -e \"$TOOLS_DIR\"/target/scala-$SCALA_VERSION/*assembly*[0-9Tg].jar ]; then which does not parse correctly if the path resolves to multiple files. The error is non-fatal, but a nuisance and presumably breaks whatever SPARK_TOOLS_JAR is used for. Currently, we error if multiple Spark assemblies are found, so we could do something similar for tools assemblies. The only issue is that means that the user will always have to go through both errors (clean the assembly/ jars then tools/ jar) when it appears that the tools/ jar is not actually important for normal operation. The second possibility is to infer the correct tools jar using the single available assembly jar, but this is slightly complicated by the code path if $FWDIR/RELEASE exists. Since I'm not 100% on what SPARK_TOOLS_JAR is even for, I'm assigning this to Josh who wrote the code initially.\n\nComments (4):\n1. Josh Rosen: SPARK_TOOLS_JAR is the assembly for the spark-tools subproject, which contains developer tools like JavaAPICompletenessChecker; it was added as an alternative to placing those tools in the examples subproject.\n2. Josh Rosen: Since the Spark Tools project contains tools for use by Spark developers and not ordinary users, we could probably just require developers to access those tools through sbt. In the sbt shell:  project tools run  I actually prefer to use JavaAPICompletenessChecker this was since it doesn't require me to go through a whole assembly cycle when I make changes. If nobody has any objections, I'll submit a PR to remove the assemblies for spark-tools and update the wiki to describe the sbt run method.\n3. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/4181\n4. Josh Rosen: It looks like this setting was removed as part of SPARK-4924, Marcelo's Spark Launcher PR, so I'm going to mark this as resolved with 1.4.0 as the fix version.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.100305"}}
{"id": "e110ca776ea398b64c0be4e423915711", "issue_key": "SPARK-985", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support Job Cancellation on Mesos Scheduler", "description": "https://github.com/apache/incubator-spark/pull/29 added job cancellation but may still need support for Mesos scheduler backends: Quote:  This looks good except that MesosSchedulerBackend isn't yet calling Mesos's killTask. Do you want to add that too or are you planning to push it till later? I don't think it's a huge change.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-12-07T13:46:06.000+0000", "updated": "2020-05-17T17:48:52.000+0000", "resolved": "2015-02-07T22:46:09.000+0000", "labels": [], "components": ["Mesos", "Scheduler", "Spark Core"], "comments": [{"author": "Patrick Wendell", "body": "Some more notes on this from a related thread: Task killing is not supported in the fine-grained mode on mesos because, in that mode, we use Mesos's built in support for all of the control plane messages relating to tasks. So we'll have to figure out how to support killing tasks in that model. There are two questions, one is who actually sends the \"kill\" message to the executor and the other is how we tell Mesos that the cores are freed which were in use by the task. In the course of normal operation that's handled by using the Mesos launchTask and sendStatusUpdate interfaces.", "created": "2014-06-25T21:34:20.293+0000"}, {"author": "Josh Rosen", "body": "I'm pretty sure that this was resolved by SPARK-3597 in 1.1.1 and 1.2.0: now that MesosSchedulerBackend implements killTask, I think we now have support for job cancellation on Mesos. I'm going to mark this as \"Resolved\", but feel free to re-open if there's still work to be done.", "created": "2015-02-07T22:46:09.783+0000"}], "num_comments": 2, "text": "Issue: SPARK-985\nSummary: Support Job Cancellation on Mesos Scheduler\nDescription: https://github.com/apache/incubator-spark/pull/29 added job cancellation but may still need support for Mesos scheduler backends: Quote:  This looks good except that MesosSchedulerBackend isn't yet calling Mesos's killTask. Do you want to add that too or are you planning to push it till later? I don't think it's a huge change.\n\nComments (2):\n1. Patrick Wendell: Some more notes on this from a related thread: Task killing is not supported in the fine-grained mode on mesos because, in that mode, we use Mesos's built in support for all of the control plane messages relating to tasks. So we'll have to figure out how to support killing tasks in that model. There are two questions, one is who actually sends the \"kill\" message to the executor and the other is how we tell Mesos that the cores are freed which were in use by the task. In the course of normal operation that's handled by using the Mesos launchTask and sendStatusUpdate interfaces.\n2. Josh Rosen: I'm pretty sure that this was resolved by SPARK-3597 in 1.1.1 and 1.2.0: now that MesosSchedulerBackend implements killTask, I think we now have support for job cancellation on Mesos. I'm going to mark this as \"Resolved\", but feel free to re-open if there's still work to be done.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.100305"}}
{"id": "246eb478a210462bd6367f596fe48f8b", "issue_key": "SPARK-986", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add job cancellation to PySpark", "description": "We should add support for job cancellation to PySpark. It would also be nice to be able to cancel jobs via ctrl-c in the PySpark shell.", "reporter": "Josh Rosen", "assignee": "Ahir Reddy", "created": "2013-12-07T13:47:13.000+0000", "updated": "2014-04-25T03:21:56.000+0000", "resolved": "2014-04-25T03:21:56.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-986\nSummary: Add job cancellation to PySpark\nDescription: We should add support for job cancellation to PySpark. It would also be nice to be able to cancel jobs via ctrl-c in the PySpark shell.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.100305"}}
{"id": "6b384fbbdc98be620ba5b73036bf06f6", "issue_key": "SPARK-987", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Cannot start workers successfully with hadoop 2.2.0", "description": "Cannot start workers successfully with hadoop 2.2.0. I build with: $make-distribution.sh -hadoop 2.2.0 P.S. Can work well with hadoop 2.0.5-alpha. But cannot connect the hadoop 2.2.0 successfully with this exception : scala> var lines = sc.textFile(\"hdfs://localhost:9000/user/hadoop/hadoop/hadoop-hadoop-jobtracker-master.log\") lines: org.apache.spark.rdd.RDD[String] = MappedRDD[3] at textFile at <console>:12 scala> lines.count java.io.IOException: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Message missing required fields: callId, status; Host Details : local host is: \"master/192.168.3.103\"; destination host is: \"localhost\":9000;", "reporter": "刘旭", "assignee": null, "created": "2013-12-07T18:51:22.000+0000", "updated": "2014-11-08T09:51:12.000+0000", "resolved": "2014-11-08T09:51:12.000+0000", "labels": ["2.2.0", "hadoop"], "components": ["Spark Core"], "comments": [{"author": "Sean R. Owen", "body": "This just looks like a classic version mismatch between client and server. The app perhaps has embedded Hadoop libs instead of using 'provided' libs from the server installation.", "created": "2014-11-08T09:51:12.531+0000"}], "num_comments": 1, "text": "Issue: SPARK-987\nSummary: Cannot start workers successfully with hadoop 2.2.0\nDescription: Cannot start workers successfully with hadoop 2.2.0. I build with: $make-distribution.sh -hadoop 2.2.0 P.S. Can work well with hadoop 2.0.5-alpha. But cannot connect the hadoop 2.2.0 successfully with this exception : scala> var lines = sc.textFile(\"hdfs://localhost:9000/user/hadoop/hadoop/hadoop-hadoop-jobtracker-master.log\") lines: org.apache.spark.rdd.RDD[String] = MappedRDD[3] at textFile at <console>:12 scala> lines.count java.io.IOException: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Message missing required fields: callId, status; Host Details : local host is: \"master/192.168.3.103\"; destination host is: \"localhost\":9000;\n\nComments (1):\n1. Sean R. Owen: This just looks like a classic version mismatch between client and server. The app perhaps has embedded Hadoop libs instead of using 'provided' libs from the server installation.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.100305"}}
{"id": "eae8666860fc858a270a2b78eb830cd6", "issue_key": "SPARK-988", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Write PySpark profiling guide", "description": "Write a guide on profiling PySpark applications. I've done this in the past by modifying the workers to make cProfile dumps, then using various tools to collect and merge those dumps into an overall performance profile.", "reporter": "Josh Rosen", "assignee": null, "created": "2013-12-08T13:58:56.000+0000", "updated": "2015-01-03T22:45:29.000+0000", "resolved": "2015-01-03T22:45:29.000+0000", "labels": [], "components": ["Documentation", "PySpark"], "comments": [{"author": "Josh Rosen", "body": "We added distributed Python profiling support in 1.2 (see SPARK-3478).", "created": "2015-01-03T22:45:29.790+0000"}], "num_comments": 1, "text": "Issue: SPARK-988\nSummary: Write PySpark profiling guide\nDescription: Write a guide on profiling PySpark applications. I've done this in the past by modifying the workers to make cProfile dumps, then using various tools to collect and merge those dumps into an overall performance profile.\n\nComments (1):\n1. Josh Rosen: We added distributed Python profiling support in 1.2 (see SPARK-3478).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.100305"}}
{"id": "49ab1254e9f2722e16dd762de1e351fe", "issue_key": "SPARK-989", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Executors table in application web ui is wrong", "description": "See attached picture. The (executorId, address) pairs are not associated with the right state data. The most obvious sign of this is that the <driver> executor has run tasks while another executor has not. This happens for other executors as well.", "reporter": "Aaron Davidson", "assignee": null, "created": "2013-12-08T17:21:23.000+0000", "updated": "2013-12-08T17:51:11.000+0000", "resolved": "2013-12-08T17:51:11.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Reynold Xin", "body": "I think this one was fixed in https://github.com/apache/incubator-spark/pull/181 ?", "created": "2013-12-08T17:22:35.065+0000"}, {"author": "Aaron Davidson", "body": "My apologies, I did not realize that was what that patch was fixing. Also I checked the wrong place for recent updates to the file. Double fail on my part! Anyway, I've confirmed that that patch works, at least. Sorry about the trouble.", "created": "2013-12-08T17:50:44.957+0000"}], "num_comments": 2, "text": "Issue: SPARK-989\nSummary: Executors table in application web ui is wrong\nDescription: See attached picture. The (executorId, address) pairs are not associated with the right state data. The most obvious sign of this is that the <driver> executor has run tasks while another executor has not. This happens for other executors as well.\n\nComments (2):\n1. Reynold Xin: I think this one was fixed in https://github.com/apache/incubator-spark/pull/181 ?\n2. Aaron Davidson: My apologies, I did not realize that was what that patch was fixing. Also I checked the wrong place for recent updates to the file. Double fail on my part! Anyway, I've confirmed that that patch works, at least. Sorry about the trouble.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.100305"}}
{"id": "cb357b443e3428996bf5c3cc86481831", "issue_key": "SPARK-990", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "JavaPairRDD.top produces ClassNotFoundException", "description": "For a class the extends a PairFlatMapFunction and implements the Comparator interface, the flatMap operation is able to load and execute this class, while the top operation produces a ClassNotFound exception. i.e.  public class TopTester { // declare an inner class as both a function and comparator. private static final class ScoreFunction extends PairFlatMapFunction<Tuple2<String,Tuple2<UUID,Double>> UUID, Double> implements Comparator<Tuple2<UUID, Double>> { public Iterable<Tuple2<UUID, Double>> call(Tuple2<String, Tuple2<UUID, Double>> match) throws Exception { return null; } public int compare(Tuple2<UUID, Double> o1, Tuple2<UUID, Double> o2) { return o1._2.compareTo(o2._2); } } public static void main(String[] argv) { // some setup ... // this operations passes, to prove that the env is setup correctly. JavaPairRDD<UUID,Double> scoreRdd = matchRdd.flatMap<new ScoreFunction()); // ClassNotFoundException ! scoreRdd.top(20, new ScoreFunction()); } }   Caused by: java.lang.ClassNotFoundException: controllers.KeywordIndex$ScoreFunction at java.net.URLClassLoader$1.run(URLClassLoader.java:366) ~[na:1.7.0_45] at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[na:1.7.0_45] at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_45] at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[na:1.7.0_45] at java.lang.ClassLoader.loadClass(ClassLoader.java:425) ~[na:1.7.0_45] at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ~[na:1.7.0_45]", "reporter": "Albert Kwong", "assignee": null, "created": "2013-12-09T19:58:12.000+0000", "updated": "2014-01-04T15:04:46.000+0000", "resolved": "2014-01-04T15:04:46.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "Due to Spark's lazy evaluation, no distributed computations take place until you call the top() action (which triggers the execution of the flatMap() transformation). You can call scoreRDD.foreach{} to force evaluation of the flatMap() without calling top(), but I think you'd run into the same error. The error suggests that the Spark worker can't load your custom class, so my bet is that you need to add a JAR containing your code to your JavaSparkContext (see https://spark.incubator.apache.org/docs/0.8.0/scala-programming-guide.html#deploying-code-on-a-cluster for the relevant section of the Scala programming guide, which describes the same concept). There's an example of this in the JavaWordCount example, where we add the Spark examples JAR to SparkContext: https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java#L39", "created": "2013-12-09T22:48:16.542+0000"}, {"author": "Albert Kwong", "body": "Hi Josh, The jar is loaded successfully into the JavaSparkContext. This is proven by the fact that the first call to flatMap is successful (which loads the same inner class that implements both PairFlatMapFunction and Comparator). Yes, I have been using count() / collect() to force execution. And yes, the code runs fine if I comment out the top() call and replace it with a collect() + local sort routine. // List<Tuple2<UUID, Double>> collect = resultRdd.top(20, new ScoreFunction()); List<Tuple2<UUID, Double>> collect = resultRdd.collect(); Collections.sort(collect, new Comparator<Tuple2<UUID, Double>>() { @Override public int compare(Tuple2<UUID, Double> o1, Tuple2<UUID, Double> o2) { return o2._2.compareTo(o1._2); // descending } });", "created": "2013-12-09T23:49:02.077+0000"}, {"author": "Josh Rosen", "body": "I wonder if the top() call is somehow trying to load that class using a different class loader that doesn't include JARs added through addJars.", "created": "2013-12-10T09:32:34.957+0000"}, {"author": "Albert Kwong", "body": "I am wondering whether this sc.clean(f) is doing the trick, which is kinda missing from def top.  /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */ def flatMap[U: ClassManifest](f: T => TraversableOnce[U]): RDD[U] = new FlatMappedRDD(this, sc.clean(f)) /** * Returns the top K elements from this RDD as defined by * the specified implicit Ordering[T]. * @param num the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def top(num: Int)(implicit ord: Ordering[T]): Array[T] = { mapPartitions { items => val queue = new BoundedPriorityQueue[T](num) queue ++= items Iterator.single(queue) }.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord.reverse) }   /** * Clean a closure to make it ready to serialized and send to tasks * (removes unreferenced variables in $outer's, updates REPL variables) */ private[spark] def clean[F <: AnyRef](f: F): F = { ClosureCleaner.clean(f) return f }", "created": "2013-12-10T18:09:07.114+0000"}, {"author": "Albert Kwong", "body": "Seems the problem may not be related to spark after all. I have modified the JavaWordCount example with a top() step and it runs well.", "created": "2013-12-11T20:05:23.313+0000"}, {"author": "Albert Kwong", "body": "Confirmed that the classpath problem is on the driver side, not the worker side. In particular, the driver side was running in a web framework (Play 2.1.1) that is interfering with spark. Moving away from Play to pure embedded Jetty solved the problem.", "created": "2013-12-12T18:44:20.168+0000"}], "num_comments": 6, "text": "Issue: SPARK-990\nSummary: JavaPairRDD.top produces ClassNotFoundException\nDescription: For a class the extends a PairFlatMapFunction and implements the Comparator interface, the flatMap operation is able to load and execute this class, while the top operation produces a ClassNotFound exception. i.e.  public class TopTester { // declare an inner class as both a function and comparator. private static final class ScoreFunction extends PairFlatMapFunction<Tuple2<String,Tuple2<UUID,Double>> UUID, Double> implements Comparator<Tuple2<UUID, Double>> { public Iterable<Tuple2<UUID, Double>> call(Tuple2<String, Tuple2<UUID, Double>> match) throws Exception { return null; } public int compare(Tuple2<UUID, Double> o1, Tuple2<UUID, Double> o2) { return o1._2.compareTo(o2._2); } } public static void main(String[] argv) { // some setup ... // this operations passes, to prove that the env is setup correctly. JavaPairRDD<UUID,Double> scoreRdd = matchRdd.flatMap<new ScoreFunction()); // ClassNotFoundException ! scoreRdd.top(20, new ScoreFunction()); } }   Caused by: java.lang.ClassNotFoundException: controllers.KeywordIndex$ScoreFunction at java.net.URLClassLoader$1.run(URLClassLoader.java:366) ~[na:1.7.0_45] at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[na:1.7.0_45] at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_45] at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[na:1.7.0_45] at java.lang.ClassLoader.loadClass(ClassLoader.java:425) ~[na:1.7.0_45] at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ~[na:1.7.0_45]\n\nComments (6):\n1. Josh Rosen: Due to Spark's lazy evaluation, no distributed computations take place until you call the top() action (which triggers the execution of the flatMap() transformation). You can call scoreRDD.foreach{} to force evaluation of the flatMap() without calling top(), but I think you'd run into the same error. The error suggests that the Spark worker can't load your custom class, so my bet is that you need to add a JAR containing your code to your JavaSparkContext (see https://spark.incubator.apache.org/docs/0.8.0/scala-programming-guide.html#deploying-code-on-a-cluster for the relevant section of the Scala programming guide, which describes the same concept). There's an example of this in the JavaWordCount example, where we add the Spark examples JAR to SparkContext: https://github.com/apache/incubator-spark/blob/v0.8.0-incubating/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java#L39\n2. Albert Kwong: Hi Josh, The jar is loaded successfully into the JavaSparkContext. This is proven by the fact that the first call to flatMap is successful (which loads the same inner class that implements both PairFlatMapFunction and Comparator). Yes, I have been using count() / collect() to force execution. And yes, the code runs fine if I comment out the top() call and replace it with a collect() + local sort routine. // List<Tuple2<UUID, Double>> collect = resultRdd.top(20, new ScoreFunction()); List<Tuple2<UUID, Double>> collect = resultRdd.collect(); Collections.sort(collect, new Comparator<Tuple2<UUID, Double>>() { @Override public int compare(Tuple2<UUID, Double> o1, Tuple2<UUID, Double> o2) { return o2._2.compareTo(o1._2); // descending } });\n3. Josh Rosen: I wonder if the top() call is somehow trying to load that class using a different class loader that doesn't include JARs added through addJars.\n4. Albert Kwong: I am wondering whether this sc.clean(f) is doing the trick, which is kinda missing from def top.  /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */ def flatMap[U: ClassManifest](f: T => TraversableOnce[U]): RDD[U] = new FlatMappedRDD(this, sc.clean(f)) /** * Returns the top K elements from this RDD as defined by * the specified implicit Ordering[T]. * @param num the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def top(num: Int)(implicit ord: Ordering[T]): Array[T] = { mapPartitions { items => val queue = new BoundedPriorityQueue[T](num) queue ++= items Iterator.single(queue) }.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord.reverse) }   /** * Clean a closure to make it ready to serialized and send to tasks * (removes unreferenced variables in $outer's, updates REPL variables) */ private[spark] def clean[F <: AnyRef](f: F): F = { ClosureCleaner.clean(f) return f }\n5. Albert Kwong: Seems the problem may not be related to spark after all. I have modified the JavaWordCount example with a top() step and it runs well.\n6. Albert Kwong: Confirmed that the classpath problem is on the driver side, not the worker side. In particular, the driver side was running in a web framework (Play 2.1.1) that is interfering with spark. Moving away from Play to pure embedded Jetty solved the problem.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "e7f38020d254b0c28140c9dbf4753a62", "issue_key": "SPARK-991", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Report call sites of operators in Python", "description": "Similar to our call site reporting in Java, we can get a stack trace in Python with traceback.extract_stack. It would be nice to set that stack trace as the name on the corresponding RDDs so it can appear in the UI.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-12-10T19:16:28.000+0000", "updated": "2018-03-29T14:39:17.000+0000", "resolved": "2014-02-11T11:30:13.000+0000", "labels": ["Starter"], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "This was implemented by Tor Myklebust in https://github.com/apache/incubator-spark/pull/311", "created": "2014-02-11T11:30:13.586+0000"}, {"author": "Michael Mior", "body": "Maybe I'm missing something here, but I'm not seeing a Python stack trace captured when I look at the RDD call site information.", "created": "2018-03-29T14:39:17.853+0000"}], "num_comments": 2, "text": "Issue: SPARK-991\nSummary: Report call sites of operators in Python\nDescription: Similar to our call site reporting in Java, we can get a stack trace in Python with traceback.extract_stack. It would be nice to set that stack trace as the name on the corresponding RDDs so it can appear in the UI.\n\nComments (2):\n1. Josh Rosen: This was implemented by Tor Myklebust in https://github.com/apache/incubator-spark/pull/311\n2. Michael Mior: Maybe I'm missing something here, but I'm not seeing a Python stack trace captured when I look at the RDD call site information.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "68942a6568fa38c32681c0e2687af214", "issue_key": "SPARK-992", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Implement toString for Python and Java RDDs", "description": "They should return the toString() of the underlying Scala RDD, which includes the RDD and the place it was created.", "reporter": "Matei Alexandru Zaharia", "assignee": "Nicholas Pentreath", "created": "2013-12-10T19:20:00.000+0000", "updated": "2013-12-19T10:39:59.000+0000", "resolved": "2013-12-19T10:39:59.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Nicholas Pentreath", "body": "See: https://github.com/apache/incubator-spark/pull/278", "created": "2013-12-19T04:41:36.730+0000"}], "num_comments": 1, "text": "Issue: SPARK-992\nSummary: Implement toString for Python and Java RDDs\nDescription: They should return the toString() of the underlying Scala RDD, which includes the RDD and the place it was created.\n\nComments (1):\n1. Nicholas Pentreath: See: https://github.com/apache/incubator-spark/pull/278", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "bae0b203d14aeced5e3051942e1e8c37", "issue_key": "SPARK-993", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Don't reuse Writable objects in HadoopRDDs by default", "description": "Right now we reuse them as an optimization, which leads to weird results when you call collect() on a file with distinct items. We should instead make that behavior optional through a flag.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-12-10T20:25:22.000+0000", "updated": "2014-11-06T17:40:12.000+0000", "resolved": "2014-11-06T17:39:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Arun Ramakrishnan", "body": "How does one reproduce this issue ? I tried a few things on the spark shell locally  import java.io.File import com.google.common.io.Files import org.apache.hadoop.io._ val tempDir = Files.createTempDir() val outputDir = new File(tempDir, \"output\").getAbsolutePath val num = 100 val nums = sc.makeRDD(1 to num).map(x => (\"a\" * x, x)) nums.saveAsSequenceFile(outputDir) val output = sc.sequenceFile[String,Int](outputDir) assert(output.collect().toSet.size == num) val t = sc.sequenceFile(outputDir, classOf[Text], classOf[IntWritable]) assert( t.map { case (k,v) => (k.toString, v.get) }.collect().toSet.size == num )  But, asserts seem to be fine.", "created": "2014-04-25T07:05:14.731+0000"}, {"author": "Matei Alexandru Zaharia", "body": "We investigated this for 1.0 but found that many InputFormats behave wrongly if you try to clone the object, so we won't fix it.", "created": "2014-11-06T17:39:26.926+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Arun, you'd see this issue if you do collect() or take() and then println. The problem is that the same Text object (for example) is referenced for all records in the dataset. The counts will be okay.", "created": "2014-11-06T17:40:12.203+0000"}], "num_comments": 3, "text": "Issue: SPARK-993\nSummary: Don't reuse Writable objects in HadoopRDDs by default\nDescription: Right now we reuse them as an optimization, which leads to weird results when you call collect() on a file with distinct items. We should instead make that behavior optional through a flag.\n\nComments (3):\n1. Arun Ramakrishnan: How does one reproduce this issue ? I tried a few things on the spark shell locally  import java.io.File import com.google.common.io.Files import org.apache.hadoop.io._ val tempDir = Files.createTempDir() val outputDir = new File(tempDir, \"output\").getAbsolutePath val num = 100 val nums = sc.makeRDD(1 to num).map(x => (\"a\" * x, x)) nums.saveAsSequenceFile(outputDir) val output = sc.sequenceFile[String,Int](outputDir) assert(output.collect().toSet.size == num) val t = sc.sequenceFile(outputDir, classOf[Text], classOf[IntWritable]) assert( t.map { case (k,v) => (k.toString, v.get) }.collect().toSet.size == num )  But, asserts seem to be fine.\n2. Matei Alexandru Zaharia: We investigated this for 1.0 but found that many InputFormats behave wrongly if you try to clone the object, so we won't fix it.\n3. Matei Alexandru Zaharia: Arun, you'd see this issue if you do collect() or take() and then println. The problem is that the same Text object (for example) is referenced for all records in the dataset. The counts will be okay.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "561882c7e0bc7f250d517c06313919e7", "issue_key": "SPARK-994", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Upgrade Spark to Scala 2.10", "description": "Tracking for the ongoing work upgrading Spark to Scala 2.10.", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "created": "2013-12-11T09:25:07.000+0000", "updated": "2020-02-07T17:26:38.000+0000", "resolved": "2013-12-14T01:30:31.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-994\nSummary: Upgrade Spark to Scala 2.10\nDescription: Tracking for the ongoing work upgrading Spark to Scala 2.10.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "c470816d30653b82e18ac7c8d058c81c", "issue_key": "SPARK-995", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Improve Support for YARN 2.2", "description": "Due to a nasty conflict relating to the respective protocol buffer libraries in YARN 2.2 and Akka, we had to build a workaround in 0.8.1 for people wanting to build with YARN 2.2 involving different build profiles in Maven and SBT. The problem stems from the fact that Hadoop 2.2+ versions pull in a dependency on protobuf 2.5. The akka version (2.0.5) we are currently using depends on protobuf 2.4.1 and they are not binary compatible. This is difficult to solve with classloader isolation because some parts of the code use both akka and the Hadoop libraries simultaneously. The solution in the 0.8 branch was to publish our own port of akka 2.0.5 which depends on protobuf 2.5. Then we added a special case in the build where it relies on this special akka version. For 0.9 we are pursuing a different solution. We are going to publish or own shaded version of protobuf 2.4 and a version of akka 2.2.4 which relies on that shaded protobof. This will completely isolate the protobuf used in akka vs in other pats of the code. There will no longer be a special case in the build... every build will rely on this special version of akka.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-12-11T09:29:04.000+0000", "updated": "2013-12-30T16:17:53.000+0000", "resolved": "2013-12-30T16:17:53.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Raymond Valencia", "body": "Hi Patrick You mean , we will change protobuf 2.4 to some other name, and a home made akka 2.2.4 to depends on this renamed protobuf thus it will not conflict upon assemble and can coexist on runtime with other part who use protobuf 2.5, is that right? Great. However, I don't understand how the upgrade of akka can help in the future to deprecate this solution. when akka upgrades protobuf version in formal 2.3 release, if without this isolation solution, then, say we use the public 2.3 release, it work with yarn 2.2 and protobuf 2.5, We still need another version to work with yarn 2.0 for protobuf2.4. So as far as what I understand, either depends on two version akka or a homemade akka with shaded protobuf to isolate from the other part. we will always need one of these two solution?", "created": "2013-12-12T21:35:57.947+0000"}, {"author": "Patrick McFadin", "body": "Hey Raymond, Ah you are right - that last line was a mistake. This doesn't totally get fixed by the newer akka because then we'll have the \"opposite\" problem for older hadoop versions. I think you understand correctly the proposal. Ya so we are going to publish protobuf 2.4 under a new package name and make a home-made akka that uses our specially named protobuf library. Basically this means multiple versions of protobuf will exist in the JVM under different package names. This is a pretty hacky solution from the compile time perspective. But it makes the runtime situation and the user's experience much simpler.", "created": "2013-12-12T21:43:25.426+0000"}, {"author": "Raymond Valencia", "body": "Thanks Patrick. I am fully understand now. So, it seems to that this solution do not impact our yarn src dir reorganize works. Can we not remove the yarn 2.2 support in source code and just disable it from build scripts or leave it as it is? So that I can continue the src dir reorganize works upon 2.10 merge. And then for those who need to run upon yarn 2.2. they will just need a very small patch to change several line of code to meet akka api change, and enable the build. it works. ;)", "created": "2013-12-12T22:17:37.732+0000"}, {"author": "Patrick McFadin", "body": "Hey Raymond, If you look at the current scala-2.10 branch it doesn't delete the YARN stuff it just disables it for 2.2 in the build. So we can merge your re-factoring change in on top of that after it goes in. Then later we can completely remove new-yarn. - Patrick", "created": "2013-12-12T22:24:54.395+0000"}, {"author": "Raymond Valencia", "body": "Yep, I saw that. Just to double confirm we won't remove it further. Cool, That sounds a good plan. This multiple dimension of code developing on yarn 2.0/2.2; scala 2.9/2.10 and protobuf this and that version situation really need a clean end. ;)", "created": "2013-12-12T22:31:47.440+0000"}, {"author": "Patrick McFadin", "body": "This was fixed using shaded protobuf library.", "created": "2013-12-30T16:17:53.961+0000"}], "num_comments": 6, "text": "Issue: SPARK-995\nSummary: Improve Support for YARN 2.2\nDescription: Due to a nasty conflict relating to the respective protocol buffer libraries in YARN 2.2 and Akka, we had to build a workaround in 0.8.1 for people wanting to build with YARN 2.2 involving different build profiles in Maven and SBT. The problem stems from the fact that Hadoop 2.2+ versions pull in a dependency on protobuf 2.5. The akka version (2.0.5) we are currently using depends on protobuf 2.4.1 and they are not binary compatible. This is difficult to solve with classloader isolation because some parts of the code use both akka and the Hadoop libraries simultaneously. The solution in the 0.8 branch was to publish our own port of akka 2.0.5 which depends on protobuf 2.5. Then we added a special case in the build where it relies on this special akka version. For 0.9 we are pursuing a different solution. We are going to publish or own shaded version of protobuf 2.4 and a version of akka 2.2.4 which relies on that shaded protobof. This will completely isolate the protobuf used in akka vs in other pats of the code. There will no longer be a special case in the build... every build will rely on this special version of akka.\n\nComments (6):\n1. Raymond Valencia: Hi Patrick You mean , we will change protobuf 2.4 to some other name, and a home made akka 2.2.4 to depends on this renamed protobuf thus it will not conflict upon assemble and can coexist on runtime with other part who use protobuf 2.5, is that right? Great. However, I don't understand how the upgrade of akka can help in the future to deprecate this solution. when akka upgrades protobuf version in formal 2.3 release, if without this isolation solution, then, say we use the public 2.3 release, it work with yarn 2.2 and protobuf 2.5, We still need another version to work with yarn 2.0 for protobuf2.4. So as far as what I understand, either depends on two version akka or a homemade akka with shaded protobuf to isolate from the other part. we will always need one of these two solution?\n2. Patrick McFadin: Hey Raymond, Ah you are right - that last line was a mistake. This doesn't totally get fixed by the newer akka because then we'll have the \"opposite\" problem for older hadoop versions. I think you understand correctly the proposal. Ya so we are going to publish protobuf 2.4 under a new package name and make a home-made akka that uses our specially named protobuf library. Basically this means multiple versions of protobuf will exist in the JVM under different package names. This is a pretty hacky solution from the compile time perspective. But it makes the runtime situation and the user's experience much simpler.\n3. Raymond Valencia: Thanks Patrick. I am fully understand now. So, it seems to that this solution do not impact our yarn src dir reorganize works. Can we not remove the yarn 2.2 support in source code and just disable it from build scripts or leave it as it is? So that I can continue the src dir reorganize works upon 2.10 merge. And then for those who need to run upon yarn 2.2. they will just need a very small patch to change several line of code to meet akka api change, and enable the build. it works. ;)\n4. Patrick McFadin: Hey Raymond, If you look at the current scala-2.10 branch it doesn't delete the YARN stuff it just disables it for 2.2 in the build. So we can merge your re-factoring change in on top of that after it goes in. Then later we can completely remove new-yarn. - Patrick\n5. Raymond Valencia: Yep, I saw that. Just to double confirm we won't remove it further. Cool, That sounds a good plan. This multiple dimension of code developing on yarn 2.0/2.2; scala 2.9/2.10 and protobuf this and that version situation really need a clean end. ;)\n6. Patrick McFadin: This was fixed using shaded protobuf library.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "be9321d965522dec35ce938278630cc2", "issue_key": "SPARK-996", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Delete Underlying Blocks When Cleaning Shuffle Meta-Data", "description": "The proposal here is very basic. When we delete shuffle blocks during the existing cleaning process, we should clear the shuffle files as well.", "reporter": "Patrick McFadin", "assignee": "Aaron Davidson", "created": "2013-12-11T09:30:44.000+0000", "updated": "2014-01-05T23:16:56.000+0000", "resolved": "2014-01-05T23:16:56.000+0000", "labels": [], "components": [], "comments": [{"author": "Mark Hamstra", "body": "Where and when are you proposing to do this cleanup? My concern is with regard to {{DAGScheduler#newOrUsedStage}}, which wants to find a {{ShuffleDependency}} for a {{Stage}} computed in a job that already finished and whose {{DAGScheduler}} data structures have already been cleaned up -- e.g. an action was run on an RDD, and only after that job completed and was cleaned up after, the driver decided to launch a job joining that RDD with another. If the {{ShuffleDependency}} is no longer present, then it will need to be re-computed. But maybe you're not talking about {{MapOutputTracker}} cleanup...", "created": "2013-12-11T11:00:49.265+0000"}, {"author": "Patrick McFadin", "body": "Hey Mark the proposal here is very modest. It's not to implement proper reference tracking or clean-up of shuffles. It's just that if people have enabled shuffle meta data clean-up, in `ShuffleBlockManager` when it cleans up meta-data for a particular shuffle, it might as well delete the files also. At this point the blocks relating to that shuffle are inaccessible anyways so you might as well clean them from disk.", "created": "2013-12-11T11:46:29.121+0000"}, {"author": "Mark Hamstra", "body": "Got it. Sounds like that shouldn't cause the {{DAGScheduler}} to do extra re-computation.", "created": "2013-12-11T11:50:53.839+0000"}, {"author": "Reynold Xin", "body": "I propose that we organize the directory structure slightly differently to better link the files with the metadata - so we don't need to traverse a large number of inodes to get to the right shuffle files. (If we are already doing that - great!)", "created": "2013-12-11T11:51:42.200+0000"}], "num_comments": 4, "text": "Issue: SPARK-996\nSummary: Delete Underlying Blocks When Cleaning Shuffle Meta-Data\nDescription: The proposal here is very basic. When we delete shuffle blocks during the existing cleaning process, we should clear the shuffle files as well.\n\nComments (4):\n1. Mark Hamstra: Where and when are you proposing to do this cleanup? My concern is with regard to {{DAGScheduler#newOrUsedStage}}, which wants to find a {{ShuffleDependency}} for a {{Stage}} computed in a job that already finished and whose {{DAGScheduler}} data structures have already been cleaned up -- e.g. an action was run on an RDD, and only after that job completed and was cleaned up after, the driver decided to launch a job joining that RDD with another. If the {{ShuffleDependency}} is no longer present, then it will need to be re-computed. But maybe you're not talking about {{MapOutputTracker}} cleanup...\n2. Patrick McFadin: Hey Mark the proposal here is very modest. It's not to implement proper reference tracking or clean-up of shuffles. It's just that if people have enabled shuffle meta data clean-up, in `ShuffleBlockManager` when it cleans up meta-data for a particular shuffle, it might as well delete the files also. At this point the blocks relating to that shuffle are inaccessible anyways so you might as well clean them from disk.\n3. Mark Hamstra: Got it. Sounds like that shouldn't cause the {{DAGScheduler}} to do extra re-computation.\n4. Reynold Xin: I propose that we organize the directory structure slightly differently to better link the files with the metadata - so we don't need to traverse a large number of inodes to get to the right shuffle files. (If we are already doing that - great!)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "cbb410a46051bb90ea7d0b8aebfcda33", "issue_key": "SPARK-997", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Write integration tests for HDFS-based recovery", "description": "Task here is to write thorough tests for master recovery when using HDFS-based input.", "reporter": "Patrick McFadin", "assignee": "Tathagata Das", "created": "2013-12-11T09:33:28.000+0000", "updated": "2014-01-24T14:00:52.000+0000", "resolved": "2014-01-24T14:00:52.000+0000", "labels": [], "components": ["DStreams"], "comments": [{"author": "Tathagata Das", "body": "This has been done and will be added to spark-perf shortly", "created": "2014-01-24T14:00:52.833+0000"}], "num_comments": 1, "text": "Issue: SPARK-997\nSummary: Write integration tests for HDFS-based recovery\nDescription: Task here is to write thorough tests for master recovery when using HDFS-based input.\n\nComments (1):\n1. Tathagata Das: This has been done and will be added to spark-perf shortly", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "862fb7f4b651dff0fb3438c48b0f1a28", "issue_key": "SPARK-998", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support Launching Driver Inside of Standalone Mode", "description": "We should support launching a driver inside of the cluster when using Standalone mode. We should also support a mode which restarts the driver program on failures. Users would use a similar client to the YARN client to submit their jobs.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-12-11T09:35:13.000+0000", "updated": "2014-01-17T05:07:47.000+0000", "resolved": "2014-01-15T22:43:48.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Nan Zhu", "body": "How about making worker create two kinds of executor: TaskExecutor (original Executor) and DriverExecutor (the within-cluster driver)?", "created": "2014-01-15T20:37:12.301+0000"}, {"author": "Patrick McFadin", "body": "This was actually fixed in 0.9.0.", "created": "2014-01-15T22:44:04.148+0000"}, {"author": "Nan Zhu", "body": "oh, great, I checked the code, but I think the current implementation does not handle the recovery of the dead driver, right? I'm interested in adding this feature", "created": "2014-01-17T05:07:47.261+0000"}], "num_comments": 3, "text": "Issue: SPARK-998\nSummary: Support Launching Driver Inside of Standalone Mode\nDescription: We should support launching a driver inside of the cluster when using Standalone mode. We should also support a mode which restarts the driver program on failures. Users would use a similar client to the YARN client to submit their jobs.\n\nComments (3):\n1. Nan Zhu: How about making worker create two kinds of executor: TaskExecutor (original Executor) and DriverExecutor (the within-cluster driver)?\n2. Patrick McFadin: This was actually fixed in 0.9.0.\n3. Nan Zhu: oh, great, I checked the code, but I think the current implementation does not handle the recovery of the dead driver, right? I'm interested in adding this feature", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.102308"}}
{"id": "e88e53226d0d7af1a6b8c886fdc81278", "issue_key": "SPARK-999", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Report More Instrumentation for Task Execution Time in UI", "description": "We should report finer-grained information about task execution time inside of the Spark UI. Here is a proposal of exactly what we should report: Task execution goes through a few stages on the Executor. 1. Deserializing the task 2. Executing the task. This pipelines a few things: --> Reading shuffle input --> Running whatever function on the RDD --> Writing shuffle output 3. Serializing the result I'd propose we should report the following five timing metrics. Man of these are already tracked in TaskMetrics. - Time spent deserializing the task on the executor (executorDeserializeTime) - Total execution time for the task (executorRunTime) -- Time spent blocking on shuffle reads during the task (fetchWaitTime) -- Time spent blocking on shuffle writes during the task (shuffleWriteTime) - Time spent serializing the result (not currently tracked) Reporting all of these in the Stage UI table would be great. Bonus points if you can find some better way to visualize them. Note that the time spent serializing the result is currently not tracked. We should figure out if we can do this in a simple way - it seems like you could modify TaskResult to contain an already serialized buffer instead of the result itself. Then you could first serialize that result, update the TaskMetrics and then serialize them (we wouldn't track the time to serialize the metrics themselves). If this is too much performance overhead we could also write a custom serializer for the broader result struct (containing the accumulators, metrics, and result). One other missing thing here is the ability to track various metrics if the task is reading or writing from HDFS or doing some other expensive thing within it's own execution. It would be nice to add support for counters and such in there, but we can keep that outside of the scope of this JIRA.", "reporter": "Patrick Wendell", "assignee": null, "created": "2013-12-12T12:07:53.000+0000", "updated": "2014-03-30T04:15:06.000+0000", "resolved": "2014-01-05T23:17:02.000+0000", "labels": [], "components": [], "comments": [{"author": "xiajunluan", "body": "Hi Patrick I would like to finish this improvement, could you assign this one to me?", "created": "2013-12-13T21:28:28.265+0000"}, {"author": "Patrick McFadin", "body": "Hey - someone else actually wanted to do this and asked me to write up an outline so they could get started. Let me double check with them though, if they aren't still planning to do this I can assign it to you.", "created": "2013-12-13T21:43:42.362+0000"}], "num_comments": 2, "text": "Issue: SPARK-999\nSummary: Report More Instrumentation for Task Execution Time in UI\nDescription: We should report finer-grained information about task execution time inside of the Spark UI. Here is a proposal of exactly what we should report: Task execution goes through a few stages on the Executor. 1. Deserializing the task 2. Executing the task. This pipelines a few things: --> Reading shuffle input --> Running whatever function on the RDD --> Writing shuffle output 3. Serializing the result I'd propose we should report the following five timing metrics. Man of these are already tracked in TaskMetrics. - Time spent deserializing the task on the executor (executorDeserializeTime) - Total execution time for the task (executorRunTime) -- Time spent blocking on shuffle reads during the task (fetchWaitTime) -- Time spent blocking on shuffle writes during the task (shuffleWriteTime) - Time spent serializing the result (not currently tracked) Reporting all of these in the Stage UI table would be great. Bonus points if you can find some better way to visualize them. Note that the time spent serializing the result is currently not tracked. We should figure out if we can do this in a simple way - it seems like you could modify TaskResult to contain an already serialized buffer instead of the result itself. Then you could first serialize that result, update the TaskMetrics and then serialize them (we wouldn't track the time to serialize the metrics themselves). If this is too much performance overhead we could also write a custom serializer for the broader result struct (containing the accumulators, metrics, and result). One other missing thing here is the ability to track various metrics if the task is reading or writing from HDFS or doing some other expensive thing within it's own execution. It would be nice to add support for counters and such in there, but we can keep that outside of the scope of this JIRA.\n\nComments (2):\n1. xiajunluan: Hi Patrick I would like to finish this improvement, could you assign this one to me?\n2. Patrick McFadin: Hey - someone else actually wanted to do this and asked me to write up an outline so they could get started. Let me double check with them though, if they aren't still planning to do this I can assign it to you.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.104052"}}
{"id": "114b4ce044c0a6bc9ab7e1df4850f20c", "issue_key": "SPARK-1000", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Crash when running SparkPi example with local-cluster", "description": "when I run SparkPi with local-cluster[2,2,512], it will throw following exception at the end of job. WARNING: An exception was thrown by an exception handler. java.util.concurrent.RejectedExecutionException at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.start(AbstractNioWorker.java:184) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:330) at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:313) at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35) at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34) at org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:504) at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:47) at org.jboss.netty.channel.Channels.fireChannelOpen(Channels.java:170) at org.jboss.netty.channel.socket.nio.NioClientSocketChannel.<init>(NioClientSocketChannel.java:79) at org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.newChannel(NioClientSocketChannelFactory.java:176) at org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.newChannel(NioClientSocketChannelFactory.java:82) at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:213) at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:183) at akka.remote.netty.ActiveRemoteClient$$anonfun$connect$1.apply$mcV$sp(Client.scala:173) at akka.util.Switch.liftedTree1$1(LockUtil.scala:33) at akka.util.Switch.transcend(LockUtil.scala:32) at akka.util.Switch.switchOn(LockUtil.scala:55) at akka.remote.netty.ActiveRemoteClient.connect(Client.scala:158) at akka.remote.netty.NettyRemoteTransport.send(NettyRemoteSupport.scala:153) at akka.remote.RemoteActorRef.$bang(RemoteActorRefProvider.scala:247) at akka.actor.LocalDeathWatch$$anonfun$publish$1.apply(ActorRefProvider.scala:559) at akka.actor.LocalDeathWatch$$anonfun$publish$1.apply(ActorRefProvider.scala:559) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.immutable.VectorIterator.foreach(Vector.scala:648) at scala.collection.IterableLike$class.foreach(IterableLike.scala:73) at scala.collection.immutable.Vector.foreach(Vector.scala:63) at akka.actor.LocalDeathWatch.publish(ActorRefProvider.scala:559) at akka.remote.RemoteDeathWatch.publish(RemoteActorRefProvider.scala:280) at akka.remote.RemoteDeathWatch.publish(RemoteActorRefProvider.scala:262) at akka.actor.ActorCell.doTerminate(ActorCell.scala:701) at akka.actor.ActorCell.handleChildTerminated(ActorCell.scala:747) at akka.actor.ActorCell.systemInvoke(ActorCell.scala:608) at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:209) at akka.dispatch.Mailbox.run(Mailbox.scala:178) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)", "reporter": "xiajunluan", "assignee": "Andrew Or", "created": "2013-12-16T05:19:02.000+0000", "updated": "2014-11-06T17:41:13.000+0000", "resolved": "2014-11-06T17:41:13.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Henry Saputra", "body": "I am not able to repro this with master branch. When you see in http://localhost:8080 do you see if worker has ALIVE status?", "created": "2014-06-25T18:50:30.462+0000"}], "num_comments": 1, "text": "Issue: SPARK-1000\nSummary: Crash when running SparkPi example with local-cluster\nDescription: when I run SparkPi with local-cluster[2,2,512], it will throw following exception at the end of job. WARNING: An exception was thrown by an exception handler. java.util.concurrent.RejectedExecutionException at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.start(AbstractNioWorker.java:184) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:330) at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35) at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:313) at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35) at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34) at org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:504) at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:47) at org.jboss.netty.channel.Channels.fireChannelOpen(Channels.java:170) at org.jboss.netty.channel.socket.nio.NioClientSocketChannel.<init>(NioClientSocketChannel.java:79) at org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.newChannel(NioClientSocketChannelFactory.java:176) at org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory.newChannel(NioClientSocketChannelFactory.java:82) at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:213) at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:183) at akka.remote.netty.ActiveRemoteClient$$anonfun$connect$1.apply$mcV$sp(Client.scala:173) at akka.util.Switch.liftedTree1$1(LockUtil.scala:33) at akka.util.Switch.transcend(LockUtil.scala:32) at akka.util.Switch.switchOn(LockUtil.scala:55) at akka.remote.netty.ActiveRemoteClient.connect(Client.scala:158) at akka.remote.netty.NettyRemoteTransport.send(NettyRemoteSupport.scala:153) at akka.remote.RemoteActorRef.$bang(RemoteActorRefProvider.scala:247) at akka.actor.LocalDeathWatch$$anonfun$publish$1.apply(ActorRefProvider.scala:559) at akka.actor.LocalDeathWatch$$anonfun$publish$1.apply(ActorRefProvider.scala:559) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.immutable.VectorIterator.foreach(Vector.scala:648) at scala.collection.IterableLike$class.foreach(IterableLike.scala:73) at scala.collection.immutable.Vector.foreach(Vector.scala:63) at akka.actor.LocalDeathWatch.publish(ActorRefProvider.scala:559) at akka.remote.RemoteDeathWatch.publish(RemoteActorRefProvider.scala:280) at akka.remote.RemoteDeathWatch.publish(RemoteActorRefProvider.scala:262) at akka.actor.ActorCell.doTerminate(ActorCell.scala:701) at akka.actor.ActorCell.handleChildTerminated(ActorCell.scala:747) at akka.actor.ActorCell.systemInvoke(ActorCell.scala:608) at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:209) at akka.dispatch.Mailbox.run(Mailbox.scala:178) at akka.dispatch.ForkJoinExecutorConfigurator$MailboxExecutionTask.exec(AbstractDispatcher.scala:516) at akka.jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:259) at akka.jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:975) at akka.jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479) at akka.jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n\nComments (1):\n1. Henry Saputra: I am not able to repro this with master branch. When you see in http://localhost:8080 do you see if worker has ALIVE status?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.104052"}}
{"id": "e9bdf76506980629819837f75ef3fee0", "issue_key": "SPARK-1001", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Memory leak when reading sequence file and then sorting", "description": "Spark appears to build up a backlog of unreachable byte arrays when an RDD is constructed from a sequence file, and then that RDD is sorted. I have a class that wraps a Java ArrayList, that can be serialized and written to a Hadoop SequenceFile (I.e. Implements the Writable interface). Let's call it WritableDataRow. It can take a Java List as its argument to wrap around, and also has a copy constructor. Setup: 10 slaves, launched via EC2, 65.9GB RAM each, dataset is 100GB of text, 120GB when in sequence file format (not using compression to compact the bytes). CDH4.2.0-backed hadoop cluster. First, building the RDD from a CSV and then sorting on index 1 works fine:  scala> import scala.collection.JavaConversions._ // Other imports here as well import scala.collection.JavaConversions._ scala> val rddAsTextFile = sc.textFile(\"s3n://some-bucket/events-*.csv\") rddAsTextFile: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at <console>:14 scala> val rddAsWritableDataRows = rddAsTextFile.map(x => new WritableDataRow(x.split(\"\\\\|\").toList)) rddAsWritableDataRows: org.apache.spark.rdd.RDD[com.palantir.finance.datatable.server.spark.WritableDataRow] = MappedRDD[2] at map at <console>:19 scala> val rddAsKeyedWritableDataRows = rddAsWritableDataRows.map(x => (x.getContents().get(1).toString(), x)); rddAsKeyedWritableDataRows: org.apache.spark.rdd.RDD[(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = MappedRDD[4] at map at <console>:22 scala> val orderedFunct = new org.apache.spark.rdd.OrderedRDDFunctions[String, WritableDataRow, (String, WritableDataRow)](rddAsKeyedWritableDataRows) orderedFunct: org.apache.spark.rdd.OrderedRDDFunctions[String,com.palantir.finance.datatable.server.spark.WritableDataRow,(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = org.apache.spark.rdd.OrderedRDDFunctions@587acb54 scala> orderedFunct.sortByKey(true).count(); // Actually triggers the computation, as stated in a different e-mail thread res0: org.apache.spark.rdd.RDD[(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = MapPartitionsRDD[8] at sortByKey at <console>:27  The above works without too many surprises. I then save it as a Sequence File (using JavaPairRDD as a way to more easily call saveAsHadoopFile(), and this is how it's done in our Java-based application):  scala> val pairRDD = new JavaPairRDD(rddAsWritableDataRows.map(x => (NullWritable.get(), x))); pairRDD: org.apache.spark.api.java.JavaPairRDD[org.apache.hadoop.io.NullWritable,com.palantir.finance.datatable.server.spark.WritableDataRow] = org.apache.spark.api.java.JavaPairRDD@8d2e9d9 scala> pairRDD.saveAsHadoopFile(\"hdfs://<hdfs-master-url>:9010/blah\", classOf[NullWritable], classOf[WritableDataRow], classOf[org.apache.hadoop.mapred.SequenceFileOutputFormat[NullWritable, WritableDataRow]]); … 2013-12-11 20:09:14,444 [main] INFO org.apache.spark.SparkContext - Job finished: saveAsHadoopFile at <console>:26, took 1052.116712748 s  And now I want to get the RDD from the sequence file and sort THAT, and this is when I monitor Ganglia and \"ps aux\" and notice the memory usage climbing ridiculously:  scala> val rddAsSequenceFile = sc.sequenceFile(\"hdfs://<hdfs-master-url>:9010/blah\", classOf[NullWritable], classOf[WritableDataRow]).map(x => new WritableDataRow(x._2)); // Invokes copy constructor to get around re-use of writable objects rddAsSequenceFile: org.apache.spark.rdd.RDD[com.palantir.finance.datatable.server.spark.WritableDataRow] = MappedRDD[19] at map at <console>:19 scala> val orderedFunct = new org.apache.spark.rdd.OrderedRDDFunctions[String, WritableDataRow, (String, WritableDataRow)](rddAsSequenceFile.map(x => (x.getContents().get(1).toString(), x))) orderedFunct: org.apache.spark.rdd.OrderedRDDFunctions[String,com.palantir.finance.datatable.server.spark.WritableDataRow,(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = org.apache.spark.rdd.OrderedRDDFunctions@6262a9a6 scala>orderedFunct.sortByKey().count();  (On the necessity to copy writables from hadoop RDDs, see: https://mail-archives.apache.org/mod_mbox/spark-user/201308.mbox/%3CCAF_KkPzrq4OTyQVwcOC6pLAz9X9_SFo33u4ySatki5PTqoYEDA@mail.gmail.com%3E ) I got a memory dump from one worker node but can't share it. I've attached a screenshot from YourKit. At the point where around 5GB of RAM is being used on the worker, 3GB of unreachable byte arrays have accumulated. Furthermore, they're all exactly the same size, and seem to be the same size as most of the byte arrays that are strong reachable. The strong reachable byte arrays are referenced from output streams in the block output writers. Let me know if you require any more information. Thanks.", "reporter": "Matthew Cheah", "assignee": null, "created": "2013-12-16T13:27:17.000+0000", "updated": "2015-04-01T11:09:17.000+0000", "resolved": "2015-04-01T11:09:17.000+0000", "labels": ["Hadoop", "Memory"], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Matthew Cheah", "body": "Yikes, I wasn't aware that square brackets would linkify all the things. If someone wants to go in and hack the description to change that, that would be fantastic.", "created": "2013-12-16T13:34:47.131+0000"}, {"author": "OuyangJin", "body": "Hi @Matthew Cheah. I wonder the moment you got that memory dump on one worker node which is analyzed by Yourkit in the pictures above? Is it at job runtime or after all job is finished? If you are using HPROF dump file, can you provide which jvm process in that worker node you are dumping, the executor process or worker process if you are using standalone?(likewise, the pid you provide to jmap is from which jvm process). Becuase if it's at runtime, unreachable byte arrays will be deleted by GC. Otherwise ,if memory leak really happens , we should search it amony strong reachable objects , because they will not be deleted by GC. So I wonder it's the unreachable byte arrays you are worried about for memory leak or the reachable ones? Thanks!", "created": "2014-02-01T02:30:06.888+0000"}, {"author": "Matthew Cheah", "body": "It's happening as the job is running. At job runtime, the memory keeps continuously climbing. It appears that the unreachable byte arrays are what keeps increasing in size, so that's what I believed was causing the leak. It IS possible for unreachable objects to NOT be cleaned up by GC for various reasons. I took a dump of the executor process, I believe. (I've since moved on from the project that was dealing with this issue though. Went back to school. It's been a little while since I looked at this, so I might not remember the specifics.)", "created": "2014-02-01T16:13:09.305+0000"}, {"author": "Nicholas Chammas", "body": "It's probably tough given how long ago this was reported, but can anyone confirm whether this is still an issue on the latest release (1.2.1)?", "created": "2015-02-23T00:23:29.991+0000"}], "num_comments": 4, "text": "Issue: SPARK-1001\nSummary: Memory leak when reading sequence file and then sorting\nDescription: Spark appears to build up a backlog of unreachable byte arrays when an RDD is constructed from a sequence file, and then that RDD is sorted. I have a class that wraps a Java ArrayList, that can be serialized and written to a Hadoop SequenceFile (I.e. Implements the Writable interface). Let's call it WritableDataRow. It can take a Java List as its argument to wrap around, and also has a copy constructor. Setup: 10 slaves, launched via EC2, 65.9GB RAM each, dataset is 100GB of text, 120GB when in sequence file format (not using compression to compact the bytes). CDH4.2.0-backed hadoop cluster. First, building the RDD from a CSV and then sorting on index 1 works fine:  scala> import scala.collection.JavaConversions._ // Other imports here as well import scala.collection.JavaConversions._ scala> val rddAsTextFile = sc.textFile(\"s3n://some-bucket/events-*.csv\") rddAsTextFile: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at <console>:14 scala> val rddAsWritableDataRows = rddAsTextFile.map(x => new WritableDataRow(x.split(\"\\\\|\").toList)) rddAsWritableDataRows: org.apache.spark.rdd.RDD[com.palantir.finance.datatable.server.spark.WritableDataRow] = MappedRDD[2] at map at <console>:19 scala> val rddAsKeyedWritableDataRows = rddAsWritableDataRows.map(x => (x.getContents().get(1).toString(), x)); rddAsKeyedWritableDataRows: org.apache.spark.rdd.RDD[(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = MappedRDD[4] at map at <console>:22 scala> val orderedFunct = new org.apache.spark.rdd.OrderedRDDFunctions[String, WritableDataRow, (String, WritableDataRow)](rddAsKeyedWritableDataRows) orderedFunct: org.apache.spark.rdd.OrderedRDDFunctions[String,com.palantir.finance.datatable.server.spark.WritableDataRow,(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = org.apache.spark.rdd.OrderedRDDFunctions@587acb54 scala> orderedFunct.sortByKey(true).count(); // Actually triggers the computation, as stated in a different e-mail thread res0: org.apache.spark.rdd.RDD[(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = MapPartitionsRDD[8] at sortByKey at <console>:27  The above works without too many surprises. I then save it as a Sequence File (using JavaPairRDD as a way to more easily call saveAsHadoopFile(), and this is how it's done in our Java-based application):  scala> val pairRDD = new JavaPairRDD(rddAsWritableDataRows.map(x => (NullWritable.get(), x))); pairRDD: org.apache.spark.api.java.JavaPairRDD[org.apache.hadoop.io.NullWritable,com.palantir.finance.datatable.server.spark.WritableDataRow] = org.apache.spark.api.java.JavaPairRDD@8d2e9d9 scala> pairRDD.saveAsHadoopFile(\"hdfs://<hdfs-master-url>:9010/blah\", classOf[NullWritable], classOf[WritableDataRow], classOf[org.apache.hadoop.mapred.SequenceFileOutputFormat[NullWritable, WritableDataRow]]); … 2013-12-11 20:09:14,444 [main] INFO org.apache.spark.SparkContext - Job finished: saveAsHadoopFile at <console>:26, took 1052.116712748 s  And now I want to get the RDD from the sequence file and sort THAT, and this is when I monitor Ganglia and \"ps aux\" and notice the memory usage climbing ridiculously:  scala> val rddAsSequenceFile = sc.sequenceFile(\"hdfs://<hdfs-master-url>:9010/blah\", classOf[NullWritable], classOf[WritableDataRow]).map(x => new WritableDataRow(x._2)); // Invokes copy constructor to get around re-use of writable objects rddAsSequenceFile: org.apache.spark.rdd.RDD[com.palantir.finance.datatable.server.spark.WritableDataRow] = MappedRDD[19] at map at <console>:19 scala> val orderedFunct = new org.apache.spark.rdd.OrderedRDDFunctions[String, WritableDataRow, (String, WritableDataRow)](rddAsSequenceFile.map(x => (x.getContents().get(1).toString(), x))) orderedFunct: org.apache.spark.rdd.OrderedRDDFunctions[String,com.palantir.finance.datatable.server.spark.WritableDataRow,(String, com.palantir.finance.datatable.server.spark.WritableDataRow)] = org.apache.spark.rdd.OrderedRDDFunctions@6262a9a6 scala>orderedFunct.sortByKey().count();  (On the necessity to copy writables from hadoop RDDs, see: https://mail-archives.apache.org/mod_mbox/spark-user/201308.mbox/%3CCAF_KkPzrq4OTyQVwcOC6pLAz9X9_SFo33u4ySatki5PTqoYEDA@mail.gmail.com%3E ) I got a memory dump from one worker node but can't share it. I've attached a screenshot from YourKit. At the point where around 5GB of RAM is being used on the worker, 3GB of unreachable byte arrays have accumulated. Furthermore, they're all exactly the same size, and seem to be the same size as most of the byte arrays that are strong reachable. The strong reachable byte arrays are referenced from output streams in the block output writers. Let me know if you require any more information. Thanks.\n\nComments (4):\n1. Matthew Cheah: Yikes, I wasn't aware that square brackets would linkify all the things. If someone wants to go in and hack the description to change that, that would be fantastic.\n2. OuyangJin: Hi @Matthew Cheah. I wonder the moment you got that memory dump on one worker node which is analyzed by Yourkit in the pictures above? Is it at job runtime or after all job is finished? If you are using HPROF dump file, can you provide which jvm process in that worker node you are dumping, the executor process or worker process if you are using standalone?(likewise, the pid you provide to jmap is from which jvm process). Becuase if it's at runtime, unreachable byte arrays will be deleted by GC. Otherwise ,if memory leak really happens , we should search it amony strong reachable objects , because they will not be deleted by GC. So I wonder it's the unreachable byte arrays you are worried about for memory leak or the reachable ones? Thanks!\n3. Matthew Cheah: It's happening as the job is running. At job runtime, the memory keeps continuously climbing. It appears that the unreachable byte arrays are what keeps increasing in size, so that's what I believed was causing the leak. It IS possible for unreachable objects to NOT be cleaned up by GC for various reasons. I took a dump of the executor process, I believe. (I've since moved on from the project that was dealing with this issue though. Went back to school. It's been a little while since I looked at this, so I might not remember the specifics.)\n4. Nicholas Chammas: It's probably tough given how long ago this was reported, but can anyone confirm whether this is still an issue on the latest release (1.2.1)?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.104052"}}
{"id": "810e91d8f6e282635f7a3979d5174c7d", "issue_key": "SPARK-1002", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Remove binary artifacts from build", "description": "This is a requirement from Apache and will block our 0.9 release unless dealt with. We should publish any jars we need through sonatype and remove sbt. - Patrick", "reporter": "Patrick Wendell", "assignee": null, "created": "2013-12-17T10:11:19.000+0000", "updated": "2014-03-30T04:13:50.000+0000", "resolved": "2014-01-05T23:17:12.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1002\nSummary: Remove binary artifacts from build\nDescription: This is a requirement from Apache and will block our 0.9 release unless dealt with. We should publish any jars we need through sonatype and remove sbt. - Patrick", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.104052"}}
{"id": "b37492d4209278ef1c3003bcf01b060a", "issue_key": "SPARK-1003", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark application is blocked when running on yarn", "description": "When running Spark Application on yarn, it is blocked on client side: SPARK_JAR=./assembly/target/scala-2.9.3/spark-assembly-0.8.1-incubating-hadoop2.2.0.jar \\ ./spark-class org.apache.spark.deploy.yarn.Client \\ --jar ./assembly/target/scala-2.9.3/spark-examples-assembly-0.8.1-incubating.jar \\ --class org.apache.spark.examples.SparkPi \\ --args yarn-standalone \\ --name pi \\ --num-workers 3 \\ --master-memory 2g \\ --worker-memory 2g \\ --worker-cores 1 I check the source and find a bug: // position: new-yarn/src/main/scala/org/apache/spark/deploy/yarn/ClientArguments.scala // line 91 case (\"--name\") :: value :: tail => appName = value args = tail the last “args = tail” is missed.", "reporter": "Xicheng Dong", "assignee": "Thomas Graves", "created": "2013-12-18T00:18:54.000+0000", "updated": "2014-01-23T23:41:16.000+0000", "resolved": "2014-01-23T23:41:16.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Josh Rosen", "body": "This was fixed in https://github.com/apache/incubator-spark/pull/257", "created": "2014-01-23T23:41:16.444+0000"}], "num_comments": 1, "text": "Issue: SPARK-1003\nSummary: Spark application is blocked when running on yarn\nDescription: When running Spark Application on yarn, it is blocked on client side: SPARK_JAR=./assembly/target/scala-2.9.3/spark-assembly-0.8.1-incubating-hadoop2.2.0.jar \\ ./spark-class org.apache.spark.deploy.yarn.Client \\ --jar ./assembly/target/scala-2.9.3/spark-examples-assembly-0.8.1-incubating.jar \\ --class org.apache.spark.examples.SparkPi \\ --args yarn-standalone \\ --name pi \\ --num-workers 3 \\ --master-memory 2g \\ --worker-memory 2g \\ --worker-cores 1 I check the source and find a bug: // position: new-yarn/src/main/scala/org/apache/spark/deploy/yarn/ClientArguments.scala // line 91 case (\"--name\") :: value :: tail => appName = value args = tail the last “args = tail” is missed.\n\nComments (1):\n1. Josh Rosen: This was fixed in https://github.com/apache/incubator-spark/pull/257", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.104052"}}
{"id": "220093403a0604c92e9df3349b44051d", "issue_key": "SPARK-1004", "issue_type": "Sub-task", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "PySpark on YARN", "description": "This is for tracking progress on supporting YARN in PySpark. We might be able to use {{yarn-client}} mode (https://spark.incubator.apache.org/docs/latest/running-on-yarn.html#launch-spark-application-with-yarn-client-mode).", "reporter": "Josh Rosen", "assignee": "Sandy Ryza", "created": "2013-12-20T12:02:04.000+0000", "updated": "2016-06-15T07:46:04.000+0000", "resolved": "2014-04-30T06:25:05.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Diana Carroll", "body": "I couldn't find anywhere it was documented how to do this. Eventually I figured it out through trial-and-error. It would be good if this could make it into the docs. I was able to run a simple pyspark program on YARN (CDH5.0b1 installed in pseudo-distributed mode) using the following command:  SPARK_JAR=$SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.1-incubating-hadoop2.2.0.jar \\ SPARK_YARN_APP_JAR=~/testdata.txt pyspark \\ test1.py  in the program I call  sc = SparkContext(\"yarn-client\", \"Simple App\")  It works. I filed a bug regarding the requirement for SPARK_YARN_APP_JAR https://spark-project.atlassian.net/browse/SPARK-1030", "created": "2014-01-17T11:24:53.665+0000"}, {"author": "Josh Rosen", "body": "Thanks for reporting this. I think that PySpark should be able to automatically set SPARK_JAR and SPARK_YARN_APP_JAR when running under yarn-client mode, so I added some code to do that here: https://github.com/JoshRosen/incubator-spark/compare/pyspark-yarn This works fine with a local Hadoop 2.0.5-alpha psuedo-distributed cluster, but I don't think it work correctly on an actual YARN cluster since the PySpark python libraries may not be installed. One approach might be to package the PySpark files into a .egg and distribute it using SPARK_YARN_APP_JAR, then modify the worker launch code to use that PYTHONPATH.", "created": "2014-01-24T10:26:00.700+0000"}, {"author": "Josh Rosen", "body": "I just pushed my work-in-progress commit into my pyspark-yarn branch: https://github.com/JoshRosen/incubator-spark/compare/pyspark-yarn The core changes are: - Remove reliance on SPARK_HOME on the workers. Only the driver should know about SPARK_HOME. On the workers, we ensure that the PySpark Python libraries are added to the PYTHONPATH. - Add a Makefile for generating a \"fat zip\" that contains PySpark's Python dependencies. This is a bit of a hack and I'd be open to better packaging tools, but this doesn't require any extra Python libraries. This use case doesn't seem to be well-addressed by the existing Python packaging tools: there are plenty of tools to package complete Python environments (such as pyinstaller and virtualenv) or to bundle *individual* libraries (e.g. distutils), but few to generate portable fat zips or eggs. My last WIP commit might not actually compile and I haven't tested it on an actual YARN cluster yet. I'm a bit busy right now and may not have time to work on this in the next couple of weeks, so anyone who's interested should feel free to pick this up and finish it.", "created": "2014-02-10T23:49:11.178+0000"}, {"author": "Sanford Ryza", "body": "Thanks Josh. I'm going to try to take a stab at this. The WIP commit seems to be missing the setup.py referenced in the Makefile. Did you possibly forget to git add it?", "created": "2014-02-18T16:45:42.522+0000"}, {"author": "Josh Rosen", "body": "Whoops, looks like I forgot to include the setup.py file; I pushed a new commit to add it: https://github.com/JoshRosen/incubator-spark/commit/c70f92ba79041f11f93097a3b9a9da20fc7bf316", "created": "2014-02-18T22:28:55.591+0000"}, {"author": "Sanford Ryza", "body": "Thanks. I was able to get your patch to work with a few changes. I took pyspark-assembly.zip out of SPARK_YARN_USER_JAR (this won't be required after SPARK-1053) and added it to SPARK_YARN_DIST_FILES. A couple changes that need to happen after this: * We shouldn't override PYTHONPATH. We should just add to it. * We shouldn't override SPARK_YARN_USER_ENV. We should just add to it. * yarn-standalone support ** SPARK_YARN_DIST_FILES isn't used in yarn-standalone mode so we'll need to have some other way of getting pyspark-assembly.zip there ** user should be able to give python script that gets distributed and run as well ** Right now yarn-standalone runs a java/scala class inside the application master process, so not sure what the best way to use a python process as the driver is", "created": "2014-02-22T13:11:01.057+0000"}, {"author": "Sanford Ryza", "body": "https://github.com/apache/incubator-spark/pull/640", "created": "2014-02-23T23:34:02.428+0000"}, {"author": "Patrick Wendell", "body": "Issue resolved by pull request 30 [https://github.com/apache/spark/pull/30]", "created": "2014-04-30T06:25:05.416+0000"}, {"author": "Apache Spark", "body": "User 'sryza' has created a pull request for this issue: https://github.com/apache/spark/pull/30", "created": "2016-06-15T07:46:04.481+0000"}], "num_comments": 9, "text": "Issue: SPARK-1004\nSummary: PySpark on YARN\nDescription: This is for tracking progress on supporting YARN in PySpark. We might be able to use {{yarn-client}} mode (https://spark.incubator.apache.org/docs/latest/running-on-yarn.html#launch-spark-application-with-yarn-client-mode).\n\nComments (9):\n1. Diana Carroll: I couldn't find anywhere it was documented how to do this. Eventually I figured it out through trial-and-error. It would be good if this could make it into the docs. I was able to run a simple pyspark program on YARN (CDH5.0b1 installed in pseudo-distributed mode) using the following command:  SPARK_JAR=$SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.1-incubating-hadoop2.2.0.jar \\ SPARK_YARN_APP_JAR=~/testdata.txt pyspark \\ test1.py  in the program I call  sc = SparkContext(\"yarn-client\", \"Simple App\")  It works. I filed a bug regarding the requirement for SPARK_YARN_APP_JAR https://spark-project.atlassian.net/browse/SPARK-1030\n2. Josh Rosen: Thanks for reporting this. I think that PySpark should be able to automatically set SPARK_JAR and SPARK_YARN_APP_JAR when running under yarn-client mode, so I added some code to do that here: https://github.com/JoshRosen/incubator-spark/compare/pyspark-yarn This works fine with a local Hadoop 2.0.5-alpha psuedo-distributed cluster, but I don't think it work correctly on an actual YARN cluster since the PySpark python libraries may not be installed. One approach might be to package the PySpark files into a .egg and distribute it using SPARK_YARN_APP_JAR, then modify the worker launch code to use that PYTHONPATH.\n3. Josh Rosen: I just pushed my work-in-progress commit into my pyspark-yarn branch: https://github.com/JoshRosen/incubator-spark/compare/pyspark-yarn The core changes are: - Remove reliance on SPARK_HOME on the workers. Only the driver should know about SPARK_HOME. On the workers, we ensure that the PySpark Python libraries are added to the PYTHONPATH. - Add a Makefile for generating a \"fat zip\" that contains PySpark's Python dependencies. This is a bit of a hack and I'd be open to better packaging tools, but this doesn't require any extra Python libraries. This use case doesn't seem to be well-addressed by the existing Python packaging tools: there are plenty of tools to package complete Python environments (such as pyinstaller and virtualenv) or to bundle *individual* libraries (e.g. distutils), but few to generate portable fat zips or eggs. My last WIP commit might not actually compile and I haven't tested it on an actual YARN cluster yet. I'm a bit busy right now and may not have time to work on this in the next couple of weeks, so anyone who's interested should feel free to pick this up and finish it.\n4. Sanford Ryza: Thanks Josh. I'm going to try to take a stab at this. The WIP commit seems to be missing the setup.py referenced in the Makefile. Did you possibly forget to git add it?\n5. Josh Rosen: Whoops, looks like I forgot to include the setup.py file; I pushed a new commit to add it: https://github.com/JoshRosen/incubator-spark/commit/c70f92ba79041f11f93097a3b9a9da20fc7bf316\n6. Sanford Ryza: Thanks. I was able to get your patch to work with a few changes. I took pyspark-assembly.zip out of SPARK_YARN_USER_JAR (this won't be required after SPARK-1053) and added it to SPARK_YARN_DIST_FILES. A couple changes that need to happen after this: * We shouldn't override PYTHONPATH. We should just add to it. * We shouldn't override SPARK_YARN_USER_ENV. We should just add to it. * yarn-standalone support ** SPARK_YARN_DIST_FILES isn't used in yarn-standalone mode so we'll need to have some other way of getting pyspark-assembly.zip there ** user should be able to give python script that gets distributed and run as well ** Right now yarn-standalone runs a java/scala class inside the application master process, so not sure what the best way to use a python process as the driver is\n7. Sanford Ryza: https://github.com/apache/incubator-spark/pull/640\n8. Patrick Wendell: Issue resolved by pull request 30 [https://github.com/apache/spark/pull/30]\n9. Apache Spark: User 'sryza' has created a pull request for this issue: https://github.com/apache/spark/pull/30", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.104052"}}
{"id": "589b627bc391571a8ed7f4c5013b4a37", "issue_key": "SPARK-1005", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Update Ning compress package to 1.0.0", "description": "I saw 1.0.0 is now out, it might have some improvements", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "created": "2013-12-21T09:24:52.000+0000", "updated": "2020-02-07T17:26:38.000+0000", "resolved": "2014-01-06T11:41:39.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1005\nSummary: Update Ning compress package to 1.0.0\nDescription: I saw 1.0.0 is now out, it might have some improvements", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "9a0a171bf4f61d72b10208f1ff0a8ab8", "issue_key": "SPARK-1006", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "MLlib ALS gets stack overflow with too many iterations", "description": "The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have. We might also be able to fix DAGScheduler to not be recursive.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2013-12-21T11:02:48.000+0000", "updated": "2016-01-06T17:19:07.000+0000", "resolved": "2015-02-23T22:12:46.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Same issue will affect Bagel at around 100 iterations.", "created": "2013-12-21T11:03:41.954+0000"}, {"author": "xiajunluan", "body": "Hi Matei ALS will be StackOverflow as too long linage chain(deserialization cause stackoverflow in executor), but I am not sure if we will fix this issue by changing DAGScheduler to not be recursive, because we don't break the linage chain. but it will avoid potential stackoverflow in driver(I have encountered this exception when recursively building stages)", "created": "2013-12-31T00:25:08.254+0000"}, {"author": "Michelangelo D'Agostino", "body": "Any plans to work on this or any pointers how one would go about making the needed modification? I'm working with a dataset that doesn't appear to be converging before it runs into this limitation...", "created": "2014-11-05T23:08:25.847+0000"}, {"author": "Xiangrui Meng", "body": "This is fixed as part of SPARK-5955, where we use checkpointing to cut off lineage.", "created": "2015-03-24T02:31:51.053+0000"}, {"author": "Anuradha Uduwage", "body": "Is there away to find out which build fix this problem: I am on 1.3.1 and error 5/08/11 14:08:20 ERROR TaskSetManager: Task 1 in stage 4204.0 failed 4 times; aborting job Exception in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4204.0 failed 4 times, most recent failure: Lost task 1.3 in stage 4204.0 (COMPANYDOMAIN-TAKENOUT): java.lang.StackOverflowError at java.io.SerialCallbackContext.<init>(SerialCallbackContext.java:48) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1890) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) I am doing 50 iterations val numIterations = 50 ALS.trainImplicit(trainData, rank, numIterations, 0.01, 0.01)", "created": "2015-08-11T18:26:19.752+0000"}, {"author": "Alger Remirata", "body": "First of all, I would like to thank you guys for developing spark and putting it open source that we can use. I'm new to Spark and Scala, and working in a project involving matrix factorizations in Spark. I have a problem regarding running ALS in Spark. It has a stackoverflow due to long linage chain as per comments on the internet. One of their suggestion is to use the setCheckpointInterval so that for every 10-20 iterations, we can checkpoint the RDDs and it prevents the error. Just want to ask details on how to do checkpointing with ALS. I am using spark-kernel developed by IBM: https://github.com/ibm-et/spark-kernel instead of spark-shell. Here are some of my specific questions regarding details on checkpoint: 1. In setting checkpoint directory through SparkContext.setCheckPointDir(), it needs to be a hadoop compatible directory. Can we use any available hdfs-compatible directory? 2. What do you mean by this comment on the code in ALS checkpointing: If the checkpoint directory is not set in [[org.apache.spark.SparkContext]], * this setting is ignored. 3. Is the use of setCheckPointInterval the only code I needed to add to have checkpointing for ALS work? 4. I am getting this error: Name: java.lang.IllegalArgumentException, Message: Wrong FS: expected file :///. How can I solve this? What is the proper way of using checkpointing. Thanks a lot!", "created": "2016-01-06T17:19:07.768+0000"}], "num_comments": 6, "text": "Issue: SPARK-1006\nSummary: MLlib ALS gets stack overflow with too many iterations\nDescription: The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have. We might also be able to fix DAGScheduler to not be recursive.\n\nComments (6):\n1. Matei Alexandru Zaharia: Same issue will affect Bagel at around 100 iterations.\n2. xiajunluan: Hi Matei ALS will be StackOverflow as too long linage chain(deserialization cause stackoverflow in executor), but I am not sure if we will fix this issue by changing DAGScheduler to not be recursive, because we don't break the linage chain. but it will avoid potential stackoverflow in driver(I have encountered this exception when recursively building stages)\n3. Michelangelo D'Agostino: Any plans to work on this or any pointers how one would go about making the needed modification? I'm working with a dataset that doesn't appear to be converging before it runs into this limitation...\n4. Xiangrui Meng: This is fixed as part of SPARK-5955, where we use checkpointing to cut off lineage.\n5. Anuradha Uduwage: Is there away to find out which build fix this problem: I am on 1.3.1 and error 5/08/11 14:08:20 ERROR TaskSetManager: Task 1 in stage 4204.0 failed 4 times; aborting job Exception in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4204.0 failed 4 times, most recent failure: Lost task 1.3 in stage 4204.0 (COMPANYDOMAIN-TAKENOUT): java.lang.StackOverflowError at java.io.SerialCallbackContext.<init>(SerialCallbackContext.java:48) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1890) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) I am doing 50 iterations val numIterations = 50 ALS.trainImplicit(trainData, rank, numIterations, 0.01, 0.01)\n6. Alger Remirata: First of all, I would like to thank you guys for developing spark and putting it open source that we can use. I'm new to Spark and Scala, and working in a project involving matrix factorizations in Spark. I have a problem regarding running ALS in Spark. It has a stackoverflow due to long linage chain as per comments on the internet. One of their suggestion is to use the setCheckpointInterval so that for every 10-20 iterations, we can checkpoint the RDDs and it prevents the error. Just want to ask details on how to do checkpointing with ALS. I am using spark-kernel developed by IBM: https://github.com/ibm-et/spark-kernel instead of spark-shell. Here are some of my specific questions regarding details on checkpoint: 1. In setting checkpoint directory through SparkContext.setCheckPointDir(), it needs to be a hadoop compatible directory. Can we use any available hdfs-compatible directory? 2. What do you mean by this comment on the code in ALS checkpointing: If the checkpoint directory is not set in [[org.apache.spark.SparkContext]], * this setting is ignored. 3. Is the use of setCheckPointInterval the only code I needed to add to have checkpointing for ALS work? 4. I am getting this error: Name: java.lang.IllegalArgumentException, Message: Wrong FS: expected file :///. How can I solve this? What is the proper way of using checkpointing. Thanks a lot!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "3e7950507b41c3d89d84b33fa3901b14", "issue_key": "SPARK-1007", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "spark-class2.cmd should change SCALA_VERSION to be 2.10", "description": "Most likely a mistake, spark-class2.cm still set scala version to be 2.9.3, it should be 2.10 instead. This is from spark git trunk.", "reporter": "Qiuzhuang Lian", "assignee": "Patrick McFadin", "created": "2013-12-25T22:38:15.000+0000", "updated": "2013-12-29T19:01:13.000+0000", "resolved": "2013-12-29T19:01:13.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Patrick McFadin", "body": "Thanks for reporting this.", "created": "2013-12-26T23:22:22.043+0000"}], "num_comments": 1, "text": "Issue: SPARK-1007\nSummary: spark-class2.cmd should change SCALA_VERSION to be 2.10\nDescription: Most likely a mistake, spark-class2.cm still set scala version to be 2.9.3, it should be 2.10 instead. This is from spark git trunk.\n\nComments (1):\n1. Patrick McFadin: Thanks for reporting this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "447f6a2c34cb6a34cd97bf36af6517c7", "issue_key": "SPARK-1008", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Provide good default logging if log4j properties is not present", "description": "This is an issue now that we've excluded log4j files from our assembly jar.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2013-12-29T19:08:31.000+0000", "updated": "2014-03-09T17:54:53.000+0000", "resolved": "2014-03-09T17:54:53.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1008\nSummary: Provide good default logging if log4j properties is not present\nDescription: This is an issue now that we've excluded log4j files from our assembly jar.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "126429971d5cceb0e91ab092fc2c41c8", "issue_key": "SPARK-1009", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Updated MLlib docs to show how to use it in Python", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Hossein Falaki", "created": "2013-12-30T19:29:31.000+0000", "updated": "2014-01-07T23:24:37.000+0000", "resolved": "2014-01-07T23:24:37.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Hossein Falaki", "body": "PR #322", "created": "2014-01-07T23:24:37.689+0000"}], "num_comments": 1, "text": "Issue: SPARK-1009\nSummary: Updated MLlib docs to show how to use it in Python\n\nComments (1):\n1. Hossein Falaki: PR #322", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "4fe0bee6b6c918db94a2b0b97c3c390e", "issue_key": "SPARK-1010", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Update all unit tests to use SparkConf instead of system properties", "description": "", "reporter": "Patrick Wendell", "assignee": "Josh Rosen", "created": "2013-12-31T15:13:22.000+0000", "updated": "2015-01-31T05:54:28.000+0000", "resolved": "2014-12-31T19:52:02.000+0000", "labels": [], "components": [], "comments": [{"author": "Nirmal", "body": "Assign me this issue. I can fix this.", "created": "2014-01-08T04:09:52.023+0000"}, {"author": "Patrick McFadin", "body": "Done!", "created": "2014-01-08T10:49:37.162+0000"}, {"author": "Nirmal", "body": "Hey Patrick By executing the following command i could find only 2 results. *find -iname '*suite.scala' -type f -exec grep -nHI -e 'System.getProperty' {} +* results:  *./core/src/test/scala/org/apache/spark/scheduler/JobLoggerSuite.scala:98*: val user = System.getProperty(\"user.name\", SparkContext.SPARK_UNKNOWN_USER) *./repl/src/test/scala/org/apache/spark/repl/ReplSuite.scala:30*: val separator = System.getProperty(\"path.separator\")", "created": "2014-01-08T22:20:45.229+0000"}, {"author": "Ryan Fishel", "body": "Is this issue still unresolved?", "created": "2014-07-01T22:14:49.637+0000"}, {"author": "Sean R. Owen", "body": "Yes, lots of usage in tests still. A lot looks intentional.  find . -name \"*Suite.scala\" -type f -exec grep -E \"System\\.[gs]etProperty\" {} \\; ... \"\"\".format(System.getProperty(\"user.name\", \"<unknown>\"), \"\"\".format(System.getProperty(\"user.name\", \"<unknown>\")).stripMargin System.setProperty(\"spark.testing\", \"true\") System.setProperty(\"spark.reducer.maxMbInFlight\", \"1\") System.setProperty(\"spark.storage.memoryFraction\", \"0.0001\") System.setProperty(\"spark.storage.memoryFraction\", \"0.01\") System.setProperty(\"spark.authenticate\", \"false\") System.setProperty(\"spark.authenticate\", \"false\") System.setProperty(\"spark.shuffle.manager\", \"hash\") System.setProperty(\"spark.scheduler.mode\", \"FIFO\") System.setProperty(\"spark.scheduler.mode\", \"FAIR\") ...", "created": "2014-10-13T18:39:43.334+0000"}, {"author": "liu chang", "body": "please assign to me, I will fix it.", "created": "2014-12-04T15:24:44.773+0000"}, {"author": "Nicholas Chammas", "body": "[~joshrosen] Haven't you been doing some work towards this goal?", "created": "2014-12-30T19:32:47.022+0000"}, {"author": "Josh Rosen", "body": "Yeah, didn't realize there was already a JIRA for this. I'll link this from my PR.", "created": "2014-12-30T19:52:15.838+0000"}, {"author": "Apache Spark", "body": "User 'JoshRosen' has created a pull request for this issue: https://github.com/apache/spark/pull/3739", "created": "2014-12-30T19:54:52.935+0000"}, {"author": "Josh Rosen", "body": "I've merged my PR, which fixes this, into master and I'll followup later today or tomorrow with backports to the maintenance branches. (Whoops, didn't mean to edit the issue description)", "created": "2014-12-31T02:15:41.232+0000"}, {"author": "Josh Rosen", "body": "I've finished backporting this to all of the maintenance branches.", "created": "2014-12-31T19:52:02.375+0000"}, {"author": "Josh Rosen", "body": "The pull request for SPARK-5425 fixes a bug in the ResetSystemProperties trait added here. My original implementation of that trait didn't properly reset the system properties because it didn't perform a proper clone: https://github.com/apache/spark/pull/4220#issuecomment-71992373.", "created": "2015-01-31T05:54:28.469+0000"}], "num_comments": 12, "text": "Issue: SPARK-1010\nSummary: Update all unit tests to use SparkConf instead of system properties\n\nComments (12):\n1. Nirmal: Assign me this issue. I can fix this.\n2. Patrick McFadin: Done!\n3. Nirmal: Hey Patrick By executing the following command i could find only 2 results. *find -iname '*suite.scala' -type f -exec grep -nHI -e 'System.getProperty' {} +* results:  *./core/src/test/scala/org/apache/spark/scheduler/JobLoggerSuite.scala:98*: val user = System.getProperty(\"user.name\", SparkContext.SPARK_UNKNOWN_USER) *./repl/src/test/scala/org/apache/spark/repl/ReplSuite.scala:30*: val separator = System.getProperty(\"path.separator\")\n4. Ryan Fishel: Is this issue still unresolved?\n5. Sean R. Owen: Yes, lots of usage in tests still. A lot looks intentional.  find . -name \"*Suite.scala\" -type f -exec grep -E \"System\\.[gs]etProperty\" {} \\; ... \"\"\".format(System.getProperty(\"user.name\", \"<unknown>\"), \"\"\".format(System.getProperty(\"user.name\", \"<unknown>\")).stripMargin System.setProperty(\"spark.testing\", \"true\") System.setProperty(\"spark.reducer.maxMbInFlight\", \"1\") System.setProperty(\"spark.storage.memoryFraction\", \"0.0001\") System.setProperty(\"spark.storage.memoryFraction\", \"0.01\") System.setProperty(\"spark.authenticate\", \"false\") System.setProperty(\"spark.authenticate\", \"false\") System.setProperty(\"spark.shuffle.manager\", \"hash\") System.setProperty(\"spark.scheduler.mode\", \"FIFO\") System.setProperty(\"spark.scheduler.mode\", \"FAIR\") ...\n6. liu chang: please assign to me, I will fix it.\n7. Nicholas Chammas: [~joshrosen] Haven't you been doing some work towards this goal?\n8. Josh Rosen: Yeah, didn't realize there was already a JIRA for this. I'll link this from my PR.\n9. Apache Spark: User 'JoshRosen' has created a pull request for this issue: https://github.com/apache/spark/pull/3739\n10. Josh Rosen: I've merged my PR, which fixes this, into master and I'll followup later today or tomorrow with backports to the maintenance branches. (Whoops, didn't mean to edit the issue description)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "0d6122061189026bbc3111fc628ec1eb", "issue_key": "SPARK-1011", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "MatrixFactorizationModel in pyspark throws serialization error", "description": "When running the following sample code in pyspark,  from pyspark.mllib.recommendation import ALS from numpy import array # Load and parse the data data = sc.textFile(\"mllib/data/als/test.data\") ratings = data.map(lambda line: array([float(x) for x in line.split(',')])) # Build the recommendation model using Alternating Least Squares model = ALS.train(sc, ratings, 1, 20) # Evaluate the model on training data ratesAndPreds = ratings.map(lambda p: (p[2], model.predict(int(p[0]), int(p[1])))) ratesAndPreds.take(1)  I get:  >>> ratesAndPreds.take(1) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py\", line 585, in take for partition in range(mapped._jrdd.splits().size()): File \"/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py\", line 984, in _jrdd pickled_command = CloudPickleSerializer().dumps(command) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/serializers.py\", line 248, in dumps def dumps(self, obj): return cloudpickle.dumps(obj, 2) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 801, in dumps cp.dump(obj) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 140, in dump return pickle.Pickler.dump(self, obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 224, in dump self.save(obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 548, in save_tuple save(element) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function self.save_function_tuple(obj, [themodule]) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple save(closure) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list self._batch_appends(iter(obj)) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 633, in _batch_appends save(x) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function self.save_function_tuple(obj, [themodule]) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple save(closure) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list self._batch_appends(iter(obj)) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 633, in _batch_appends save(x) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function self.save_function_tuple(obj, [themodule]) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple save(closure) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list self._batch_appends(iter(obj)) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 636, in _batch_appends save(tmp[0]) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 254, in save_function self.save_function_tuple(obj, modList) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 314, in save_function_tuple save(f_globals) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 181, in save_dict pickle.Pickler.save_dict(self, obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 649, in save_dict self._batch_setitems(obj.iteritems()) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 686, in _batch_setitems save(v) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 331, in save self.save_reduce(obj=obj, *rv) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 631, in save_reduce save(state) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 181, in save_dict pickle.Pickler.save_dict(self, obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 649, in save_dict self._batch_setitems(obj.iteritems()) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 681, in _batch_setitems save(v) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 306, in save rv = reduce(self.proto) File \"build/bdist.macosx-10.8-intel/egg/py4j/java_gateway.py\", line 500, in __call__ File \"build/bdist.macosx-10.8-intel/egg/py4j/protocol.py\", line 304, in get_return_value py4j.protocol.Py4JError: An error occurred while calling o37.__getnewargs__. Trace: py4j.Py4JException: Method __getnewargs__([]) does not exist at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333) at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342) at py4j.Gateway.invoke(Gateway.java:251) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:695)", "reporter": "Hossein Falaki", "assignee": "Hossein Falaki", "created": "2014-01-02T12:42:56.000+0000", "updated": "2014-07-25T22:53:26.000+0000", "resolved": "2014-07-25T22:42:13.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "This error suggests that a Py4J object is being referenced inside of a function closure, because pickle is attempting to call __getnewargs__ on a Java object. I haven't tried running your example, but my hunch is that the private _java_model field in MatrixFactorizationModel is being serialized. Since we're not using Py4J on the workers, I think we'd need to find a different way to pass model objects to PySpark UDFs for further processing. This could be tricky and I don't know of any easy solution off the top of my head.", "created": "2014-01-02T15:42:13.731+0000"}, {"author": "Hossein Falaki", "body": "I was working on examples for MLLib in python that I encountered this issue. One solution might be to add a predict() function to ALS that takes an RDD and returns predicted ratings. If we had such a method, I could avoid passing the model in the closure and void this issue for now (until we fix it properly).", "created": "2014-01-02T15:45:47.351+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yeah, actually MatrixFactorizationModel.predict() cannot be called on a cluster even in Scala, as far as I know. This is because it uses RDD operations inside it, and you can't call those within a closure on the cluster. We'd have to add a predict on RDDs if that's what you'd like to do.", "created": "2014-01-03T09:22:38.617+0000"}, {"author": "Hossein Falaki", "body": "Yes, I added that. After thorough testing I will send the pull request. Hopefully someone can add the python binding for it later.", "created": "2014-01-03T09:52:22.616+0000"}, {"author": "Hossein Falaki", "body": "It would be great to have python binding for the method added with <https://github.com/apache/incubator-spark/pull/328>", "created": "2014-01-03T16:40:50.637+0000"}, {"author": "Josh Rosen", "body": "This was fixed in Spark 0.9.1 with the addition of a new {{predictAll}} method for performing bulk predictions. This was added in commit https://github.com/apache/spark/commit/b2e690f839e7ee47f405135d35170173386c5d13.", "created": "2014-07-25T22:42:13.465+0000"}, {"author": "Josh Rosen", "body": "Oh, and if you want to combine the actual vs. predicted ratings:  from pyspark.mllib.recommendation import ALS from numpy import array # Load and parse the data data = sc.textFile(\"mllib/data/als/test.data\") ratings = data.map(lambda line: array([float(x) for x in line.split(',')])) # Build the recommendation model using Alternating Least Squares model = ALS.train(ratings, 1, 20) # Evaluate the model on training data predictedRatings = model.predictAll(ratings.map(lambda x: (x[0], x[1]))) trueVsPredicted = ratings.map(lambda x: ((x[0], x[1]), x[2])).join(predictedRatings.map(lambda x: ((x[0], x[1]), x[2]))) trueVsPredicted.take(1)", "created": "2014-07-25T22:53:26.134+0000"}], "num_comments": 7, "text": "Issue: SPARK-1011\nSummary: MatrixFactorizationModel in pyspark throws serialization error\nDescription: When running the following sample code in pyspark,  from pyspark.mllib.recommendation import ALS from numpy import array # Load and parse the data data = sc.textFile(\"mllib/data/als/test.data\") ratings = data.map(lambda line: array([float(x) for x in line.split(',')])) # Build the recommendation model using Alternating Least Squares model = ALS.train(sc, ratings, 1, 20) # Evaluate the model on training data ratesAndPreds = ratings.map(lambda p: (p[2], model.predict(int(p[0]), int(p[1])))) ratesAndPreds.take(1)  I get:  >>> ratesAndPreds.take(1) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py\", line 585, in take for partition in range(mapped._jrdd.splits().size()): File \"/Users/hossein/Projects/incubator-spark/python/pyspark/rdd.py\", line 984, in _jrdd pickled_command = CloudPickleSerializer().dumps(command) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/serializers.py\", line 248, in dumps def dumps(self, obj): return cloudpickle.dumps(obj, 2) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 801, in dumps cp.dump(obj) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 140, in dump return pickle.Pickler.dump(self, obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 224, in dump self.save(obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 548, in save_tuple save(element) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function self.save_function_tuple(obj, [themodule]) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple save(closure) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list self._batch_appends(iter(obj)) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 633, in _batch_appends save(x) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function self.save_function_tuple(obj, [themodule]) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple save(closure) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list self._batch_appends(iter(obj)) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 633, in _batch_appends save(x) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 259, in save_function self.save_function_tuple(obj, [themodule]) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 316, in save_function_tuple save(closure) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 600, in save_list self._batch_appends(iter(obj)) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 636, in _batch_appends save(tmp[0]) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 254, in save_function self.save_function_tuple(obj, modList) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 314, in save_function_tuple save(f_globals) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 181, in save_dict pickle.Pickler.save_dict(self, obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 649, in save_dict self._batch_setitems(obj.iteritems()) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 686, in _batch_setitems save(v) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 331, in save self.save_reduce(obj=obj, *rv) File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 631, in save_reduce save(state) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/Users/hossein/Projects/incubator-spark/python/pyspark/cloudpickle.py\", line 181, in save_dict pickle.Pickler.save_dict(self, obj) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 649, in save_dict self._batch_setitems(obj.iteritems()) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 681, in _batch_setitems save(v) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 306, in save rv = reduce(self.proto) File \"build/bdist.macosx-10.8-intel/egg/py4j/java_gateway.py\", line 500, in __call__ File \"build/bdist.macosx-10.8-intel/egg/py4j/protocol.py\", line 304, in get_return_value py4j.protocol.Py4JError: An error occurred while calling o37.__getnewargs__. Trace: py4j.Py4JException: Method __getnewargs__([]) does not exist at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333) at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342) at py4j.Gateway.invoke(Gateway.java:251) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:695)\n\nComments (7):\n1. Josh Rosen: This error suggests that a Py4J object is being referenced inside of a function closure, because pickle is attempting to call __getnewargs__ on a Java object. I haven't tried running your example, but my hunch is that the private _java_model field in MatrixFactorizationModel is being serialized. Since we're not using Py4J on the workers, I think we'd need to find a different way to pass model objects to PySpark UDFs for further processing. This could be tricky and I don't know of any easy solution off the top of my head.\n2. Hossein Falaki: I was working on examples for MLLib in python that I encountered this issue. One solution might be to add a predict() function to ALS that takes an RDD and returns predicted ratings. If we had such a method, I could avoid passing the model in the closure and void this issue for now (until we fix it properly).\n3. Matei Alexandru Zaharia: Yeah, actually MatrixFactorizationModel.predict() cannot be called on a cluster even in Scala, as far as I know. This is because it uses RDD operations inside it, and you can't call those within a closure on the cluster. We'd have to add a predict on RDDs if that's what you'd like to do.\n4. Hossein Falaki: Yes, I added that. After thorough testing I will send the pull request. Hopefully someone can add the python binding for it later.\n5. Hossein Falaki: It would be great to have python binding for the method added with <https://github.com/apache/incubator-spark/pull/328>\n6. Josh Rosen: This was fixed in Spark 0.9.1 with the addition of a new {{predictAll}} method for performing bulk predictions. This was added in commit https://github.com/apache/spark/commit/b2e690f839e7ee47f405135d35170173386c5d13.\n7. Josh Rosen: Oh, and if you want to combine the actual vs. predicted ratings:  from pyspark.mllib.recommendation import ALS from numpy import array # Load and parse the data data = sc.textFile(\"mllib/data/als/test.data\") ratings = data.map(lambda line: array([float(x) for x in line.split(',')])) # Build the recommendation model using Alternating Least Squares model = ALS.train(ratings, 1, 20) # Evaluate the model on training data predictedRatings = model.predictAll(ratings.map(lambda x: (x[0], x[1]))) trueVsPredicted = ratings.map(lambda x: ((x[0], x[1]), x[2])).join(predictedRatings.map(lambda x: ((x[0], x[1]), x[2]))) trueVsPredicted.take(1)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "1e084dd2f6b83fdbca78e437d4a1a552", "issue_key": "SPARK-1012", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "DAGScheduler Exception", "description": "I get When running the following code:  import org.apache.spark.mllib.recommendation.ALS import org.apache.spark.mllib.recommendation.Rating // Load and parse the data val data = sc.textFile(\"mllib/data/als/test.data\") val ratings = data.map(_.split(',') match { case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble) }) // Build the recommendation model using ALS val numIterations = 20 val model = ALS.train(ratings, 1, 20, 0.01) // Evaluate the model on rating data val ratesAndPreds = ratings.map{ case Rating(user, item, rate) => (rate, model.predict(user, item))} val MSE = ratesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/ratesAndPreds.count  I get:  org.apache.spark.SparkException: Job aborted: Task 2.0:0 failed 1 times (most recent failure: Exception failure: scala.MatchError: null) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1024) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1024) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:205) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  The problem is deterministic. In addition ratesAndPreds has exactly 16 elements:  scala> ratesAndPreds.take(16) res1: Array[(Double, Double)] = Array((5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928)) scala> ratesAndPreds.count()  Throws the exception again.", "reporter": "Hossein Falaki", "assignee": "Hossein Falaki", "created": "2014-01-02T16:19:27.000+0000", "updated": "2014-01-07T22:02:31.000+0000", "resolved": "2014-01-07T22:02:31.000+0000", "labels": ["DAGScheduler", "MLLib,"], "components": ["Spark Core"], "comments": [{"author": "Hossein Falaki", "body": "The problem is that MatrixFactorizationModel has a reference to two RDDs (userFeatures, and productFeatures). As a result, when passed inside a closure all the bad things happen. The solution is augmenting MatrixFactorizaitonModel with a bulk prediction method that takes an RDD of test data and returns a prediction RDD.", "created": "2014-01-02T16:52:34.082+0000"}, {"author": "Hossein Falaki", "body": "Submitted this pull request to offer a viable method for bulk prediction.", "created": "2014-01-03T16:02:52.144+0000"}, {"author": "Hossein Falaki", "body": "Fixed with PR #328", "created": "2014-01-07T22:02:31.418+0000"}], "num_comments": 3, "text": "Issue: SPARK-1012\nSummary: DAGScheduler Exception\nDescription: I get When running the following code:  import org.apache.spark.mllib.recommendation.ALS import org.apache.spark.mllib.recommendation.Rating // Load and parse the data val data = sc.textFile(\"mllib/data/als/test.data\") val ratings = data.map(_.split(',') match { case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble) }) // Build the recommendation model using ALS val numIterations = 20 val model = ALS.train(ratings, 1, 20, 0.01) // Evaluate the model on rating data val ratesAndPreds = ratings.map{ case Rating(user, item, rate) => (rate, model.predict(user, item))} val MSE = ratesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/ratesAndPreds.count  I get:  org.apache.spark.SparkException: Job aborted: Task 2.0:0 failed 1 times (most recent failure: Exception failure: scala.MatchError: null) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1024) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1024) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:205) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  The problem is deterministic. In addition ratesAndPreds has exactly 16 elements:  scala> ratesAndPreds.take(16) res1: Array[(Double, Double)] = Array((5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928)) scala> ratesAndPreds.count()  Throws the exception again.\n\nComments (3):\n1. Hossein Falaki: The problem is that MatrixFactorizationModel has a reference to two RDDs (userFeatures, and productFeatures). As a result, when passed inside a closure all the bad things happen. The solution is augmenting MatrixFactorizaitonModel with a bulk prediction method that takes an RDD of test data and returns a prediction RDD.\n2. Hossein Falaki: Submitted this pull request to offer a viable method for bulk prediction.\n3. Hossein Falaki: Fixed with PR #328", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "a5326f31abeeb18fdc8a6e439f020a17", "issue_key": "SPARK-1213", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "loss function error of logistic loss", "description": "There might be a error in incubator-spark/mllib/src/main/scala/org/apache/spark/mllib/optimizatio/Gradient.scala The loss function of class LogisticGradient might be wrong. The original one is : val loss = if (margin > 0) { math.log(1 + math.exp(0 - margin)) } else { math.log(1 + math.exp(margin)) - margin } But when we use this kind of loss function, we will find that the loss is increasing when optimizing, such as LogisticRegressionWithSGD. I think it should be something like this: val loss = if (label > 0) { math.log(1 + math.exp(margin)) } else { math.log(1 + math.exp(margin)) - margin } I tested the loss function. It works well.", "reporter": "Xusen Yin", "assignee": null, "created": "2014-01-05T03:02:00.000+0000", "updated": "2014-04-01T06:34:13.000+0000", "resolved": "2014-04-01T06:34:13.000+0000", "labels": ["MLLib,"], "components": ["MLlib"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1213\nSummary: loss function error of logistic loss\nDescription: There might be a error in incubator-spark/mllib/src/main/scala/org/apache/spark/mllib/optimizatio/Gradient.scala The loss function of class LogisticGradient might be wrong. The original one is : val loss = if (margin > 0) { math.log(1 + math.exp(0 - margin)) } else { math.log(1 + math.exp(margin)) - margin } But when we use this kind of loss function, we will find that the loss is increasing when optimizing, such as LogisticRegressionWithSGD. I think it should be something like this: val loss = if (label > 0) { math.log(1 + math.exp(margin)) } else { math.log(1 + math.exp(margin)) - margin } I tested the loss function. It works well.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "197e75c7f5745b1c5c10117416d6f1d7", "issue_key": "SPARK-1013", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Have DEVELOPERS.txt file with documentation for developers", "description": "Some things to include would be: - How to run tests - How to run a single test in sbt or maven - How to build spark with assemble-deps", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2014-01-06T11:51:32.000+0000", "updated": "2014-03-25T15:12:25.000+0000", "resolved": "2014-03-25T15:12:25.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1013\nSummary: Have DEVELOPERS.txt file with documentation for developers\nDescription: Some things to include would be: - How to run tests - How to run a single test in sbt or maven - How to build spark with assemble-deps", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "4054cb99bf6f08766c63f1b8e9566ec2", "issue_key": "SPARK-1014", "issue_type": "New Feature", "status": "Closed", "priority": "Major", "resolution": null, "summary": "MultilogisticRegressionWithSGD", "description": "Multilogistic Regression With SGD based on mllib packages Use labeledpoint, gradientDescent to train the model", "reporter": "Kun Yang", "assignee": null, "created": "2014-01-06T21:01:56.000+0000", "updated": "2015-02-20T23:50:52.000+0000", "resolved": "2015-02-20T23:50:52.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Kun Yang", "body": "pr sent", "created": "2014-01-06T21:13:01.970+0000"}, {"author": "Kun Yang", "body": "source code", "created": "2014-01-06T21:30:00.200+0000"}, {"author": "Kun Yang", "body": "There is a typo in the comments, should be: /** * Classification model trained using Multinomial Logistic Regression with category {0, 1, ..., k - 1}. * * @param weights Weights [w_00, w_01, ..., w_0m; w_10, w_11, ..., w_1m; ...; w_{k-2, 0}, ..., w_{k-2, m}] * where w_i0 is the intercept, i = 0, 1, ..., k - 2 */", "created": "2014-01-07T11:27:07.066+0000"}, {"author": "Sean R. Owen", "body": "I'm curious if this is still active -- where was the PR? was this just one-vs-all LR ?", "created": "2014-11-12T16:38:14.800+0000"}, {"author": "Kun Yang", "body": "I am not sure if you can find the pr on the repository. Please find it on my github: https://github.com/kunyang1987/incubator-spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/MultilogisticRegression.scala", "created": "2014-11-12T17:59:40.769+0000"}, {"author": "Xiangrui Meng", "body": "We support multinomial logistic regression with LBFGS in 1.3. I marked this JIRA as duplicated.", "created": "2015-02-20T23:50:52.351+0000"}], "num_comments": 6, "text": "Issue: SPARK-1014\nSummary: MultilogisticRegressionWithSGD\nDescription: Multilogistic Regression With SGD based on mllib packages Use labeledpoint, gradientDescent to train the model\n\nComments (6):\n1. Kun Yang: pr sent\n2. Kun Yang: source code\n3. Kun Yang: There is a typo in the comments, should be: /** * Classification model trained using Multinomial Logistic Regression with category {0, 1, ..., k - 1}. * * @param weights Weights [w_00, w_01, ..., w_0m; w_10, w_11, ..., w_1m; ...; w_{k-2, 0}, ..., w_{k-2, m}] * where w_i0 is the intercept, i = 0, 1, ..., k - 2 */\n4. Sean R. Owen: I'm curious if this is still active -- where was the PR? was this just one-vs-all LR ?\n5. Kun Yang: I am not sure if you can find the pr on the repository. Please find it on my github: https://github.com/kunyang1987/incubator-spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/MultilogisticRegression.scala\n6. Xiangrui Meng: We support multinomial logistic regression with LBFGS in 1.3. I marked this JIRA as duplicated.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "245ba21a95f90281f8d7f0eb949c1ab3", "issue_key": "SPARK-1015", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Visualize the DAG of RDD", "description": "The DAG of RDD can help user understand the data flow and how spark get the final RDD executed. It could help user to find chances to optimize the execution of some complex RDD. I will leverage graphviz to visualize the DAG. For this task, I plan to split it into 2 steps. Step 1. Just visualize the simple DAG graph. Each RDD is one node, and there will be one edge between the parent RDD and child RDD. ( I attach one simple graph in the attachments ) Step 2. Put RDD in the same stage into one sub graph. This may need to extract the splitting staging related code in DAGSchduler.", "reporter": "Jeff Zhang", "assignee": null, "created": "2014-01-07T06:13:56.000+0000", "updated": "2015-02-27T09:52:14.000+0000", "resolved": "2015-02-27T09:52:14.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "liancheng", "body": "Hi, you may want to have a look at the [Spark Replay Debugger|http://spark-replay-debugger-overview.readthedocs.org/en/latest/], which has already implemented RDD DAG visualization and more :-) See [the overview document|http://spark-replay-debugger-overview.readthedocs.org/en/latest/], [JIRA-975|https://spark-project.atlassian.net/browse/SPARK-975], [PR-224|https://github.com/apache/incubator-spark/pull/224] & [PR-284|https://github.com/apache/incubator-spark/pull/284] for more details. A word about GraphViz. GraphVis is excellent for drawing RDD DAGs, but it can't handle RDD DAG together with stage information. Take this DAG of MLlib ALS as an example: RDD #0, #1, #2 & #3 form a stage, #0, #1, #2 & #4 form another, but in the figure, there is only #3 in the first stage. These two stages share 3 RDDs, GraphViz can't handle this kind of overlapped subgraphs. I am considering other visualization techniques, maybe some JS library that can be easily integrated into Spark Web UI.", "created": "2014-01-08T06:57:25.821+0000"}, {"author": "Sean R. Owen", "body": "[~zjffdu] are you planning on working on this? We also have {{toDebugString}} which prints some of this info. How would the visualization work with spark-shell? Is this just a utility you can host outside Spark?", "created": "2015-02-26T11:07:59.218+0000"}, {"author": "Jeff Zhang", "body": "[~sowen] I may not have time for this recently. bq. How would the visualization work with spark-shell? Is this just a utility you can host outside Spark? I would prefer to use graphviz for visualize the RDD. And spark just build the dot file for graphviz and let the graphviz to visualize it. Besides, I think integrating the DAG view to spark ui may be helpful for users to debug the RDD (especially on performance perspective )", "created": "2015-02-27T04:00:59.852+0000"}, {"author": "Sean R. Owen", "body": "That sounds great, but also sounds like something non-core enough that it can be a separate package that users could bring in and which could be advertised at http://spark-packages.org/", "created": "2015-02-27T09:52:14.773+0000"}], "num_comments": 4, "text": "Issue: SPARK-1015\nSummary: Visualize the DAG of RDD\nDescription: The DAG of RDD can help user understand the data flow and how spark get the final RDD executed. It could help user to find chances to optimize the execution of some complex RDD. I will leverage graphviz to visualize the DAG. For this task, I plan to split it into 2 steps. Step 1. Just visualize the simple DAG graph. Each RDD is one node, and there will be one edge between the parent RDD and child RDD. ( I attach one simple graph in the attachments ) Step 2. Put RDD in the same stage into one sub graph. This may need to extract the splitting staging related code in DAGSchduler.\n\nComments (4):\n1. liancheng: Hi, you may want to have a look at the [Spark Replay Debugger|http://spark-replay-debugger-overview.readthedocs.org/en/latest/], which has already implemented RDD DAG visualization and more :-) See [the overview document|http://spark-replay-debugger-overview.readthedocs.org/en/latest/], [JIRA-975|https://spark-project.atlassian.net/browse/SPARK-975], [PR-224|https://github.com/apache/incubator-spark/pull/224] & [PR-284|https://github.com/apache/incubator-spark/pull/284] for more details. A word about GraphViz. GraphVis is excellent for drawing RDD DAGs, but it can't handle RDD DAG together with stage information. Take this DAG of MLlib ALS as an example: RDD #0, #1, #2 & #3 form a stage, #0, #1, #2 & #4 form another, but in the figure, there is only #3 in the first stage. These two stages share 3 RDDs, GraphViz can't handle this kind of overlapped subgraphs. I am considering other visualization techniques, maybe some JS library that can be easily integrated into Spark Web UI.\n2. Sean R. Owen: [~zjffdu] are you planning on working on this? We also have {{toDebugString}} which prints some of this info. How would the visualization work with spark-shell? Is this just a utility you can host outside Spark?\n3. Jeff Zhang: [~sowen] I may not have time for this recently. bq. How would the visualization work with spark-shell? Is this just a utility you can host outside Spark? I would prefer to use graphviz for visualize the RDD. And spark just build the dot file for graphviz and let the graphviz to visualize it. Besides, I think integrating the DAG view to spark ui may be helpful for users to debug the RDD (especially on performance perspective )\n4. Sean R. Owen: That sounds great, but also sounds like something non-core enough that it can be a separate package that users could bring in and which could be advertised at http://spark-packages.org/", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "9f9fc018b4c4920d5e1647a916f05db7", "issue_key": "SPARK-1016", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "When running examples jar (compiled with maven) logs don't initialize properly", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "2014-01-07T22:20:09.000+0000", "updated": "2015-03-02T15:19:07.000+0000", "resolved": "2015-03-02T15:19:07.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Sean R. Owen", "body": "Is this resolved now? examples should be logging via log4j just fine at this point.", "created": "2014-11-25T10:18:59.442+0000"}, {"author": "Sean R. Owen", "body": "At this point, logging seems just fine when I'm running examples, using Maven-built artifacts.", "created": "2015-03-02T15:19:07.380+0000"}], "num_comments": 2, "text": "Issue: SPARK-1016\nSummary: When running examples jar (compiled with maven) logs don't initialize properly\n\nComments (2):\n1. Sean R. Owen: Is this resolved now? examples should be logging via log4j just fine at this point.\n2. Sean R. Owen: At this point, logging seems just fine when I'm running examples, using Maven-built artifacts.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "3b23bdcaedfbe90405b377af046c87ff", "issue_key": "SPARK-1017", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Set the permgen even if we are calling the users sbt (via SBT_OPTS)", "description": "Now we will call the users sbt installation if they have one. But users might run into the permgen issues... so we should force the permgen unless the user explicitly overrides it.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2014-01-08T14:22:28.000+0000", "updated": "2014-10-13T18:14:39.000+0000", "resolved": "2014-10-13T18:14:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Sean R. Owen", "body": "As I understand, only {{sbt/sbt}} is supported for building Spark with SBT, rather than a local {{sbt}}. Maven is the primary build, and it sets {{MaxPermSize}} and {{PermGen}} for scalac and scalatest. I think this is obsolete and/or already covered then?", "created": "2014-10-13T18:14:39.555+0000"}], "num_comments": 1, "text": "Issue: SPARK-1017\nSummary: Set the permgen even if we are calling the users sbt (via SBT_OPTS)\nDescription: Now we will call the users sbt installation if they have one. But users might run into the permgen issues... so we should force the permgen unless the user explicitly overrides it.\n\nComments (1):\n1. Sean R. Owen: As I understand, only {{sbt/sbt}} is supported for building Spark with SBT, rather than a local {{sbt}}. Maven is the primary build, and it sets {{MaxPermSize}} and {{PermGen}} for scalac and scalatest. I think this is obsolete and/or already covered then?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "07e46dcc2a776adb81d0f458182a6f67", "issue_key": "SPARK-1018", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "take and collect don't work on HadoopRDD", "description": "I am reading a simple text file using hadoopFile as follows: var hrdd1 = sc.hadoopFile(\"/home/training/testdata.txt\",classOf[TextInputFormat], classOf[LongWritable], classOf[Text]) Testing using this simple text file: 001 this is line 1 002 this is line two 003 yet another line the data read is correct, as I can tell using println scala> hrdd1.foreach(println): (0,001 this is line 1) (19,002 this is line two) (40,003 yet another line) But neither collect nor take work properly. Take prints out the key (byte offset) of the last (non-existent) line repeatedly: scala> hrdd1.take(4): res146: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((61,), (61,), (61,)) Collect is even worse: it complains: java.io.NotSerializableException: org.apache.hadoop.io.LongWritable at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183) The problem appears to be the LongWritable in both cases, because if I map to a new RDD, converting the values from Text objects to strings, it works: scala> hrdd1.map(pair => (pair._1.toString,pair._2.toString)).take(4) res148: Array[(java.lang.String, java.lang.String)] = Array((0,001 this is line 1), (19,002 this is line two), (40,003 yet another line)) Seems to me either rdd.collect and rdd.take ought to handle non-serializable types gracefully, or hadoopFile should return a mapped RDD that converts the hadoop types into the appropriate serializable Java objects. (Or at very least the docs for the API should indicate that the usual RDD methods don't work on HadoopRDDs). BTW, this behavior is the same for both the old and new API versions of hadoopFile. It also is the same whether the file is from HDFS or a plain old text file.", "reporter": "Diana Carroll", "assignee": null, "created": "2014-01-09T10:43:29.000+0000", "updated": "2016-09-19T14:16:56.000+0000", "resolved": "2016-01-16T13:21:07.000+0000", "labels": ["hadoop"], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Hey [~dcarroll@cloudera.com] just wanted to clarify. When you create a Hadoop file and pass it specific types, it is assuming that the file is already encoded with those types (e.g. someone else wrote out as a LongWritable). I'm not sure you can just write your own text file and expect it to deserialize correctly as, e.g. a LongWritable. Usually people do this type of de-serialization in Spark itself if the data is just in textual format. E.g. they to sc.textFile(XX).map(line => line.split(\"\\t\")).map(arr => (arr(0), arr(1).toInt))", "created": "2014-02-27T12:04:31.371+0000"}, {"author": "Seng Tang", "body": "I've been encountering a similar issue. I've attached a custom Hadoop InputFormat (trivial example, basically a LineReader) along with the input data and the output logs. This is the behavior I'm seeing whether I use my custom InputFormat or Hadoop's TextInputFormat: scala> val data = sc.hadoopFile[LongWritable, Text, XmlInputFormat](\"test.txt\") data: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = HadoopRDD[0] at hadoopFile at <console>:13 scala> data.take(1) res0: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((311,This is line 1)) scala> data.take(2) res1: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((311,This is line 2), (311,This is line 2)) As to Patrick's comment, the [K,V] returned by the InputFormats are wrapped inside [LongWritable, Text] already if that's what you mean.", "created": "2014-02-27T13:01:21.928+0000"}, {"author": "Diana Carroll", "body": "I'm not sure what you mean. The usual way of reading a text file is, as you say, sc.textFile. The code for that method is  def textFile(path: String, minSplits: Int = defaultMinSplits): RDD[String] = { hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minSplits).map(pair => pair._2.toString) }  As you can see, it's simply calling hadoopFile() and telling it to use the Hadoop class TextInputFormat to read the file. TextInputFormat does the job of reading in text input (using LineRecordReader), and outputting a key-value pair....the key being a long writable (the byte offset of the line in the file) and the value being Text (the line itself.) textFile then ignores the key (the byte offset), and returns an RDD with just the lines from the file. In other words, textFile() is simply a more specialized form of hadoopFile() that assumes that your input is line-oriented text. So yeah, sc.textFile() works great if your input is line-oriented. I believe you are not correct about hadoopFile(). It doesn't expect the input file to be serialized java objects. to the contrary, it doesn't have any expectations about the format at all...which is why you have to pass a pointer to a Hadoop InputFormat class. That's the class that actually reads the file. (Actually, technically, that's the class that calculates the input splits, and then creates a record RecordReader object to read the the data from the input split.) My goal in testing out sc.hadoopFile was actually because I wanted to explore using a different input format than the default TextInputFormat. (Hadoop includes several out of the box, including NLineInputFormat, CSVInputFormat, etc. I also have one I've written myself called ColumnInputFormat). TextInputFormat is pretty limited...each record is assumed to begin where the last one ended, and end with a new-line. My first test was the simplest...to call hadoopFile() with the same InputFormat class textFile() does: TextInputFormat. That's where I encountered this bug. Note that the LongWritable isn't what's supposed to be in the text file...it's the type of the key for the key-value pair generated by LineRecordReader. It's the byte offset in the file. So your code example doesn't work. The long int value I want isn't *in* the text, it's meta-data *about* the text, specifically the byte offset of the text in the file. textFile() throws that data away but what if I actually cared about it and wanted to use it? That's the test case I was exploring.", "created": "2014-02-27T13:25:44.895+0000"}, {"author": "Patrick McFadin", "body": "Hey [~dcarroll@cloudera.com] sorry for misunderstanding. You are running into two distinct issues here. The first is that Hadoop writables re-use objects in a way that makes iterating over sets of them (like Spark does) difficult. We have a fairly explicit warning in the docs for hadoopFile to make this clear. Copying from the scaladoc it says:  Note: Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function.  Maybe we should improve this to say \"cache, collect, or take\" rather than just \"cache\". We actually write a whole feature to automatically clone these objects in a way that fixed this behavior, it used a cloning utility that Hadoop provides: http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/WritableUtils.html#clone(T, org.apache.hadoop.conf.Configuration) Unfortunately we found in practice that there are several Writable implementations that break when you clone them, so this didn't work either. So we had to rip this out of Spark and leave the confusing semantics: https://github.com/apache/incubator-spark/pull/502 [~sengtang] - what happens if you map your rdd using the WritableUtils clone() and then call take() does it work then? Maybe we should add a utility function to do this. A second issue is that Hadoop's types aren't themselves java serializeable. This is means Spark can't send them between nodes. For this one it's possible we could add automatic serializers but I'd have to think about this a bit. In general people tend to use the Hadoop classes when reading the data, but then they convert their data to other types before it has to be serialized. collect() and take() are an exception though (if people are exploring the data) so that might warrant trying to have more elegant behavior.", "created": "2014-03-02T21:52:37.582+0000"}, {"author": "Seng Tang", "body": "I was able to do a take() after using clone() (or doing any sort of map transformation like you mentioned) but when I try to do a collect() I get a classNotFound exception for the InputFormat class specified. I've attached the stack trace, and the same thing happens when I specify the Hadoop TextInputFormat class also.", "created": "2014-03-05T07:55:13.395+0000"}, {"author": "Patrick McFadin", "body": "--- sent from my phone On Mar 5, 2014 7:56 AM, \"Seng Tang (JIRA)\" <jira@spark-project.atlassian.net>", "created": "2014-03-05T08:08:28.389+0000"}, {"author": "Igor Berman", "body": "Hi Patrick, We spent some time to understand why data is \"corrupted\" while working with avro objects that wasn't copied at our layer, and yes I've seen the note above newHadoopApiFile, but note doesn't describe the consequences of not copying objects, at least not for people who came to spark without deep knowledge in hadoop format(I think there are plenty of those) Do you think that even using read-avro - transform - write-avro chain can be corrupted due to not copying avro objects in the beginning? e.g. we've seen that several objects has same data when they shouldn't, this was solved by deep-copy of whole avro-object If you permit me to suggest, it would be nice to have section about working with haddopRDD or newHadoopRDD which will advice on best practices and \"do-s\" and \"don't-s\" when working with hadoop files", "created": "2015-06-05T13:21:51.046+0000"}, {"author": "Christophe Bismuth", "body": "Hi, I've spent few hours trying to understand why I had *only duplicates of my last RDD item* after calling the {{collect}} API. I'm using Apache Spark 1.6.0 with Avro files stored in HDFS. Here is my workaround, hope it helps ...  public JavaRDD<GenericRecord> readJavaRDD(final JavaSparkContext sparkContext, final Schema schema, final String path) throws IOException { final Configuration configuration = new Configuration(); configuration.set(\"avro.schema.input.key\", schema.toString()); final JavaPairRDD<AvroKey<GenericRecord>, NullWritable> rdd = sparkContext.newAPIHadoopFile( path, classHelper.classOf(new AvroKeyInputFormat<GenericRecord>()), classHelper.classOf(new AvroKey<GenericRecord>()), NullWritable.class, configuration ); return rdd.map(tuple -> tuple._1().datum()) // see the trick below - a deep copy ain't required .map(record -> new GenericData.Record((GenericData.Record) record, false)); }", "created": "2016-09-19T14:16:56.615+0000"}], "num_comments": 8, "text": "Issue: SPARK-1018\nSummary: take and collect don't work on HadoopRDD\nDescription: I am reading a simple text file using hadoopFile as follows: var hrdd1 = sc.hadoopFile(\"/home/training/testdata.txt\",classOf[TextInputFormat], classOf[LongWritable], classOf[Text]) Testing using this simple text file: 001 this is line 1 002 this is line two 003 yet another line the data read is correct, as I can tell using println scala> hrdd1.foreach(println): (0,001 this is line 1) (19,002 this is line two) (40,003 yet another line) But neither collect nor take work properly. Take prints out the key (byte offset) of the last (non-existent) line repeatedly: scala> hrdd1.take(4): res146: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((61,), (61,), (61,)) Collect is even worse: it complains: java.io.NotSerializableException: org.apache.hadoop.io.LongWritable at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183) The problem appears to be the LongWritable in both cases, because if I map to a new RDD, converting the values from Text objects to strings, it works: scala> hrdd1.map(pair => (pair._1.toString,pair._2.toString)).take(4) res148: Array[(java.lang.String, java.lang.String)] = Array((0,001 this is line 1), (19,002 this is line two), (40,003 yet another line)) Seems to me either rdd.collect and rdd.take ought to handle non-serializable types gracefully, or hadoopFile should return a mapped RDD that converts the hadoop types into the appropriate serializable Java objects. (Or at very least the docs for the API should indicate that the usual RDD methods don't work on HadoopRDDs). BTW, this behavior is the same for both the old and new API versions of hadoopFile. It also is the same whether the file is from HDFS or a plain old text file.\n\nComments (8):\n1. Patrick McFadin: Hey [~dcarroll@cloudera.com] just wanted to clarify. When you create a Hadoop file and pass it specific types, it is assuming that the file is already encoded with those types (e.g. someone else wrote out as a LongWritable). I'm not sure you can just write your own text file and expect it to deserialize correctly as, e.g. a LongWritable. Usually people do this type of de-serialization in Spark itself if the data is just in textual format. E.g. they to sc.textFile(XX).map(line => line.split(\"\\t\")).map(arr => (arr(0), arr(1).toInt))\n2. Seng Tang: I've been encountering a similar issue. I've attached a custom Hadoop InputFormat (trivial example, basically a LineReader) along with the input data and the output logs. This is the behavior I'm seeing whether I use my custom InputFormat or Hadoop's TextInputFormat: scala> val data = sc.hadoopFile[LongWritable, Text, XmlInputFormat](\"test.txt\") data: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = HadoopRDD[0] at hadoopFile at <console>:13 scala> data.take(1) res0: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((311,This is line 1)) scala> data.take(2) res1: Array[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Array((311,This is line 2), (311,This is line 2)) As to Patrick's comment, the [K,V] returned by the InputFormats are wrapped inside [LongWritable, Text] already if that's what you mean.\n3. Diana Carroll: I'm not sure what you mean. The usual way of reading a text file is, as you say, sc.textFile. The code for that method is  def textFile(path: String, minSplits: Int = defaultMinSplits): RDD[String] = { hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minSplits).map(pair => pair._2.toString) }  As you can see, it's simply calling hadoopFile() and telling it to use the Hadoop class TextInputFormat to read the file. TextInputFormat does the job of reading in text input (using LineRecordReader), and outputting a key-value pair....the key being a long writable (the byte offset of the line in the file) and the value being Text (the line itself.) textFile then ignores the key (the byte offset), and returns an RDD with just the lines from the file. In other words, textFile() is simply a more specialized form of hadoopFile() that assumes that your input is line-oriented text. So yeah, sc.textFile() works great if your input is line-oriented. I believe you are not correct about hadoopFile(). It doesn't expect the input file to be serialized java objects. to the contrary, it doesn't have any expectations about the format at all...which is why you have to pass a pointer to a Hadoop InputFormat class. That's the class that actually reads the file. (Actually, technically, that's the class that calculates the input splits, and then creates a record RecordReader object to read the the data from the input split.) My goal in testing out sc.hadoopFile was actually because I wanted to explore using a different input format than the default TextInputFormat. (Hadoop includes several out of the box, including NLineInputFormat, CSVInputFormat, etc. I also have one I've written myself called ColumnInputFormat). TextInputFormat is pretty limited...each record is assumed to begin where the last one ended, and end with a new-line. My first test was the simplest...to call hadoopFile() with the same InputFormat class textFile() does: TextInputFormat. That's where I encountered this bug. Note that the LongWritable isn't what's supposed to be in the text file...it's the type of the key for the key-value pair generated by LineRecordReader. It's the byte offset in the file. So your code example doesn't work. The long int value I want isn't *in* the text, it's meta-data *about* the text, specifically the byte offset of the text in the file. textFile() throws that data away but what if I actually cared about it and wanted to use it? That's the test case I was exploring.\n4. Patrick McFadin: Hey [~dcarroll@cloudera.com] sorry for misunderstanding. You are running into two distinct issues here. The first is that Hadoop writables re-use objects in a way that makes iterating over sets of them (like Spark does) difficult. We have a fairly explicit warning in the docs for hadoopFile to make this clear. Copying from the scaladoc it says:  Note: Because Hadoop's RecordReader class re-uses the same Writable object for each record, directly caching the returned RDD will create many references to the same object. If you plan to directly cache Hadoop writable objects, you should first copy them using a map function.  Maybe we should improve this to say \"cache, collect, or take\" rather than just \"cache\". We actually write a whole feature to automatically clone these objects in a way that fixed this behavior, it used a cloning utility that Hadoop provides: http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/WritableUtils.html#clone(T, org.apache.hadoop.conf.Configuration) Unfortunately we found in practice that there are several Writable implementations that break when you clone them, so this didn't work either. So we had to rip this out of Spark and leave the confusing semantics: https://github.com/apache/incubator-spark/pull/502 [~sengtang] - what happens if you map your rdd using the WritableUtils clone() and then call take() does it work then? Maybe we should add a utility function to do this. A second issue is that Hadoop's types aren't themselves java serializeable. This is means Spark can't send them between nodes. For this one it's possible we could add automatic serializers but I'd have to think about this a bit. In general people tend to use the Hadoop classes when reading the data, but then they convert their data to other types before it has to be serialized. collect() and take() are an exception though (if people are exploring the data) so that might warrant trying to have more elegant behavior.\n5. Seng Tang: I was able to do a take() after using clone() (or doing any sort of map transformation like you mentioned) but when I try to do a collect() I get a classNotFound exception for the InputFormat class specified. I've attached the stack trace, and the same thing happens when I specify the Hadoop TextInputFormat class also.\n6. Patrick McFadin: --- sent from my phone On Mar 5, 2014 7:56 AM, \"Seng Tang (JIRA)\" <jira@spark-project.atlassian.net>\n7. Igor Berman: Hi Patrick, We spent some time to understand why data is \"corrupted\" while working with avro objects that wasn't copied at our layer, and yes I've seen the note above newHadoopApiFile, but note doesn't describe the consequences of not copying objects, at least not for people who came to spark without deep knowledge in hadoop format(I think there are plenty of those) Do you think that even using read-avro - transform - write-avro chain can be corrupted due to not copying avro objects in the beginning? e.g. we've seen that several objects has same data when they shouldn't, this was solved by deep-copy of whole avro-object If you permit me to suggest, it would be nice to have section about working with haddopRDD or newHadoopRDD which will advice on best practices and \"do-s\" and \"don't-s\" when working with hadoop files\n8. Christophe Bismuth: Hi, I've spent few hours trying to understand why I had *only duplicates of my last RDD item* after calling the {{collect}} API. I'm using Apache Spark 1.6.0 with Avro files stored in HDFS. Here is my workaround, hope it helps ...  public JavaRDD<GenericRecord> readJavaRDD(final JavaSparkContext sparkContext, final Schema schema, final String path) throws IOException { final Configuration configuration = new Configuration(); configuration.set(\"avro.schema.input.key\", schema.toString()); final JavaPairRDD<AvroKey<GenericRecord>, NullWritable> rdd = sparkContext.newAPIHadoopFile( path, classHelper.classOf(new AvroKeyInputFormat<GenericRecord>()), classHelper.classOf(new AvroKey<GenericRecord>()), NullWritable.class, configuration ); return rdd.map(tuple -> tuple._1().datum()) // see the trick below - a deep copy ain't required .map(record -> new GenericData.Record((GenericData.Record) record, false)); }", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.109804"}}
{"id": "3fcf33148030fbb7d327c628f076f604", "issue_key": "SPARK-1019", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "pyspark RDD take() throws NPE", "description": "I'm getting sporadic NPEs from pyspark that I can't narrow down, but I'm able to reproduce it consistently from the attached data file. If I delete any single line from the file, it works; but the file as is or larger (it's a snippet of a much larger log file) causes the problem. Printing the lines read works fine...but afterwards, I get the exception. Problem occurs with vanilla pyspark and IPython, but NOT with the scala spark shell. sc.textFile(\"testlog13\").take(5) (does not seem to matter how many lines I take). In [32]: sc.textFile(\"testlog16\").take(5) 14/01/09 14:16:45 INFO MemoryStore: ensureFreeSpace(69808) called with curMem=1185875, maxMem=342526525 14/01/09 14:16:45 INFO MemoryStore: Block broadcast_16 stored as values to memory (estimated size 68.2 KB, free 325.5 MB) 14/01/09 14:16:45 INFO FileInputFormat: Total input paths to process : 1 14/01/09 14:16:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:288 14/01/09 14:16:45 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:288) with 1 output partitions (allowLocal=true) 14/01/09 14:16:45 INFO DAGScheduler: Final stage: Stage 23 (runJob at PythonRDD.scala:288) 14/01/09 14:16:45 INFO DAGScheduler: Parents of final stage: List() 14/01/09 14:16:45 INFO DAGScheduler: Missing parents: List() 14/01/09 14:16:45 INFO DAGScheduler: Computing the requested partition locally 14/01/09 14:16:45 INFO HadoopRDD: Input split: file:/home/training/testlog16:0+61124 14/01/09 14:16:45 INFO PythonRDD: Times: total = 14, boot = 1, init = 3, finish = 10 14/01/09 14:16:45 INFO SparkContext: Job finished: runJob at PythonRDD.scala:288, took 0.021146108 s Out[32]: [u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET theme.css HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET KBDOC-00076.html HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET theme.css HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" '] In [33]: Exception in thread \"stdin writer for python\" java.lang.NullPointerException at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:54) at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:60) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:246) at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:275) at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:227) at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:195) at java.io.DataInputStream.read(DataInputStream.java:100) at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:167) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:150) at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27) at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:400) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399) at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:98)", "reporter": "Diana Carroll", "assignee": "Josh Rosen", "created": "2014-01-09T14:20:36.000+0000", "updated": "2014-09-12T20:26:06.000+0000", "resolved": "2014-03-12T23:18:35.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Diana Carroll", "body": "I'm getting this reliably again using the .9 rc build, using a different dataset. Will attach for ease of reproduction. (zip codes with latitude and longitude)", "created": "2014-01-30T06:25:41.910+0000"}, {"author": "Patrick McFadin", "body": "Hey Diana, I just tried to reproduce this from a fresh spark build and couldn't. If you do a clean checkout of spark like I did below do you get the error?  cd /tmp git clone https://git-wip-us.apache.org/repos/asf/incubator-spark.git -b branch-0.9 cd incubator-spark sbt/sbt assembly wget https://spark-project.atlassian.net/secure/attachment/12402/testlog13 ./bin/pyspark >>> sc.textFile(\"testlog13\").take(5) [u'165.32.101.206 - 8 [15/Sep/2013:23:59:50 +0100] \"GET theme.css HTTP/1.0\" 200 17446 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET theme.css HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET KBDOC-00076.html HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET theme.css HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ']", "created": "2014-01-30T11:09:56.260+0000"}, {"author": "Diana Carroll", "body": "I'm not getting the problem from that sample file -- it was sporadic when I was testing. But I'm now getting it consistently from the latlon.tsv file I just uploaded (a tab-delimited list of zip codes with latitude and longitude). I just did a fresh build from your instructions and am still getting the problem In case it is helpful I will attach a log file with debugging enabled. (Doesn't seem to give much more info I'm afraid.) I notice that the actual exception is occurring in Hadoop LineRecordReader. Maybe my issue has something to do with the version of Hadoop I'm using? I've tested this in CDH5b1 and the (yet to be released) CDH5b2.", "created": "2014-01-30T16:20:40.812+0000"}, {"author": "Diana Carroll", "body": "debug log", "created": "2014-01-30T16:21:42.218+0000"}, {"author": "Diana Carroll", "body": "I just thought to test the latlon.tsv file using HDFS instead of the local file system. That works fine. It is only loading it from the local file system that is giving me problems. I also note that the take() is successful...it displays the requested records and *then* throws NPE for whatever that's worth.", "created": "2014-01-30T17:04:15.553+0000"}, {"author": "Josh Rosen", "body": "Thanks for reporting this and for including sample data. I was able to reproduce this bug using the latest master branch, Hadoop 1.0.4, and a local pyspark shell:  >>> sc.textFile(\"latlon.tsv\").take(1) 14/01/30 18:35:00 INFO MemoryStore: ensureFreeSpace(34262) called with curMem=34238, maxMem=311387750 14/01/30 18:35:00 INFO MemoryStore: Block broadcast_1 stored as values to memory (estimated size 33.5 KB, free 296.9 MB) 14/01/30 18:35:00 INFO FileInputFormat: Total input paths to process : 1 14/01/30 18:35:00 INFO SparkContext: Starting job: take at <stdin>:1 14/01/30 18:35:00 INFO DAGScheduler: Got job 1 (take at <stdin>:1) with 1 output partitions (allowLocal=true) 14/01/30 18:35:00 INFO DAGScheduler: Final stage: Stage 1 (take at <stdin>:1) 14/01/30 18:35:00 INFO DAGScheduler: Parents of final stage: List() 14/01/30 18:35:00 INFO DAGScheduler: Missing parents: List() 14/01/30 18:35:00 INFO DAGScheduler: Computing the requested partition locally 14/01/30 18:35:00 INFO HadoopRDD: Input split: file:/Users/joshrosen/Documents/spark/spark/latlon.tsv:0+1127316 14/01/30 18:35:00 INFO PythonRDD: Times: total = 39, boot = 2, init = 37, finish = 0 14/01/30 18:35:00 INFO SparkContext: Job finished: take at <stdin>:1, took 0.043407 s [u'00210\\t43.005895\\t-71.013202'] >>> Exception in thread \"stdin writer for python\" java.lang.NullPointerException at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48) at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214) at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:237) at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:189) at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158) at java.io.DataInputStream.read(DataInputStream.java:100) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:134) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:164) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:149) at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:232) at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:85)  I used this stacktrace with [BrainLeg|http://brainleg.com/], a structural stacktrace search engine, and found a few other projects that have run into similar NPEs: - https://issues.apache.org/jira/browse/HCATALOG-626 - https://github.com/facebook/presto/pull/950 The exception seems to be dependent on the size of the input file, since the job runs without any exceptions if I run on a small sample of lines from the original input. I'll check out those links to see how those other projects may have fixed this issue. I'll also try running under a debugger to see if I can track down where the null is being introduced.", "created": "2014-01-30T18:56:05.530+0000"}, {"author": "Josh Rosen", "body": "Interestingly, I can only reproduce the NPE when calling an action that only reads a portion of the input file, like first() or take(). If I call  sc.textFile(\"latlon.tsv\").collect()[0]  which forces the entire file to be read, then I don't see any exception. My current theory is that there's a race condition between the Python daemon's stdin writer consuming its iterator and the job completion callbacks closing that iterator. Imagine that I run {{sc.textFile().take(n )}} in Scala Spark. The data flow here is entirely pull-based and we should only read as much of the file as is necessary to return the first {{n}} records. When the job completes, the Hadoop iterator is closed using a job completion callback. Now, consider what happens when I run this locally using PySpark. PySpark's data flow is a mixture of push and pull with backpressure: the stdin writer thread pulls records from Hadoop or cached RDDs and pushes them into the Python worker process (where network buffer sizes provide backpressure), which pushes its output records back to Java. If the job finishes before the Python worker has consumed all of its input, the job completion callbacks might close the Hadoop iterator while the PySpark stdin writer thread is still attempting to consume it, leading to a NullPointerException because it calls a closed data stream. This theory explains why the above {{collect()}} call didn't throw a NPE: since the entire input needed to be consumed before the job could complete, the stdin writer thread would never attempt to read from a closed stream. It also explains why you didn't see NPEs for small inputs: if the input is small enough, the Python worker will have enough time and network buffer space to receive the entire input before the job completes, even if it only iterates over its first few input records. I'll try tracing through the execution in more detail to verify if this is what's causing the exception. When fixing this, we'll have to be careful to avoid introducing race conditions. If we simply added job completion callbacks to stop the PythonRDD stdin writer thread, we'd have to worry about a race between that callback and the Hadoop iterator callback. Maybe we could guarantee that completion callbacks are called in reverse order of registration. It looks like the underlying bug might have been introduced in https://github.com/apache/incubator-spark/commit/b9d6783f36d527f5082bf13a4ee6fd108e97795c, which optimized PySpark's {{take()}} to push the limit into {{mapPartitions()}} calls to avoid computing the entire first partition. That commit added a try-catch block that seems to imply that IOExceptions could occur if the Python worker didn't consume its entire input:  } catch { case e: IOException => // This can happen for legitimate reasons if the Python code stops returning data before we are done // passing elements through, e.g., for take(). Just log a message to say it happened. logInfo(\"stdin writer to Python finished early\") logDebug(\"stdin writer to Python finished early\", e) }  This seems a little fishy, since it's implicitly detecting the Python worker process being finished rather than using an explicit signal from the worker to PythonRDD. Also, IOException is too broad of an exception, which was the root cause of SPARK-1025, so we should remove this catch.", "created": "2014-01-30T20:19:37.288+0000"}, {"author": "Patrick McFadin", "body": "Okay I spent some time today playing with this. Josh I confirmed that the issue is what you suspected - the callback for the HadoopRDD is getting called and invalidating the input stream. I've proposed a patch here that I found fixed the issue for me locally. I think my patch still has a potential race, but it least decreases the odds of this exception substantially so I'd say it's strictly better than what's there now.", "created": "2014-03-09T21:07:26.642+0000"}, {"author": "Patrick McFadin", "body": "[~dcarroll@cloudera.com] If you could test this patch that would be great.", "created": "2014-03-09T21:08:34.985+0000"}, {"author": "Diana Carroll", "body": "Will do. (Just back from vacation so probably not today!) On Mon, Mar 10, 2014 at 12:09 AM, Patrick Wendell (JIRA) <", "created": "2014-03-10T08:32:28.399+0000"}, {"author": "Patrick McFadin", "body": "Forgot to link the associated pull request: https://github.com/apache/spark/pull/112", "created": "2014-03-10T14:42:48.277+0000"}, {"author": "Diana Carroll", "body": "I tested the patch and it works! I tested it side by side with unpatched 0.9. Unpatched = NPE in the latlon.tsv test file; patched = no NPE. Thanks.", "created": "2014-03-11T07:15:38.712+0000"}, {"author": "Patrick McFadin", "body": "I think the fixed proposed here is the best we can do given the current cancellation support in Spark. [~joshrosen] please feel free to re-open this if you see a nicer solution. There may very well be one. https://github.com/apache/spark/pull/112/files", "created": "2014-03-12T23:18:36.027+0000"}, {"author": "Josh Rosen", "body": "I'm still skeptical of the {{catch IOException}} code, since it's been involved in two bugs now, so we should probably revisit that at some point, but this seems like an okay fix for now for the NPE issues.", "created": "2014-03-16T11:17:18.445+0000"}], "num_comments": 14, "text": "Issue: SPARK-1019\nSummary: pyspark RDD take() throws NPE\nDescription: I'm getting sporadic NPEs from pyspark that I can't narrow down, but I'm able to reproduce it consistently from the attached data file. If I delete any single line from the file, it works; but the file as is or larger (it's a snippet of a much larger log file) causes the problem. Printing the lines read works fine...but afterwards, I get the exception. Problem occurs with vanilla pyspark and IPython, but NOT with the scala spark shell. sc.textFile(\"testlog13\").take(5) (does not seem to matter how many lines I take). In [32]: sc.textFile(\"testlog16\").take(5) 14/01/09 14:16:45 INFO MemoryStore: ensureFreeSpace(69808) called with curMem=1185875, maxMem=342526525 14/01/09 14:16:45 INFO MemoryStore: Block broadcast_16 stored as values to memory (estimated size 68.2 KB, free 325.5 MB) 14/01/09 14:16:45 INFO FileInputFormat: Total input paths to process : 1 14/01/09 14:16:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:288 14/01/09 14:16:45 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:288) with 1 output partitions (allowLocal=true) 14/01/09 14:16:45 INFO DAGScheduler: Final stage: Stage 23 (runJob at PythonRDD.scala:288) 14/01/09 14:16:45 INFO DAGScheduler: Parents of final stage: List() 14/01/09 14:16:45 INFO DAGScheduler: Missing parents: List() 14/01/09 14:16:45 INFO DAGScheduler: Computing the requested partition locally 14/01/09 14:16:45 INFO HadoopRDD: Input split: file:/home/training/testlog16:0+61124 14/01/09 14:16:45 INFO PythonRDD: Times: total = 14, boot = 1, init = 3, finish = 10 14/01/09 14:16:45 INFO SparkContext: Job finished: runJob at PythonRDD.scala:288, took 0.021146108 s Out[32]: [u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET theme.css HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET KBDOC-00076.html HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET theme.css HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" '] In [33]: Exception in thread \"stdin writer for python\" java.lang.NullPointerException at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:54) at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:60) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:246) at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:275) at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:227) at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:195) at java.io.DataInputStream.read(DataInputStream.java:100) at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:167) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:150) at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27) at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:400) at scala.collection.Iterator$class.foreach(Iterator.scala:772) at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399) at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:98)\n\nComments (14):\n1. Diana Carroll: I'm getting this reliably again using the .9 rc build, using a different dataset. Will attach for ease of reproduction. (zip codes with latitude and longitude)\n2. Patrick McFadin: Hey Diana, I just tried to reproduce this from a fresh spark build and couldn't. If you do a clean checkout of spark like I did below do you get the error?  cd /tmp git clone https://git-wip-us.apache.org/repos/asf/incubator-spark.git -b branch-0.9 cd incubator-spark sbt/sbt assembly wget https://spark-project.atlassian.net/secure/attachment/12402/testlog13 ./bin/pyspark >>> sc.textFile(\"testlog13\").take(5) [u'165.32.101.206 - 8 [15/Sep/2013:23:59:50 +0100] \"GET theme.css HTTP/1.0\" 200 17446 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET KBDOC-00087.html HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] \"GET theme.css HTTP/1.0\" 200 8681 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET KBDOC-00076.html HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ', u'182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] \"GET theme.css HTTP/1.0\" 200 17546 \"http://www.loudacre.com\" \"Loudacre CSR Browser\" ']\n3. Diana Carroll: I'm not getting the problem from that sample file -- it was sporadic when I was testing. But I'm now getting it consistently from the latlon.tsv file I just uploaded (a tab-delimited list of zip codes with latitude and longitude). I just did a fresh build from your instructions and am still getting the problem In case it is helpful I will attach a log file with debugging enabled. (Doesn't seem to give much more info I'm afraid.) I notice that the actual exception is occurring in Hadoop LineRecordReader. Maybe my issue has something to do with the version of Hadoop I'm using? I've tested this in CDH5b1 and the (yet to be released) CDH5b2.\n4. Diana Carroll: debug log\n5. Diana Carroll: I just thought to test the latlon.tsv file using HDFS instead of the local file system. That works fine. It is only loading it from the local file system that is giving me problems. I also note that the take() is successful...it displays the requested records and *then* throws NPE for whatever that's worth.\n6. Josh Rosen: Thanks for reporting this and for including sample data. I was able to reproduce this bug using the latest master branch, Hadoop 1.0.4, and a local pyspark shell:  >>> sc.textFile(\"latlon.tsv\").take(1) 14/01/30 18:35:00 INFO MemoryStore: ensureFreeSpace(34262) called with curMem=34238, maxMem=311387750 14/01/30 18:35:00 INFO MemoryStore: Block broadcast_1 stored as values to memory (estimated size 33.5 KB, free 296.9 MB) 14/01/30 18:35:00 INFO FileInputFormat: Total input paths to process : 1 14/01/30 18:35:00 INFO SparkContext: Starting job: take at <stdin>:1 14/01/30 18:35:00 INFO DAGScheduler: Got job 1 (take at <stdin>:1) with 1 output partitions (allowLocal=true) 14/01/30 18:35:00 INFO DAGScheduler: Final stage: Stage 1 (take at <stdin>:1) 14/01/30 18:35:00 INFO DAGScheduler: Parents of final stage: List() 14/01/30 18:35:00 INFO DAGScheduler: Missing parents: List() 14/01/30 18:35:00 INFO DAGScheduler: Computing the requested partition locally 14/01/30 18:35:00 INFO HadoopRDD: Input split: file:/Users/joshrosen/Documents/spark/spark/latlon.tsv:0+1127316 14/01/30 18:35:00 INFO PythonRDD: Times: total = 39, boot = 2, init = 37, finish = 0 14/01/30 18:35:00 INFO SparkContext: Job finished: take at <stdin>:1, took 0.043407 s [u'00210\\t43.005895\\t-71.013202'] >>> Exception in thread \"stdin writer for python\" java.lang.NullPointerException at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48) at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214) at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:237) at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:189) at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158) at java.io.DataInputStream.read(DataInputStream.java:100) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:134) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133) at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:164) at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:149) at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:232) at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:85)  I used this stacktrace with [BrainLeg|http://brainleg.com/], a structural stacktrace search engine, and found a few other projects that have run into similar NPEs: - https://issues.apache.org/jira/browse/HCATALOG-626 - https://github.com/facebook/presto/pull/950 The exception seems to be dependent on the size of the input file, since the job runs without any exceptions if I run on a small sample of lines from the original input. I'll check out those links to see how those other projects may have fixed this issue. I'll also try running under a debugger to see if I can track down where the null is being introduced.\n7. Josh Rosen: Interestingly, I can only reproduce the NPE when calling an action that only reads a portion of the input file, like first() or take(). If I call  sc.textFile(\"latlon.tsv\").collect()[0]  which forces the entire file to be read, then I don't see any exception. My current theory is that there's a race condition between the Python daemon's stdin writer consuming its iterator and the job completion callbacks closing that iterator. Imagine that I run {{sc.textFile().take(n )}} in Scala Spark. The data flow here is entirely pull-based and we should only read as much of the file as is necessary to return the first {{n}} records. When the job completes, the Hadoop iterator is closed using a job completion callback. Now, consider what happens when I run this locally using PySpark. PySpark's data flow is a mixture of push and pull with backpressure: the stdin writer thread pulls records from Hadoop or cached RDDs and pushes them into the Python worker process (where network buffer sizes provide backpressure), which pushes its output records back to Java. If the job finishes before the Python worker has consumed all of its input, the job completion callbacks might close the Hadoop iterator while the PySpark stdin writer thread is still attempting to consume it, leading to a NullPointerException because it calls a closed data stream. This theory explains why the above {{collect()}} call didn't throw a NPE: since the entire input needed to be consumed before the job could complete, the stdin writer thread would never attempt to read from a closed stream. It also explains why you didn't see NPEs for small inputs: if the input is small enough, the Python worker will have enough time and network buffer space to receive the entire input before the job completes, even if it only iterates over its first few input records. I'll try tracing through the execution in more detail to verify if this is what's causing the exception. When fixing this, we'll have to be careful to avoid introducing race conditions. If we simply added job completion callbacks to stop the PythonRDD stdin writer thread, we'd have to worry about a race between that callback and the Hadoop iterator callback. Maybe we could guarantee that completion callbacks are called in reverse order of registration. It looks like the underlying bug might have been introduced in https://github.com/apache/incubator-spark/commit/b9d6783f36d527f5082bf13a4ee6fd108e97795c, which optimized PySpark's {{take()}} to push the limit into {{mapPartitions()}} calls to avoid computing the entire first partition. That commit added a try-catch block that seems to imply that IOExceptions could occur if the Python worker didn't consume its entire input:  } catch { case e: IOException => // This can happen for legitimate reasons if the Python code stops returning data before we are done // passing elements through, e.g., for take(). Just log a message to say it happened. logInfo(\"stdin writer to Python finished early\") logDebug(\"stdin writer to Python finished early\", e) }  This seems a little fishy, since it's implicitly detecting the Python worker process being finished rather than using an explicit signal from the worker to PythonRDD. Also, IOException is too broad of an exception, which was the root cause of SPARK-1025, so we should remove this catch.\n8. Patrick McFadin: Okay I spent some time today playing with this. Josh I confirmed that the issue is what you suspected - the callback for the HadoopRDD is getting called and invalidating the input stream. I've proposed a patch here that I found fixed the issue for me locally. I think my patch still has a potential race, but it least decreases the odds of this exception substantially so I'd say it's strictly better than what's there now.\n9. Patrick McFadin: [~dcarroll@cloudera.com] If you could test this patch that would be great.\n10. Diana Carroll: Will do. (Just back from vacation so probably not today!) On Mon, Mar 10, 2014 at 12:09 AM, Patrick Wendell (JIRA) <", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.109804"}}
{"id": "3054af2d31b4a1fd9fb8ae3b75e929d0", "issue_key": "SPARK-1020", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "SparkListener interfaces should not expose internal types/objects", "description": "", "reporter": "Patrick McFadin", "assignee": null, "created": "2014-01-09T22:31:40.000+0000", "updated": "2014-03-17T10:20:37.000+0000", "resolved": "2014-03-17T10:20:25.000+0000", "labels": [], "components": [], "comments": [{"author": "Radosław Sypeń", "body": "Hi Patrick, Could you please describe this issue: 1. Which Spark version is affected? 2. What types/objects are exposed? Thanks in advance.", "created": "2014-02-04T00:48:51.915+0000"}, {"author": "Reynold Xin", "body": "1. In the latest versions of Spark, including 0.8, 0.9, and the latest master branch. 2. ActiveJob, StageInfo", "created": "2014-02-04T23:44:22.282+0000"}, {"author": "Patrick McFadin", "body": "I think initially the XXXInfo classes were intended to be for downstream consumption . But now they point to internal data types such as Stage, RDD, etc. We should either make XXInfo not have pointers to the internal objects, or create a new class that stores data for the stage, and have this one not contain any pointers to internal objects.", "created": "2014-02-04T23:47:28.042+0000"}, {"author": "Radosław Sypeń", "body": "I think that it could be done by extending constructors of *Info classes with required fields instead of creating new classes which in terms would lead to more complexity and types. I think keeping things small is better. I also noticed that event class names could be refactored Trait SparkListenerEvents should be named SparkListenerEvent or just SparkEvent. Event classes for SparkListener should end with \"Event\" and not start with trait name, for example: JobEndEvent instead of SparkListenerJobEnd (it is too verbose).", "created": "2014-02-05T23:43:26.579+0000"}], "num_comments": 4, "text": "Issue: SPARK-1020\nSummary: SparkListener interfaces should not expose internal types/objects\n\nComments (4):\n1. Radosław Sypeń: Hi Patrick, Could you please describe this issue: 1. Which Spark version is affected? 2. What types/objects are exposed? Thanks in advance.\n2. Reynold Xin: 1. In the latest versions of Spark, including 0.8, 0.9, and the latest master branch. 2. ActiveJob, StageInfo\n3. Patrick McFadin: I think initially the XXXInfo classes were intended to be for downstream consumption . But now they point to internal data types such as Stage, RDD, etc. We should either make XXInfo not have pointers to the internal objects, or create a new class that stores data for the stage, and have this one not contain any pointers to internal objects.\n4. Radosław Sypeń: I think that it could be done by extending constructors of *Info classes with required fields instead of creating new classes which in terms would lead to more complexity and types. I think keeping things small is better. I also noticed that event class names could be refactored Trait SparkListenerEvents should be named SparkListenerEvent or just SparkEvent. Event classes for SparkListener should end with \"Event\" and not start with trait name, for example: JobEndEvent instead of SparkListenerJobEnd (it is too verbose).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.109804"}}
{"id": "f936ef907a5031021a1d049cdc081808", "issue_key": "SPARK-1021", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "sortByKey() launches a cluster job when it shouldn't", "description": "The sortByKey() method is listed as a transformation, not an action, in the documentation. But it launches a cluster job regardless. http://spark.incubator.apache.org/docs/latest/scala-programming-guide.html Some discussion on the mailing list suggested that this is a problem with the rdd.count() call inside Partitioner.scala's rangeBounds method. https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/Partitioner.scala#L102 Josh Rosen suggests that rangeBounds should be made into a lazy variable:  I wonder whether making RangePartitoner .rangeBounds into a lazy val would fix this (https://github.com/apache/incubator-spark/blob/6169fe14a140146602fb07cfcd13eee6efad98f9/core/src/main/scala/org/apache/spark/Partitioner.scala#L95). We'd need to make sure that rangeBounds() is never called before an action is performed. This could be tricky because it's called in the RangePartitioner.equals() method. Maybe it's sufficient to just compare the number of partitions, the ids of the RDDs used to create the RangePartitioner, and the sort ordering. This still supports the case where I range-partition one RDD and pass the same partitioner to a different RDD. It breaks support for the case where two range partitioners created on different RDDs happened to have the same rangeBounds(), but it seems unlikely that this would really harm performance since it's probably unlikely that the range partitioners are equal by chance.  Can we please make this happen? I'll send a PR on GitHub to start the discussion and testing.", "reporter": "Andrew Ash", "assignee": "Erik Erlandson", "created": "2014-01-09T22:47:47.000+0000", "updated": "2015-11-12T05:19:43.000+0000", "resolved": "2015-11-03T15:31:45.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "I'm observing the same behavior in 0.9.0 and would add to the affects version/s field but I can't edit this task with my permissions. You'd think the reporter of a bug would be able to edit it!", "created": "2014-02-06T00:22:31.978+0000"}, {"author": "Reynold Xin", "body": "I just granted you permission, [~ash211].", "created": "2014-02-06T10:36:03.402+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Note that if we do this, we'll need a similar fix in Python, which may be trickier.", "created": "2014-04-07T17:16:56.714+0000"}, {"author": "Andrew Ash", "body": "I tried making the change of making that val lazy and a cluster job was still launched, so it'll need to be a more involved fix than just adding lazy. The PR for that is probably somewhere on an old github repo.", "created": "2014-04-09T14:25:40.496+0000"}, {"author": "Andrew Ash", "body": "https://github.com/ash211/spark/commit/a62e828234d5b69585495593730032f2877932ae", "created": "2014-04-09T14:32:16.705+0000"}, {"author": "Marcelo Masiero Vanzin", "body": "I actually played with the idea and just turning the {{rangeBounds}} variable into a lazy one doesn't work. That makes the variable be evaluated only when the transformation is executed on the worker nodes; at that point, you can't execute actions (which are needed to compute {{rangeBounds}}). One way to work around this would be to have something be evaluated on the RDDs when the scheduler walks the graph before submitting jobs to the workers. I'm not aware of such functionality in the code, though. Or maybe there's something cleaner that can be done here?", "created": "2014-04-09T20:58:08.850+0000"}, {"author": "Erik Erlandson", "body": "I deferred the compute of the partition bounds this way, and seems to work properly in my testing and the unit tests: https://github.com/erikerlandson/spark/compare/erikerlandson:rdd_drop_master...spark-1021", "created": "2014-07-30T23:58:55.886+0000"}, {"author": "Apache Spark", "body": "User 'erikerlandson' has created a pull request for this issue: https://github.com/apache/spark/pull/1689", "created": "2014-07-31T14:31:07.892+0000"}, {"author": "Reynold Xin", "body": "This was reverted due to the commit breaking correlationoptimizer14 https://github.com/apache/spark/pull/1689#issuecomment-57106886", "created": "2014-09-29T02:01:08.312+0000"}, {"author": "Reynold Xin", "body": "@Erik Erlandson any update on this?", "created": "2014-10-14T22:28:31.515+0000"}, {"author": "Apache Spark", "body": "User 'erikerlandson' has created a pull request for this issue: https://github.com/apache/spark/pull/3079", "created": "2014-11-03T21:50:49.016+0000"}, {"author": "Xuefu Zhang", "body": "This problem also occurred on Hive on Spark (HIVE-9370. Could we take this forward?", "created": "2015-01-17T02:45:35.196+0000"}, {"author": "Ilya Ganelin", "body": "I'd be happy to look into this and https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-4514 .", "created": "2015-04-24T15:29:43.878+0000"}], "num_comments": 13, "text": "Issue: SPARK-1021\nSummary: sortByKey() launches a cluster job when it shouldn't\nDescription: The sortByKey() method is listed as a transformation, not an action, in the documentation. But it launches a cluster job regardless. http://spark.incubator.apache.org/docs/latest/scala-programming-guide.html Some discussion on the mailing list suggested that this is a problem with the rdd.count() call inside Partitioner.scala's rangeBounds method. https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/Partitioner.scala#L102 Josh Rosen suggests that rangeBounds should be made into a lazy variable:  I wonder whether making RangePartitoner .rangeBounds into a lazy val would fix this (https://github.com/apache/incubator-spark/blob/6169fe14a140146602fb07cfcd13eee6efad98f9/core/src/main/scala/org/apache/spark/Partitioner.scala#L95). We'd need to make sure that rangeBounds() is never called before an action is performed. This could be tricky because it's called in the RangePartitioner.equals() method. Maybe it's sufficient to just compare the number of partitions, the ids of the RDDs used to create the RangePartitioner, and the sort ordering. This still supports the case where I range-partition one RDD and pass the same partitioner to a different RDD. It breaks support for the case where two range partitioners created on different RDDs happened to have the same rangeBounds(), but it seems unlikely that this would really harm performance since it's probably unlikely that the range partitioners are equal by chance.  Can we please make this happen? I'll send a PR on GitHub to start the discussion and testing.\n\nComments (13):\n1. Andrew Ash: I'm observing the same behavior in 0.9.0 and would add to the affects version/s field but I can't edit this task with my permissions. You'd think the reporter of a bug would be able to edit it!\n2. Reynold Xin: I just granted you permission, [~ash211].\n3. Matei Alexandru Zaharia: Note that if we do this, we'll need a similar fix in Python, which may be trickier.\n4. Andrew Ash: I tried making the change of making that val lazy and a cluster job was still launched, so it'll need to be a more involved fix than just adding lazy. The PR for that is probably somewhere on an old github repo.\n5. Andrew Ash: https://github.com/ash211/spark/commit/a62e828234d5b69585495593730032f2877932ae\n6. Marcelo Masiero Vanzin: I actually played with the idea and just turning the {{rangeBounds}} variable into a lazy one doesn't work. That makes the variable be evaluated only when the transformation is executed on the worker nodes; at that point, you can't execute actions (which are needed to compute {{rangeBounds}}). One way to work around this would be to have something be evaluated on the RDDs when the scheduler walks the graph before submitting jobs to the workers. I'm not aware of such functionality in the code, though. Or maybe there's something cleaner that can be done here?\n7. Erik Erlandson: I deferred the compute of the partition bounds this way, and seems to work properly in my testing and the unit tests: https://github.com/erikerlandson/spark/compare/erikerlandson:rdd_drop_master...spark-1021\n8. Apache Spark: User 'erikerlandson' has created a pull request for this issue: https://github.com/apache/spark/pull/1689\n9. Reynold Xin: This was reverted due to the commit breaking correlationoptimizer14 https://github.com/apache/spark/pull/1689#issuecomment-57106886\n10. Reynold Xin: @Erik Erlandson any update on this?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.109804"}}
{"id": "af4052e833e5a01e04d36c903ad97501", "issue_key": "SPARK-1022", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add unit tests for kafka streaming", "description": "It would be nice if we could add unit tests to verify elements of kafka's stream. Right now we do integration tests only which makes it hard to upgrade versions of kafka. The place to start here would be to look at how kafka tests itself and see if the functionality can be exposed to third party users.", "reporter": "Patrick Wendell", "assignee": "Saisai Shao", "created": "2014-01-11T09:43:02.000+0000", "updated": "2015-08-06T00:21:53.000+0000", "resolved": "2014-08-06T06:43:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Apache Spark", "body": "User 'tdas' has created a pull request for this issue: [https://github.com/apache/spark/pull/557|https://github.com/apache/spark/pull/557]", "created": "2014-07-20T00:05:37.537+0000"}, {"author": "Apache Spark", "body": "User 'jerryshao' has created a pull request for this issue: https://github.com/apache/spark/pull/1751", "created": "2014-08-03T09:21:44.833+0000"}, {"author": "Apache Spark", "body": "User 'tdas' has created a pull request for this issue: https://github.com/apache/spark/pull/1797", "created": "2014-08-06T01:41:46.112+0000"}, {"author": "Patrick Wendell", "body": "There was a follow up to this issue: https://github.com/apache/spark/pull/1797", "created": "2014-08-06T06:43:48.879+0000"}, {"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/1804", "created": "2014-08-06T08:30:20.954+0000"}, {"author": "Anuj Ojha", "body": "[~tdas] and [~jerryshao2015] I was working on writing Integration test for my application, which reads data from kafka, performs transformation on the data and then writes the Dstream back to other kafka topic. To do this I had to get KafkaTestUtils and some other dependent classes locally as I needed embedded zookeeper and kafa queue. I also had to modify sendMessage to accept my specific type of ket and value. I was wondering if it is possible to make the KafkaTestUtils generic? so that others can use it for in-memory unit/integration testing. I have created generic sendMessage method hopefully it's of some use.  class KafkaTestUtils [T,U]{ ....... // Kafka producer private var producer: Producer[T, U] = _ .......... def sendMessages(topic: String, messageToFreq: JMap[T, U]): Unit = { import scala.collection.JavaConversions._ producer = new Producer[T, U](new ProducerConfig(producerConfiguration)) for ((k,v) <- messageToFreq) { var message = new KeyedMessage(topic, k,v) producer.send(message) } producer.close() producer = null } ......... }  I also had to change producer properties to provide key and value serialization class. It might be worth making possible to set these values from user test. I did not create JIRA for the request as I wanted to see if this is even a acceptable request. Please let me know.", "created": "2015-08-05T21:46:05.448+0000"}, {"author": "Saisai Shao", "body": "Hi [~anujojha], {{KafkaTestUtils}} is used for Kafka and Spark integration test, it is not public to users to use it, so the API design is not so generic for everyone use. I think Spark itself will not provide such functionality for user to do integration test, if you want to do such thing, from my point it would be better for you to do it yourself, not replying on Spark.", "created": "2015-08-06T00:21:53.589+0000"}], "num_comments": 7, "text": "Issue: SPARK-1022\nSummary: Add unit tests for kafka streaming\nDescription: It would be nice if we could add unit tests to verify elements of kafka's stream. Right now we do integration tests only which makes it hard to upgrade versions of kafka. The place to start here would be to look at how kafka tests itself and see if the functionality can be exposed to third party users.\n\nComments (7):\n1. Apache Spark: User 'tdas' has created a pull request for this issue: [https://github.com/apache/spark/pull/557|https://github.com/apache/spark/pull/557]\n2. Apache Spark: User 'jerryshao' has created a pull request for this issue: https://github.com/apache/spark/pull/1751\n3. Apache Spark: User 'tdas' has created a pull request for this issue: https://github.com/apache/spark/pull/1797\n4. Patrick Wendell: There was a follow up to this issue: https://github.com/apache/spark/pull/1797\n5. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/1804\n6. Anuj Ojha: [~tdas] and [~jerryshao2015] I was working on writing Integration test for my application, which reads data from kafka, performs transformation on the data and then writes the Dstream back to other kafka topic. To do this I had to get KafkaTestUtils and some other dependent classes locally as I needed embedded zookeeper and kafa queue. I also had to modify sendMessage to accept my specific type of ket and value. I was wondering if it is possible to make the KafkaTestUtils generic? so that others can use it for in-memory unit/integration testing. I have created generic sendMessage method hopefully it's of some use.  class KafkaTestUtils [T,U]{ ....... // Kafka producer private var producer: Producer[T, U] = _ .......... def sendMessages(topic: String, messageToFreq: JMap[T, U]): Unit = { import scala.collection.JavaConversions._ producer = new Producer[T, U](new ProducerConfig(producerConfiguration)) for ((k,v) <- messageToFreq) { var message = new KeyedMessage(topic, k,v) producer.send(message) } producer.close() producer = null } ......... }  I also had to change producer properties to provide key and value serialization class. It might be worth making possible to set these values from user test. I did not create JIRA for the request as I wanted to see if this is even a acceptable request. Please let me know.\n7. Saisai Shao: Hi [~anujojha], {{KafkaTestUtils}} is used for Kafka and Spark integration test, it is not public to users to use it, so the API design is not so generic for everyone use. I think Spark itself will not provide such functionality for user to do integration test, if you want to do such thing, from my point it would be better for you to do it yourself, not replying on Spark.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "6a1d82e7e60def570d746d40ff32c13c", "issue_key": "SPARK-1023", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Remove Thread.sleep(5000) from TaskSchedulerImpl", "description": "This causes the unit tests to take super long. We should figure out why this exists and see if we can lower it or do something smarter.", "reporter": "Patrick Wendell", "assignee": null, "created": "2014-01-11T16:02:27.000+0000", "updated": "2014-11-06T17:42:16.000+0000", "resolved": "2014-11-06T17:42:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "created": "2014-01-11T16:03:43.784+0000"}, {"author": "Reynold Xin", "body": "I think @Mridul Muralidharan added this. Perhaps he can comment on this.", "created": "2014-01-16T11:09:57.877+0000"}, {"author": "Reynold Xin", "body": "From Mridul: Hi, Cant seem to login to that site, but the rationale is to ensure that all pending messages (on the fly) are actually sent out before we shutdown : as in, not just write to output streams/buffers, but to wire and reasonably ensure received on other side. It is not a graceful fix - just to alleviate immediate concerns : not relevant for local mode, more relevant in cluster mode - particularly helps in congested clusters. Regards, Mridul", "created": "2014-01-17T00:11:25.292+0000"}, {"author": "Nan Zhu", "body": "I found that we have actually waited for a response to StopDriver from the driver if (driverActor != null) { val future = driverActor.ask(StopDriver)(timeout) Await.ready(future, timeout) } By intuitive, we should not need to wait for the other 1000 ms. However, when I removed Thread.sleep() and tested on my laptop, it failed... then I checked the driverActor side code case StopDriver => sender ! true context.stop(self) it sends the response first and then stop itself...so I guess the failed testcase is caused by the scheduling order of threads in my laptop Thread 1 -> sender!true Thread 2-> TaskSchedulerImpl.scala, goes forward and jumps out from override def stop() {} -> test case Thread 1-> context.stop(itself) so the test case failed because context.stop(itself) hasn't been executed we cannot simply change the order of sender ! true context.stop(self) I'm thinking about how to do this...", "created": "2014-01-17T05:31:29.777+0000"}, {"author": "Nan Zhu", "body": "confused... I cannot reproduce the bug in both local mode and cluster mode", "created": "2014-01-17T08:41:13.080+0000"}], "num_comments": 5, "text": "Issue: SPARK-1023\nSummary: Remove Thread.sleep(5000) from TaskSchedulerImpl\nDescription: This causes the unit tests to take super long. We should figure out why this exists and see if we can lower it or do something smarter.\n\nComments (5):\n1. Patrick McFadin: https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala\n2. Reynold Xin: I think @Mridul Muralidharan added this. Perhaps he can comment on this.\n3. Reynold Xin: From Mridul: Hi, Cant seem to login to that site, but the rationale is to ensure that all pending messages (on the fly) are actually sent out before we shutdown : as in, not just write to output streams/buffers, but to wire and reasonably ensure received on other side. It is not a graceful fix - just to alleviate immediate concerns : not relevant for local mode, more relevant in cluster mode - particularly helps in congested clusters. Regards, Mridul\n4. Nan Zhu: I found that we have actually waited for a response to StopDriver from the driver if (driverActor != null) { val future = driverActor.ask(StopDriver)(timeout) Await.ready(future, timeout) } By intuitive, we should not need to wait for the other 1000 ms. However, when I removed Thread.sleep() and tested on my laptop, it failed... then I checked the driverActor side code case StopDriver => sender ! true context.stop(self) it sends the response first and then stop itself...so I guess the failed testcase is caused by the scheduling order of threads in my laptop Thread 1 -> sender!true Thread 2-> TaskSchedulerImpl.scala, goes forward and jumps out from override def stop() {} -> test case Thread 1-> context.stop(itself) so the test case failed because context.stop(itself) hasn't been executed we cannot simply change the order of sender ! true context.stop(self) I'm thinking about how to do this...\n5. Nan Zhu: confused... I cannot reproduce the bug in both local mode and cluster mode", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "588065443d7ccd139af8fdd88b19625c", "issue_key": "SPARK-1024", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "-XX:+UseCompressedStrings is actually dropped in jdk7", "description": "hi, guys, -XX:+UseCompressedStrings is actually dropped in jdk7, but the Tuning Guide(http://spark.incubator.apache.org/docs/latest/tuning.html) is still recommend users to use this. JVM will warn us \"Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0\" if we use this option. so,maybe we can remove the point from the guide now. If the option is added back in the future, we also can add it back to the guide. more details can access this url : http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7129417", "reporter": "Chen Chao", "assignee": "Reynold Xin", "created": "2014-01-14T01:18:26.000+0000", "updated": "2014-02-09T22:32:53.000+0000", "resolved": "2014-02-09T22:32:53.000+0000", "labels": ["JVM", "UseCompressedStrings"], "components": ["Documentation"], "comments": [{"author": "Reynold Xin", "body": "Thanks for reporting. Do you mind submitting a pull request to fix the doc?", "created": "2014-01-14T22:08:46.613+0000"}, {"author": "Chen Chao", "body": "ok, i will fix it.", "created": "2014-01-15T05:36:35.952+0000"}, {"author": "Chen Chao", "body": "pull request SPARK-1024 : https://github.com/apache/incubator-spark/pull/439", "created": "2014-01-15T06:51:25.475+0000"}, {"author": "Andrew Ash", "body": "Merged by Reynold on Jan 15", "created": "2014-02-09T22:32:53.925+0000"}], "num_comments": 4, "text": "Issue: SPARK-1024\nSummary: -XX:+UseCompressedStrings is actually dropped in jdk7\nDescription: hi, guys, -XX:+UseCompressedStrings is actually dropped in jdk7, but the Tuning Guide(http://spark.incubator.apache.org/docs/latest/tuning.html) is still recommend users to use this. JVM will warn us \"Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0\" if we use this option. so,maybe we can remove the point from the guide now. If the option is added back in the future, we also can add it back to the guide. more details can access this url : http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7129417\n\nComments (4):\n1. Reynold Xin: Thanks for reporting. Do you mind submitting a pull request to fix the doc?\n2. Chen Chao: ok, i will fix it.\n3. Chen Chao: pull request SPARK-1024 : https://github.com/apache/incubator-spark/pull/439\n4. Andrew Ash: Merged by Reynold on Jan 15", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "399daabc623a18d927bd385ed2a2daa1", "issue_key": "SPARK-1025", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "pyspark hangs when parent base file is unavailable", "description": "Working with a file in pyspark. These steps work fine:  mydata = sc.textFile(\"somefile\") myfiltereddata = mydata.filter(some filter) myfiltereddata.count()  But then I delete \"somefile\" from the file system and attempt to run  myfiltereddata.count()  This hangs indefinitely. I eventually hit Ctrl-C and it displayed a stack trace. (I will attach the output as a file.) It works in scala though. If I do the same thing, the last line produces an expected error message:  14/01/14 08:41:43 ERROR Executor: Exception in task ID 4 java.io.FileNotFoundException: File file:somefile does not exist at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)", "reporter": "Diana Carroll", "assignee": "Josh Rosen", "created": "2014-01-14T08:55:29.000+0000", "updated": "2014-01-26T00:23:49.000+0000", "resolved": "2014-01-26T00:23:49.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "I turned on debug-level logging with log4j.properties and found the problem: PythonRDD's stdin writer thread was catching, logging, and ignoring the FileNotFoundException rather than allowing it to propagate and fail the job:  14/01/23 17:42:54 INFO HadoopRDD: Input split: file:/tmp/test.txt:0+0 14/01/23 17:42:54 INFO PythonRDD: stdin writer to Python finished early 14/01/23 17:42:54 DEBUG PythonRDD: stdin writer to Python finished early java.io.FileNotFoundException: File file:/tmp/test.txt does not exist. at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:397) at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:125) at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:283) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427) at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:78) at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:51) at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:168) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:161) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:73) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:81)  I submitted a pull request to fix this: https://github.com/apache/incubator-spark/pull/504", "created": "2014-01-23T18:27:16.653+0000"}], "num_comments": 1, "text": "Issue: SPARK-1025\nSummary: pyspark hangs when parent base file is unavailable\nDescription: Working with a file in pyspark. These steps work fine:  mydata = sc.textFile(\"somefile\") myfiltereddata = mydata.filter(some filter) myfiltereddata.count()  But then I delete \"somefile\" from the file system and attempt to run  myfiltereddata.count()  This hangs indefinitely. I eventually hit Ctrl-C and it displayed a stack trace. (I will attach the output as a file.) It works in scala though. If I do the same thing, the last line produces an expected error message:  14/01/14 08:41:43 ERROR Executor: Exception in task ID 4 java.io.FileNotFoundException: File file:somefile does not exist at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)\n\nComments (1):\n1. Josh Rosen: I turned on debug-level logging with log4j.properties and found the problem: PythonRDD's stdin writer thread was catching, logging, and ignoring the FileNotFoundException rather than allowing it to propagate and fail the job:  14/01/23 17:42:54 INFO HadoopRDD: Input split: file:/tmp/test.txt:0+0 14/01/23 17:42:54 INFO PythonRDD: stdin writer to Python finished early 14/01/23 17:42:54 DEBUG PythonRDD: stdin writer to Python finished early java.io.FileNotFoundException: File file:/tmp/test.txt does not exist. at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:397) at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:125) at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:283) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427) at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:78) at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:51) at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:168) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:161) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:73) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:81)  I submitted a pull request to fix this: https://github.com/apache/incubator-spark/pull/504", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "572dece575dab5cdc11fc87caa325aed", "issue_key": "SPARK-1026", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "PySpark using deprecated mapPartitionsWithSplit", "description": "In the Scala API, mapPartitionsWithSplit has been deprecated for a long time and really should be removed. However, PySpark uses mapPartitionsWithSplit instead of mapPartitionsWithIndex, so mapPartitionsWithSplit now can't be removed until it has first been deprecated in PySpark for at least one release.", "reporter": "Mark Hamstra", "assignee": "Josh Rosen", "created": "2014-01-14T08:56:44.000+0000", "updated": "2014-01-23T21:08:22.000+0000", "resolved": "2014-01-23T21:08:22.000+0000", "labels": [], "components": ["PySpark", "Spark Core"], "comments": [{"author": "Josh Rosen", "body": "I've submitted a pull request for this: https://github.com/apache/incubator-spark/pull/505", "created": "2014-01-23T19:41:55.424+0000"}], "num_comments": 1, "text": "Issue: SPARK-1026\nSummary: PySpark using deprecated mapPartitionsWithSplit\nDescription: In the Scala API, mapPartitionsWithSplit has been deprecated for a long time and really should be removed. However, PySpark uses mapPartitionsWithSplit instead of mapPartitionsWithIndex, so mapPartitionsWithSplit now can't be removed until it has first been deprecated in PySpark for at least one release.\n\nComments (1):\n1. Josh Rosen: I've submitted a pull request for this: https://github.com/apache/incubator-spark/pull/505", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "095b5bc58c254f31f663bc5dc54e347b", "issue_key": "SPARK-1027", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Standalone cluster should use default spark home if not specified by user", "description": "I added a quick fix to this in PR #442 (only in 0.9.0). But a nicer fix would be to have SparkHome be an Option[String] in ApplicationDescription and then to percolate that option all the way to the worker. Whatever fix we get should merge into both 0.8 and 0.9 branches.", "reporter": "Patrick McFadin", "assignee": "Nan Zhu", "created": "2014-01-15T11:03:41.000+0000", "updated": "2014-01-22T18:58:54.000+0000", "resolved": "2014-01-22T18:58:54.000+0000", "labels": ["starter"], "components": ["Deploy"], "comments": [{"author": "Nan Zhu", "body": "made a PR in https://github.com/apache/incubator-spark/pull/447", "created": "2014-01-15T17:47:40.277+0000"}], "num_comments": 1, "text": "Issue: SPARK-1027\nSummary: Standalone cluster should use default spark home if not specified by user\nDescription: I added a quick fix to this in PR #442 (only in 0.9.0). But a nicer fix would be to have SparkHome be an Option[String] in ApplicationDescription and then to percolate that option all the way to the worker. Whatever fix we get should merge into both 0.8 and 0.9 branches.\n\nComments (1):\n1. Nan Zhu: made a PR in https://github.com/apache/incubator-spark/pull/447", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "aa23869248cfb5b882b2845bf8a41a99", "issue_key": "SPARK-1028", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark-shell automatically set MASTER fails", "description": "spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem. The condition is \"if [[ \"x\" != \"x$SPARK_MASTER_IP\" && \"y\" != \"y$SPARK_MASTER_PORT\" ]];\" we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077. So if we do not set SPARK_MASTER_PORT, the condition will never be true. We should just use default port if users do not set port explicitly I think.", "reporter": "Chen Chao", "assignee": null, "created": "2014-01-15T19:05:26.000+0000", "updated": "2014-03-02T18:56:01.000+0000", "resolved": "2014-02-09T23:53:59.000+0000", "labels": ["spark-shell"], "components": ["Spark Core"], "comments": [{"author": "Chen Chao", "body": "if needed , i'll provide a patch.", "created": "2014-01-15T19:06:40.770+0000"}, {"author": "Reynold Xin", "body": "Sure. Please do. Thanks!", "created": "2014-01-15T19:09:20.138+0000"}, {"author": "Chen Chao", "body": "OK, i'll pull request today.", "created": "2014-01-15T19:13:31.052+0000"}, {"author": "Chen Chao", "body": "SPARK-1028 pull request at https://github.com/apache/incubator-spark/pull/449/files", "created": "2014-01-15T20:04:31.916+0000"}, {"author": "Chen Chao", "body": "already merged to master. please close.", "created": "2014-02-09T23:46:53.823+0000"}, {"author": "Chen Chao", "body": "@Reynold, it seems that 0.9.0 did not fix this ?", "created": "2014-03-02T18:54:15.638+0000"}, {"author": "Reynold Xin", "body": "I think it was merged into master for 1.0.0 and 0.9.1.", "created": "2014-03-02T18:56:01.640+0000"}], "num_comments": 7, "text": "Issue: SPARK-1028\nSummary: spark-shell automatically set MASTER fails\nDescription: spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem. The condition is \"if [[ \"x\" != \"x$SPARK_MASTER_IP\" && \"y\" != \"y$SPARK_MASTER_PORT\" ]];\" we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077. So if we do not set SPARK_MASTER_PORT, the condition will never be true. We should just use default port if users do not set port explicitly I think.\n\nComments (7):\n1. Chen Chao: if needed , i'll provide a patch.\n2. Reynold Xin: Sure. Please do. Thanks!\n3. Chen Chao: OK, i'll pull request today.\n4. Chen Chao: SPARK-1028 pull request at https://github.com/apache/incubator-spark/pull/449/files\n5. Chen Chao: already merged to master. please close.\n6. Chen Chao: @Reynold, it seems that 0.9.0 did not fix this ?\n7. Reynold Xin: I think it was merged into master for 1.0.0 and 0.9.1.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "d1c1258f6699e5606e78464ce9f17f8d", "issue_key": "SPARK-1029", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "spark Window shell script errors regarding shell script location reference", "description": "When launch spark-shell.cmd in Window 7, I got following errors E:\\projects\\amplab\\incubator-spark>bin\\spark-shell.cmd 'E:\\projects\\amplab\\incubator-spark\\bin\\..\\sbin\\spark-class2.cmd' is not recognized as an internal or external command, operable program or batch file. E:\\projects\\amplab\\incubator-spark>bin\\spark-shell.cmd '\"E:\\projects\\amplab\\incubator-spark\\bin\\..\\sbin\\compute-classpath.cmd\"' is not recognized as an internal or external co mmand, operable program or batch file. Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/repl/Main I am attaching my patches,", "reporter": "Qiuzhuang Lian", "assignee": null, "created": "2014-01-15T23:10:46.000+0000", "updated": "2015-01-24T22:17:24.000+0000", "resolved": "2015-01-24T22:17:24.000+0000", "labels": [], "components": ["Windows"], "comments": [{"author": "Reynold Xin", "body": "Thanks for submitting a patch. Do you mind submitting a github pull request against https://github.com/apache/incubator-spark ?", "created": "2014-01-15T23:20:24.519+0000"}, {"author": "Qiuzhuang Lian", "body": "PR #: https://github.com/apache/incubator-spark/pull/451", "created": "2014-01-16T22:36:12.554+0000"}, {"author": "Sean R. Owen", "body": "Looks like this was fixed in https://github.com/apache/spark/commit/4e510b0b0c8a69cfe0ee037b37661caf9bf1d057 for 1.0.0", "created": "2015-01-24T22:17:24.283+0000"}], "num_comments": 3, "text": "Issue: SPARK-1029\nSummary: spark Window shell script errors regarding shell script location reference\nDescription: When launch spark-shell.cmd in Window 7, I got following errors E:\\projects\\amplab\\incubator-spark>bin\\spark-shell.cmd 'E:\\projects\\amplab\\incubator-spark\\bin\\..\\sbin\\spark-class2.cmd' is not recognized as an internal or external command, operable program or batch file. E:\\projects\\amplab\\incubator-spark>bin\\spark-shell.cmd '\"E:\\projects\\amplab\\incubator-spark\\bin\\..\\sbin\\compute-classpath.cmd\"' is not recognized as an internal or external co mmand, operable program or batch file. Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/spark/repl/Main I am attaching my patches,\n\nComments (3):\n1. Reynold Xin: Thanks for submitting a patch. Do you mind submitting a github pull request against https://github.com/apache/incubator-spark ?\n2. Qiuzhuang Lian: PR #: https://github.com/apache/incubator-spark/pull/451\n3. Sean R. Owen: Looks like this was fixed in https://github.com/apache/spark/commit/4e510b0b0c8a69cfe0ee037b37661caf9bf1d057 for 1.0.0", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.111808"}}
{"id": "81d91f150a75c58c3fe011c8e24e7c02", "issue_key": "SPARK-1215", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Clustering: Index out of bounds error", "description": "code: import org.apache.spark.mllib.clustering._ val test = sc.makeRDD(Array(4,4,4,4,4).map(e => Array(e.toDouble))) val kmeans = new KMeans().setK(4) kmeans.run(test) evals with java.lang.ArrayIndexOutOfBoundsException error: 14/01/17 12:35:54 INFO scheduler.DAGScheduler: Stage 25 (collectAsMap at KMeans.scala:243) finished in 0.047 s 14/01/17 12:35:54 INFO spark.SparkContext: Job finished: collectAsMap at KMeans.scala:243, took 16.389537116 s Exception in thread \"main\" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at com.simontuffs.onejar.Boot.run(Boot.java:340) at com.simontuffs.onejar.Boot.main(Boot.java:166) Caused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.spark.mllib.clustering.LocalKMeans$.kMeansPlusPlus(LocalKMeans.scala:47) at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:247) at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.immutable.Range.foreach(Range.scala:81) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.immutable.Range.map(Range.scala:46) at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:244) at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:124) at Clustering$$anonfun$1.apply$mcDI$sp(Clustering.scala:21) at Clustering$$anonfun$1.apply(Clustering.scala:19) at Clustering$$anonfun$1.apply(Clustering.scala:19) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.immutable.Range.foreach(Range.scala:78) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.immutable.Range.map(Range.scala:46) at Clustering$.main(Clustering.scala:19) at Clustering.main(Clustering.scala) ... 6 more", "reporter": "dewshick", "assignee": "Joseph K. Bradley", "created": "2014-01-17T05:24:22.000+0000", "updated": "2014-08-27T08:57:00.000+0000", "resolved": "2014-07-17T22:05:30.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "The error was due to small number of points and large k. The k-means|| initialization doesn't collect more than k candidates. This is very unlikely to appear in practice because k is much smaller than number of points. I will re-visit this issue once we implement better weighted sampling algorithms.", "created": "2014-04-15T16:46:28.032+0000"}, {"author": "Denis Serduik", "body": "I don't think that the problem is about size of dataset. I've faced with similar issue on dataset with about 900 items. As a workaround we've decided to fallback with random init mode.", "created": "2014-05-23T12:26:55.112+0000"}, {"author": "Denis Serduik", "body": "attach test dataset MLLib failed to find 4 centers with k-means|| init mode on this data", "created": "2014-05-23T12:28:24.947+0000"}, {"author": "Joseph K. Bradley", "body": "Submitted fix as PR 1407: https://github.com/apache/spark/pull/1407 Made default behavior to return k clusters still, with some duplicated", "created": "2014-07-14T19:35:06.935+0000"}, {"author": "Xiangrui Meng", "body": "Issue resolved by pull request 1468 [https://github.com/apache/spark/pull/1468]", "created": "2014-07-17T22:05:30.942+0000"}], "num_comments": 5, "text": "Issue: SPARK-1215\nSummary: Clustering: Index out of bounds error\nDescription: code: import org.apache.spark.mllib.clustering._ val test = sc.makeRDD(Array(4,4,4,4,4).map(e => Array(e.toDouble))) val kmeans = new KMeans().setK(4) kmeans.run(test) evals with java.lang.ArrayIndexOutOfBoundsException error: 14/01/17 12:35:54 INFO scheduler.DAGScheduler: Stage 25 (collectAsMap at KMeans.scala:243) finished in 0.047 s 14/01/17 12:35:54 INFO spark.SparkContext: Job finished: collectAsMap at KMeans.scala:243, took 16.389537116 s Exception in thread \"main\" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at com.simontuffs.onejar.Boot.run(Boot.java:340) at com.simontuffs.onejar.Boot.main(Boot.java:166) Caused by: java.lang.ArrayIndexOutOfBoundsException: -1 at org.apache.spark.mllib.clustering.LocalKMeans$.kMeansPlusPlus(LocalKMeans.scala:47) at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:247) at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.immutable.Range.foreach(Range.scala:81) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.immutable.Range.map(Range.scala:46) at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:244) at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:124) at Clustering$$anonfun$1.apply$mcDI$sp(Clustering.scala:21) at Clustering$$anonfun$1.apply(Clustering.scala:19) at Clustering$$anonfun$1.apply(Clustering.scala:19) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233) at scala.collection.immutable.Range.foreach(Range.scala:78) at scala.collection.TraversableLike$class.map(TraversableLike.scala:233) at scala.collection.immutable.Range.map(Range.scala:46) at Clustering$.main(Clustering.scala:19) at Clustering.main(Clustering.scala) ... 6 more\n\nComments (5):\n1. Xiangrui Meng: The error was due to small number of points and large k. The k-means|| initialization doesn't collect more than k candidates. This is very unlikely to appear in practice because k is much smaller than number of points. I will re-visit this issue once we implement better weighted sampling algorithms.\n2. Denis Serduik: I don't think that the problem is about size of dataset. I've faced with similar issue on dataset with about 900 items. As a workaround we've decided to fallback with random init mode.\n3. Denis Serduik: attach test dataset MLLib failed to find 4 centers with k-means|| init mode on this data\n4. Joseph K. Bradley: Submitted fix as PR 1407: https://github.com/apache/spark/pull/1407 Made default behavior to return k clusters still, with some duplicated\n5. Xiangrui Meng: Issue resolved by pull request 1468 [https://github.com/apache/spark/pull/1468]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.113812"}}
{"id": "71db4dd058210b538572cb622799681b", "issue_key": "SPARK-1030", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "unneeded file required when running pyspark program using yarn-client", "description": "I can successfully run a pyspark program using the yarn-client master using the following command:  SPARK_JAR=$SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.1-incubating-hadoop2.2.0.jar \\ SPARK_YARN_APP_JAR=~/testdata.txt pyspark \\ test1.py  However, the SPARK_YARN_APP_JAR doesn't make any sense; it's a Python program, and therefore there's no JAR. If I don't set the value, or if I set the value to a non-existent files, Spark gives me an error message.  py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext. : org.apache.spark.SparkException: env SPARK_YARN_APP_JAR is not set at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:46)  or  py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext. : java.io.FileNotFoundException: File file:dummy.txt does not exist at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)  My program is very simple:  from pyspark import SparkContext def main(): sc = SparkContext(\"yarn-client\", \"Simple App\") logData = sc.textFile(\"hdfs://localhost/user/training/weblogs/2013-09-15.log\") numjpgs = logData.filter(lambda s: '.jpg' in s).count() print \"Number of JPG requests: \" + str(numjpgs)  Although it reads the SPARK_YARN_APP_JAR file, it doesn't use the file at all; I can point it at anything, as long as it's a valid, accessible file, and it works the same. Although there's an obvious workaround for this bug, it's high priority from my perspective because I'm working on a course to teach people how to do this, and it's really hard to explain why this variable is needed!", "reporter": "Diana Carroll", "assignee": "Josh Rosen", "created": "2014-01-17T11:19:38.000+0000", "updated": "2014-07-25T01:42:23.000+0000", "resolved": "2014-07-25T01:42:23.000+0000", "labels": [], "components": ["Deploy", "PySpark", "YARN"], "comments": [{"author": "Matthew Farrellee", "body": "using pyspark to submit is deprecated in spark 1.0 in favor of spark-submit. i think this should be closed as resolved/workfix. /cc: [~pwendell] [~joshrosen]", "created": "2014-07-02T13:57:24.495+0000"}, {"author": "Josh Rosen", "body": "Closing this now, since it was addressed as part of Spark 1.0's PySpark on YARN patches (including SPARK-1004).", "created": "2014-07-25T01:42:23.456+0000"}], "num_comments": 2, "text": "Issue: SPARK-1030\nSummary: unneeded file required when running pyspark program using yarn-client\nDescription: I can successfully run a pyspark program using the yarn-client master using the following command:  SPARK_JAR=$SPARK_HOME/assembly/target/scala-2.9.3/spark-assembly_2.9.3-0.8.1-incubating-hadoop2.2.0.jar \\ SPARK_YARN_APP_JAR=~/testdata.txt pyspark \\ test1.py  However, the SPARK_YARN_APP_JAR doesn't make any sense; it's a Python program, and therefore there's no JAR. If I don't set the value, or if I set the value to a non-existent files, Spark gives me an error message.  py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext. : org.apache.spark.SparkException: env SPARK_YARN_APP_JAR is not set at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:46)  or  py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext. : java.io.FileNotFoundException: File file:dummy.txt does not exist at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)  My program is very simple:  from pyspark import SparkContext def main(): sc = SparkContext(\"yarn-client\", \"Simple App\") logData = sc.textFile(\"hdfs://localhost/user/training/weblogs/2013-09-15.log\") numjpgs = logData.filter(lambda s: '.jpg' in s).count() print \"Number of JPG requests: \" + str(numjpgs)  Although it reads the SPARK_YARN_APP_JAR file, it doesn't use the file at all; I can point it at anything, as long as it's a valid, accessible file, and it works the same. Although there's an obvious workaround for this bug, it's high priority from my perspective because I'm working on a course to teach people how to do this, and it's really hard to explain why this variable is needed!\n\nComments (2):\n1. Matthew Farrellee: using pyspark to submit is deprecated in spark 1.0 in favor of spark-submit. i think this should be closed as resolved/workfix. /cc: [~pwendell] [~joshrosen]\n2. Josh Rosen: Closing this now, since it was addressed as part of Spark 1.0's PySpark on YARN patches (including SPARK-1004).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.113812"}}
{"id": "7f371fd8041b02b9e6fd6f1b587dceae", "issue_key": "SPARK-1031", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "FileNotFoundException when running simple Spark app on Yarn", "description": "I hit a FileNotFoundException in the application master when running the SparkPi example as described in the docs against Yarn 2.2. The problem appears to be that the app master requires the app jar to be in its working directory with the same name as returned by SparkContext.jarOfClass. A symlink called app.jar to the app jar exists in the working directory, but jar itself lives in the NodeManager local cache dir with the name it was submitted with (e.g. spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar). When adding the jar, the SparkContext only uses the last component in its path:  if (SparkHadoopUtil.get.isYarnMode() && master == \"yarn-standalone\") { // In order for this to work in yarn standalone mode the user must specify the // --addjars option to the client to upload the file into the distributed cache // of the AM to make it show up in the current working directory. val fileName = new Path(uri.getPath).getName() try { env.httpFileServer.addJar(new File(fileName))  If SparkContext.jarOfClass returns the jar in its linked-to location, the file will not be found. When I modified things to set fileName to use the jar's full path, my app completed successfully.", "reporter": "Sanford Ryza", "assignee": null, "created": "2014-01-19T01:20:50.000+0000", "updated": "2014-01-20T15:05:33.000+0000", "resolved": "2014-01-19T21:58:30.000+0000", "labels": [], "components": ["Spark Core", "YARN"], "comments": [{"author": "Thomas Graves", "body": "You can work around this by specifying the --addJars option. To make this more user friendly and not have to specify the --addJars option we made a small change: https://github.com/apache/incubator-spark/pull/470/", "created": "2014-01-19T10:23:13.541+0000"}, {"author": "Sanford Ryza", "body": "With that change, an error will still be logged in normal operation, right? If I am giving a jar through --jar, should I also need to --addJar it?", "created": "2014-01-19T10:37:07.910+0000"}, {"author": "Thomas Graves", "body": "yeah it will still log the error and your application will most likely fail later on when it tries to use it. To clarify this is non spark examples case. The spark examples don't really need the jar distributed again since its the app jar. In a \"real\" application that uses sc.addJar it will log an error if it doesn't exist and then later on will most likely fail since the jar won't have been distributed.", "created": "2014-01-19T11:06:21.075+0000"}, {"author": "Thomas Graves", "body": "not sure what you mean by --jar option.. If you sc.addJar something then for yarn you have to specify --addJars to distributed it to the application master where it can be distributed through the normal spark addJar HttpFileServer mechanism.", "created": "2014-01-19T11:07:13.944+0000"}, {"author": "Sanford Ryza", "body": "Spoke to Patrick offline and I think I understand the fundamental issue behind the strangeness. Let me know if this is incorrect: The app jar is distributed through a different mechanism than additional added jars. The app jar gets to every worker node as a Yarn local resource. Additional jars only get to the app master, and the app master serves them to workers with the HTTP file server. The strangeness comes when an application addJar's the app jar, which is a natural thing to do in mesos or standalone mode, but in Yarn mode, will try to distribute the same jar through a different mechanism. Would it make sense to use the same mechanism for both of these? I.e. to ship the app jar to workers through the HTTP file server or to ship additional jars to workers as Yarn localized resources?", "created": "2014-01-19T11:31:14.350+0000"}, {"author": "Sanford Ryza", "body": "An additional benefit of using the same mechanism is that it would greatly simplify debugging ClassNotFoundExceptions in workers. This is a painful exercise on any framework, but gets harder with each different way that bits can get sent over.", "created": "2014-01-19T11:35:47.840+0000"}, {"author": "Thomas Graves", "body": "This is something Patrick and I spoke about and I plan to look into.", "created": "2014-01-19T11:54:42.982+0000"}, {"author": "Thomas Graves", "body": "I should add feel free to take a look at it if you want. I was planning on looking more to distributed the app.jar via the spark mechanism rather then the yarn mechamism", "created": "2014-01-19T11:58:57.163+0000"}, {"author": "Sanford Ryza", "body": "Cool, I will try to take a look. Filed SPARK-1035 for this. Is there a reason you think the app.jar mechanism is preferable to the Yarn mechanism? With the Yarn mechanism we could avoid sending the jars if another app already has them cached. It also requires fewer copies even when they aren't already cached.", "created": "2014-01-20T00:20:47.083+0000"}, {"author": "Thomas Graves", "body": "The main reason I was going to start there was to keep it consistent with the way spark does it. Should be easier to maintain, less changes for the on yarn side, easier for people to understand what is going on if they have used standalone/mesos mode, etc. But as you say the performance needs to be evaluated. If its slower or if we thinking people will use the public distributed cache then we should leave it using yarn. I generally don't see many people using the public distributed cache for their application jars. Either on Spark or MR. That might not be true with other people though.", "created": "2014-01-20T06:59:56.010+0000"}, {"author": "Sanford Ryza", "body": "bq. I generally don't see many people using the public distributed cache for their application jars. YARN-1492 should make the shared distributed cache a more attractive (and hopefully the default) option. I've spoken to a few people who say that, on MapReduce, loading jars which many jobs have in common is a significant part of their job time. It would be nice for Spark not to be behind MR and Tez in this regard.", "created": "2014-01-20T13:19:33.467+0000"}, {"author": "Thomas Graves", "body": "I agree. Even without YARN-1492 the yarn distributed cache should be better, especially if the incase of a large # of workers. With the spark HttpFileServer they are fetching from one server vs being distributed to multiple nodes from HDFS. The downside again is that its different, even between yarn-client and yarn-standalone mode. I guess we could add the functionality to yarn-client mode and if they specify the --addJars type parameter then it uses the distributed cache instead of the HttpFileServer.", "created": "2014-01-20T15:05:33.656+0000"}], "num_comments": 12, "text": "Issue: SPARK-1031\nSummary: FileNotFoundException when running simple Spark app on Yarn\nDescription: I hit a FileNotFoundException in the application master when running the SparkPi example as described in the docs against Yarn 2.2. The problem appears to be that the app master requires the app jar to be in its working directory with the same name as returned by SparkContext.jarOfClass. A symlink called app.jar to the app jar exists in the working directory, but jar itself lives in the NodeManager local cache dir with the name it was submitted with (e.g. spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar). When adding the jar, the SparkContext only uses the last component in its path:  if (SparkHadoopUtil.get.isYarnMode() && master == \"yarn-standalone\") { // In order for this to work in yarn standalone mode the user must specify the // --addjars option to the client to upload the file into the distributed cache // of the AM to make it show up in the current working directory. val fileName = new Path(uri.getPath).getName() try { env.httpFileServer.addJar(new File(fileName))  If SparkContext.jarOfClass returns the jar in its linked-to location, the file will not be found. When I modified things to set fileName to use the jar's full path, my app completed successfully.\n\nComments (12):\n1. Thomas Graves: You can work around this by specifying the --addJars option. To make this more user friendly and not have to specify the --addJars option we made a small change: https://github.com/apache/incubator-spark/pull/470/\n2. Sanford Ryza: With that change, an error will still be logged in normal operation, right? If I am giving a jar through --jar, should I also need to --addJar it?\n3. Thomas Graves: yeah it will still log the error and your application will most likely fail later on when it tries to use it. To clarify this is non spark examples case. The spark examples don't really need the jar distributed again since its the app jar. In a \"real\" application that uses sc.addJar it will log an error if it doesn't exist and then later on will most likely fail since the jar won't have been distributed.\n4. Thomas Graves: not sure what you mean by --jar option.. If you sc.addJar something then for yarn you have to specify --addJars to distributed it to the application master where it can be distributed through the normal spark addJar HttpFileServer mechanism.\n5. Sanford Ryza: Spoke to Patrick offline and I think I understand the fundamental issue behind the strangeness. Let me know if this is incorrect: The app jar is distributed through a different mechanism than additional added jars. The app jar gets to every worker node as a Yarn local resource. Additional jars only get to the app master, and the app master serves them to workers with the HTTP file server. The strangeness comes when an application addJar's the app jar, which is a natural thing to do in mesos or standalone mode, but in Yarn mode, will try to distribute the same jar through a different mechanism. Would it make sense to use the same mechanism for both of these? I.e. to ship the app jar to workers through the HTTP file server or to ship additional jars to workers as Yarn localized resources?\n6. Sanford Ryza: An additional benefit of using the same mechanism is that it would greatly simplify debugging ClassNotFoundExceptions in workers. This is a painful exercise on any framework, but gets harder with each different way that bits can get sent over.\n7. Thomas Graves: This is something Patrick and I spoke about and I plan to look into.\n8. Thomas Graves: I should add feel free to take a look at it if you want. I was planning on looking more to distributed the app.jar via the spark mechanism rather then the yarn mechamism\n9. Sanford Ryza: Cool, I will try to take a look. Filed SPARK-1035 for this. Is there a reason you think the app.jar mechanism is preferable to the Yarn mechanism? With the Yarn mechanism we could avoid sending the jars if another app already has them cached. It also requires fewer copies even when they aren't already cached.\n10. Thomas Graves: The main reason I was going to start there was to keep it consistent with the way spark does it. Should be easier to maintain, less changes for the on yarn side, easier for people to understand what is going on if they have used standalone/mesos mode, etc. But as you say the performance needs to be evaluated. If its slower or if we thinking people will use the public distributed cache then we should leave it using yarn. I generally don't see many people using the public distributed cache for their application jars. Either on Spark or MR. That might not be true with other people though.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.113812"}}
{"id": "c88d33326cec087a94935ff898b9df23", "issue_key": "SPARK-1032", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "If Yarn app fails before registering, app master stays around long after", "description": "My Spark/YARN app hit an error while initializing its SparkContext (YARN-1031). The app stayed around for another 5 minutes continually logging \"14/01/18 23:58:59 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.\" until YARN finally killed it. This is probably because it died before registering in the first place, but still tried to unregister.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-01-19T01:37:20.000+0000", "updated": "2014-04-04T20:51:08.000+0000", "resolved": "2014-02-28T08:52:18.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Sanford Ryza", "body": "https://github.com/apache/incubator-spark/pull/648", "created": "2014-02-24T22:28:54.043+0000"}, {"author": "Thomas Graves", "body": "pr https://github.com/apache/spark/pull/28 committed.", "created": "2014-02-28T08:52:18.477+0000"}], "num_comments": 2, "text": "Issue: SPARK-1032\nSummary: If Yarn app fails before registering, app master stays around long after\nDescription: My Spark/YARN app hit an error while initializing its SparkContext (YARN-1031). The app stayed around for another 5 minutes continually logging \"14/01/18 23:58:59 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.\" until YARN finally killed it. This is probably because it died before registering in the first place, but still tried to unregister.\n\nComments (2):\n1. Sanford Ryza: https://github.com/apache/incubator-spark/pull/648\n2. Thomas Graves: pr https://github.com/apache/spark/pull/28 committed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.113812"}}
{"id": "2249f9cf549f5e8977c8f7ae502608fc", "issue_key": "SPARK-1033", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Ask for cores in Yarn container requests", "description": "Yarn 2.2 has support for requesting cores in addition to memory. Spark against Yarn 2.2 should include cores in its resource requests in the same way it includes memory.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-01-19T01:39:17.000+0000", "updated": "2014-04-04T20:51:05.000+0000", "resolved": "2014-02-18T13:41:08.000+0000", "labels": [], "components": ["YARN"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1033\nSummary: Ask for cores in Yarn container requests\nDescription: Yarn 2.2 has support for requesting cores in addition to memory. Spark against Yarn 2.2 should include cores in its resource requests in the same way it includes memory.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.113812"}}
{"id": "8d974caba91304bdeb53b42174d74dc9", "issue_key": "SPARK-1034", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Py4JException on PySpark Cartesian Result", "description": "RDD operations on results of the Pyspark Cartesian method return Py4JException. Here's a few examples  >>> rdd1=sc.parallelize([1,2,3,4,5,1]) >>> rdd2=sc.parallelize([11,12,13,14,15,11]) >>> rdd1.cartesian(rdd2).map(lambda x: x[0] + x[1]).collect() Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 446, in collect bytesInJava = self._jrdd.collect().iterator() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 1041, in _jrdd class_tag) File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\", line 669, in __call__ File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\", line 304, in get_return_value py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace: py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184) at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202) at py4j.Gateway.invoke(Gateway.java:213) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:744) >>> >>> rdd1.cartesian(rdd2).count() Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 525, in count return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 516, in sum return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add) File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 482, in reduce vals = self.mapPartitions(func).collect() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 446, in collect bytesInJava = self._jrdd.collect().iterator() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 1041, in _jrdd class_tag) File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\", line 669, in __call__ File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\", line 304, in get_return_value py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace: py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184) at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202) at py4j.Gateway.invoke(Gateway.java:213) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:744)  I see this issue after the custom serializer change. https://github.com/apache/incubator-spark/commit/cbb7f04aef2220ece93dea9f3fa98b5db5f270d6", "reporter": "Taka Shinagawa", "assignee": "Josh Rosen", "created": "2014-01-19T12:27:51.000+0000", "updated": "2014-07-26T22:26:26.000+0000", "resolved": "2014-01-23T19:47:49.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Michael Broxton", "body": "I can confirm that I see this bug as well using Spark 0.9.1. It appears that collect() works correctly: >>> test_rdd1 = sc.parallelize(range(5), 2) >>> test_rdd2 = test_rdd1.cartesian(test_rdd1) >>> print test_rdd2.collect() [(0, 0), (0, 1), (1, 0), (1, 1), (0, 2), (0, 3), (1, 2), (1, 3), (0, 4), (1, 4), (2, 0), (2, 1), (3, 0), (3, 1), (4, 0), (4, 1), (2, 2), (2, 3), (3, 2), (3, 3), (2, 4), (3, 4), (4, 2), (4, 3), (4, 4)] However, other operations like map(), count(), and reduce() produce the same exception that Taka has reported above.", "created": "2014-01-22T23:12:22.589+0000"}, {"author": "Michael Broxton", "body": "After spending a little bit of time delving into the code, I can say that this problem appears to occur at line 1038 of rdd.py when the PythonRDD object is constructed. python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), bytearray(pickled_command), env, includes, self.preservesPartitioning, self.ctx.pythonExec, broadcast_vars, self.ctx._javaAccumulator, class_tag) It appears that Py4J can't find the corresponding Java constructor when this function is called on a Cartesian RDD. I added the following debugging code immediately before line 1038: print '---' print self._prev_jrdd.rdd() print len(bytearray(pickled_command)) print env print includes print self.preservesPartitioning print self.ctx.pythonExec print broadcast_vars print self.ctx._javaAccumulator print class_tag print '---' This prints out each argument that is passed to the PythonRDD constructor. Having made this change, I ran this in Spark: test_rdd = sc.parallelize(range(5), 2) print test_rdd.count() # THIS WORKS!! which produced this debugging output on the console: ------ ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:206 1431 {} [] False python [] [] Array[byte] --- However, when I run this: test_rdd1 = sc.parallelize(range(5), 2) test_rdd2= test_rdd1.cartesian(test_rdd1) print test_rdd2.count() # FAILS WITH Py4J EXCEPTION I get this debugging output: --- CartesianRDD[1] at cartesian at NativeMethodAccessorImpl.java:-2 1506 {} [] False python [] [] Object --- Since both ParallelCollectionRDD and CartesianRDD are subclasses of the same RDD superclass, I suspect that the meaningful difference between the \"working\" and \"not working\" version is the last argument to the constructor: the class_tag. It appears that the class_tag of a cartesian RDD is a Scala object of some sort, whereas the class_tag of the first RDD was Array[Byte]. I spent a little bit of time poking around PythonRDD.scala to see if I could understand why the constructor only worked with the first set of arguments, but I'm afraid I cannot see what is causing this bug. But, hopefully you will find this info useful when you fix this bug. Thanks!", "created": "2014-01-23T01:29:41.040+0000"}, {"author": "Taka Shinagawa", "body": "Michael, thanks for the info. I'm also looking into the same place.", "created": "2014-01-23T01:56:22.851+0000"}, {"author": "Josh Rosen", "body": "Thanks for reporting this bug, and for the detailed investigation so far. Michael, in both cases in your example, {{class_tag}} is actually a Scala ClassTag object and that {{Array\\[Byte\\]}} and {{Object}} are the results of calling {{toString()}} on those ClassTags. In cartesian, we're trying to wrap a {{JavaPairRDD<Array<Byte>, Array<Byte>>}} into a PythonRDD. The ClassTag for this {{JavaRDD<Tuple2<Array<Byte>, Array<Byte>>>}} should be a {{ClassTag<Tuple2<Array<Byte>, Array<Byte>>}} (or a {{ClassTag<Tuple2<?, ?>>}}). The root problem here may actually be in JavaPairRDD's classTag:  scala> val x = sc.parallelize(Seq(Array[Byte](0))).map(x => (x, x)) x: org.apache.spark.rdd.RDD[(Array[Byte], Array[Byte])] = MappedRDD[7] at map at <console>:18 scala> JavaPairRDD.fromJavaRDD(x).classTag res15: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = Object scala> JavaRDD.fromRDD(x).classTag res17: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = scala.Tuple2  This turns out to be caused by a line in JavaPairRDD that constructed its ClassTag by casting an AnyRef ClassTag, when it should have just used the underlying ScalaRDD's {{elementClassTag}} (since it's already of the right type):  override val classTag: ClassTag[(K, V)] = implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[Tuple2[K, V]]]  should be  override val classTag: ClassTag[(K, V)] = rdd.elementClassTag  I've submitted this fix as part of a pull request: https://github.com/apache/incubator-spark/pull/501", "created": "2014-01-23T15:19:49.484+0000"}, {"author": "Taka Shinagawa", "body": "Josh, thanks for the quick fix! I've confirmed this change fixes the problem.", "created": "2014-01-23T15:55:15.044+0000"}, {"author": "Michael Broxton", "body": "Hi Josh, Yes, I can confirm that this fixes it for me as well. Tremendous thanks for taking a look at this, and for taking the time to explain the fix. Now back to data analysis! Best, -Michael", "created": "2014-01-23T16:47:39.170+0000"}], "num_comments": 6, "text": "Issue: SPARK-1034\nSummary: Py4JException on PySpark Cartesian Result\nDescription: RDD operations on results of the Pyspark Cartesian method return Py4JException. Here's a few examples  >>> rdd1=sc.parallelize([1,2,3,4,5,1]) >>> rdd2=sc.parallelize([11,12,13,14,15,11]) >>> rdd1.cartesian(rdd2).map(lambda x: x[0] + x[1]).collect() Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 446, in collect bytesInJava = self._jrdd.collect().iterator() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 1041, in _jrdd class_tag) File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\", line 669, in __call__ File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\", line 304, in get_return_value py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace: py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184) at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202) at py4j.Gateway.invoke(Gateway.java:213) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:744) >>> >>> rdd1.cartesian(rdd2).count() Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 525, in count return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 516, in sum return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add) File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 482, in reduce vals = self.mapPartitions(func).collect() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 446, in collect bytesInJava = self._jrdd.collect().iterator() File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/pyspark/rdd.py\", line 1041, in _jrdd class_tag) File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\", line 669, in __call__ File \"/Users/tshinagawa/Documents/Spark/RCs/spark-0.9.0-incubating-rc2/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\", line 304, in get_return_value py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.python.PythonRDD. Trace: py4j.Py4JException: Constructor org.apache.spark.api.python.PythonRDD([class org.apache.spark.rdd.CartesianRDD, class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.Boolean, class java.lang.String, class java.util.ArrayList, class org.apache.spark.Accumulator, class scala.reflect.ManifestFactory$$anon$2]) does not exist at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:184) at py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:202) at py4j.Gateway.invoke(Gateway.java:213) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Thread.java:744)  I see this issue after the custom serializer change. https://github.com/apache/incubator-spark/commit/cbb7f04aef2220ece93dea9f3fa98b5db5f270d6\n\nComments (6):\n1. Michael Broxton: I can confirm that I see this bug as well using Spark 0.9.1. It appears that collect() works correctly: >>> test_rdd1 = sc.parallelize(range(5), 2) >>> test_rdd2 = test_rdd1.cartesian(test_rdd1) >>> print test_rdd2.collect() [(0, 0), (0, 1), (1, 0), (1, 1), (0, 2), (0, 3), (1, 2), (1, 3), (0, 4), (1, 4), (2, 0), (2, 1), (3, 0), (3, 1), (4, 0), (4, 1), (2, 2), (2, 3), (3, 2), (3, 3), (2, 4), (3, 4), (4, 2), (4, 3), (4, 4)] However, other operations like map(), count(), and reduce() produce the same exception that Taka has reported above.\n2. Michael Broxton: After spending a little bit of time delving into the code, I can say that this problem appears to occur at line 1038 of rdd.py when the PythonRDD object is constructed. python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), bytearray(pickled_command), env, includes, self.preservesPartitioning, self.ctx.pythonExec, broadcast_vars, self.ctx._javaAccumulator, class_tag) It appears that Py4J can't find the corresponding Java constructor when this function is called on a Cartesian RDD. I added the following debugging code immediately before line 1038: print '---' print self._prev_jrdd.rdd() print len(bytearray(pickled_command)) print env print includes print self.preservesPartitioning print self.ctx.pythonExec print broadcast_vars print self.ctx._javaAccumulator print class_tag print '---' This prints out each argument that is passed to the PythonRDD constructor. Having made this change, I ran this in Spark: test_rdd = sc.parallelize(range(5), 2) print test_rdd.count() # THIS WORKS!! which produced this debugging output on the console: ------ ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:206 1431 {} [] False python [] [] Array[byte] --- However, when I run this: test_rdd1 = sc.parallelize(range(5), 2) test_rdd2= test_rdd1.cartesian(test_rdd1) print test_rdd2.count() # FAILS WITH Py4J EXCEPTION I get this debugging output: --- CartesianRDD[1] at cartesian at NativeMethodAccessorImpl.java:-2 1506 {} [] False python [] [] Object --- Since both ParallelCollectionRDD and CartesianRDD are subclasses of the same RDD superclass, I suspect that the meaningful difference between the \"working\" and \"not working\" version is the last argument to the constructor: the class_tag. It appears that the class_tag of a cartesian RDD is a Scala object of some sort, whereas the class_tag of the first RDD was Array[Byte]. I spent a little bit of time poking around PythonRDD.scala to see if I could understand why the constructor only worked with the first set of arguments, but I'm afraid I cannot see what is causing this bug. But, hopefully you will find this info useful when you fix this bug. Thanks!\n3. Taka Shinagawa: Michael, thanks for the info. I'm also looking into the same place.\n4. Josh Rosen: Thanks for reporting this bug, and for the detailed investigation so far. Michael, in both cases in your example, {{class_tag}} is actually a Scala ClassTag object and that {{Array\\[Byte\\]}} and {{Object}} are the results of calling {{toString()}} on those ClassTags. In cartesian, we're trying to wrap a {{JavaPairRDD<Array<Byte>, Array<Byte>>}} into a PythonRDD. The ClassTag for this {{JavaRDD<Tuple2<Array<Byte>, Array<Byte>>>}} should be a {{ClassTag<Tuple2<Array<Byte>, Array<Byte>>}} (or a {{ClassTag<Tuple2<?, ?>>}}). The root problem here may actually be in JavaPairRDD's classTag:  scala> val x = sc.parallelize(Seq(Array[Byte](0))).map(x => (x, x)) x: org.apache.spark.rdd.RDD[(Array[Byte], Array[Byte])] = MappedRDD[7] at map at <console>:18 scala> JavaPairRDD.fromJavaRDD(x).classTag res15: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = Object scala> JavaRDD.fromRDD(x).classTag res17: scala.reflect.ClassTag[(Array[Byte], Array[Byte])] = scala.Tuple2  This turns out to be caused by a line in JavaPairRDD that constructed its ClassTag by casting an AnyRef ClassTag, when it should have just used the underlying ScalaRDD's {{elementClassTag}} (since it's already of the right type):  override val classTag: ClassTag[(K, V)] = implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[Tuple2[K, V]]]  should be  override val classTag: ClassTag[(K, V)] = rdd.elementClassTag  I've submitted this fix as part of a pull request: https://github.com/apache/incubator-spark/pull/501\n5. Taka Shinagawa: Josh, thanks for the quick fix! I've confirmed this change fixes the problem.\n6. Michael Broxton: Hi Josh, Yes, I can confirm that this fixes it for me as well. Tremendous thanks for taking a look at this, and for taking the time to explain the fix. Now back to data analysis! Best, -Michael", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.113812"}}
{"id": "1854dd231290b9640d66e255b72f11b4", "issue_key": "SPARK-1035", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Use a single mechanism for distributing jars on Yarn", "description": "When running Spark on Yarn, the app jar is distributed through a different mechanism than additional added jars. The app jar gets to every worker node as a Yarn local resource. Additional jars only get to the app master, and the app master serves them to workers with the HTTP file server. Strangeness comes when an application addJar's the app jar, which is a natural thing to do in mesos or standalone mode, but in Yarn mode, will try to distribute the same jar through a different mechanism. Using the same mechanism for both would eliminate this issue, as well as greatly simplify debugging ClassNotFoundExceptions in workers.", "reporter": "Sanford Ryza", "assignee": null, "created": "2014-01-20T00:16:38.000+0000", "updated": "2014-04-07T20:06:24.000+0000", "resolved": "2014-04-07T20:06:24.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Sandy Ryza", "body": "When I originally filed this, I didn't realize that jars could be added at runtime. In light of this, I don't think we can do much better than the current state of things.", "created": "2014-04-07T20:06:24.244+0000"}], "num_comments": 1, "text": "Issue: SPARK-1035\nSummary: Use a single mechanism for distributing jars on Yarn\nDescription: When running Spark on Yarn, the app jar is distributed through a different mechanism than additional added jars. The app jar gets to every worker node as a Yarn local resource. Additional jars only get to the app master, and the app master serves them to workers with the HTTP file server. Strangeness comes when an application addJar's the app jar, which is a natural thing to do in mesos or standalone mode, but in Yarn mode, will try to distribute the same jar through a different mechanism. Using the same mechanism for both would eliminate this issue, as well as greatly simplify debugging ClassNotFoundExceptions in workers.\n\nComments (1):\n1. Sandy Ryza: When I originally filed this, I didn't realize that jars could be added at runtime. In light of this, I don't think we can do much better than the current state of things.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.115817"}}
{"id": "5dc73ca0ac4055e3b24a3381d4d1ac5c", "issue_key": "SPARK-1036", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": ".gitignore is overly aggressive", "description": "The .gitignore file ignores \"lib/\" but this includes directories named lib anywhere in the tree, including the following:  ./python/lib ./graphx/src/main/scala/org/apache/spark/graphx/lib ./graphx/src/test/scala/org/apache/spark/graphx/lib  It would be confusing and incorrect if any patches accidentally omitted changes to the contents of these directories.", "reporter": "Sean Mackrory", "assignee": "Patrick Wendell", "created": "2014-01-20T14:02:20.000+0000", "updated": "2020-02-07T17:19:49.000+0000", "resolved": "2014-07-26T20:22:01.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "Ooops ....", "created": "2014-01-20T14:07:55.439+0000"}, {"author": "Sean Mackrory", "body": "Here's one suggestion. I don't know if there are other lib/ directories that show up after some actions that should also be ignored, and there are probably other entries that are intended to be ignored in the root directory only, like /logs/, etc...", "created": "2014-01-20T14:08:23.227+0000"}, {"author": "Reynold Xin", "body": "I'm not even sure why we want to exclude the lib folder. Unlike lib_managed, jars in lib are meant to be included. Looks like [~prashant] included it originally. Can you comment on this, [~prashant]?", "created": "2014-01-20T14:12:30.079+0000"}, {"author": "Patrick McFadin", "body": "[~mackrorysd] Could you submit this patch as a PR?", "created": "2014-01-20T14:45:52.886+0000"}, {"author": "Prashant Sharma", "body": "Hey Reynold, I added it initially thinking of the ability to \"provide\" jars in sbt, as in sbt puts everything kept in lib folder on the classpath for run. Since at that point we were not having anything in lib/ so I added it in ignore so that it does not get accidentally committed. But now since we want it we can surely remove the entry from .gitignore or I even liked [~mackrorysd]'s suggestion too.", "created": "2014-01-20T20:36:44.431+0000"}, {"author": "Sean Mackrory", "body": "Thanks, everyone. I've submitted Pull Request #488 to remove this line entirely, given Prashant's comments.", "created": "2014-01-21T07:42:18.535+0000"}, {"author": "Josh Rosen", "body": "Fixed by Patrick in https://github.com/apache/spark/commit/e437069dcecea5b89c2a37f0b7b1f8dd5796b8df", "created": "2014-07-26T20:22:01.558+0000"}], "num_comments": 7, "text": "Issue: SPARK-1036\nSummary: .gitignore is overly aggressive\nDescription: The .gitignore file ignores \"lib/\" but this includes directories named lib anywhere in the tree, including the following:  ./python/lib ./graphx/src/main/scala/org/apache/spark/graphx/lib ./graphx/src/test/scala/org/apache/spark/graphx/lib  It would be confusing and incorrect if any patches accidentally omitted changes to the contents of these directories.\n\nComments (7):\n1. Reynold Xin: Ooops ....\n2. Sean Mackrory: Here's one suggestion. I don't know if there are other lib/ directories that show up after some actions that should also be ignored, and there are probably other entries that are intended to be ignored in the root directory only, like /logs/, etc...\n3. Reynold Xin: I'm not even sure why we want to exclude the lib folder. Unlike lib_managed, jars in lib are meant to be included. Looks like [~prashant] included it originally. Can you comment on this, [~prashant]?\n4. Patrick McFadin: [~mackrorysd] Could you submit this patch as a PR?\n5. Prashant Sharma: Hey Reynold, I added it initially thinking of the ability to \"provide\" jars in sbt, as in sbt puts everything kept in lib folder on the classpath for run. Since at that point we were not having anything in lib/ so I added it in ignore so that it does not get accidentally committed. But now since we want it we can surely remove the entry from .gitignore or I even liked [~mackrorysd]'s suggestion too.\n6. Sean Mackrory: Thanks, everyone. I've submitted Pull Request #488 to remove this line entirely, given Prashant's comments.\n7. Josh Rosen: Fixed by Patrick in https://github.com/apache/spark/commit/e437069dcecea5b89c2a37f0b7b1f8dd5796b8df", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.115817"}}
{"id": "5a2a324e615d913259deebef86846f6b", "issue_key": "SPARK-1037", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "the name of findTaskFromList & findTask in TaskSetManager.scala is confusing", "description": "the name of these two functions is confusing though in the comments the author said that the method does \"dequeue\" tasks from the list but from the name, it is not explicitly indicating that the method will mutate the parameter in  private def findTaskFromList(list: ArrayBuffer[Int]): Option[Int] = { while (!list.isEmpty) { val index = list.last list.trimEnd(1) if (copiesRunning(index) == 0 && !successful(index)) { return Some(index) } } None }", "reporter": "Nan Zhu", "assignee": "Ilya Ganelin", "created": "2014-01-21T11:01:05.000+0000", "updated": "2014-12-15T22:51:56.000+0000", "resolved": "2014-12-15T22:51:32.000+0000", "labels": ["starter"], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "at least findAndDequeueTasks is better? It just confused me when I read the source code... \"why I didn't see any method removing those launched tasks?\"..finally I found them in find* methods Hi, administrators, any comments? If you think it worth changing, please assign it to me, I'm glad to make a quick fix", "created": "2014-01-21T11:06:27.851+0000"}, {"author": "Nan Zhu", "body": "also, I personally think it is better to make findTaskFromList an inner function of findTask?", "created": "2014-01-21T11:16:42.193+0000"}, {"author": "Nan Zhu", "body": "the same problem on findSpeculativeTask..", "created": "2014-01-21T11:24:14.119+0000"}, {"author": "Apache Spark", "body": "User 'ilganeli' has created a pull request for this issue: https://github.com/apache/spark/pull/3665", "created": "2014-12-10T17:48:57.393+0000"}, {"author": "Josh Rosen", "body": "Issue resolved by pull request 3665 [https://github.com/apache/spark/pull/3665]", "created": "2014-12-15T22:51:32.690+0000"}], "num_comments": 5, "text": "Issue: SPARK-1037\nSummary: the name of findTaskFromList & findTask in TaskSetManager.scala is confusing\nDescription: the name of these two functions is confusing though in the comments the author said that the method does \"dequeue\" tasks from the list but from the name, it is not explicitly indicating that the method will mutate the parameter in  private def findTaskFromList(list: ArrayBuffer[Int]): Option[Int] = { while (!list.isEmpty) { val index = list.last list.trimEnd(1) if (copiesRunning(index) == 0 && !successful(index)) { return Some(index) } } None }\n\nComments (5):\n1. Nan Zhu: at least findAndDequeueTasks is better? It just confused me when I read the source code... \"why I didn't see any method removing those launched tasks?\"..finally I found them in find* methods Hi, administrators, any comments? If you think it worth changing, please assign it to me, I'm glad to make a quick fix\n2. Nan Zhu: also, I personally think it is better to make findTaskFromList an inner function of findTask?\n3. Nan Zhu: the same problem on findSpeculativeTask..\n4. Apache Spark: User 'ilganeli' has created a pull request for this issue: https://github.com/apache/spark/pull/3665\n5. Josh Rosen: Issue resolved by pull request 3665 [https://github.com/apache/spark/pull/3665]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.115817"}}
{"id": "0975a8889fcaaf807b1743db26ec581f", "issue_key": "SPARK-1038", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add more fields in JsonProtocol and add tests that verify the JSON itself", "description": "Over time we've added fields to various types that aren't exposed in the protocol, we should do an inventory and add important ones. Also, we should have tests that actually verify the parsing. For instance, say someone changes a field type from T to Option[T]... these will render differently but right now there is no test to verify the stringified output.", "reporter": "Patrick McFadin", "assignee": "OuyangJin", "created": "2014-01-22T15:29:32.000+0000", "updated": "2014-02-21T19:38:52.000+0000", "resolved": "2014-02-21T19:38:52.000+0000", "labels": ["starter"], "components": ["Deploy"], "comments": [{"author": "OuyangJin", "body": "I'll try on this. Can someone put me as Assignee. Thanks!", "created": "2014-02-05T01:55:06.421+0000"}, {"author": "OuyangJin", "body": "PR is here: https://github.com/apache/incubator-spark/pull/551", "created": "2014-02-06T01:37:15.095+0000"}, {"author": "Fabrizio Milo", "body": "This issue should be closed", "created": "2014-02-21T19:07:10.872+0000"}], "num_comments": 3, "text": "Issue: SPARK-1038\nSummary: Add more fields in JsonProtocol and add tests that verify the JSON itself\nDescription: Over time we've added fields to various types that aren't exposed in the protocol, we should do an inventory and add important ones. Also, we should have tests that actually verify the parsing. For instance, say someone changes a field type from T to Option[T]... these will render differently but right now there is no test to verify the stringified output.\n\nComments (3):\n1. OuyangJin: I'll try on this. Can someone put me as Assignee. Thanks!\n2. OuyangJin: PR is here: https://github.com/apache/incubator-spark/pull/551\n3. Fabrizio Milo: This issue should be closed", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.115817"}}
{"id": "dea4460547fc67c153cfd4a4980813d9", "issue_key": "SPARK-1039", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "In-cluster driver will retry infinitely when failed to start unless the user kill it manually", "description": "in the current implementation, the in-cluster driver will fall into a dead loop when it fails to start unless the user kill it manually in DriverRunner.scala: Line 209: keepTrying = supervise && exitCode != 0 && !killed", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-01-22T20:11:25.000+0000", "updated": "2014-03-09T17:41:04.000+0000", "resolved": "2014-03-09T17:41:04.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Nan Zhu", "body": "made a PR https://github.com/apache/incubator-spark/pull/472 there I set an upper bound for the retry times of the in-cluster driver users specify that with spark.driver.maxretrynum", "created": "2014-01-22T20:12:31.601+0000"}, {"author": "Patrick McFadin", "body": "The intended design is that if you select \"supervise\" you want it to retry indefinitely. So I don't think this is a bug.", "created": "2014-03-09T17:40:54.657+0000"}], "num_comments": 2, "text": "Issue: SPARK-1039\nSummary: In-cluster driver will retry infinitely when failed to start unless the user kill it manually\nDescription: in the current implementation, the in-cluster driver will fall into a dead loop when it fails to start unless the user kill it manually in DriverRunner.scala: Line 209: keepTrying = supervise && exitCode != 0 && !killed\n\nComments (2):\n1. Nan Zhu: made a PR https://github.com/apache/incubator-spark/pull/472 there I set an upper bound for the retry times of the in-cluster driver users specify that with spark.driver.maxretrynum\n2. Patrick McFadin: The intended design is that if you select \"supervise\" you want it to retry indefinitely. So I don't think this is a bug.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.115817"}}
{"id": "e5c43a237ab53da0c0e75703fc2e5cc6", "issue_key": "SPARK-1040", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Collect as Map throws a casting exception when run on a JavaPairRDD object", "description": "The error that arises  Exception in thread \"main\" java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lscala.Tuple2; at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:427) at org.apache.spark.api.java.JavaPairRDD.collectAsMap(JavaPairRDD.scala:409)  The code being executed  public static String ImageSummary(final JavaPairRDD<Integer,int[]> inImg) { final Set<Integer> keyList=inImg.collectAsMap().keySet(); for(Integer cVal: keyList) outString+=cVal+\",\"; return outString; }  The line 426-427 from PairRDDFunctions.scala  def collectAsMap(): Map[K, V] = { val data = self.toArray()", "reporter": "Kevin Mader", "assignee": "Josh Rosen", "created": "2014-01-23T02:03:47.000+0000", "updated": "2015-09-23T14:29:16.000+0000", "resolved": "2014-01-26T00:24:06.000+0000", "labels": [], "components": ["Java API"], "comments": [{"author": "Josh Rosen", "body": "I wonder where that cast is being performed, since there's no explicit cast at that line. Does the traceback have any more detail? JavaPairRDD.collectAsMap() is tested at least once in JavaAPISuite: https://github.com/apache/incubator-spark/blob/master/core/src/test/scala/org/apache/spark/JavaAPISuite.java#L290, so I wonder why that test works but your code fails.", "created": "2014-01-23T11:02:08.406+0000"}, {"author": "Kevin Mader", "body": "I tried it again and it's definitely throws the same error. Could it be a type issue since I am using an array of primitives (int[]) in the JavaPairRDD, which Java doesn't always so eagerly support particularly as a generic?", "created": "2014-01-25T14:43:44.951+0000"}, {"author": "Josh Rosen", "body": "I was able to reproduce this:  @Test public void collectAsMapWithIntArrayValues() { // Regression test for SPARK-1040 JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(new Integer[] { 1 })); JavaPairRDD<Integer, int[]> pairRDD = rdd.map(new PairFunction<Integer, Integer, int[]>() { @Override public Tuple2<Integer, int[]> call(Integer x) throws Exception { return new Tuple2<Integer, int[]>(x, new int[] { x }); } }); pairRDD.collect(); // works fine Map<Integer, int[]> map = pairRDD.collectAsMap(); // crashed originally }  I see the same traceback that you reported above. If I create a JavaPairRDD using SparkContext.parallelizePairs(), collectAsMap() works fine; the problem seems to only affect JavaPairRDDs that have been derived by transforming non-JavaPairRDDs. Also, it only seems to affect collectAsMap(); calling collect() on the same RDD works fine. From the error, I suspect that the failing cast has to do with an implicit conversion to PairRDDFunctions.", "created": "2014-01-25T16:23:03.318+0000"}, {"author": "Josh Rosen", "body": "I found a fix and opened a pull request here: https://github.com/apache/incubator-spark/pull/511", "created": "2014-01-25T16:45:59.879+0000"}, {"author": "Pulkit Bhuwalka", "body": "The github link above is not functional anymore.", "created": "2014-09-28T21:36:25.839+0000"}, {"author": "Glenn Strycker", "body": "I am getting a similar error in Spark 1.3.0... see a new ticket I created: https://issues.apache.org/jira/browse/SPARK-10762", "created": "2015-09-22T22:08:02.600+0000"}, {"author": "Glenn Strycker", "body": "My ticket SPARK-10762 may have just been a user error, but was interesting none-the-less... evidently Scala or Spark is not correctly reporting the type of an ArrayBuffer as ArrayBuffer[Any], but claimed it had correctly been cast to ArrayBuffer[(Int,String)].", "created": "2015-09-23T14:29:16.871+0000"}], "num_comments": 7, "text": "Issue: SPARK-1040\nSummary: Collect as Map throws a casting exception when run on a JavaPairRDD object\nDescription: The error that arises  Exception in thread \"main\" java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lscala.Tuple2; at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:427) at org.apache.spark.api.java.JavaPairRDD.collectAsMap(JavaPairRDD.scala:409)  The code being executed  public static String ImageSummary(final JavaPairRDD<Integer,int[]> inImg) { final Set<Integer> keyList=inImg.collectAsMap().keySet(); for(Integer cVal: keyList) outString+=cVal+\",\"; return outString; }  The line 426-427 from PairRDDFunctions.scala  def collectAsMap(): Map[K, V] = { val data = self.toArray()\n\nComments (7):\n1. Josh Rosen: I wonder where that cast is being performed, since there's no explicit cast at that line. Does the traceback have any more detail? JavaPairRDD.collectAsMap() is tested at least once in JavaAPISuite: https://github.com/apache/incubator-spark/blob/master/core/src/test/scala/org/apache/spark/JavaAPISuite.java#L290, so I wonder why that test works but your code fails.\n2. Kevin Mader: I tried it again and it's definitely throws the same error. Could it be a type issue since I am using an array of primitives (int[]) in the JavaPairRDD, which Java doesn't always so eagerly support particularly as a generic?\n3. Josh Rosen: I was able to reproduce this:  @Test public void collectAsMapWithIntArrayValues() { // Regression test for SPARK-1040 JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(new Integer[] { 1 })); JavaPairRDD<Integer, int[]> pairRDD = rdd.map(new PairFunction<Integer, Integer, int[]>() { @Override public Tuple2<Integer, int[]> call(Integer x) throws Exception { return new Tuple2<Integer, int[]>(x, new int[] { x }); } }); pairRDD.collect(); // works fine Map<Integer, int[]> map = pairRDD.collectAsMap(); // crashed originally }  I see the same traceback that you reported above. If I create a JavaPairRDD using SparkContext.parallelizePairs(), collectAsMap() works fine; the problem seems to only affect JavaPairRDDs that have been derived by transforming non-JavaPairRDDs. Also, it only seems to affect collectAsMap(); calling collect() on the same RDD works fine. From the error, I suspect that the failing cast has to do with an implicit conversion to PairRDDFunctions.\n4. Josh Rosen: I found a fix and opened a pull request here: https://github.com/apache/incubator-spark/pull/511\n5. Pulkit Bhuwalka: The github link above is not functional anymore.\n6. Glenn Strycker: I am getting a similar error in Spark 1.3.0... see a new ticket I created: https://issues.apache.org/jira/browse/SPARK-10762\n7. Glenn Strycker: My ticket SPARK-10762 may have just been a user error, but was interesting none-the-less... evidently Scala or Spark is not correctly reporting the type of an ArrayBuffer as ArrayBuffer[Any], but claimed it had correctly been cast to ArrayBuffer[(Int,String)].", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.115817"}}
{"id": "9aa12f8c33f2b517b8436b0036f2a98a", "issue_key": "SPARK-1041", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "ec2-related lines in start-*.sh no longer work", "description": "Due to the change of hostname of EC2 instances, the following lines in the start-* scripts no longer work # Set SPARK_PUBLIC_DNS so slaves can be linked in master web UI if [ \"$SPARK_PUBLIC_DNS\" = \"\" ]; then # If we appear to be running on EC2, use the public address by default: # NOTE: ec2-metadata is installed on Amazon Linux AMI. Check based on that and hostname if command -v ec2-metadata > /dev/null || [[ `hostname` == *ec2.internal ]]; then export SPARK_PUBLIC_DNS=`wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname` fi fi since we have spark-ec2, suggest removing these lines", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-01-23T08:27:11.000+0000", "updated": "2014-02-22T20:37:41.000+0000", "resolved": "2014-02-22T20:35:58.000+0000", "labels": [], "components": [], "comments": [{"author": "Nan Zhu", "body": "PR: https://github.com/apache/incubator-spark/pull/588", "created": "2014-02-21T18:55:36.289+0000"}], "num_comments": 1, "text": "Issue: SPARK-1041\nSummary: ec2-related lines in start-*.sh no longer work\nDescription: Due to the change of hostname of EC2 instances, the following lines in the start-* scripts no longer work # Set SPARK_PUBLIC_DNS so slaves can be linked in master web UI if [ \"$SPARK_PUBLIC_DNS\" = \"\" ]; then # If we appear to be running on EC2, use the public address by default: # NOTE: ec2-metadata is installed on Amazon Linux AMI. Check based on that and hostname if command -v ec2-metadata > /dev/null || [[ `hostname` == *ec2.internal ]]; then export SPARK_PUBLIC_DNS=`wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname` fi fi since we have spark-ec2, suggest removing these lines\n\nComments (1):\n1. Nan Zhu: PR: https://github.com/apache/incubator-spark/pull/588", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.115817"}}
{"id": "7099e5e664cb8723b8634ee21bfcd2a7", "issue_key": "SPARK-1042", "issue_type": "Bug", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "spark cleans all java broadcast variables when it hits the spark.cleaner.ttl", "description": "When setting spark.cleaner.ttl, spark performs the cleanup on time - but it cleans all broadcast variables, not just the ones that are older than the ttl. This creates an exception when the next mapPartitions runs because it cannot find the broadcast variable, even when it was created immediately before running the task. Our temp workaround - not set the ttl and suffer from an ongoing memory leak (forces a restart). We are using JavaSparkContext and our broadcast variables are Java HashMaps.", "reporter": "Tal Sliwowicz", "assignee": "OuyangJin", "created": "2014-01-23T09:29:17.000+0000", "updated": "2014-10-21T06:53:25.000+0000", "resolved": "2014-10-21T06:53:25.000+0000", "labels": ["memory_leak"], "components": ["Java API", "Spark Core"], "comments": [{"author": "OuyangJin", "body": "I'll try to solve this . But I can't put myself as Assignee now.", "created": "2014-01-25T06:38:08.211+0000"}, {"author": "Reynold Xin", "body": "Thanks. I assigned you the ticket.", "created": "2014-01-25T09:43:16.117+0000"}, {"author": "Yinan Li", "body": "If I'm right, you can disable cleaning broadcast variables by setting the property spark.cleaner.ttl.BROADCAST_VARS=-1, when you set spark.cleaner.ttl. Take a look at MetadataCleaner.scala and BlockManager.scala.", "created": "2014-01-28T10:34:42.407+0000"}, {"author": "Yinan Li", "body": "Just realized the fix I mentioned above is only in 0.9. So this still needs to be fixed in 0.8.0 and 0.8.1.", "created": "2014-01-28T10:36:11.677+0000"}, {"author": "Tal Sliwowicz", "body": "Thanks. I'd rather not disable broadcast vars entirely but have some ttl for them too. The issue is that with 0.8.1, once the ttl comes, all the broadcast variables get cleaned, not just the old ones. Was this fixed too in 0.9?", "created": "2014-01-28T12:16:50.679+0000"}, {"author": "OuyangJin", "body": "Sorry for latet comment. @Tal Sliwowicz I wonder which mode are you running spark? local or cluster. for local mode, broadcast variables will only be wriiten in BlockManager as a MEMORY_AND_DISK storage level. Otherwise, they will be written both in BlockManager and local files under control of HttpBroadCast (if you didn't change default broadcast), and both ways have their own clean up function callbacks period done by MetadataCleaner. Could u plz show your detail config for spark mode and BroadCast type, so I can do a further lookup of the src", "created": "2014-01-30T00:37:19.180+0000"}, {"author": "Tal Sliwowicz", "body": "We are running spark 0.8.1 in a mesos 0.13 cluster (with zookeeper - so mesos runs in multi master). We did not change any of the cleaner defaults and are using the fault storage level (memory). Thanks!", "created": "2014-01-30T00:50:42.410+0000"}, {"author": "OuyangJin", "body": "@Tal Sliwowicz I wonder the value you set for spark.cleaner.ttl (in seconds) and the interval between each time you create broadcast valuse. Becuase I found that the way MetaCleaner works is a little tricky: 1 there is a delay which equal to value set to ttl 2 there is a period = max(10, delay / 10), so this period is at least 10 seconds 3 there is a Timer schedule for clean tasks (for clean up broadcast vars under ttl. Cleanup tasks first execute after (period) secnods and then do every (period) seconds. (code is timer.schedule(task, periodSeconds * 1000, periodSeconds * 1000) What I found is that there is no bugs in cleanup function implementation because each time this task check the formula: ( ( currTime - varCreateTime) > ttl ) == true ), if this happens ,all this var entry will be cleaned. But the period may have some blocks. For examples , if period = 10 seconds, and your create 3 broadcast vars just in 10 seconds, and your ttl is 5 seconds. So the first time the clean up task is executed after (period = 10 ) seconds, all broadcast variables just hit the ttl , and will be cleaned. So maybe your setting for ttl and the resulting period value will affect the clean up result. So check whether you ttl is smaller than the resulting period , and whether it is the reason which cause your issue description. And if you find out that it 's not the reason, I'll just look for other reasons and solutions.Thanks!", "created": "2014-01-31T05:20:18.862+0000"}, {"author": "Tal Sliwowicz", "body": "Thanks for the detailed reply! We tried with ttl = 300 . We have a loop were we periodically, each iteration, create new RDDs and run calculations over them. Each loop iteration we also create new broadcast variables. These variables and RDDs are only used inside the specific loop iteration, and never again. When we use the ttl it kept giving exceptions on broadcast variables that were not available to the worker nodes. Is this some thing that does not reproduce for you?", "created": "2014-02-03T00:02:56.700+0000"}, {"author": "OuyangJin", "body": "Hi,@Tal, I'm working on this, it would be wonderful if you give me the detail print stack for what you describle of 'exceptions on broadcast variables that were not available to the worker nodes', at least the class name of the Exception,so I can do a detail debug . Thanks!", "created": "2014-02-05T19:13:48.027+0000"}, {"author": "Tal Sliwowicz", "body": "Hi @OuyangJin We tried to reproduce again (on a different driver jar, but it runs the same way as described in the ticket) - this time received the error below. We tried several times, and it is very consistent. Notice - the \"No space left on device\" is bogus. There is 100G+ free space. We are using Spark 0.8.1. java.io.FileNotFoundException (java.io.FileNotFoundException: /var/lib/spark/data/spark-local-20140218081428-ae01/2e/merged_shuffle_17562_361_0 (No space left on device)) java.io.FileOutputStream.openAppend(Native Method) java.io.FileOutputStream.<init>(FileOutputStream.java:192) org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:114) org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:173) org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:162) org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:159) scala.collection.Iterator$class.foreach(Iterator.scala:772) scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399) org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:159) org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:100) org.apache.spark.scheduler.Task.run(Task.scala:53) org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:215) org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:50) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182) java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) java.lang.Thread.run(Thread.java:662)", "created": "2014-02-18T04:25:04.355+0000"}, {"author": "Tal Sliwowicz", "body": "We are planning to test this week with 0.9. I will update.", "created": "2014-03-03T06:10:26.396+0000"}, {"author": "OuyangJin", "body": "Hi @Tal, sorry for late reply . What I can see from your exception which was posted days ago happened during shuffle phase. And this code segment (exception report in ShuffleMapTask.runTask) is a logic which write map output to its associate buckets. This is a pre-phase for standard shuffle which in latter phase ResultTask can read this output using socket or netty depending on your config. I wonder your broadcast variable are for shuffle use? You say\"Each loop iteration we also create new broadcast variables\" You use the broadcast to store a result of RDD?(like sc.broadcast(rdd.collect().toMap()) and use it inside of a closure of another RDD? Maybe you can show me the code logic inside of your loop(you can use simplified pseudocode if you think it's not suitable to show your real code relating some copyright) Thanks!", "created": "2014-03-03T19:24:34.182+0000"}, {"author": "Tal Sliwowicz", "body": "You are correct - there is a mismatch. It seems that there was a corruption of the filesystem which caused the error above. It is now fixed. We will try to reproduce again the issue with the broadcast using the 0.9 we now have running (the original exception was specific to broadcast variables, not a general one)", "created": "2014-03-03T21:55:54.339+0000"}, {"author": "OuyangJin", "body": "@Tal. Thanks for your progress. When you mean fixed, you mean you fix a bug in your user code or in your local filesystem?", "created": "2014-03-03T22:14:22.067+0000"}, {"author": "Tal Sliwowicz", "body": "Local Filesystem (reformatted)", "created": "2014-03-03T22:28:30.582+0000"}, {"author": "OuyangJin", "body": "Great. So any progress in your testing for 0.9.0, please let me know so that I can help you track this bug. If this ttl feature really don't work as it declare, I'll fix it with you test support Thanks", "created": "2014-03-03T22:38:28.473+0000"}, {"author": "Tal Sliwowicz", "body": "Tested on 0.9 This is an exception stack with ttl=120 (the job takes under 60 seconds and then gets rid of all the broadcast variables. It creates new ones each iteration) 2014-03-05 09:01:59,165 ERROR [Executor task launch worker-17] Executor - Exception in task ID 225722 java.io.FileNotFoundException: http://192.168.10.85:54174/broadcast_312 at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1457) at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156) at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56) at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:69) at org.apache.spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:138) at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1795) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1754) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:662) 2014-03-05 09:01:59,168 ERROR [Executor task launch worker-11] Executor - Exception in task ID 225734 java.io.FileNotFoundException: http://192.168.10.85:54174/broadcast_312 at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1457) at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:156) at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56) at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:969) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1852) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1950) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1874) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1756) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:69) at org.apache.spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:138) at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1795) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1754) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1326) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:662)", "created": "2014-03-05T06:39:45.760+0000"}, {"author": "OuyangJin", "body": "Great, I'll look into it. Any progress will be reported ASAP when I found the reason for it", "created": "2014-03-05T19:42:27.218+0000"}, {"author": "OuyangJin", "body": "@Tal, according to your execption reported, I found out that it happens during task deserialization.So it looks like you put a broadcast variable inside a rdd closure ,and when excutor what to run a task, it has to deserialize a rdd closure together with broadcast variable, and the broadcast varialbe can't be found in the HttpServer I'm still not sure the reason exactly causes this problem. I'm sorry for that. There are 3 problems I want to be sure for your user code. 1 You say that you create new broadcast variables each time your enter the loop, and each boradcast variable has a ttl for 120 seconds. Are you sure that your rdd closre just use the newly created broadcast variables, not old ones in previous loop . 2 You say your job runs for 60 seconds and all broadcast varialbe cleaned up. In this 60 seconds, how many iterations of your loop happens? Just 1 round of your loop or serveral rounds. 3 When broadcast variables are cleaned up, there will be logs like \"Deleted broadcast file\" , And broadcast variables will be stored on machines which run your driver program. Have your ever see these log in your console or driver machine Thanks. Sorry for still not finding the exact reason. But previously community also reported several user cases that program surprisingly clean up their broadcast varialbe which are still in use, due to httpserver down which storing the broadcast varialbes, or time not correctly caculated. So now some people are developing a new clean up way which will replace MetadataCleaner and TTL config, in this PR:https://github.com/apache/spark/pull/126 But I still want to figure out the exact reason because many people are still using 0.9 version. It's best if you can provide some detail information", "created": "2014-03-15T07:33:52.586+0000"}, {"author": "Tal Sliwowicz", "body": "1. Yes, I'm sure it creates and uses a news instance 2. Just 1 round. I can increase the ttl, but it it still happens. 3. I turned logs to WARN. Can you be more specific about the classes I should increase logging on? I will move them to info or even debug", "created": "2014-04-16T08:59:32.793+0000"}, {"author": "Tal Sliwowicz", "body": "About 3 - the package org.apache.spark.broadcast , right?", "created": "2014-04-16T09:05:03.209+0000"}, {"author": "OuyangJin", "body": "yes is in org.apache.spark.broadcast, \"Deleted broadcast file\" ,this is a info log", "created": "2014-04-16T09:35:31.154+0000"}, {"author": "Tal Sliwowicz", "body": "[~qqsun8819] I think the issue was resolved in 0.9.2. We are not experiencing it any more. Thanks!", "created": "2014-10-20T08:30:27.838+0000"}, {"author": "Patrick Wendell", "body": "I think this was fixed back in 0.9.2", "created": "2014-10-21T06:53:25.783+0000"}], "num_comments": 25, "text": "Issue: SPARK-1042\nSummary: spark cleans all java broadcast variables when it hits the spark.cleaner.ttl\nDescription: When setting spark.cleaner.ttl, spark performs the cleanup on time - but it cleans all broadcast variables, not just the ones that are older than the ttl. This creates an exception when the next mapPartitions runs because it cannot find the broadcast variable, even when it was created immediately before running the task. Our temp workaround - not set the ttl and suffer from an ongoing memory leak (forces a restart). We are using JavaSparkContext and our broadcast variables are Java HashMaps.\n\nComments (25):\n1. OuyangJin: I'll try to solve this . But I can't put myself as Assignee now.\n2. Reynold Xin: Thanks. I assigned you the ticket.\n3. Yinan Li: If I'm right, you can disable cleaning broadcast variables by setting the property spark.cleaner.ttl.BROADCAST_VARS=-1, when you set spark.cleaner.ttl. Take a look at MetadataCleaner.scala and BlockManager.scala.\n4. Yinan Li: Just realized the fix I mentioned above is only in 0.9. So this still needs to be fixed in 0.8.0 and 0.8.1.\n5. Tal Sliwowicz: Thanks. I'd rather not disable broadcast vars entirely but have some ttl for them too. The issue is that with 0.8.1, once the ttl comes, all the broadcast variables get cleaned, not just the old ones. Was this fixed too in 0.9?\n6. OuyangJin: Sorry for latet comment. @Tal Sliwowicz I wonder which mode are you running spark? local or cluster. for local mode, broadcast variables will only be wriiten in BlockManager as a MEMORY_AND_DISK storage level. Otherwise, they will be written both in BlockManager and local files under control of HttpBroadCast (if you didn't change default broadcast), and both ways have their own clean up function callbacks period done by MetadataCleaner. Could u plz show your detail config for spark mode and BroadCast type, so I can do a further lookup of the src\n7. Tal Sliwowicz: We are running spark 0.8.1 in a mesos 0.13 cluster (with zookeeper - so mesos runs in multi master). We did not change any of the cleaner defaults and are using the fault storage level (memory). Thanks!\n8. OuyangJin: @Tal Sliwowicz I wonder the value you set for spark.cleaner.ttl (in seconds) and the interval between each time you create broadcast valuse. Becuase I found that the way MetaCleaner works is a little tricky: 1 there is a delay which equal to value set to ttl 2 there is a period = max(10, delay / 10), so this period is at least 10 seconds 3 there is a Timer schedule for clean tasks (for clean up broadcast vars under ttl. Cleanup tasks first execute after (period) secnods and then do every (period) seconds. (code is timer.schedule(task, periodSeconds * 1000, periodSeconds * 1000) What I found is that there is no bugs in cleanup function implementation because each time this task check the formula: ( ( currTime - varCreateTime) > ttl ) == true ), if this happens ,all this var entry will be cleaned. But the period may have some blocks. For examples , if period = 10 seconds, and your create 3 broadcast vars just in 10 seconds, and your ttl is 5 seconds. So the first time the clean up task is executed after (period = 10 ) seconds, all broadcast variables just hit the ttl , and will be cleaned. So maybe your setting for ttl and the resulting period value will affect the clean up result. So check whether you ttl is smaller than the resulting period , and whether it is the reason which cause your issue description. And if you find out that it 's not the reason, I'll just look for other reasons and solutions.Thanks!\n9. Tal Sliwowicz: Thanks for the detailed reply! We tried with ttl = 300 . We have a loop were we periodically, each iteration, create new RDDs and run calculations over them. Each loop iteration we also create new broadcast variables. These variables and RDDs are only used inside the specific loop iteration, and never again. When we use the ttl it kept giving exceptions on broadcast variables that were not available to the worker nodes. Is this some thing that does not reproduce for you?\n10. OuyangJin: Hi,@Tal, I'm working on this, it would be wonderful if you give me the detail print stack for what you describle of 'exceptions on broadcast variables that were not available to the worker nodes', at least the class name of the Exception,so I can do a detail debug . Thanks!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.117821"}}
{"id": "cee45c28535c155cb9ec45b97ebdc4ed", "issue_key": "SPARK-1043", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Pyspark RDD's cannot deal with strings greater than 64K bytes.", "description": "Pyspark uses java.io.DataOutputStream.writeUTF to send data to the python world which causes a problem since java.io.DataOutputStream.writeUTF fails if you pass it strings above 64K bytes. Furthermore a fix to this issue is not straight forward since the java to python protocol actually relies on this and uses it as the separator between items. The offending write happens in core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:220 and the reliance on this to separate items can be found in the MUTF8Deserializer class in python/pyspark/serializers.py:264 The only solution I currently have in mind would be to change the protocol to either extend the number of bytes used to specify the length of the item or to add a boolean flag to every \"packet\" to indicate wether the item is split into multiple parts (although the second option might result in bad data if multiple things are writing to these steams)", "reporter": "Erik Selin", "assignee": "Josh Rosen", "created": "2014-01-24T13:18:33.000+0000", "updated": "2014-01-28T21:32:55.000+0000", "resolved": "2014-01-28T21:32:55.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Thanks for catching this. I'll look into alternative encodings that support longer strings and add a test case for large strings to make sure this doesn't break again in the future.", "created": "2014-01-24T22:28:02.084+0000"}, {"author": "Erik Selin", "body": "I've submitted a quick fix to this https://github.com/apache/incubator-spark/pull/512 not sure if you guys usually discuss things here or on github so I'm linking this to the pr and the pr to this :)", "created": "2014-01-26T11:57:50.563+0000"}, {"author": "Bouke van der Bijl", "body": "Note that the current encoding will also break if any of the modified characters described here: http://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#modified-utf-8 are used. The python side doesn't implement anything to mitigate this, it's probably safest to switch to vanilla UTF-8", "created": "2014-01-26T14:15:21.395+0000"}, {"author": "Josh Rosen", "body": "There's no reason that PySpark has to use MUTF-8; this was a bad choice that I made when implementing custom serializer support (https://github.com/apache/incubator-spark/pull/146), so I'd switch to vanilla UTF-8 instead.", "created": "2014-01-26T14:24:05.461+0000"}, {"author": "Josh Rosen", "body": "Fixed in https://github.com/apache/incubator-spark/pull/523", "created": "2014-01-28T21:32:55.841+0000"}], "num_comments": 5, "text": "Issue: SPARK-1043\nSummary: Pyspark RDD's cannot deal with strings greater than 64K bytes.\nDescription: Pyspark uses java.io.DataOutputStream.writeUTF to send data to the python world which causes a problem since java.io.DataOutputStream.writeUTF fails if you pass it strings above 64K bytes. Furthermore a fix to this issue is not straight forward since the java to python protocol actually relies on this and uses it as the separator between items. The offending write happens in core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala:220 and the reliance on this to separate items can be found in the MUTF8Deserializer class in python/pyspark/serializers.py:264 The only solution I currently have in mind would be to change the protocol to either extend the number of bytes used to specify the length of the item or to add a boolean flag to every \"packet\" to indicate wether the item is split into multiple parts (although the second option might result in bad data if multiple things are writing to these steams)\n\nComments (5):\n1. Josh Rosen: Thanks for catching this. I'll look into alternative encodings that support longer strings and add a test case for large strings to make sure this doesn't break again in the future.\n2. Erik Selin: I've submitted a quick fix to this https://github.com/apache/incubator-spark/pull/512 not sure if you guys usually discuss things here or on github so I'm linking this to the pr and the pr to this :)\n3. Bouke van der Bijl: Note that the current encoding will also break if any of the modified characters described here: http://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#modified-utf-8 are used. The python side doesn't implement anything to mitigate this, it's probably safest to switch to vanilla UTF-8\n4. Josh Rosen: There's no reason that PySpark has to use MUTF-8; this was a bad choice that I made when implementing custom serializer support (https://github.com/apache/incubator-spark/pull/146), so I'd switch to vanilla UTF-8 instead.\n5. Josh Rosen: Fixed in https://github.com/apache/incubator-spark/pull/523", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.117821"}}
{"id": "cc46388f37b22ccf37750f538b6cc7b0", "issue_key": "SPARK-1044", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Default spark logs location in EC2 AMI leads to out-of-disk space pretty soon", "description": "The default log location is SPARK_HOME/work/ and this leads to disk space running out pretty quickly. The spark-ec2 scripts should configure the cluster to automatically set the logging directory to /mnt/spark-work/ or something like that on the mounted disks. The SPARK_HOME/work may also be symlinked to that directory to maintain the existing setup.", "reporter": "Tathagata Das", "assignee": null, "created": "2014-01-24T13:29:23.000+0000", "updated": "2016-01-12T13:45:13.000+0000", "resolved": "2016-01-12T13:45:13.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Andrew Ash", "body": "Filling up the work dir could be alleviated by fixing https://issues.apache.org/jira/browse/SPARK-1860 so we could enable worker dir cleanup automatically. If we had automatic worker dir cleanup, would you still want to move the work directory to somewhere else?", "created": "2014-07-25T01:19:30.837+0000"}, {"author": "Allan Douglas R. de Oliveira", "body": "I think it is still a good idea even if the automatic cleanup is implemented. One large job or many small jobs can fill many gigabytes before the cleanup can kick in.", "created": "2014-07-25T01:31:34.556+0000"}, {"author": "Sean R. Owen", "body": "I'm assuming lots of smaller EC2 issues are obsolete as it's moving out of Spark", "created": "2016-01-12T13:45:13.656+0000"}], "num_comments": 3, "text": "Issue: SPARK-1044\nSummary: Default spark logs location in EC2 AMI leads to out-of-disk space pretty soon\nDescription: The default log location is SPARK_HOME/work/ and this leads to disk space running out pretty quickly. The spark-ec2 scripts should configure the cluster to automatically set the logging directory to /mnt/spark-work/ or something like that on the mounted disks. The SPARK_HOME/work may also be symlinked to that directory to maintain the existing setup.\n\nComments (3):\n1. Andrew Ash: Filling up the work dir could be alleviated by fixing https://issues.apache.org/jira/browse/SPARK-1860 so we could enable worker dir cleanup automatically. If we had automatic worker dir cleanup, would you still want to move the work directory to somewhere else?\n2. Allan Douglas R. de Oliveira: I think it is still a good idea even if the automatic cleanup is implemented. One large job or many small jobs can fill many gigabytes before the cleanup can kick in.\n3. Sean R. Owen: I'm assuming lots of smaller EC2 issues are obsolete as it's moving out of Spark", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.117821"}}
{"id": "7e1e5956b33914b3311bdb87b486aa07", "issue_key": "SPARK-1045", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ExternalAppendOnlyMap Iterator throw no such element on joining two large rdd", "description": "On latest master branch 05be7047744c88e64e7e6bd973f9bcfacd00da5f, I keep getting no such element from a single shuffle task when performance join on two large rdd (memory spill 10G, disk spill 800m for a single task) the code is here: https://gist.github.com/guojc/8643741 the exception is java.util.NoSuchElementException (java.util.NoSuchElementException) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:304) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:239) org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29) scala.collection.Iterator$$anon$11.next(Iterator.scala:328) scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371) scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:724) org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:734) org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:734) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109) org.apache.spark.scheduler.Task.run(Task.scala:53) org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) java.lang.Thread.run(Thread.java:662)", "reporter": "Jiacheng Guo", "assignee": null, "created": "2014-01-26T22:28:49.000+0000", "updated": "2020-05-17T18:30:42.000+0000", "resolved": "2014-03-09T17:43:04.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Jiacheng Guo", "body": "I was able to finally identify the bug as StreamBuffer.compareTo method's ill defined behavior when key's hashCode equals to Int.MaxValue. Though this only occur in aboue 1/2^32 chance, it can happen a lot when your key size approach 2^32. I have create a pull request for the bug fix https://github.com/apache/incubator-spark/pull/612", "created": "2014-02-18T02:58:36.557+0000"}], "num_comments": 1, "text": "Issue: SPARK-1045\nSummary: ExternalAppendOnlyMap Iterator throw no such element on joining two large rdd\nDescription: On latest master branch 05be7047744c88e64e7e6bd973f9bcfacd00da5f, I keep getting no such element from a single shuffle task when performance join on two large rdd (memory spill 10G, disk spill 800m for a single task) the code is here: https://gist.github.com/guojc/8643741 the exception is java.util.NoSuchElementException (java.util.NoSuchElementException) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:304) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:239) org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29) scala.collection.Iterator$$anon$11.next(Iterator.scala:328) scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371) scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) org.apache.spark.rdd.PairRDDFunctions.org$apache$spark$rdd$PairRDDFunctions$$writeToFile$1(PairRDDFunctions.scala:724) org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:734) org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$2.apply(PairRDDFunctions.scala:734) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109) org.apache.spark.scheduler.Task.run(Task.scala:53) org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) java.lang.Thread.run(Thread.java:662)\n\nComments (1):\n1. Jiacheng Guo: I was able to finally identify the bug as StreamBuffer.compareTo method's ill defined behavior when key's hashCode equals to Int.MaxValue. Though this only occur in aboue 1/2^32 chance, it can happen a lot when your key size approach 2^32. I have create a pull request for the bug fix https://github.com/apache/incubator-spark/pull/612", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.117821"}}
{"id": "b18ef990478a68e0cc88aefbcf022ed6", "issue_key": "SPARK-1046", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Enable to build behind a proxy.", "description": "I tried to build spark-0.8.1 behind proxy and failed although I set http/https.proxyHost, proxyPort, proxyUser, proxyPassword. I found it's caused by accessing github using git protocol (git://). The URL is hard-corded in SparkPluginBuild.scala as follows.  lazy val junitXmlListener = uri(\"git://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce\")  After I rewrite the URL as follows, I could build successfully.  lazy val junitXmlListener = uri(\"https://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce\")  I think we should be able to build whether we are behind a proxy or not.", "reporter": "Kousuke Saruta", "assignee": null, "created": "2014-01-26T22:37:43.000+0000", "updated": "2014-09-27T19:07:03.000+0000", "resolved": "2014-09-27T19:07:03.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Reynold Xin", "body": "Thanks for reporting this. Do you mind submitting a pull request for this?", "created": "2014-01-26T22:41:37.701+0000"}, {"author": "Kousuke Saruta", "body": "Sure! I'll submit a pull request later.", "created": "2014-01-26T23:26:00.177+0000"}, {"author": "Sean R. Owen", "body": "Is this stale / resolved? I don't see this in the code at this point.", "created": "2014-06-21T21:00:14.580+0000"}], "num_comments": 3, "text": "Issue: SPARK-1046\nSummary: Enable to build behind a proxy.\nDescription: I tried to build spark-0.8.1 behind proxy and failed although I set http/https.proxyHost, proxyPort, proxyUser, proxyPassword. I found it's caused by accessing github using git protocol (git://). The URL is hard-corded in SparkPluginBuild.scala as follows.  lazy val junitXmlListener = uri(\"git://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce\")  After I rewrite the URL as follows, I could build successfully.  lazy val junitXmlListener = uri(\"https://github.com/ijuma/junit_xml_listener.git#fe434773255b451a38e8d889536ebc260f4225ce\")  I think we should be able to build whether we are behind a proxy or not.\n\nComments (3):\n1. Reynold Xin: Thanks for reporting this. Do you mind submitting a pull request for this?\n2. Kousuke Saruta: Sure! I'll submit a pull request later.\n3. Sean R. Owen: Is this stale / resolved? I don't see this in the code at this point.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.117821"}}
{"id": "e6a7ecd2bf09ba65b4aacbb17e57a7be", "issue_key": "SPARK-1047", "issue_type": "Bug", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Ability to disable the spark ui server (unit tests)", "description": "Provide the ability to disable the internal jetty server that serves the web ui. It's not needed when running unit tests and often conflicts may other ports on the system.", "reporter": "Heiko Braun", "assignee": "Andrew Or", "created": "2014-01-27T07:12:12.000+0000", "updated": "2015-01-08T23:10:12.000+0000", "resolved": "2014-09-12T00:24:26.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Heiko Braun", "body": "The PR is here: https://github.com/apache/incubator-spark/pull/518/", "created": "2014-01-27T08:09:48.167+0000"}, {"author": "Andrew Or", "body": "Here's the more updated PR: https://github.com/apache/spark/pull/2363", "created": "2014-09-11T19:05:13.440+0000"}], "num_comments": 2, "text": "Issue: SPARK-1047\nSummary: Ability to disable the spark ui server (unit tests)\nDescription: Provide the ability to disable the internal jetty server that serves the web ui. It's not needed when running unit tests and often conflicts may other ports on the system.\n\nComments (2):\n1. Heiko Braun: The PR is here: https://github.com/apache/incubator-spark/pull/518/\n2. Andrew Or: Here's the more updated PR: https://github.com/apache/spark/pull/2363", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.117821"}}
{"id": "11cf7ce585c83004ce32bea2e18a51be", "issue_key": "SPARK-1048", "issue_type": "New Feature", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Create spark-site.xml or spark-site.yaml for configuration", "description": "Currently, spark doesn't provide the ability to pass configuration as a file, people have to explicitly specify them on the command line. This becomes a bigger issue when deploying spark on a cluster and people want to specify parameters other than default value. As of now, spark is being a top apache project and we need to pay more attention to the configuration. Most apache projects provide a xml configuration file (like hdfs, hadoop, hbase, etc.) and probably we want to do the same thing for spark. The advantages are obvious, it helps developers to specify their own spark configuration for the cluster and add/remove configuration parameters will be much easier via file than via system property.", "reporter": "Shengzhe Yao", "assignee": null, "created": "2014-01-27T16:49:57.000+0000", "updated": "2014-09-21T09:25:52.000+0000", "resolved": "2014-05-13T21:22:52.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Shengzhe Yao", "body": "I can work on it if community like this idea and there is no work have been done.", "created": "2014-01-27T16:52:59.041+0000"}, {"author": "Reynold Xin", "body": "Take a look at http://apache-spark-developers-list.1001551.n3.nabble.com/Moving-to-Typesafe-Config-td381.html", "created": "2014-01-27T16:58:29.466+0000"}, {"author": "Matei Alexandru Zaharia", "body": "As you might see on the previous discussion on this, we actually had some issues with config files. The main one is how to pass the file to the application -- should it just be on the classpath for example? A lot of Spark's properties are very specific to each application (e.g. spark.serializer) and do not make sense to configure globally. But if you suggest a design that 1) does not break the existing forms of configuration and 2) is easy to understand regarding what's application-specific and global, we can add it in.", "created": "2014-01-27T21:23:28.323+0000"}, {"author": "Evan Chan", "body": "The application can always slurp in a config file itself and pass it in programmatically. It would be super nice to have a fromFile method in SparkConf however.", "created": "2014-01-28T17:04:27.365+0000"}, {"author": "Shengzhe Yao", "body": "As Evan mentioned, I would prefer to 1) Add an environment variable SPARK_CONFIG_FILE to point the configuration file path. 2) In SparkConf class, add logic to read SPARK_CONFIG_FILE variable and parse the configuration file in the beginning of the if statement. SparkConf.scala Line 50 if (loadDefaults) { // Add logic to read SPARK_CONFIG_FILE variable and parse the configuration file // Load any spark.* system properties for ((k, v) <- System.getProperties.asScala if k.startsWith(\"spark.\")) { settings(k) = v } } In this way, spark will first read the given configuration file and then apply the system property; therefore, properties specified in command line always have the highest priority. Note: this design doesn't solve the problem Matei mentioned: is easy to understand regarding what's application-specific and global. But again, I think the point is to free programmer's hands by providing a way to pass configurations via a config file. For application specific ones, developer could still specify them in command line or provide their own mechanisms to pass into program. BTW: I use following command to find all configurations we use in spark, it seems we have 129 configurations and it would be better to allow people pass them via a configuration file (if they want to use a different value other than default). find . -name \"\\*.scala\" | xargs -I{} grep \"get.\\*(\\\"&#92;\\|set.*(\\\"\" {} | perl -pe 's|^.\\*?(get&#92;|set).\\*?&#92;(\\\"(.\\*?)\\\".\\*$|\\2|' | grep \"^spark\" | sort | uniq", "created": "2014-01-28T20:25:35.348+0000"}, {"author": "Thomas Graves", "body": "The original discussion about using the type safe configs was to pick up configs like: java system proprties -> application specific file -> cluster config file (spark.conf). Along the way it seems it changed to just java system properties -> cluster file (spark.conf). Was there a reason it dropped the application file?", "created": "2014-01-29T06:14:22.298+0000"}, {"author": "Kousuke Saruta", "body": "Any update on this issue? Is someone trying to address?", "created": "2014-05-06T23:47:49.670+0000"}, {"author": "Shengzhe Yao", "body": "[~sarutak] I am supposed to do the job but didn't figure out all use cases. This could be as simple as add a fromFile method in SparkConf as [~velvia] mentioned, but config file format should be addressed cleanly (like Hadoop/HBase's xml config file or something else ?). I might to add a xml based config file in next few weeks, but please feel free if you have ideas and want to do it now. I am happy to see people like this idea: config file for Spark.", "created": "2014-05-06T23:54:46.859+0000"}, {"author": "Shengzhe Yao", "body": "Patrick submit a patch which introduces spark-defaults.conf to set system properties. This seems to achieve our original purpose: allow user to set configuration values in a well-known config file and Spark will automatically pick it up. commit fb98488fc8e68cc84f6e0750fd4e9e29029879d2 Author: Patrick Wendell <pwendell@gmail.com> Date: Mon Apr 21 10:26:33 2014 -0700 Clean up and simplify Spark configuration Over time as we've added more deployment modes, this have gotten a bit unwieldy with user-facing configuration options in Spark. Going forward we'll advise all users to run `spark-submit` to launch applications. This is a WIP patch but it makes the following improvements: 1. Improved `spark-env.sh.template` which was missing a lot of things users now set in that file. 2. Removes the shipping of SPARK_CLASSPATH, SPARK_JAVA_OPTS, and SPARK_LIBRARY_PATH to the executors on the cluster. This was an ugly hack. Instead it introduces config variables spark.executor.extraJavaOpts, spark.executor.extraLibraryPath, and spark.executor.extraClassPath. 3. Adds ability to set these same variables for the driver using `spark-submit`. 4. Allows you to load system properties from a `spark-defaults.conf` file when running `spark-submit`. This will allow setting both SparkConf options and other system properties utilized by `spark-submit`. 5. Made `SPARK_LOCAL_IP` an environment variable rather than a SparkConf property. This is more consistent with it being set on each node. Author: Patrick Wendell <pwendell@gmail.com>", "created": "2014-05-13T21:22:52.486+0000"}], "num_comments": 9, "text": "Issue: SPARK-1048\nSummary: Create spark-site.xml or spark-site.yaml for configuration\nDescription: Currently, spark doesn't provide the ability to pass configuration as a file, people have to explicitly specify them on the command line. This becomes a bigger issue when deploying spark on a cluster and people want to specify parameters other than default value. As of now, spark is being a top apache project and we need to pay more attention to the configuration. Most apache projects provide a xml configuration file (like hdfs, hadoop, hbase, etc.) and probably we want to do the same thing for spark. The advantages are obvious, it helps developers to specify their own spark configuration for the cluster and add/remove configuration parameters will be much easier via file than via system property.\n\nComments (9):\n1. Shengzhe Yao: I can work on it if community like this idea and there is no work have been done.\n2. Reynold Xin: Take a look at http://apache-spark-developers-list.1001551.n3.nabble.com/Moving-to-Typesafe-Config-td381.html\n3. Matei Alexandru Zaharia: As you might see on the previous discussion on this, we actually had some issues with config files. The main one is how to pass the file to the application -- should it just be on the classpath for example? A lot of Spark's properties are very specific to each application (e.g. spark.serializer) and do not make sense to configure globally. But if you suggest a design that 1) does not break the existing forms of configuration and 2) is easy to understand regarding what's application-specific and global, we can add it in.\n4. Evan Chan: The application can always slurp in a config file itself and pass it in programmatically. It would be super nice to have a fromFile method in SparkConf however.\n5. Shengzhe Yao: As Evan mentioned, I would prefer to 1) Add an environment variable SPARK_CONFIG_FILE to point the configuration file path. 2) In SparkConf class, add logic to read SPARK_CONFIG_FILE variable and parse the configuration file in the beginning of the if statement. SparkConf.scala Line 50 if (loadDefaults) { // Add logic to read SPARK_CONFIG_FILE variable and parse the configuration file // Load any spark.* system properties for ((k, v) <- System.getProperties.asScala if k.startsWith(\"spark.\")) { settings(k) = v } } In this way, spark will first read the given configuration file and then apply the system property; therefore, properties specified in command line always have the highest priority. Note: this design doesn't solve the problem Matei mentioned: is easy to understand regarding what's application-specific and global. But again, I think the point is to free programmer's hands by providing a way to pass configurations via a config file. For application specific ones, developer could still specify them in command line or provide their own mechanisms to pass into program. BTW: I use following command to find all configurations we use in spark, it seems we have 129 configurations and it would be better to allow people pass them via a configuration file (if they want to use a different value other than default). find . -name \"\\*.scala\" | xargs -I{} grep \"get.\\*(\\\"&#92;\\|set.*(\\\"\" {} | perl -pe 's|^.\\*?(get&#92;|set).\\*?&#92;(\\\"(.\\*?)\\\".\\*$|\\2|' | grep \"^spark\" | sort | uniq\n6. Thomas Graves: The original discussion about using the type safe configs was to pick up configs like: java system proprties -> application specific file -> cluster config file (spark.conf). Along the way it seems it changed to just java system properties -> cluster file (spark.conf). Was there a reason it dropped the application file?\n7. Kousuke Saruta: Any update on this issue? Is someone trying to address?\n8. Shengzhe Yao: [~sarutak] I am supposed to do the job but didn't figure out all use cases. This could be as simple as add a fromFile method in SparkConf as [~velvia] mentioned, but config file format should be addressed cleanly (like Hadoop/HBase's xml config file or something else ?). I might to add a xml based config file in next few weeks, but please feel free if you have ideas and want to do it now. I am happy to see people like this idea: config file for Spark.\n9. Shengzhe Yao: Patrick submit a patch which introduces spark-defaults.conf to set system properties. This seems to achieve our original purpose: allow user to set configuration values in a well-known config file and Spark will automatically pick it up. commit fb98488fc8e68cc84f6e0750fd4e9e29029879d2 Author: Patrick Wendell <pwendell@gmail.com> Date: Mon Apr 21 10:26:33 2014 -0700 Clean up and simplify Spark configuration Over time as we've added more deployment modes, this have gotten a bit unwieldy with user-facing configuration options in Spark. Going forward we'll advise all users to run `spark-submit` to launch applications. This is a WIP patch but it makes the following improvements: 1. Improved `spark-env.sh.template` which was missing a lot of things users now set in that file. 2. Removes the shipping of SPARK_CLASSPATH, SPARK_JAVA_OPTS, and SPARK_LIBRARY_PATH to the executors on the cluster. This was an ugly hack. Instead it introduces config variables spark.executor.extraJavaOpts, spark.executor.extraLibraryPath, and spark.executor.extraClassPath. 3. Adds ability to set these same variables for the driver using `spark-submit`. 4. Allows you to load system properties from a `spark-defaults.conf` file when running `spark-submit`. This will allow setting both SparkConf options and other system properties utilized by `spark-submit`. 5. Made `SPARK_LOCAL_IP` an environment variable rather than a SparkConf property. This is more consistent with it being set on each node. Author: Patrick Wendell <pwendell@gmail.com>", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.117821"}}
{"id": "d4a430e2d7f8cb780069c212d74c6cca", "issue_key": "SPARK-1049", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark on yarn - yarn-client mode doesn't always exit properly", "description": "When running in spark on yarn in yarn-client mode, the application master doesn't always exit in a timely manner if its still waiting on containers from the yarn resource manager.", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-01-30T08:27:04.000+0000", "updated": "2014-04-08T20:40:08.000+0000", "resolved": "2014-04-08T20:40:08.000+0000", "labels": [], "components": [], "comments": [{"author": "Thomas Graves", "body": "https://github.com/apache/incubator-spark/pull/526", "created": "2014-01-31T09:45:56.366+0000"}, {"author": "Tathagata Das", "body": "[~tgraves] Wasnt this resolved, in master as well as branch 0.9? This commit https://github.com/apache/spark/commit/b044b0b If so, shall we mark this resolved?", "created": "2014-04-08T20:03:26.769+0000"}, {"author": "Thomas Graves", "body": "Yes it was, go ahead and resolve it.", "created": "2014-04-08T20:37:24.945+0000"}], "num_comments": 3, "text": "Issue: SPARK-1049\nSummary: spark on yarn - yarn-client mode doesn't always exit properly\nDescription: When running in spark on yarn in yarn-client mode, the application master doesn't always exit in a timely manner if its still waiting on containers from the yarn resource manager.\n\nComments (3):\n1. Thomas Graves: https://github.com/apache/incubator-spark/pull/526\n2. Tathagata Das: [~tgraves] Wasnt this resolved, in master as well as branch 0.9? This commit https://github.com/apache/spark/commit/b044b0b If so, shall we mark this resolved?\n3. Thomas Graves: Yes it was, go ahead and resolve it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.119823"}}
{"id": "3ed54b59c6a66662e989783ed4a73ea1", "issue_key": "SPARK-1050", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Investigate AnyRefMap", "description": "Once Spark is built using Scala 2.11, we should investigate (particularly on heavily-used, performance-critical code paths) replacing usage of HashMap with the new, higher-performance AnyRefMap.", "reporter": "Mark Hamstra", "assignee": null, "created": "2014-01-30T10:23:46.000+0000", "updated": "2015-02-23T03:07:11.000+0000", "resolved": "2015-02-23T03:07:11.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Reynold Xin", "body": "I suspect the openhashset and hashmap we use in GraphX would be faster than this. They are both open addressing hash maps, and mine actually does specialization.", "created": "2014-01-30T10:26:42.109+0000"}, {"author": "Mark Hamstra", "body": "Perhaps so, but it is worth reaching a conclusion more solid than a guess. Also, there are a lot of instances of HashMap in Spark that haven't been (and probably can't be) replaced with your newer alternatives. Anyway, there's nothing much to be done on this issue until we're using Scala 2.11, so this is mostly just a placeholder and a reminder for down the road.", "created": "2014-01-30T10:36:42.891+0000"}, {"author": "Nicholas Chammas", "body": "Since Spark can now be built with Scala 2.11, I believe this issue can now be looked into. Tagging this with a 1.4.0 target so it can be reviewed by then.", "created": "2015-02-23T00:25:53.499+0000"}, {"author": "Reynold Xin", "body": "I'm going to close this for now. If somebody has time to review the new Scala hash map, we can reopen it. In general the performance characteristics of the Scala collection library is not very satisfying. I have some thoughts on performance optimizations in general and I will post something soon.", "created": "2015-02-23T03:06:48.768+0000"}], "num_comments": 4, "text": "Issue: SPARK-1050\nSummary: Investigate AnyRefMap\nDescription: Once Spark is built using Scala 2.11, we should investigate (particularly on heavily-used, performance-critical code paths) replacing usage of HashMap with the new, higher-performance AnyRefMap.\n\nComments (4):\n1. Reynold Xin: I suspect the openhashset and hashmap we use in GraphX would be faster than this. They are both open addressing hash maps, and mine actually does specialization.\n2. Mark Hamstra: Perhaps so, but it is worth reaching a conclusion more solid than a guess. Also, there are a lot of instances of HashMap in Spark that haven't been (and probably can't be) replaced with your newer alternatives. Anyway, there's nothing much to be done on this issue until we're using Scala 2.11, so this is mostly just a placeholder and a reminder for down the road.\n3. Nicholas Chammas: Since Spark can now be built with Scala 2.11, I believe this issue can now be looked into. Tagging this with a 1.4.0 target so it can be reviewed by then.\n4. Reynold Xin: I'm going to close this for now. If somebody has time to review the new Scala hash map, we can reopen it. In general the performance characteristics of the Scala collection library is not very satisfying. I have some thoughts on performance optimizations in general and I will post something soon.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.119823"}}
{"id": "21e987e8ef69324271ad03d90abbb2a6", "issue_key": "SPARK-1051", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "On Yarn, executors don't doAs as submitting user", "description": "This means that they can't write/read from files that the yarn user doesn't have permissions to but the submitting user does. I don't think this isn't a problem when running with Hadoop security, because the executor processes will be run as the submitting user.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-02-03T13:33:38.000+0000", "updated": "2020-04-24T06:36:20.000+0000", "resolved": "2014-02-28T10:44:33.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Kevin Markey", "body": "I'm not yet a Spark contributor, primarily a Spark user on Yarn. This sounds very much like an issue we've had with Spark 0.8.1 on Yarn 2.2.0. It is described on the Spark user mailing list in a posting 2/1/2014 entitled \"File write ownership conflicts for Spark 0.8.1 in YARN modes.\" I can post more details here if desired. We've replicated the problem in several environments: - CDH5.0.0-beta-1 or plain vanilla Apache Hadoop 2.2.0 - A small CDH5-beta-1 cluster or a single-CPU Yarn/HDFS pseudocluster - In yarn-standalone or yarn-client mode, although there are subtle differences in ownership of directories, temporary files written to the output directories, and (when not halted by permission errors) final output result files. In our tests, any file or directory created in RDD.saveAsTextFile(path:String) or FSDataOutputStream by a YARN process (the driver in yarn-standalone, or the worker nodes in either yarn mode) is owned by the UID of the user reported by System.properties(\"user.name\") and any other file or directory -- staging files or directories apparently created by the driver in yarn-client mode as destination containers for saveAsTextFile temporary and part* files are owned by the UID corresponding to \"appUser\" in logs reported by ...spark.deploy.yarn.Client using Yarn's ...hadoop.yarn.api.records.ApplicationReport. Depending on UMASK and other settings, this can result in a ...hadoop.security.AccessControlException because of permission denied. I've found simple workarounds for yarn-standalone mode because the same user creates both files and destination directories, but there are rarely good workarounds (that don't violate good security practices) for yarn-client, because files and destination directories are usually owned by different users. This is particularly true for the typical CDH5 installation.", "created": "2014-02-04T15:38:15.883+0000"}, {"author": "Sanford Ryza", "body": "https://github.com/apache/incubator-spark/pull/538", "created": "2014-02-06T15:18:22.286+0000"}, {"author": "Kevin Markey", "body": "This solves the issue for yarn-client, but not for yarn-standalone. In all the following cases, Spark is started by UID \"kmarkey\". This is reflected by the appUser as reported in the log. If starting a job in yarn-standalone as any user, the Master (driver) process is owned by \"yarn\", container directories are owned by \"yarn\" and Spark worker process write to files with the SPARK_USER or appUser UID. An ownership exception occurs if this is any user other than the \"yarn\" UID. Ownership of destination directories is immaterial. yarn-standalone JIRA-1051 patch appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000/_temporary drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000/_temporary/0 ... exception stops process when kmarkey tries to write to this directory ... destination: /user/kmarkey/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000/_temporary drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000/_temporary/0 ... exception stops process when kmarkey tries to write to this directory ... In yarn-client mode, things are as they should be. SPARK_USER, appUser, and the worker process UIDs now match. yarn-client JIRA-1051 patch appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-Client drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary -rw-r--r-- 1 kmarkey supergroup 41220 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary/summary.json -rw-r--r-- 1 kmarkey supergroup 25528 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/types drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/part-00001 So, it seems that in yarn-standalone, only the \"yarn\" user can start or own the Master process, and this proposed fix should not apply. Or the fix should be applied to the master (driver) process. But in retrospect, that seems impossible to do, because it is a child of the ResourceMaster. Therefore, this fix should only apply to yarn-client mode.", "created": "2014-02-09T16:36:39.915+0000"}, {"author": "Sanford Ryza", "body": "Updated the pull request to fix this issue", "created": "2014-02-10T13:16:26.828+0000"}, {"author": "Kevin Markey", "body": "Thank you. That is an improvement. Both yarn-client and yarn-standalone work similarly -- at least the portions controlled by Spark. The only remaining files with the wrong ownership are written by Java using FSDataOutputStream, and I need to modify how I initialize FileSystem to fix those. That is not your responsibility.  yarn-client JIRA-1051 patch #2 appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-Client drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2 drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary -rw-r--r-- 1 kmarkey supergroup 41220 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary/summary.json -rw-r--r-- 1 kmarkey supergroup 25528 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/types drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/part-00001 yarn-standalone JIRA-1051 patch #2 appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-YarnStandAlone2 * = files written using FSDataOutputStream in Java code unrelated to Spark. All other files created by Spark. (I will need to handle the FileSystem initializer differently.) drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2 * drwxr-xr-x - hduser supergroup 0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary * -rw-r--r-- 1 hduser supergroup 41220 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary/summary.json * -rw-r--r-- 1 hduser supergroup 25528 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/part-00001", "created": "2014-02-10T21:52:11.185+0000"}, {"author": "Kevin Markey", "body": "I was able to fix the remaining ownership issues. In the \"summary.*\" files above and below, I was using a small Java utility I authored to write a String or List of Strings to an HDFS file. When instantiating the FileSystem object, I was using get(URI,configuration,user), naively passing the value of System.getProperty(\"user.name\") as user. But this property is unaffected by the runAsUser(sparkuser) code added by the patch. Using instead the get(URI,configuration) factory method, the default authority for the specified URI is determined from the correct context, and the file is saved with sparkuser (appUser) ownership instead of yarn user ownership. The runAsUser(sparkuser) has the desired effect here, too, not just in the saveAsTextFile() instances on the worker nodes. Hooray! Tested on pseudocluster, now running on full cluster.  yarn-standalone JIRA-1051 patch #2 with fix in CuHdfsUtil appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-YarnStandAlone3 drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3 drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:36 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary -rw-r--r-- 1 kmarkey supergroup 41220 2014-02-11 14:36 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary/summary.json -rw-r--r-- 1 kmarkey supergroup 25528 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/part-00001", "created": "2014-02-11T14:01:00.949+0000"}, {"author": "Kevin Markey", "body": "When testing on real cluster, running *yarn-standalone* with user *klmarkey*, two cases observed: When explicitly defining \"export SPARK_USER=klmarkey\", files all saved with \"klmarkey\" as owner. When *NOT* explicitly defining SPARK_USER, files all saved with \"*yarn*\" as owner.", "created": "2014-02-11T17:36:58.460+0000"}, {"author": "Kevin Markey", "body": "I'm not sure why the 2nd patch works for yarn-standalone in a pseudocluster but not on the real cluster. The pseudocluster is plain vanilla Apache Hadoop 2.2.0. The real cluster is CDH5 Hadoop 2.2.0 Beta-1. I checked the cluster configuration, and there's nothing that stands out. I also don't quite understand why \"appUser\" is consistently reported by ApplicationReport as \"kmarkey\" whether SPARK_USER is defined or not. Is this a documentation issue? Or should the *spark-class* shell initialize it?", "created": "2014-02-11T19:38:29.835+0000"}, {"author": "Sanford Ryza", "body": "Very strange. I'll look into this.", "created": "2014-02-11T20:07:40.508+0000"}, {"author": "Kevin Markey", "body": "Indeed, I was surprised, too. At first I thought I had run in a shell I sudoed into as \"yarn\" to clean up some of the debris left behind from previous tests! But I hadn't. I repeated each of the results twice. The good news is that the user -- whichever it was -- is now *consistent* over all files, directories, and temporary files written to the output destination. Before patch 2, jobs were failing because files written by worker node processes were conflicting with those written by the appMaster processes.", "created": "2014-02-11T20:16:06.508+0000"}, {"author": "Kevin Markey", "body": "User error! After reconfiguring my cluster with up-to-date runtime scripts and other configuration data, the patched code worked as advertised, even *without* explicitly defining SPARK_USER. Thanks, Sandy, for your work on this issue.", "created": "2014-02-12T14:53:40.976+0000"}, {"author": "Thomas Graves", "body": "pr https://github.com/apache/spark/pull/29 committed.", "created": "2014-02-28T08:51:12.112+0000"}, {"author": "Thomas Graves", "body": "sorry pr needs upmerge first.", "created": "2014-02-28T08:51:37.329+0000"}, {"author": "Thomas Graves", "body": "I committed this to branch-0.9 also. commit 748f002b3a58af118f7de0b6ea9170895b571c78", "created": "2014-03-20T12:49:15.986+0000"}, {"author": "Apache Spark", "body": "User 'sryza' has created a pull request for this issue: https://github.com/apache/spark/pull/29", "created": "2016-06-15T20:55:05.808+0000"}], "num_comments": 15, "text": "Issue: SPARK-1051\nSummary: On Yarn, executors don't doAs as submitting user\nDescription: This means that they can't write/read from files that the yarn user doesn't have permissions to but the submitting user does. I don't think this isn't a problem when running with Hadoop security, because the executor processes will be run as the submitting user.\n\nComments (15):\n1. Kevin Markey: I'm not yet a Spark contributor, primarily a Spark user on Yarn. This sounds very much like an issue we've had with Spark 0.8.1 on Yarn 2.2.0. It is described on the Spark user mailing list in a posting 2/1/2014 entitled \"File write ownership conflicts for Spark 0.8.1 in YARN modes.\" I can post more details here if desired. We've replicated the problem in several environments: - CDH5.0.0-beta-1 or plain vanilla Apache Hadoop 2.2.0 - A small CDH5-beta-1 cluster or a single-CPU Yarn/HDFS pseudocluster - In yarn-standalone or yarn-client mode, although there are subtle differences in ownership of directories, temporary files written to the output directories, and (when not halted by permission errors) final output result files. In our tests, any file or directory created in RDD.saveAsTextFile(path:String) or FSDataOutputStream by a YARN process (the driver in yarn-standalone, or the worker nodes in either yarn mode) is owned by the UID of the user reported by System.properties(\"user.name\") and any other file or directory -- staging files or directories apparently created by the driver in yarn-client mode as destination containers for saveAsTextFile temporary and part* files are owned by the UID corresponding to \"appUser\" in logs reported by ...spark.deploy.yarn.Client using Yarn's ...hadoop.yarn.api.records.ApplicationReport. Depending on UMASK and other settings, this can result in a ...hadoop.security.AccessControlException because of permission denied. I've found simple workarounds for yarn-standalone mode because the same user creates both files and destination directories, but there are rarely good workarounds (that don't violate good security practices) for yarn-client, because files and destination directories are usually owned by different users. This is particularly true for the typical CDH5 installation.\n2. Sanford Ryza: https://github.com/apache/incubator-spark/pull/538\n3. Kevin Markey: This solves the issue for yarn-client, but not for yarn-standalone. In all the following cases, Spark is started by UID \"kmarkey\". This is reflected by the appUser as reported in the log. If starting a job in yarn-standalone as any user, the Master (driver) process is owned by \"yarn\", container directories are owned by \"yarn\" and Spark worker process write to files with the SPARK_USER or appUser UID. An ownership exception occurs if this is any user other than the \"yarn\" UID. Ownership of destination directories is immaterial. yarn-standalone JIRA-1051 patch appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000/_temporary drwxr-xr-x - hduser supergroup 0 2014-02-09 16:39 /user/hduser/output/Spark-Jira1051/values/column-0000/_temporary/0 ... exception stops process when kmarkey tries to write to this directory ... destination: /user/kmarkey/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000 drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000/_temporary drwxr-xr-x - hduser supergroup 0 2014-02-09 16:45 /user/kmarkey/output/Spark-Jira1051/values/column-0000/_temporary/0 ... exception stops process when kmarkey tries to write to this directory ... In yarn-client mode, things are as they should be. SPARK_USER, appUser, and the worker process UIDs now match. yarn-client JIRA-1051 patch appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-Client drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary -rw-r--r-- 1 kmarkey supergroup 41220 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary/summary.json -rw-r--r-- 1 kmarkey supergroup 25528 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:59 /user/kmarkey/output/Spark-Jira1051-Client/types drwxr-xr-x - kmarkey supergroup 0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-09 16:56 /user/kmarkey/output/Spark-Jira1051-Client/types/column-0000/part-00001 So, it seems that in yarn-standalone, only the \"yarn\" user can start or own the Master process, and this proposed fix should not apply. Or the fix should be applied to the master (driver) process. But in retrospect, that seems impossible to do, because it is a child of the ResourceMaster. Therefore, this fix should only apply to yarn-client mode.\n4. Sanford Ryza: Updated the pull request to fix this issue\n5. Kevin Markey: Thank you. That is an improvement. Both yarn-client and yarn-standalone work similarly -- at least the portions controlled by Spark. The only remaining files with the wrong ownership are written by Java using FSDataOutputStream, and I need to modify how I initialize FileSystem to fix those. That is not your responsibility.  yarn-client JIRA-1051 patch #2 appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-Client drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2 drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary -rw-r--r-- 1 kmarkey supergroup 41220 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary/summary.json -rw-r--r-- 1 kmarkey supergroup 25528 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:08 /user/kmarkey/output/Spark-Jira1051-Client2/types drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-10 22:05 /user/kmarkey/output/Spark-Jira1051-Client2/types/column-0000/part-00001 yarn-standalone JIRA-1051 patch #2 appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-YarnStandAlone2 * = files written using FSDataOutputStream in Java code unrelated to Spark. All other files created by Spark. (I will need to handle the FileSystem initializer differently.) drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2 * drwxr-xr-x - hduser supergroup 0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary * -rw-r--r-- 1 hduser supergroup 41220 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary/summary.json * -rw-r--r-- 1 hduser supergroup 25528 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:33 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types drwxr-xr-x - kmarkey supergroup 0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-10 22:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone2/types/column-0000/part-00001\n6. Kevin Markey: I was able to fix the remaining ownership issues. In the \"summary.*\" files above and below, I was using a small Java utility I authored to write a String or List of Strings to an HDFS file. When instantiating the FileSystem object, I was using get(URI,configuration,user), naively passing the value of System.getProperty(\"user.name\") as user. But this property is unaffected by the runAsUser(sparkuser) code added by the patch. Using instead the get(URI,configuration) factory method, the default authority for the specified URI is determined from the correct context, and the file is saved with sparkuser (appUser) ownership instead of yarn user ownership. The runAsUser(sparkuser) has the desired effect here, too, not just in the saveAsTextFile() instances on the worker nodes. Hooray! Tested on pseudocluster, now running on full cluster.  yarn-standalone JIRA-1051 patch #2 with fix in CuHdfsUtil appUser UID: kmarkey yarn UID: hduser destination: /user/hduser/output/Spark-Jira1051-YarnStandAlone3 drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3 drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:36 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary -rw-r--r-- 1 kmarkey supergroup 41220 2014-02-11 14:36 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary/summary.json -rw-r--r-- 1 kmarkey supergroup 25528 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/summary/summary.txt drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:35 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types drwxr-xr-x - kmarkey supergroup 0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000 -rw-r--r-- 1 kmarkey supergroup 0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/_SUCCESS -rw-r--r-- 1 kmarkey supergroup 0 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/part-00000 -rw-r--r-- 1 kmarkey supergroup 15 2014-02-11 14:31 /user/kmarkey/output/Spark-Jira1051-YarnStandAlone3/types/column-0000/part-00001\n7. Kevin Markey: When testing on real cluster, running *yarn-standalone* with user *klmarkey*, two cases observed: When explicitly defining \"export SPARK_USER=klmarkey\", files all saved with \"klmarkey\" as owner. When *NOT* explicitly defining SPARK_USER, files all saved with \"*yarn*\" as owner.\n8. Kevin Markey: I'm not sure why the 2nd patch works for yarn-standalone in a pseudocluster but not on the real cluster. The pseudocluster is plain vanilla Apache Hadoop 2.2.0. The real cluster is CDH5 Hadoop 2.2.0 Beta-1. I checked the cluster configuration, and there's nothing that stands out. I also don't quite understand why \"appUser\" is consistently reported by ApplicationReport as \"kmarkey\" whether SPARK_USER is defined or not. Is this a documentation issue? Or should the *spark-class* shell initialize it?\n9. Sanford Ryza: Very strange. I'll look into this.\n10. Kevin Markey: Indeed, I was surprised, too. At first I thought I had run in a shell I sudoed into as \"yarn\" to clean up some of the debris left behind from previous tests! But I hadn't. I repeated each of the results twice. The good news is that the user -- whichever it was -- is now *consistent* over all files, directories, and temporary files written to the output destination. Before patch 2, jobs were failing because files written by worker node processes were conflicting with those written by the appMaster processes.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.119823"}}
{"id": "7473ab1f122c20258b11e131db994115", "issue_key": "SPARK-1052", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark on Mesos with CDH4.5.0 cannot start the Tasks properly", "description": "Steps : Start the mesos cluster with the deb 0.15 from mesosphere. Install cdh 4.5.0. Start the spark-shell with this code : val data = 1 to 10000 val distData = sc.parallelize(data) distData.filter(_< 10).collect() The tasks die because of an exception. It used to work with Spark 0.8.0", "reporter": "Alberto Miorin", "assignee": "Bijay Singh Bisht", "created": "2014-02-04T13:48:02.000+0000", "updated": "2014-02-16T16:55:31.000+0000", "resolved": "2014-02-16T16:55:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Marek Wiewiorka", "body": "I have exactly the same problem using Hadoop 1.2.1, Spark 0.9 and Mesos 0.15.", "created": "2014-02-05T15:01:46.308+0000"}, {"author": "Vinod Kone", "body": "I'm able to repro too. I suspect it's networking binding issue.", "created": "2014-02-08T20:15:02.551+0000"}, {"author": "Bijay Singh Bisht", "body": "Fix is available in the pull request #568 Bug caused by using the new Conf object, for setting spark.dirver.host etc. But the user of those properties still using the System.properties to access it.", "created": "2014-02-10T10:59:38.132+0000"}, {"author": "Aaron Davidson", "body": "PR #568 merged.", "created": "2014-02-16T16:55:31.906+0000"}], "num_comments": 4, "text": "Issue: SPARK-1052\nSummary: Spark on Mesos with CDH4.5.0 cannot start the Tasks properly\nDescription: Steps : Start the mesos cluster with the deb 0.15 from mesosphere. Install cdh 4.5.0. Start the spark-shell with this code : val data = 1 to 10000 val distData = sc.parallelize(data) distData.filter(_< 10).collect() The tasks die because of an exception. It used to work with Spark 0.8.0\n\nComments (4):\n1. Marek Wiewiorka: I have exactly the same problem using Hadoop 1.2.1, Spark 0.9 and Mesos 0.15.\n2. Vinod Kone: I'm able to repro too. I suspect it's networking binding issue.\n3. Bijay Singh Bisht: Fix is available in the pull request #568 Bug caused by using the new Conf object, for setting spark.dirver.host etc. But the user of those properties still using the System.properties to access it.\n4. Aaron Davidson: PR #568 merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.119823"}}
{"id": "885d2c6dbd13d723f954ce298cf1f7eb", "issue_key": "SPARK-1053", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Should not require SPARK_YARN_APP_JAR when running on YARN", "description": "org.apache.spark.SparkException: env SPARK_YARN_APP_JAR is not set at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:49) at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:126) at org.apache.spark.SparkContext.<init>(SparkContext.scala:200) at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:954)", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-02-04T23:22:31.000+0000", "updated": "2014-04-04T20:49:15.000+0000", "resolved": "2014-02-27T01:19:19.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Josh Rosen", "body": "This also causes problems for PySpark on YARN (SPARK-1030). This isn't the only issue blocking Pyspark on YARN; see SPARK-1004 for more discussion.", "created": "2014-02-05T17:19:17.404+0000"}, {"author": "Sanford Ryza", "body": "Hadn't seen that. Are you already working on this? If not, I was planning to give it a shot.", "created": "2014-02-05T17:22:32.849+0000"}, {"author": "Josh Rosen", "body": "Go ahead; I'm not working on the SPARK_YARN_APP_JAR issue (I have a work in progress branch for fixing PySpark on YARN, but it's a more extensive set of changes that are unrelated to this issue).", "created": "2014-02-05T17:35:05.580+0000"}, {"author": "Sanford Ryza", "body": "https://github.com/apache/incubator-spark/pull/553", "created": "2014-02-06T18:06:32.749+0000"}], "num_comments": 4, "text": "Issue: SPARK-1053\nSummary: Should not require SPARK_YARN_APP_JAR when running on YARN\nDescription: org.apache.spark.SparkException: env SPARK_YARN_APP_JAR is not set at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:49) at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:126) at org.apache.spark.SparkContext.<init>(SparkContext.scala:200) at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:954)\n\nComments (4):\n1. Josh Rosen: This also causes problems for PySpark on YARN (SPARK-1030). This isn't the only issue blocking Pyspark on YARN; see SPARK-1004 for more discussion.\n2. Sanford Ryza: Hadn't seen that. Are you already working on this? If not, I was planning to give it a shot.\n3. Josh Rosen: Go ahead; I'm not working on the SPARK_YARN_APP_JAR issue (I have a work in progress branch for fixing PySpark on YARN, but it's a more extensive set of changes that are unrelated to this issue).\n4. Sanford Ryza: https://github.com/apache/incubator-spark/pull/553", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.119823"}}
{"id": "da90c0a2daa343f3c5d971465b83dfc7", "issue_key": "SPARK-1054", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Get Cassandra support in Spark Core/Spark Cassandra Module", "description": "Calliope is a library providing an interface to consume data from Cassandra to spark and store RDDs from Spark to Cassandra. Building as wrapper over Cassandra's Hadoop I/O it provides a simplified and very generic API to consume and produces data from and to Cassandra. It allows you to consume data from Legacy as well as CQL3 Cassandra Storage. It can also harness C* to speed up your process by fetching only the relevant data from C* harnessing CQL3 and C*'s secondary indexes. Though it currently uses only the Hadoop I/O formats for Cassandra in near future we see the same API harnessing other means of consuming Cassandra data like using the StorageProxy or even reading from SSTables directly. Over the basic data fetch functionality, the Calliope API harnesses Scala and it's implicit parameters and conversions for you to work on a higher abstraction dealing with tuples/objects instead of Cassandra's Row/Columns in your MapRed jobs. Over past few months we have seen the combination of Spark+Cassandra gaining a lot of traction. And we feel Calliope provides the path of least friction for developers to start working with this combination. We have been using this ins production for over a year now and the Calliope early access repository has 30+ users. I am putting this issue to start a discussion around whether we would want Calliope to be a part of Spark and if yes, what will be involved in doing so. You can read more about Calliope here - http://tuplejump.github.io/calliope", "reporter": "Rohit Rai", "assignee": null, "created": "2014-02-05T00:31:02.000+0000", "updated": "2015-03-01T11:53:23.000+0000", "resolved": "2015-03-01T11:53:23.000+0000", "labels": ["calliope", "cassandra"], "components": ["Spark Core"], "comments": [{"author": "Rohit Rai", "body": "With the https://github.com/datastax/cassandra-driver-spark from Datastax, we should work on getting a united standard API in Spark, getting good things from both worlds.", "created": "2014-07-02T18:39:55.848+0000"}, {"author": "Sean R. Owen", "body": "Like HBase, there are already examples of using Cassandra from Spark via standard Hadoop APIs. This sounds like a great stand-alone project. I believe this is another one that should remain external and be linked at http://spark-packages.org/ ? At least I don't see activity here and it seems like this is perfectly usable without requiring Spark to package it.", "created": "2015-03-01T11:53:23.979+0000"}], "num_comments": 2, "text": "Issue: SPARK-1054\nSummary: Get Cassandra support in Spark Core/Spark Cassandra Module\nDescription: Calliope is a library providing an interface to consume data from Cassandra to spark and store RDDs from Spark to Cassandra. Building as wrapper over Cassandra's Hadoop I/O it provides a simplified and very generic API to consume and produces data from and to Cassandra. It allows you to consume data from Legacy as well as CQL3 Cassandra Storage. It can also harness C* to speed up your process by fetching only the relevant data from C* harnessing CQL3 and C*'s secondary indexes. Though it currently uses only the Hadoop I/O formats for Cassandra in near future we see the same API harnessing other means of consuming Cassandra data like using the StorageProxy or even reading from SSTables directly. Over the basic data fetch functionality, the Calliope API harnesses Scala and it's implicit parameters and conversions for you to work on a higher abstraction dealing with tuples/objects instead of Cassandra's Row/Columns in your MapRed jobs. Over past few months we have seen the combination of Spark+Cassandra gaining a lot of traction. And we feel Calliope provides the path of least friction for developers to start working with this combination. We have been using this ins production for over a year now and the Calliope early access repository has 30+ users. I am putting this issue to start a discussion around whether we would want Calliope to be a part of Spark and if yes, what will be involved in doing so. You can read more about Calliope here - http://tuplejump.github.io/calliope\n\nComments (2):\n1. Rohit Rai: With the https://github.com/datastax/cassandra-driver-spark from Datastax, we should work on getting a united standard API in Spark, getting good things from both worlds.\n2. Sean R. Owen: Like HBase, there are already examples of using Cassandra from Spark via standard Hadoop APIs. This sounds like a great stand-alone project. I believe this is another one that should remain external and be linked at http://spark-packages.org/ ? At least I don't see activity here and it seems like this is perfectly usable without requiring Spark to package it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.119823"}}
{"id": "a0ef132384c6457371996138cc8e86e3", "issue_key": "SPARK-1055", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark version on Dockerfile does not match release", "description": "The used Spark version in the .../base/Dockerfile is stale on 0.8.1 and should be updated to 0.9.x to match the release. See: https://github.com/apache/incubator-spark/blob/branch-0.9/docker/spark-test/base/Dockerfile PS: Should we add a component for 'Docker'? - not sure under which component I should file this bug.", "reporter": "Gerard Maas", "assignee": "Nan Zhu", "created": "2014-02-05T03:12:36.000+0000", "updated": "2014-02-22T16:59:56.000+0000", "resolved": "2014-02-22T16:35:14.000+0000", "labels": [], "components": ["Examples"], "comments": [{"author": "Gerard Maas", "body": "Scala version is also incorrect", "created": "2014-02-05T03:22:45.659+0000"}], "num_comments": 1, "text": "Issue: SPARK-1055\nSummary: Spark version on Dockerfile does not match release\nDescription: The used Spark version in the .../base/Dockerfile is stale on 0.8.1 and should be updated to 0.9.x to match the release. See: https://github.com/apache/incubator-spark/blob/branch-0.9/docker/spark-test/base/Dockerfile PS: Should we add a component for 'Docker'? - not sure under which component I should file this bug.\n\nComments (1):\n1. Gerard Maas: Scala version is also incorrect", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.119823"}}
{"id": "fce6b73b7d2893cd510cf9d45c795136", "issue_key": "SPARK-1212", "issue_type": "Improvement", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Support sparse data in MLlib", "description": "MLlib's NaiveBayes, SGD, and KMeans accept RDD[LabeledPoint] for training and RDD[Array[Double]] for prediction, where LabeledPoint is a wrapper of (Double, Array[Double]). Using Array[Double] could have good performance, but sparse data appears quite often in practice. So I created this JIRA to discuss the plan of adding sparse data support to MLlib and track its progress. The goal is to support sparse data for training and prediction in all existing algorithms in MLlib: * Gradient Descent * K-Means * Naive Bayes Previous discussions and pull requests: * https://github.com/mesos/spark/pull/736", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-02-05T15:30:41.000+0000", "updated": "2014-04-03T07:22:23.000+0000", "resolved": "2014-04-03T07:22:23.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Debasish Das", "body": "The previous pull request was not closed. Dmitry's mahout math project has scala bindings...code is available in mahout apache git beginning 0.9...kmeans-sparse, sgd sparse and nb sparse can be developed through adding mahout math dependency...although cloudera mentioned that they are not going to support whole mahout in cdh5...but I guess mahout math is clean enough for apache, cdh and hdp ? If you can create a pull request with the right dependency (dmitry's project or mahout math) as Matei mentioned we can build upon it ?", "created": "2014-02-05T16:16:06.819+0000"}, {"author": "Xiangrui Meng", "body": "Which pull request did you mean? The sparse vector in mahout is backed by a primitive-typed map, which is not efficient for sequential access used in kmeans, sgd, and nb. I understand that mahout-math is not optimized for performance, but its performance downgrade is not negligible.", "created": "2014-02-06T10:16:02.377+0000"}, {"author": "Imran Rashid", "body": "I think there are multiple versions of sparse vector in mahout math. This one is better for sequential access: http://svn.apache.org/repos/asf/mahout/trunk/math/src/main/java/org/apache/mahout/math/SequentialAccessSparseVector.java the actual data is stored in this: http://svn.apache.org/repos/asf/mahout/trunk/math/src/main/java/org/apache/mahout/math/OrderedIntDoubleMapping.java there are still some layers of indirection in there, so its not ideal, but maybe good enough. Might be worth profiling to see the difference vs a more \"bare-metal\" array based approach.", "created": "2014-02-06T10:31:17.754+0000"}, {"author": "Xiangrui Meng", "body": "Thanks for pointing out there is one sequential access vector in mahout. I will put some benchmark results later. In the meantime, I would suggest separating the user interface from implementation. We don't use types defined outside scala/spark/mllib in public interfaces and utilize either breeze-math or mahout-math in the implementation. That means we need data models that can be easily converted into breeze or mahout vectors/matrices. The overhead is small if we don't need to copy the data, and the benefit is that we still have the freedom to choose which package to use.", "created": "2014-02-06T10:46:34.801+0000"}, {"author": "Xiangrui Meng", "body": "I just sent out a PR (https://github.com/apache/incubator-spark/pull/575, comments attached) for discussion. This PR added sparse data support to KMeans as a pilot. For KMeans, the centers are stored as dense vectors while the input data could be either sparse vectors or dense vectors. The operation needed are Euclidean distance between two vectors and adding a sparse/dense vector to a dense vector (center). I tested the performance of adding a sparse vector to a dense vector using mahout-math-0.9, breeze-0.5.2, and a naive implementation without any fancy stuff. The sparse vector is casted to a base Vector class (because we want to support both dense and sparse input). The results are the following: Adding a sparse vector of size 1,000,000 and sparsity 0.01 to a dense vector for 100,000 times. ~~~ mahout: 4.292476s breeze: 56.132823s naive : 3.504810s ~~~ Adding a sparse vector of size 1,000,000 and sparsity 0.05 to a dense vector for 100,000 times. ~~~ mahout: 33.308203s breeze: 364.451941s naive : 27.890856s ~~~ The issue with breeze is that it uses a generic implementation the input is a base Vector class. If the input is a SparseVector instance, the performance is between mahout and the bare-bone implementation. This issue was fixed (https://github.com/scalanlp/breeze/issues/151) but we need a release from breeze to use in MLlib. ==================== This is a proposal for sparse data support in MLlib (https://spark-project.atlassian.net/browse/MLLIB-18). So please ignore minor stuff in this PR. If most people agree with the plan, I will go and add sparse data support to other MLlib algorithms. The idea of the proposal is that we define simple data models and factory methods for user to provide sparse input. Then instead of writing a linear algebra library for mllib, we take leverage on an existing linear algebra package in implementing algorithms. So we can change the underlying implementation without breaking the interfaces in the future. We need the following: data models for sparse vectors. We need data models for dense vector, sequential access sparse vector (backed by two parallel arrays), and random access sparse vector (backed by a primitive-typed hash map, not in this pull request.). Those are defined in the Vec class in this PR. a linear algebra package. We are considering either breeze or mahout-math. Both have pros and cons, and we can discuss more in the JIRA. This PR uses mahout-math. Mahout vectors do not implement serializable, so we need a serializable wrapper class (MahoutVectorWrapper) to use in spark. As a result, we added not only mahout-math but also mathout-core into dependencies because we need VectorWritable defined mahout-core for the wrapper class. But we can certainly remove most transitive dependencies of mahout-core. lightweight converters. The conversion between our data models and Mahout vectors shouldn't involve data copying. However, Mahout vectors hide their members. In this PR, Java reflection is used to get the private fields out. This doesn't seem to be a good solution, but I didn't figure out a better one. ===================", "created": "2014-02-10T10:13:48.203+0000"}, {"author": "Xiangrui Meng", "body": "Part II adds sparse data support to GLMs and Naive Bayes. PR: https://github.com/apache/spark/pull/245", "created": "2014-03-31T08:18:41.412+0000"}], "num_comments": 6, "text": "Issue: SPARK-1212\nSummary: Support sparse data in MLlib\nDescription: MLlib's NaiveBayes, SGD, and KMeans accept RDD[LabeledPoint] for training and RDD[Array[Double]] for prediction, where LabeledPoint is a wrapper of (Double, Array[Double]). Using Array[Double] could have good performance, but sparse data appears quite often in practice. So I created this JIRA to discuss the plan of adding sparse data support to MLlib and track its progress. The goal is to support sparse data for training and prediction in all existing algorithms in MLlib: * Gradient Descent * K-Means * Naive Bayes Previous discussions and pull requests: * https://github.com/mesos/spark/pull/736\n\nComments (6):\n1. Debasish Das: The previous pull request was not closed. Dmitry's mahout math project has scala bindings...code is available in mahout apache git beginning 0.9...kmeans-sparse, sgd sparse and nb sparse can be developed through adding mahout math dependency...although cloudera mentioned that they are not going to support whole mahout in cdh5...but I guess mahout math is clean enough for apache, cdh and hdp ? If you can create a pull request with the right dependency (dmitry's project or mahout math) as Matei mentioned we can build upon it ?\n2. Xiangrui Meng: Which pull request did you mean? The sparse vector in mahout is backed by a primitive-typed map, which is not efficient for sequential access used in kmeans, sgd, and nb. I understand that mahout-math is not optimized for performance, but its performance downgrade is not negligible.\n3. Imran Rashid: I think there are multiple versions of sparse vector in mahout math. This one is better for sequential access: http://svn.apache.org/repos/asf/mahout/trunk/math/src/main/java/org/apache/mahout/math/SequentialAccessSparseVector.java the actual data is stored in this: http://svn.apache.org/repos/asf/mahout/trunk/math/src/main/java/org/apache/mahout/math/OrderedIntDoubleMapping.java there are still some layers of indirection in there, so its not ideal, but maybe good enough. Might be worth profiling to see the difference vs a more \"bare-metal\" array based approach.\n4. Xiangrui Meng: Thanks for pointing out there is one sequential access vector in mahout. I will put some benchmark results later. In the meantime, I would suggest separating the user interface from implementation. We don't use types defined outside scala/spark/mllib in public interfaces and utilize either breeze-math or mahout-math in the implementation. That means we need data models that can be easily converted into breeze or mahout vectors/matrices. The overhead is small if we don't need to copy the data, and the benefit is that we still have the freedom to choose which package to use.\n5. Xiangrui Meng: I just sent out a PR (https://github.com/apache/incubator-spark/pull/575, comments attached) for discussion. This PR added sparse data support to KMeans as a pilot. For KMeans, the centers are stored as dense vectors while the input data could be either sparse vectors or dense vectors. The operation needed are Euclidean distance between two vectors and adding a sparse/dense vector to a dense vector (center). I tested the performance of adding a sparse vector to a dense vector using mahout-math-0.9, breeze-0.5.2, and a naive implementation without any fancy stuff. The sparse vector is casted to a base Vector class (because we want to support both dense and sparse input). The results are the following: Adding a sparse vector of size 1,000,000 and sparsity 0.01 to a dense vector for 100,000 times. ~~~ mahout: 4.292476s breeze: 56.132823s naive : 3.504810s ~~~ Adding a sparse vector of size 1,000,000 and sparsity 0.05 to a dense vector for 100,000 times. ~~~ mahout: 33.308203s breeze: 364.451941s naive : 27.890856s ~~~ The issue with breeze is that it uses a generic implementation the input is a base Vector class. If the input is a SparseVector instance, the performance is between mahout and the bare-bone implementation. This issue was fixed (https://github.com/scalanlp/breeze/issues/151) but we need a release from breeze to use in MLlib. ==================== This is a proposal for sparse data support in MLlib (https://spark-project.atlassian.net/browse/MLLIB-18). So please ignore minor stuff in this PR. If most people agree with the plan, I will go and add sparse data support to other MLlib algorithms. The idea of the proposal is that we define simple data models and factory methods for user to provide sparse input. Then instead of writing a linear algebra library for mllib, we take leverage on an existing linear algebra package in implementing algorithms. So we can change the underlying implementation without breaking the interfaces in the future. We need the following: data models for sparse vectors. We need data models for dense vector, sequential access sparse vector (backed by two parallel arrays), and random access sparse vector (backed by a primitive-typed hash map, not in this pull request.). Those are defined in the Vec class in this PR. a linear algebra package. We are considering either breeze or mahout-math. Both have pros and cons, and we can discuss more in the JIRA. This PR uses mahout-math. Mahout vectors do not implement serializable, so we need a serializable wrapper class (MahoutVectorWrapper) to use in spark. As a result, we added not only mahout-math but also mathout-core into dependencies because we need VectorWritable defined mahout-core for the wrapper class. But we can certainly remove most transitive dependencies of mahout-core. lightweight converters. The conversion between our data models and Mahout vectors shouldn't involve data copying. However, Mahout vectors hide their members. In this PR, Java reflection is used to get the private fields out. This doesn't seem to be a good solution, but I didn't figure out a better one. ===================\n6. Xiangrui Meng: Part II adds sparse data support to GLMs and Naive Bayes. PR: https://github.com/apache/spark/pull/245", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.121826"}}
{"id": "ae3d1451a0d7184b0a999d35331d55ab", "issue_key": "SPARK-1224", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Improving Documentation for MLlib", "description": "Users would need some better documentation the ML models that we support. (For usability and e.g. for comparing different algorithms for the same models). This ticket could be used to track some progress on this. I have something prepared already for the classification & regression parts, but we definitely need some math formulas in the .md files for that. sth like this PR could do it? https://github.com/shivaram/spark/compare/mathjax As for the user guide, we could split the current file http://spark.incubator.apache.org/docs/latest/mllib-guide.html into separate ones for a) classification and regression, b) clustering, c) collaborative filtering and d) optimization methods. Any thoughts on this?", "reporter": "Martin Jaggi", "assignee": "Martin Jaggi", "created": "2014-02-06T03:11:12.000+0000", "updated": "2014-03-19T05:51:38.000+0000", "resolved": "2014-03-19T05:51:38.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Martin Jaggi", "body": "here is a pull request: https://github.com/apache/incubator-spark/pull/552", "created": "2014-02-06T09:10:35.655+0000"}, {"author": "Martin Jaggi", "body": "and here is another one, making use of these fancy new tex capabilities :-) https://github.com/apache/incubator-spark/pull/563", "created": "2014-02-08T11:37:01.637+0000"}, {"author": "Martin Jaggi", "body": "had to rebase it, the active PR now is https://github.com/apache/incubator-spark/pull/566", "created": "2014-02-09T05:35:40.327+0000"}, {"author": "Xiangrui Meng", "body": "Martin, shall I mark this JIRA as fixed?", "created": "2014-03-18T19:24:29.087+0000"}], "num_comments": 4, "text": "Issue: SPARK-1224\nSummary: Improving Documentation for MLlib\nDescription: Users would need some better documentation the ML models that we support. (For usability and e.g. for comparing different algorithms for the same models). This ticket could be used to track some progress on this. I have something prepared already for the classification & regression parts, but we definitely need some math formulas in the .md files for that. sth like this PR could do it? https://github.com/shivaram/spark/compare/mathjax As for the user guide, we could split the current file http://spark.incubator.apache.org/docs/latest/mllib-guide.html into separate ones for a) classification and regression, b) clustering, c) collaborative filtering and d) optimization methods. Any thoughts on this?\n\nComments (4):\n1. Martin Jaggi: here is a pull request: https://github.com/apache/incubator-spark/pull/552\n2. Martin Jaggi: and here is another one, making use of these fancy new tex capabilities :-) https://github.com/apache/incubator-spark/pull/563\n3. Martin Jaggi: had to rebase it, the active PR now is https://github.com/apache/incubator-spark/pull/566\n4. Xiangrui Meng: Martin, shall I mark this JIRA as fixed?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.121826"}}
{"id": "61bc5d7405bb838d689687bc42d864d5", "issue_key": "SPARK-1227", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Diagnostics for Classification&Regression", "description": "Currently, the attained objective function is not computed (for efficiency reasons, as one evaluation requires one full pass through the data). For diagnostics and comparing different algorithms, we should however provide this as a separate function (one MR). Doing this requires the loss and regularizer functions themselves, not only their gradients (which are currently in the Gradient class). How about adding the new function directly on the corresponding models in classification/* and regression/* ? Any thoughts?", "reporter": "Martin Jaggi", "assignee": "Martin Jaggi", "created": "2014-02-06T03:20:45.000+0000", "updated": "2016-11-07T18:03:27.000+0000", "resolved": "2015-11-05T09:32:44.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Martin Jaggi", "body": "here is a related discussion for ROC AUC and Average precision metrics for Classification https://github.com/apache/incubator-spark/pull/550 though these are for diagnosis of the testing (of any model candidate), while we also diagnosis for the training (optimization of the model)", "created": "2014-02-09T05:33:51.155+0000"}, {"author": "Sean R. Owen", "body": "Is this still relevant now that conventional classifier and regressor metrics are implemented in MLlib? You wouldn't be able to compare models by their loss function in general anyway.", "created": "2014-11-08T10:04:03.958+0000"}, {"author": "Martin Jaggi", "body": "actually this is still relevant, as looking at the training objective value is the only way to tell if a chosen model has been properly trained or not. without this, the user will not know if a resulting bad test error should be blamed on poor training of a good model, or on choosing a wrong model (e.g. wrong regularization parameter). both happens often. or imagine a deep net, where the same question is even harder to tell apart. now that MLlib starts offering different training algorithms for the same models (e.g. SGD and L-BFGS), and also different ways of distributing training, the training objective would definitely be useful compare algorithms. (or also when comparing different step-size regimes, such as here: https://issues.apache.org/jira/browse/SPARK-3942 ) maybe the nicest way around this would be to provide a nice *benchmarking suite* for regression and classification, which could be used to judge different algorithms in this respect, also if new contributed algorithms need to be compared for efficiency. a benchmarking suite should include a small selection of standard datasets for both sparse and dense data. this is also related to the currently not so nice way of passing around the regularizer values (which are part of the training optimization objective) through the updater function, which is currently quite different in SGD as compared to L-BFGS, see the issue here: https://issues.apache.org/jira/browse/SPARK-2505 and the spark L-BFBS implementation: https://github.com/apache/spark/pull/353 irrespective of training error, the classifier methods would also benefit from adding the test accuracy percentage as a function (see current code examples, where this still has to be calculated manually, as it's not implemented yet in {{BinaryClassificationMetrics}}.", "created": "2014-11-10T12:31:56.420+0000"}, {"author": "Sean R. Owen", "body": "OK you're interested in detecting overfitting, for one? You can't compare algorithms with a different objective function this way though, right? a different regularization param alone means a different objective. So I don't see it can be a general benchmark, and other classifier metrics are more appropriate. But yeah you could compare two runs that share the exact same objective, and that's a way of comparing training algorithms. Accuracy is in MulticlassMetrics which can be used with binary classification but it would be a nice to have for BinaryClassificationMetrics.", "created": "2014-11-10T13:23:13.723+0000"}, {"author": "Martin Jaggi", "body": "yes, this can sometimes help to detect overfitting, if combined with test error. and tells us if the model is over-trained or under-trained (such as when using early stopping). and yes, comparison of methods is only possible for the same objective function. it's not a replacement for test error, but a benchmark suite would benefit if it could keep track of both test and train objectives.", "created": "2014-11-10T14:43:49.438+0000"}, {"author": "Joseph K. Bradley", "body": "I agree it will be nice to provide loss classes. Even though *Metrics classes exist already, loss classes might be nice as we provide more functionality for diagnosis during learning (e.g., for early stopping, model selection, etc.). Added link to related JIRA on optimization APIs.", "created": "2015-04-13T04:06:48.919+0000"}], "num_comments": 6, "text": "Issue: SPARK-1227\nSummary: Diagnostics for Classification&Regression\nDescription: Currently, the attained objective function is not computed (for efficiency reasons, as one evaluation requires one full pass through the data). For diagnostics and comparing different algorithms, we should however provide this as a separate function (one MR). Doing this requires the loss and regularizer functions themselves, not only their gradients (which are currently in the Gradient class). How about adding the new function directly on the corresponding models in classification/* and regression/* ? Any thoughts?\n\nComments (6):\n1. Martin Jaggi: here is a related discussion for ROC AUC and Average precision metrics for Classification https://github.com/apache/incubator-spark/pull/550 though these are for diagnosis of the testing (of any model candidate), while we also diagnosis for the training (optimization of the model)\n2. Sean R. Owen: Is this still relevant now that conventional classifier and regressor metrics are implemented in MLlib? You wouldn't be able to compare models by their loss function in general anyway.\n3. Martin Jaggi: actually this is still relevant, as looking at the training objective value is the only way to tell if a chosen model has been properly trained or not. without this, the user will not know if a resulting bad test error should be blamed on poor training of a good model, or on choosing a wrong model (e.g. wrong regularization parameter). both happens often. or imagine a deep net, where the same question is even harder to tell apart. now that MLlib starts offering different training algorithms for the same models (e.g. SGD and L-BFGS), and also different ways of distributing training, the training objective would definitely be useful compare algorithms. (or also when comparing different step-size regimes, such as here: https://issues.apache.org/jira/browse/SPARK-3942 ) maybe the nicest way around this would be to provide a nice *benchmarking suite* for regression and classification, which could be used to judge different algorithms in this respect, also if new contributed algorithms need to be compared for efficiency. a benchmarking suite should include a small selection of standard datasets for both sparse and dense data. this is also related to the currently not so nice way of passing around the regularizer values (which are part of the training optimization objective) through the updater function, which is currently quite different in SGD as compared to L-BFGS, see the issue here: https://issues.apache.org/jira/browse/SPARK-2505 and the spark L-BFBS implementation: https://github.com/apache/spark/pull/353 irrespective of training error, the classifier methods would also benefit from adding the test accuracy percentage as a function (see current code examples, where this still has to be calculated manually, as it's not implemented yet in {{BinaryClassificationMetrics}}.\n4. Sean R. Owen: OK you're interested in detecting overfitting, for one? You can't compare algorithms with a different objective function this way though, right? a different regularization param alone means a different objective. So I don't see it can be a general benchmark, and other classifier metrics are more appropriate. But yeah you could compare two runs that share the exact same objective, and that's a way of comparing training algorithms. Accuracy is in MulticlassMetrics which can be used with binary classification but it would be a nice to have for BinaryClassificationMetrics.\n5. Martin Jaggi: yes, this can sometimes help to detect overfitting, if combined with test error. and tells us if the model is over-trained or under-trained (such as when using early stopping). and yes, comparison of methods is only possible for the same objective function. it's not a replacement for test error, but a benchmark suite would benefit if it could keep track of both test and train objectives.\n6. Joseph K. Bradley: I agree it will be nice to provide loss classes. Even though *Metrics classes exist already, loss classes might be nice as we provide more functionality for diagnosis during learning (e.g., for early stopping, model selection, etc.). Added link to related JIRA on optimization APIs.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.121826"}}
{"id": "6444bd579038e959e0244afcccb8b0d0", "issue_key": "SPARK-1056", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Header comment in Executor incorrectly implies it's not used for YARN", "description": "/** * Spark executor used with Mesos and the standalone scheduler. */", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-02-06T15:02:39.000+0000", "updated": "2014-04-04T20:51:01.000+0000", "resolved": "2014-02-18T13:42:09.000+0000", "labels": [], "components": ["YARN"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1056\nSummary: Header comment in Executor incorrectly implies it's not used for YARN\nDescription: /** * Spark executor used with Mesos and the standalone scheduler. */", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.121826"}}
{"id": "974d8c85acc0a70c6c838a23674f9805", "issue_key": "SPARK-1057", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Remove Fastutil", "description": "Really simple request, to upgrade fastutil to 6.5.x. The current version, 6.4.x, has some minor API's in the Object2xxOpenHashMap structures which is used in many places in Spark, and has been marked deprecated. Plus there is a conflict with another library we are using (saddle -- http://saddle.github.io/) which uses a newer version of fastutil. I'd be happy to send a PR. I guess a bigger question is do you want to keep using fastutil (SPARK-681) but Spark uses more than just hashmaps, so that probably requires another discussion.", "reporter": "Evan Chan", "assignee": "Sean R. Owen", "created": "2014-02-06T15:30:32.000+0000", "updated": "2014-04-12T05:49:42.000+0000", "resolved": "2014-04-12T05:49:42.000+0000", "labels": ["Starter"], "components": ["Spark Core"], "comments": [{"author": "Piyush Kansal", "body": "How about upgrading fastutil to 6.5.x till any decision regarding SPARK-681 is made?", "created": "2014-03-04T23:39:40.760+0000"}, {"author": "Evan Chan", "body": "This sounds good. I'll open a PR soon, guess I can assign this to myself.", "created": "2014-03-13T10:28:14.886+0000"}, {"author": "Evan Chan", "body": "Help, can't figure out how to assign this to myself.", "created": "2014-03-13T10:34:50.280+0000"}, {"author": "Usman Ghani", "body": "[~velvia] How did you end up assigning this?", "created": "2014-03-22T16:39:34.661+0000"}, {"author": "Evan Chan", "body": "Usman, I didn't assign it, a committer did. On Sat, Mar 22, 2014 at 4:42 PM, Usman Ghani (JIRA) -- -- Evan Chan Staff Engineer ev@ooyala.com |", "created": "2014-03-23T23:13:20.593+0000"}, {"author": "Evan Chan", "body": "PR created: https://github.com/apache/spark/pull/215", "created": "2014-03-24T00:07:19.924+0000"}, {"author": "Patrick Wendell", "body": "I changed the title of the JIRA to reflect the ultimate decision to remove fastutil. It was removed here: https://github.com/apache/spark/pull/266", "created": "2014-04-12T05:49:29.536+0000"}], "num_comments": 7, "text": "Issue: SPARK-1057\nSummary: Remove Fastutil\nDescription: Really simple request, to upgrade fastutil to 6.5.x. The current version, 6.4.x, has some minor API's in the Object2xxOpenHashMap structures which is used in many places in Spark, and has been marked deprecated. Plus there is a conflict with another library we are using (saddle -- http://saddle.github.io/) which uses a newer version of fastutil. I'd be happy to send a PR. I guess a bigger question is do you want to keep using fastutil (SPARK-681) but Spark uses more than just hashmaps, so that probably requires another discussion.\n\nComments (7):\n1. Piyush Kansal: How about upgrading fastutil to 6.5.x till any decision regarding SPARK-681 is made?\n2. Evan Chan: This sounds good. I'll open a PR soon, guess I can assign this to myself.\n3. Evan Chan: Help, can't figure out how to assign this to myself.\n4. Usman Ghani: [~velvia] How did you end up assigning this?\n5. Evan Chan: Usman, I didn't assign it, a committer did. On Sat, Mar 22, 2014 at 4:42 PM, Usman Ghani (JIRA) -- -- Evan Chan Staff Engineer ev@ooyala.com |\n6. Evan Chan: PR created: https://github.com/apache/spark/pull/215\n7. Patrick Wendell: I changed the title of the JIRA to reflect the ultimate decision to remove fastutil. It was removed here: https://github.com/apache/spark/pull/266", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.121826"}}
{"id": "7dd2459f9a3d1ceea44cdc538ca1ed39", "issue_key": "SPARK-1058", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fix Style Errors and Add Scala Style to Spark Build", "description": "Style errors eat up a huge amount of our review time. We should add scalastye to the build and fail builds if people make style violations. A corollary of this is that we need to bring style up to par in cases where it isn't. Scalastyle is planning to make a 4.0.0 release soon, which will allow us to exclude imports from the line limit. I set up an example style file here: https://github.com/pwendell/incubator-spark/compare/style?expand=1 https://github.com/scalastyle/scalastyle-plugin It would be nice to fix up our existing style errors and integrate scala style into the build.", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "created": "2014-02-06T17:10:39.000+0000", "updated": "2020-02-07T17:26:44.000+0000", "resolved": "2014-02-14T17:52:24.000+0000", "labels": [], "components": ["Project Infra"], "comments": [{"author": "Patrick McFadin", "body": "Thanks Prashant for this!", "created": "2014-02-14T17:52:24.668+0000"}], "num_comments": 1, "text": "Issue: SPARK-1058\nSummary: Fix Style Errors and Add Scala Style to Spark Build\nDescription: Style errors eat up a huge amount of our review time. We should add scalastye to the build and fail builds if people make style violations. A corollary of this is that we need to bring style up to par in cases where it isn't. Scalastyle is planning to make a 4.0.0 release soon, which will allow us to exclude imports from the line limit. I set up an example style file here: https://github.com/pwendell/incubator-spark/compare/style?expand=1 https://github.com/scalastyle/scalastyle-plugin It would be nice to fix up our existing style errors and integrate scala style into the build.\n\nComments (1):\n1. Patrick McFadin: Thanks Prashant for this!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.121826"}}
{"id": "f32e74ec0b1d05385eb8ec3e22c9f893", "issue_key": "SPARK-1059", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Now that we submit core requests to YARN, fix usage message in ClientArguments", "description": "\"Number of cores for the workers (Default: 1). This is unsused right now.\"", "reporter": "Sanford Ryza", "assignee": null, "created": "2014-02-06T17:57:08.000+0000", "updated": "2014-04-07T20:04:25.000+0000", "resolved": "2014-04-07T20:04:25.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Sandy Ryza", "body": "This got fixed in Tom's security patch.", "created": "2014-04-07T20:04:25.962+0000"}], "num_comments": 1, "text": "Issue: SPARK-1059\nSummary: Now that we submit core requests to YARN, fix usage message in ClientArguments\nDescription: \"Number of cores for the workers (Default: 1). This is unsused right now.\"\n\nComments (1):\n1. Sandy Ryza: This got fixed in Tom's security patch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.121826"}}
{"id": "da743d9a019a2a5a64443e441fb965c6", "issue_key": "SPARK-1060", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "JettyUtil is not using host information to start server", "description": "In the current implementation, the webserver in Master/Worker is started with (MasterUI/WorkerUI) val (srv, bPort) = JettyUtils.startJettyServer(\"0.0.0.0\", port, handlers) inside startJettyServer val server = new Server(currentPort) //the Server will take \"0.0.0.0\" as the hostname, i.e. will always bind to the IP address of the first NIC which is actually not using host information got by val host = Utils.localHostName() this can cause wrong IP binding, e.g. if the host has two NICs, N1 and N2, the user specify the SPARK_LOCAL_IP as the N2's IP address, however, when starting the web server, for the reason stated above, it will always bind to the N1's address", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-02-06T18:55:42.000+0000", "updated": "2014-02-08T23:39:47.000+0000", "resolved": "2014-02-08T23:39:47.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Nan Zhu", "body": "made a PR https://github.com/apache/incubator-spark/pull/556", "created": "2014-02-06T18:57:44.890+0000"}], "num_comments": 1, "text": "Issue: SPARK-1060\nSummary: JettyUtil is not using host information to start server\nDescription: In the current implementation, the webserver in Master/Worker is started with (MasterUI/WorkerUI) val (srv, bPort) = JettyUtils.startJettyServer(\"0.0.0.0\", port, handlers) inside startJettyServer val server = new Server(currentPort) //the Server will take \"0.0.0.0\" as the hostname, i.e. will always bind to the IP address of the first NIC which is actually not using host information got by val host = Utils.localHostName() this can cause wrong IP binding, e.g. if the host has two NICs, N1 and N2, the user specify the SPARK_LOCAL_IP as the N2's IP address, however, when starting the web server, for the reason stated above, it will always bind to the N1's address\n\nComments (1):\n1. Nan Zhu: made a PR https://github.com/apache/incubator-spark/pull/556", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "8060593c35b8d5f1dc312d56d6de8fa8", "issue_key": "SPARK-1061", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "allow Hadoop RDDs to be read w/ a partitioner", "description": "Using partitioners to get narrow dependencies can save tons of time on a shuffle. However, after saving an RDD to hdfs, and then reloading it, all partitioner information is lost. This means that you can never get a narrow dependency when loading data from hadoop. I think we could get around this by: 1) having a modified version of hadoop rdd that kept track of original part file (or maybe just prevent splits altogether ...) 2) add a \"assumePartition(partitioner:Partitioner, verify: Boolean)\" function to RDD. It would create a new RDD, which had the exact same data but just pretended that the RDD had the given partitioner applied to it. And if verify=true, it could add a mapPartitionsWithIndex to check that each record was in the right partition. http://apache-spark-user-list.1001560.n3.nabble.com/setting-partitioners-with-hadoop-rdds-td976.html", "reporter": "Imran Rashid", "assignee": "Imran Rashid", "created": "2014-02-06T19:45:02.000+0000", "updated": "2016-01-05T11:50:42.000+0000", "resolved": "2016-01-05T11:50:42.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "It might be necessary to modify HadoopRDD here due to the HDFS block size. The problem is that if you save the RDD and one partition is written as part-00000, but that file is more than one HDFS block long, you'll get multiple map tasks reading it when you re-create the RDD from it. So the ideal solution would take into account that any input split from part-00000 falls into partition 1, and would probably need one mapper for that whole file in the current version of Spark.", "created": "2014-02-06T20:12:17.101+0000"}, {"author": "Imran Rashid", "body": "yeah, I think this really only makes if the FileInputFormat has isSplittable set to false. It would be really hard to keep the partitioner accurate if the part files get split. (I don't think the api gives you enough flexibility even if you wanted to.) I think as long as HadoopPartitioner.assumePartitioner has a check that the input isn't splittlable, do you think this shoudl work?", "created": "2014-02-07T16:14:55.585+0000"}, {"author": "Apache Spark", "body": "User 'squito' has created a pull request for this issue: https://github.com/apache/spark/pull/4449", "created": "2015-02-07T07:01:47.201+0000"}, {"author": "Sean R. Owen", "body": "Is this still live?", "created": "2015-12-31T09:11:30.097+0000"}], "num_comments": 4, "text": "Issue: SPARK-1061\nSummary: allow Hadoop RDDs to be read w/ a partitioner\nDescription: Using partitioners to get narrow dependencies can save tons of time on a shuffle. However, after saving an RDD to hdfs, and then reloading it, all partitioner information is lost. This means that you can never get a narrow dependency when loading data from hadoop. I think we could get around this by: 1) having a modified version of hadoop rdd that kept track of original part file (or maybe just prevent splits altogether ...) 2) add a \"assumePartition(partitioner:Partitioner, verify: Boolean)\" function to RDD. It would create a new RDD, which had the exact same data but just pretended that the RDD had the given partitioner applied to it. And if verify=true, it could add a mapPartitionsWithIndex to check that each record was in the right partition. http://apache-spark-user-list.1001560.n3.nabble.com/setting-partitioners-with-hadoop-rdds-td976.html\n\nComments (4):\n1. Matei Alexandru Zaharia: It might be necessary to modify HadoopRDD here due to the HDFS block size. The problem is that if you save the RDD and one partition is written as part-00000, but that file is more than one HDFS block long, you'll get multiple map tasks reading it when you re-create the RDD from it. So the ideal solution would take into account that any input split from part-00000 falls into partition 1, and would probably need one mapper for that whole file in the current version of Spark.\n2. Imran Rashid: yeah, I think this really only makes if the FileInputFormat has isSplittable set to false. It would be really hard to keep the partitioner accurate if the part files get split. (I don't think the api gives you enough flexibility even if you wanted to.) I think as long as HadoopPartitioner.assumePartitioner has a check that the input isn't splittlable, do you think this shoudl work?\n3. Apache Spark: User 'squito' has created a pull request for this issue: https://github.com/apache/spark/pull/4449\n4. Sean R. Owen: Is this still live?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "30d3d4c4847b066ff07e1b4b6d468972", "issue_key": "SPARK-1062", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add rdd.intersection(otherRdd) method", "description": "Based on this GitHub PR: https://github.com/apache/incubator-spark/pull/506", "reporter": "Andrew Ash", "assignee": "Andrew Ash", "created": "2014-02-06T22:31:25.000+0000", "updated": "2014-02-06T22:50:13.000+0000", "resolved": "2014-02-06T22:40:11.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "Ready to merge after several rounds of CR", "created": "2014-02-06T22:32:58.696+0000"}, {"author": "Patrick McFadin", "body": "Hey [~ash211] usually the person working on it remains the asignee even if in the short term other people help review and commit it.", "created": "2014-02-06T22:37:01.094+0000"}, {"author": "Andrew Ash", "body": "Ah ok -- I had it assigned to you because I don't think I have permissions to merge into master. Thanks for the merge!", "created": "2014-02-06T22:40:46.420+0000"}, {"author": "Patrick McFadin", "body": "ya at least in other projects i've been on. The assignee is consistently the person who contributed the code. This way after the fact people can see that you worked on it, even if I happened to commit it.", "created": "2014-02-06T22:43:39.945+0000"}, {"author": "Andrew Ash", "body": "I'm used to having it land on the person with the \"next action\" but I'm happy to do the code contributor instead. Cheers!", "created": "2014-02-06T22:45:27.405+0000"}, {"author": "Patrick McFadin", "body": "Ya that's definitely how JIRA is used in many contexts... anyways I guess we can use either approach.", "created": "2014-02-06T22:50:13.600+0000"}], "num_comments": 6, "text": "Issue: SPARK-1062\nSummary: Add rdd.intersection(otherRdd) method\nDescription: Based on this GitHub PR: https://github.com/apache/incubator-spark/pull/506\n\nComments (6):\n1. Andrew Ash: Ready to merge after several rounds of CR\n2. Patrick McFadin: Hey [~ash211] usually the person working on it remains the asignee even if in the short term other people help review and commit it.\n3. Andrew Ash: Ah ok -- I had it assigned to you because I don't think I have permissions to merge into master. Thanks for the merge!\n4. Patrick McFadin: ya at least in other projects i've been on. The assignee is consistently the person who contributed the code. This way after the fact people can see that you worked on it, even if I happened to commit it.\n5. Andrew Ash: I'm used to having it land on the person with the \"next action\" but I'm happy to do the code contributor instead. Cheers!\n6. Patrick McFadin: Ya that's definitely how JIRA is used in many contexts... anyways I guess we can use either approach.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "437df7cda9e7828289891713fa0ecfe7", "issue_key": "SPARK-1063", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add .sortBy(f) method on RDD", "description": "Based on this GitHub PR: https://github.com/apache/incubator-spark/pull/508 I've written the below so many times that I think it'd be broadly useful to have a .sortBy(f) method on RDD:  .keyBy{l => <my function> } .sortByKey() .map(_._2)", "reporter": "Andrew Ash", "assignee": "Andrew Ash", "created": "2014-02-06T22:34:36.000+0000", "updated": "2014-11-06T07:09:39.000+0000", "resolved": "2014-11-06T07:09:39.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "The old PR is gone now (and was never merged) so I'm re-aiming at the apache/spark repo here: https://github.com/apache/spark/pull/369", "created": "2014-04-09T14:40:27.958+0000"}], "num_comments": 1, "text": "Issue: SPARK-1063\nSummary: Add .sortBy(f) method on RDD\nDescription: Based on this GitHub PR: https://github.com/apache/incubator-spark/pull/508 I've written the below so many times that I think it'd be broadly useful to have a .sortBy(f) method on RDD:  .keyBy{l => <my function> } .sortByKey() .map(_._2)\n\nComments (1):\n1. Andrew Ash: The old PR is gone now (and was never merged) so I'm re-aiming at the apache/spark repo here: https://github.com/apache/spark/pull/369", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "85b6bb884f0a58465137dcdeb116db76", "issue_key": "SPARK-1064", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Make it possible to use cluster's Hadoop jars when running against YARN", "description": "YARN applications like MapReduce and Tez rely on the cluster's Hadoop jars instead of distributing their own. This has a couple advantages * Avoids sending a bunch of bits to every node for each app * Only a single version of Hadoop can be running on a cluster at one time, simplifying debugging * Easier to upgrade and apply patched versions of Hadoop", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-02-07T10:25:03.000+0000", "updated": "2014-04-04T20:50:58.000+0000", "resolved": "2014-03-12T00:49:12.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Sanford Ryza", "body": "https://github.com/apache/incubator-spark/pull/649", "created": "2014-02-24T22:31:21.591+0000"}, {"author": "Thomas Graves", "body": "https://github.com/apache/spark/pull/102", "created": "2014-03-20T12:17:45.047+0000"}], "num_comments": 2, "text": "Issue: SPARK-1064\nSummary: Make it possible to use cluster's Hadoop jars when running against YARN\nDescription: YARN applications like MapReduce and Tez rely on the cluster's Hadoop jars instead of distributing their own. This has a couple advantages * Avoids sending a bunch of bits to every node for each app * Only a single version of Hadoop can be running on a cluster at one time, simplifying debugging * Easier to upgrade and apply patched versions of Hadoop\n\nComments (2):\n1. Sanford Ryza: https://github.com/apache/incubator-spark/pull/649\n2. Thomas Graves: https://github.com/apache/spark/pull/102", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "3a48056f109da22581fd6ee5f89c80d3", "issue_key": "SPARK-1065", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "PySpark runs out of memory with large broadcast variables", "description": "PySpark's driver components may run out of memory when broadcasting large variables (say 1 gigabyte). Because PySpark's broadcast is implemented on top of Java Spark's broadcast by broadcasting a pickled Python as a byte array, we may be retaining multiple copies of the large object: a pickled copy in the JVM and a deserialized copy in the Python driver. The problem could also be due to memory requirements during pickling. PySpark is also affected by broadcast variables not being garbage collected. Adding an unpersist() method to broadcast variables may fix this: https://github.com/apache/incubator-spark/pull/543. As a first step to fixing this, we should write a failing test to reproduce the error. This was discovered by [~sandy]: [\"trouble with broadcast variables on pyspark\"|http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-broadcast-variables-on-pyspark-tp1301.html].", "reporter": "Josh Rosen", "assignee": "Davies Liu", "created": "2014-02-07T11:41:54.000+0000", "updated": "2014-08-17T00:00:30.000+0000", "resolved": "2014-08-17T00:00:30.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Sanford Ryza", "body": "Here's some code that reproduces it.  theconf = SparkConf().set(\"spark.executor.memory\", \"5g\").setAppName(\"broadcastfail\").setMaster(cluster_url) sc = SparkContext(conf=theconf) broadcast_vals = [] for i in range(5): datas = [[float(i) for i in range(200)] for i in range(100000)] val = sc.broadcast(datas).value broadcast_vals.append(val) sc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val) for val in broadcast_vals])).collect()  It generates a few arrays of floats, each of which should take about 160 MB. The executors never end up using much memory, but the driver uses an enormous amount. Both the python and java processes ramp up to multiple GB until I start seeing a bunch of \"OutOfMemoryError: java heap space\". With a single 160MB array, the job completes fine, but the driver still uses about 9 GB.", "created": "2014-02-07T18:01:27.322+0000"}, {"author": "Vlad Frolov", "body": "I am facing the same issue in my project, where I use PySpark. As a proof of that the big objects I have could easily fit into nodes' memory, I am going to use dummy solution of saving my big objects into HDFS and load them on Python nodes. Does anybody have an idea how to fix the issue in a better way? I don't have enough either Scala nor Java knowledge to fix this in Spark core. However, I feel like broadcast variables could be reimplemented on Python side though it seems a bit dangerous idea because we don't want to have separate implementations of one thing in both languages. That will also save memory, because while we use broadcasts through Scala we have 1 copy in JVM, 1 pickled copy in Python and 1 constructed object copy in Python.", "created": "2014-08-11T22:06:09.695+0000"}, {"author": "Vlad Frolov", "body": "I have finished my experiment of using HDFS as a temp storage for my big objects. It showed that my mappers do not leak memory and work pretty stable. However, it takes long time to load these objects on each node. Is there straightforward way to detect memory leaks in Spark and PySpark?", "created": "2014-08-12T17:18:09.031+0000"}, {"author": "Davies Liu", "body": "The broadcast was not used correctly in the above code, it should be used like this:  broadcast_vals = [] for i in range(5): datas = [[float(i) for i in range(200)] for i in range(100000)] val = sc.broadcast(datas) broadcast_vals.append(val) sc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val.value) for val in broadcast_vals])).collect()  The reference of object in Python driver in not necessary in most cases, we will make it optional (no reference by default), then it can reduce the memory used in Python driver.", "created": "2014-08-13T00:30:24.856+0000"}, {"author": "Apache Spark", "body": "User 'davies' has created a pull request for this issue: https://github.com/apache/spark/pull/1912", "created": "2014-08-13T00:33:10.240+0000"}, {"author": "Vlad Frolov", "body": "[~davies] Will your PR take into account this fix: [SPARK-2521] Broadcast RDD object (instead of sending it along with every task) https://github.com/apache/spark/commit/7b8cd175254d42c8e82f0aa8eb4b7f3508d8fde2 ? \"The patch uses broadcast to send RDD objects and the closures to executors\"", "created": "2014-08-13T00:45:39.268+0000"}, {"author": "Vlad Frolov", "body": "[~davies] I have not noticed that there was that mistake in the example, but I have not used that code. I run into the issue in my own code, where I use broadcasts correctly. I'm building your branch now and will try it right away. Thank you for your fix!", "created": "2014-08-13T00:54:33.783+0000"}, {"author": "Davies Liu", "body": "[~frol], I think broadcast the RDD object is already done by that PR. But the serialized closure will still be sent to JVM by py4j. After using broadcast for large datasets, the serialized closure should not be too huge, so I guess it will not be a big issue.", "created": "2014-08-13T00:57:59.782+0000"}, {"author": "Davies Liu", "body": "After this patch, the above test can run successfully with about 700M memory in Python driver, 5xxMB memory in JVM driver, and 3G memory in python worker. It may triggle another problem when run it with Mesos or YARN, because Spark does not reserve memory for Python worker, this may be fixed in 1.2 release.", "created": "2014-08-13T01:02:25.188+0000"}, {"author": "Vlad Frolov", "body": "[~davies] I understand that if you use broadcast explicitly the closure won't be huge, but the point of that PR was also \"1. Users won't need to decide what to broadcast anymore, unless they would want to use a large object multiple times in different operations\".", "created": "2014-08-13T01:04:02.776+0000"}, {"author": "Vlad Frolov", "body": "[~davies] I use YARN setup so I will see how it goes.", "created": "2014-08-13T01:05:41.088+0000"}, {"author": "Vlad Frolov", "body": "[~davies] I have compiled and run your broadcast branch against my cluster on YARN. It does not leak memory any more! And it is at least 25% faster than my dummy implementation on top of HDFS. Implementation with broadcasts takes 4.5 minutes to finish a task, where my implementation took 6 minutes. More heavy tests are still working.", "created": "2014-08-13T01:43:11.289+0000"}, {"author": "Vlad Frolov", "body": "Heavy tasks completed in 18 minutes each instead of 22 minutes, which is 20% speed up. That is nice! I don't see any problems on my YARN cluster. Java nodes eat up to 1.5GB RAM (which is my JVM limit) each and Python daemons eat around 650MB each. Though those numbers are still a bit weird, it is obvious to me that workers don't leak now. Thank you a lot!", "created": "2014-08-13T02:37:13.564+0000"}, {"author": "Davies Liu", "body": "Cool, thanks for the tests. If we can compress the data, it will be faster. I will do it in another separate PR. The closure is serialized by cloudpickle, so it will be much slower if you do not use broadcast explicitly. We can show an warning if the serialized closure is too big.", "created": "2014-08-13T04:57:18.711+0000"}], "num_comments": 14, "text": "Issue: SPARK-1065\nSummary: PySpark runs out of memory with large broadcast variables\nDescription: PySpark's driver components may run out of memory when broadcasting large variables (say 1 gigabyte). Because PySpark's broadcast is implemented on top of Java Spark's broadcast by broadcasting a pickled Python as a byte array, we may be retaining multiple copies of the large object: a pickled copy in the JVM and a deserialized copy in the Python driver. The problem could also be due to memory requirements during pickling. PySpark is also affected by broadcast variables not being garbage collected. Adding an unpersist() method to broadcast variables may fix this: https://github.com/apache/incubator-spark/pull/543. As a first step to fixing this, we should write a failing test to reproduce the error. This was discovered by [~sandy]: [\"trouble with broadcast variables on pyspark\"|http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-broadcast-variables-on-pyspark-tp1301.html].\n\nComments (14):\n1. Sanford Ryza: Here's some code that reproduces it.  theconf = SparkConf().set(\"spark.executor.memory\", \"5g\").setAppName(\"broadcastfail\").setMaster(cluster_url) sc = SparkContext(conf=theconf) broadcast_vals = [] for i in range(5): datas = [[float(i) for i in range(200)] for i in range(100000)] val = sc.broadcast(datas).value broadcast_vals.append(val) sc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val) for val in broadcast_vals])).collect()  It generates a few arrays of floats, each of which should take about 160 MB. The executors never end up using much memory, but the driver uses an enormous amount. Both the python and java processes ramp up to multiple GB until I start seeing a bunch of \"OutOfMemoryError: java heap space\". With a single 160MB array, the job completes fine, but the driver still uses about 9 GB.\n2. Vlad Frolov: I am facing the same issue in my project, where I use PySpark. As a proof of that the big objects I have could easily fit into nodes' memory, I am going to use dummy solution of saving my big objects into HDFS and load them on Python nodes. Does anybody have an idea how to fix the issue in a better way? I don't have enough either Scala nor Java knowledge to fix this in Spark core. However, I feel like broadcast variables could be reimplemented on Python side though it seems a bit dangerous idea because we don't want to have separate implementations of one thing in both languages. That will also save memory, because while we use broadcasts through Scala we have 1 copy in JVM, 1 pickled copy in Python and 1 constructed object copy in Python.\n3. Vlad Frolov: I have finished my experiment of using HDFS as a temp storage for my big objects. It showed that my mappers do not leak memory and work pretty stable. However, it takes long time to load these objects on each node. Is there straightforward way to detect memory leaks in Spark and PySpark?\n4. Davies Liu: The broadcast was not used correctly in the above code, it should be used like this:  broadcast_vals = [] for i in range(5): datas = [[float(i) for i in range(200)] for i in range(100000)] val = sc.broadcast(datas) broadcast_vals.append(val) sc.parallelize([i for i in range(80)]).map(lambda x: sum([len(val.value) for val in broadcast_vals])).collect()  The reference of object in Python driver in not necessary in most cases, we will make it optional (no reference by default), then it can reduce the memory used in Python driver.\n5. Apache Spark: User 'davies' has created a pull request for this issue: https://github.com/apache/spark/pull/1912\n6. Vlad Frolov: [~davies] Will your PR take into account this fix: [SPARK-2521] Broadcast RDD object (instead of sending it along with every task) https://github.com/apache/spark/commit/7b8cd175254d42c8e82f0aa8eb4b7f3508d8fde2 ? \"The patch uses broadcast to send RDD objects and the closures to executors\"\n7. Vlad Frolov: [~davies] I have not noticed that there was that mistake in the example, but I have not used that code. I run into the issue in my own code, where I use broadcasts correctly. I'm building your branch now and will try it right away. Thank you for your fix!\n8. Davies Liu: [~frol], I think broadcast the RDD object is already done by that PR. But the serialized closure will still be sent to JVM by py4j. After using broadcast for large datasets, the serialized closure should not be too huge, so I guess it will not be a big issue.\n9. Davies Liu: After this patch, the above test can run successfully with about 700M memory in Python driver, 5xxMB memory in JVM driver, and 3G memory in python worker. It may triggle another problem when run it with Mesos or YARN, because Spark does not reserve memory for Python worker, this may be fixed in 1.2 release.\n10. Vlad Frolov: [~davies] I understand that if you use broadcast explicitly the closure won't be huge, but the point of that PR was also \"1. Users won't need to decide what to broadcast anymore, unless they would want to use a large object multiple times in different operations\".", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "840470902a38eeea4244673031ffaa97", "issue_key": "SPARK-1066", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Improve Developer Documentation", "description": "- Port release documentation to the wiki: https://docs.google.com/document/d/15YGRQjnL6IVsULgPd4xsxqbJQiU438mU6TxR-5uYvLs/edit#heading=h.y4mkppazvqs9 - Contribute developer scripts to spark repo - Describe on wiki how to use local builds", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2014-02-07T11:45:23.000+0000", "updated": "2014-03-30T04:14:19.000+0000", "resolved": "2014-03-21T14:57:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Henry Saputra", "body": "Looks like this is fixed?", "created": "2014-02-10T11:54:20.712+0000"}], "num_comments": 1, "text": "Issue: SPARK-1066\nSummary: Improve Developer Documentation\nDescription: - Port release documentation to the wiki: https://docs.google.com/document/d/15YGRQjnL6IVsULgPd4xsxqbJQiU438mU6TxR-5uYvLs/edit#heading=h.y4mkppazvqs9 - Contribute developer scripts to spark repo - Describe on wiki how to use local builds\n\nComments (1):\n1. Henry Saputra: Looks like this is fixed?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "53385d86ad1f53fe069c948abc7a2dfd", "issue_key": "SPARK-1067", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Default log4j initialization causes errors for those not using log4j", "description": "Some users do not use slf4j-log4j... by assuming thy are using log4j this breaks their use of spark.", "reporter": "Patrick McFadin", "assignee": "Patrick McFadin", "created": "2014-02-07T18:06:38.000+0000", "updated": "2014-03-08T16:12:49.000+0000", "resolved": "2014-03-08T16:12:36.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1067\nSummary: Default log4j initialization causes errors for those not using log4j\nDescription: Some users do not use slf4j-log4j... by assuming thy are using log4j this breaks their use of spark.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "7797670824f9ac73a05d2def42465119", "issue_key": "SPARK-1068", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Worker.scala should kill drivers in method postStop()", "description": "", "reporter": "Qiuzhuang Lian", "assignee": "Qiuzhuang Lian", "created": "2014-02-08T00:05:50.000+0000", "updated": "2014-02-08T13:00:58.000+0000", "resolved": "2014-02-08T13:00:58.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Qiuzhuang Lian", "body": "PR: https://github.com/apache/incubator-spark/pull/561", "created": "2014-02-08T00:11:50.941+0000"}], "num_comments": 1, "text": "Issue: SPARK-1068\nSummary: Worker.scala should kill drivers in method postStop()\n\nComments (1):\n1. Qiuzhuang Lian: PR: https://github.com/apache/incubator-spark/pull/561", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "5506e21c67584cd17a2441b65d6caeec", "issue_key": "SPARK-1069", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Provide binary compatibility in Spark 1.X releases", "description": "It would be good if Spark offered binary compatibility between versions, meaning users do not have to recompile. This JIRA marks the intention of codifying this as a policy once we do a 1.1 release which is link-level compatible with 1.0. Examples of binary incompatibilities: https://github.com/jsuereth/binary-resilience Some good talks on this: https://skillsmatter.com/skillscasts/3269-binary-resilience http://www.slideshare.net/mircodotta/managing-binary-compatibility-in-scala", "reporter": "Patrick McFadin", "assignee": null, "created": "2014-02-08T12:50:19.000+0000", "updated": "2014-09-16T16:12:21.000+0000", "resolved": "2014-09-16T16:12:21.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick Wendell", "body": "We've implemented MIMA and a bunch of other things to help us do this, so I can close this now.", "created": "2014-09-16T16:12:21.206+0000"}], "num_comments": 1, "text": "Issue: SPARK-1069\nSummary: Provide binary compatibility in Spark 1.X releases\nDescription: It would be good if Spark offered binary compatibility between versions, meaning users do not have to recompile. This JIRA marks the intention of codifying this as a policy once we do a 1.1 release which is link-level compatible with 1.0. Examples of binary incompatibilities: https://github.com/jsuereth/binary-resilience Some good talks on this: https://skillsmatter.com/skillscasts/3269-binary-resilience http://www.slideshare.net/mircodotta/managing-binary-compatibility-in-scala\n\nComments (1):\n1. Patrick Wendell: We've implemented MIMA and a bunch of other things to help us do this, so I can close this now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "93631aa67d799be323b944719ef0a9f8", "issue_key": "SPARK-1220", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Principal Component Analysis", "description": "Build Principal Component Analysis (PCA) for Spark. Use previous SVD implementation. Current Pull Request for this: * github.com/apache/spark/pull/88", "reporter": "Reza Zadeh", "assignee": "Reza Zadeh", "created": "2014-02-08T18:12:35.000+0000", "updated": "2014-03-20T21:58:35.000+0000", "resolved": "2014-03-20T21:58:34.000+0000", "labels": [], "components": ["MLlib"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1220\nSummary: Principal Component Analysis\nDescription: Build Principal Component Analysis (PCA) for Spark. Use previous SVD implementation. Current Pull Request for this: * github.com/apache/spark/pull/88", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "3d2e3e9c010f1ed41f59cbae9c0b2c82", "issue_key": "SPARK-1070", "issue_type": "Task", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add check for JIRA ticket in the Github pull request title/summary with CI", "description": "As part of discussion in the dev@ list to add audit trail of Spark's Github pull requests (PR) to JIRA, need to add check maybe in the Jenkins CI to verify that the PRs contain JIRA ticket number in the title/ summary. There are maybe some PRs that may not need ticket so probably add support for some \"magic\" keyword to bypass the check. But this should be done in rare cases.", "reporter": "Henry Saputra", "assignee": "Mark Hamstra", "created": "2014-02-08T23:43:12.000+0000", "updated": "2016-01-05T19:43:21.000+0000", "resolved": "2016-01-05T19:43:21.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Nicholas Chammas", "body": "[~hsaputra] - The [Spark PR Board | https://spark-prs.appspot.com/] automatically parses JIRA ticket IDs in the PR titles. Does that address the need behind this request? cc [~joshrosen]", "created": "2014-11-03T19:29:24.425+0000"}, {"author": "Henry Saputra", "body": "[~nchammas], way back when Patrick propose the right way to send PR there was a discussion to \"force\" PR to have JIRA rocket prefix in the summary. This ticket is filed to address that issue/ idea.", "created": "2014-11-03T19:32:29.475+0000"}, {"author": "Josh Rosen", "body": "Resolving as \"Won't Fix.\" Most contributors now submit PRs with JIRAs, so the cost of infrequently reminding new folks to file them isn't high enough to justify automation here.", "created": "2016-01-05T19:43:21.436+0000"}], "num_comments": 3, "text": "Issue: SPARK-1070\nSummary: Add check for JIRA ticket in the Github pull request title/summary with CI\nDescription: As part of discussion in the dev@ list to add audit trail of Spark's Github pull requests (PR) to JIRA, need to add check maybe in the Jenkins CI to verify that the PRs contain JIRA ticket number in the title/ summary. There are maybe some PRs that may not need ticket so probably add support for some \"magic\" keyword to bypass the check. But this should be done in rare cases.\n\nComments (3):\n1. Nicholas Chammas: [~hsaputra] - The [Spark PR Board | https://spark-prs.appspot.com/] automatically parses JIRA ticket IDs in the PR titles. Does that address the need behind this request? cc [~joshrosen]\n2. Henry Saputra: [~nchammas], way back when Patrick propose the right way to send PR there was a discussion to \"force\" PR to have JIRA rocket prefix in the summary. This ticket is filed to address that issue/ idea.\n3. Josh Rosen: Resolving as \"Won't Fix.\" Most contributors now submit PRs with JIRAs, so the cost of infrequently reminding new folks to file them isn't high enough to justify automation here.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "45046144bf036cc4c571e3362f025aeb", "issue_key": "SPARK-1071", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Tidy logging strategy and use of log4j", "description": "Prompted by a recent thread on the mailing list, I tried and failed to see if Spark can be made independent of log4j. There are a few cases where control of the underlying logging is pretty useful, and to do that, you have to bind to a specific logger. Instead I propose some tidying that leaves Spark's use of log4j, but gets rid of warnings and should still enable downstream users to switch. The idea is to pipe everything (except log4j) through SLF4J, and have Spark use SLF4J directly when logging, and where Spark needs to output info (REPL and tests), bind from SLF4J to log4j. This leaves the same behavior in Spark. It means that downstream users who want to use something except log4j should: - Exclude dependencies on log4j, slf4j-log4j12 from Spark - Include dependency on log4j-over-slf4j - Include dependency on another logger X, and another slf4j-X - Recreate any log config that Spark does, that is needed, in the other logger's config That sounds about right. Here are the key changes: - Include the jcl-over-slf4j shim everywhere by depending on it in core. - Exclude dependencies on commons-logging from third-party libraries. - Include the jul-to-slf4j shim everywhere by depending on it in core. - Exclude slf4j-* dependencies from third-party libraries to prevent collision or warnings - Added missing slf4j-log4j12 binding to GraphX, Bagel module tests And minor/incidental changes: - Update to SLF4J 1.7.5, which happily matches Hadoop 2’s version and is a recommended update over 1.7.2 - (Remove a duplicate HBase dependency declaration in SparkBuild.scala) - (Remove a duplicate mockito dependency declaration that was causing warnings and bugging me) Pull request coming.", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "created": "2014-02-09T14:49:59.000+0000", "updated": "2015-01-15T09:08:43.000+0000", "resolved": "2014-02-23T11:41:36.000+0000", "labels": [], "components": ["Build", "Input/Output"], "comments": [{"author": "Aaron Davidson", "body": "If a user attempts to use log4j-over-slf4j without first excluding Spark's dependency on log4j and slf4j-log4j12, will they see the standard stack overflow exception between those two? If so, we should perhaps add a \"Logging with Spark\" section somewhere in the docs to be very clear how to interface with Spark's logger.", "created": "2014-02-09T16:21:38.077+0000"}, {"author": "Sean Owen", "body": "slf4j detects that situation and provides a good warning. (This isn't quite the StackOverflowError situation referred to in the conversation on the mailing list last week.) It's all par for the course in slf4j, but docs can't hurt. If the project proceeds with a change like this I'll write up a blurb on the wiki, sure.", "created": "2014-02-09T16:53:12.670+0000"}], "num_comments": 2, "text": "Issue: SPARK-1071\nSummary: Tidy logging strategy and use of log4j\nDescription: Prompted by a recent thread on the mailing list, I tried and failed to see if Spark can be made independent of log4j. There are a few cases where control of the underlying logging is pretty useful, and to do that, you have to bind to a specific logger. Instead I propose some tidying that leaves Spark's use of log4j, but gets rid of warnings and should still enable downstream users to switch. The idea is to pipe everything (except log4j) through SLF4J, and have Spark use SLF4J directly when logging, and where Spark needs to output info (REPL and tests), bind from SLF4J to log4j. This leaves the same behavior in Spark. It means that downstream users who want to use something except log4j should: - Exclude dependencies on log4j, slf4j-log4j12 from Spark - Include dependency on log4j-over-slf4j - Include dependency on another logger X, and another slf4j-X - Recreate any log config that Spark does, that is needed, in the other logger's config That sounds about right. Here are the key changes: - Include the jcl-over-slf4j shim everywhere by depending on it in core. - Exclude dependencies on commons-logging from third-party libraries. - Include the jul-to-slf4j shim everywhere by depending on it in core. - Exclude slf4j-* dependencies from third-party libraries to prevent collision or warnings - Added missing slf4j-log4j12 binding to GraphX, Bagel module tests And minor/incidental changes: - Update to SLF4J 1.7.5, which happily matches Hadoop 2’s version and is a recommended update over 1.7.2 - (Remove a duplicate HBase dependency declaration in SparkBuild.scala) - (Remove a duplicate mockito dependency declaration that was causing warnings and bugging me) Pull request coming.\n\nComments (2):\n1. Aaron Davidson: If a user attempts to use log4j-over-slf4j without first excluding Spark's dependency on log4j and slf4j-log4j12, will they see the standard stack overflow exception between those two? If so, we should perhaps add a \"Logging with Spark\" section somewhere in the docs to be very clear how to interface with Spark's logger.\n2. Sean Owen: slf4j detects that situation and provides a good warning. (This isn't quite the StackOverflowError situation referred to in the conversation on the mailing list last week.) It's all par for the course in slf4j, but docs can't hurt. If the project proceeds with a change like this I'll write up a blurb on the wiki, sure.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "b72b889ab1e95b8797093d0ec0cfed30", "issue_key": "SPARK-1072", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Use binary search for RangePartitioner when there is more than 1000 partitions", "description": "", "reporter": "Holden Karau", "assignee": "Holden Karau", "created": "2014-02-09T23:25:12.000+0000", "updated": "2014-06-18T07:57:22.000+0000", "resolved": "2014-06-18T07:57:21.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Henry Saputra", "body": "This is fixed with https://github.com/apache/incubator-spark/pull/571 ?", "created": "2014-03-19T10:58:14.334+0000"}], "num_comments": 1, "text": "Issue: SPARK-1072\nSummary: Use binary search for RangePartitioner when there is more than 1000 partitions\n\nComments (1):\n1. Henry Saputra: This is fixed with https://github.com/apache/incubator-spark/pull/571 ?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "c7a6daddcbf6490304ecf52a29f83901", "issue_key": "SPARK-1073", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "GitHub PR squasher has bad titles", "description": "https://github.com/apache/incubator-spark/pull/574", "reporter": "Andrew Ash", "assignee": "Andrew Ash", "created": "2014-02-10T00:27:10.000+0000", "updated": "2014-02-12T23:29:21.000+0000", "resolved": "2014-02-12T23:29:21.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Patrick McFadin", "body": "Thanks for this Andrew! I consider this a \"Major\" issue - good project infrastructure is important :)", "created": "2014-02-12T23:29:21.937+0000"}], "num_comments": 1, "text": "Issue: SPARK-1073\nSummary: GitHub PR squasher has bad titles\nDescription: https://github.com/apache/incubator-spark/pull/574\n\nComments (1):\n1. Patrick McFadin: Thanks for this Andrew! I consider this a \"Major\" issue - good project infrastructure is important :)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "fc0e39dc80ca53add71996548a423a43", "issue_key": "SPARK-1074", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "JavaPairRDD as Object File", "description": "So I can perform a save command on a JavaPairRDD  static public void HSave(JavaPairRDD<D3int, int[]> baseImg,String path) { final String outpath=(new File(path)).getAbsolutePath(); baseImg.saveAsObjectFile(outpath); }  When I use the objectFile command from the JavaSparkContext  static public ReadObjectFile(JavaSparkContext jsc, final String path) { JavaPairRDD<D3int, int[]> newImage=(JavaPairRDD<D3int,int[]>) jsc.objectFile(path); }  I get an error cannot cast from JavaRDD to JavaPairRDD. Is there a way to get back to JavaPairRDD or will I need to map my data to a JavaRDD, save, load, then remap the JavaRDD back to the JavaPairRDD", "reporter": "Kevin Mader", "assignee": null, "created": "2014-02-10T07:37:47.000+0000", "updated": "2014-11-08T10:00:50.000+0000", "resolved": "2014-11-08T10:00:49.000+0000", "labels": [], "components": ["Input/Output", "Java API"], "comments": [{"author": "Kevin Mader", "body": "Attempting to open up the saved objectFile using the spark shell / scala initially appears to work fine  val inObj=sc.objectFile(objPath) org.apache.spark.rdd.RDD[Nothing] = FlatMappedRDD[3] at objectFile at <console>:12  But when you take the first element it produces an Array Store Exception  inObj.take(1) java.lang.ArrayStoreException: [Ljava.lang.Object; at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:835) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:835) at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56) at org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:695) at org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:679)", "created": "2014-02-10T07:50:46.421+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Hi Kevin, In Java, try doing a map() on the JavaRDD to get a JavaPairRDD. You can use the identity function in there and it will just mark it as having pairs. Alternatively you can do new JavaPairRDD(javaRDD.rdd()).", "created": "2014-03-02T17:44:28.599+0000"}, {"author": "Sean R. Owen", "body": "Am I right in thinking that if you want to save a JavaPairRDD to HDFS, you have key-value pairs, and so you want to use JavaPairRDD.saveAsNewAPIHadoopFile, and SparkContext.sequenceFile to read it? This works. objectFile doesn't seem like the right approach anyway.", "created": "2014-11-08T10:00:50.014+0000"}], "num_comments": 3, "text": "Issue: SPARK-1074\nSummary: JavaPairRDD as Object File\nDescription: So I can perform a save command on a JavaPairRDD  static public void HSave(JavaPairRDD<D3int, int[]> baseImg,String path) { final String outpath=(new File(path)).getAbsolutePath(); baseImg.saveAsObjectFile(outpath); }  When I use the objectFile command from the JavaSparkContext  static public ReadObjectFile(JavaSparkContext jsc, final String path) { JavaPairRDD<D3int, int[]> newImage=(JavaPairRDD<D3int,int[]>) jsc.objectFile(path); }  I get an error cannot cast from JavaRDD to JavaPairRDD. Is there a way to get back to JavaPairRDD or will I need to map my data to a JavaRDD, save, load, then remap the JavaRDD back to the JavaPairRDD\n\nComments (3):\n1. Kevin Mader: Attempting to open up the saved objectFile using the spark shell / scala initially appears to work fine  val inObj=sc.objectFile(objPath) org.apache.spark.rdd.RDD[Nothing] = FlatMappedRDD[3] at objectFile at <console>:12  But when you take the first element it produces an Array Store Exception  inObj.take(1) java.lang.ArrayStoreException: [Ljava.lang.Object; at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:835) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:835) at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56) at org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:695) at org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:679)\n2. Matei Alexandru Zaharia: Hi Kevin, In Java, try doing a map() on the JavaRDD to get a JavaPairRDD. You can use the identity function in there and it will just mark it as having pairs. Alternatively you can do new JavaPairRDD(javaRDD.rdd()).\n3. Sean R. Owen: Am I right in thinking that if you want to save a JavaPairRDD to HDFS, you have key-value pairs, and so you want to use JavaPairRDD.saveAsNewAPIHadoopFile, and SparkContext.sequenceFile to read it? This works. objectFile doesn't seem like the right approach anyway.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "e9b50115f4cde7a6fe3d62debf18f2d8", "issue_key": "SPARK-1075", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Fix simple doc typo in Spark Streaming Custom Receiver", "description": "The closing parentheses in the constructor in the first code block example is reversed: diff --git a/docs/streaming-custom-receivers.md b/docs/streaming-custom-receivers.md index 4e27d65..3fb540c 100644 --- a/docs/streaming-custom-receivers.md +++ b/docs/streaming-custom-receivers.md @@ -14,7 +14,7 @@ This starts with implementing [NetworkReceiver](api/streaming/index.html#org.apa The following is a simple socket text-stream receiver. {% highlight scala %} - class SocketTextStreamReceiver(host: String, port: Int( + class SocketTextStreamReceiver(host: String, port: Int) extends NetworkReceiver[String] { protected lazy val blocksGenerator: BlockGenerator =", "reporter": "Henry Saputra", "assignee": "Henry Saputra", "created": "2014-02-10T11:56:47.000+0000", "updated": "2014-02-11T15:30:40.000+0000", "resolved": "2014-02-11T15:30:40.000+0000", "labels": ["doc"], "components": ["Documentation"], "comments": [{"author": "Henry Saputra", "body": "PR: https://github.com/apache/incubator-spark/pull/577", "created": "2014-02-10T11:59:13.152+0000"}, {"author": "Henry Saputra", "body": "PR from https://github.com/apache/incubator-spark/pull/577 merged to ASF git repo.", "created": "2014-02-11T15:30:40.280+0000"}], "num_comments": 2, "text": "Issue: SPARK-1075\nSummary: Fix simple doc typo in Spark Streaming Custom Receiver\nDescription: The closing parentheses in the constructor in the first code block example is reversed: diff --git a/docs/streaming-custom-receivers.md b/docs/streaming-custom-receivers.md index 4e27d65..3fb540c 100644 --- a/docs/streaming-custom-receivers.md +++ b/docs/streaming-custom-receivers.md @@ -14,7 +14,7 @@ This starts with implementing [NetworkReceiver](api/streaming/index.html#org.apa The following is a simple socket text-stream receiver. {% highlight scala %} - class SocketTextStreamReceiver(host: String, port: Int( + class SocketTextStreamReceiver(host: String, port: Int) extends NetworkReceiver[String] { protected lazy val blocksGenerator: BlockGenerator =\n\nComments (2):\n1. Henry Saputra: PR: https://github.com/apache/incubator-spark/pull/577\n2. Henry Saputra: PR from https://github.com/apache/incubator-spark/pull/577 merged to ASF git repo.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "1087b42f34e3785c5e45e87f62da5c98", "issue_key": "SPARK-1076", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Adding zipWithIndex and zipWithUniqueId to RDD", "description": "Assign ranks to an ordered or unordered data set is a common operation. This could be done by first counting records in each partition and then assign ranks in parallel. The purpose of assigning ranks to an unordered set is usually to get a unique id for each item, e.g., to map feature names to feature indices. In such cases, the assignment could be done without counting records, saving one spark job.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-02-10T15:28:36.000+0000", "updated": "2014-02-21T11:02:21.000+0000", "resolved": "2014-02-21T11:02:21.000+0000", "labels": [], "components": [], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/incubator-spark/pull/578", "created": "2014-02-11T23:33:44.308+0000"}], "num_comments": 1, "text": "Issue: SPARK-1076\nSummary: Adding zipWithIndex and zipWithUniqueId to RDD\nDescription: Assign ranks to an ordered or unordered data set is a common operation. This could be done by first counting records in each partition and then assign ranks in parallel. The purpose of assigning ranks to an unordered set is usually to get a unique id for each item, e.g., to map feature names to feature indices. In such cases, the assignment could be done without counting records, saving one spark job.\n\nComments (1):\n1. Xiangrui Meng: PR: https://github.com/apache/incubator-spark/pull/578", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "cda39cd6e0c13637fbde87ff51fe8ba1", "issue_key": "SPARK-1225", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "ROC AUC and Average Precision for Binary classification models", "description": "Implementation of Receiver Operator Characteristic area under the curve and Average Precision performance metrics for binary classification tasks. Currently works for - Logistic regression - SVM classification Pull request: https://github.com/apache/spark/pull/160", "reporter": "Sven Schmit", "assignee": "Sven Schmit", "created": "2014-02-10T18:18:32.000+0000", "updated": "2014-04-11T19:08:13.000+0000", "resolved": "2014-04-11T19:07:47.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Included in https://github.com/apache/spark/pull/364", "created": "2014-04-11T19:08:13.266+0000"}], "num_comments": 1, "text": "Issue: SPARK-1225\nSummary: ROC AUC and Average Precision for Binary classification models\nDescription: Implementation of Receiver Operator Characteristic area under the curve and Average Precision performance metrics for binary classification tasks. Currently works for - Logistic regression - SVM classification Pull request: https://github.com/apache/spark/pull/160\n\nComments (1):\n1. Matei Alexandru Zaharia: Included in https://github.com/apache/spark/pull/364", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "32a329bb24a4016b50265bb38f6d8b18", "issue_key": "SPARK-1077", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Highlighted codes are not displayed properly in docs", "description": "In the documentation and programming guides, the highlighted codes are not displayed properly. Something wrong with Jekyll's syntax highlighter? See here: https://github.com/jyotiska/incubator-spark/blob/master/docs/scala-programming-guide.md#initializing-spark", "reporter": "Jyotiska NK", "assignee": null, "created": "2014-02-10T22:46:36.000+0000", "updated": "2014-02-10T23:29:39.000+0000", "resolved": "2014-02-10T23:29:39.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Josh Rosen", "body": "[The docs use Pygments for syntax highlighting|https://github.com/apache/incubator-spark/tree/master/docs#pygments], not Github-flavored markdown, so they don't display properly in GitHub. You can follow the instructions in the {{docs}} folder to compile the docs yourself, or you can view the compiled docs on the Spark website: https://spark.incubator.apache.org/docs/latest/scala-programming-guide.html#initializing-spark", "created": "2014-02-10T23:00:29.517+0000"}, {"author": "Jyotiska NK", "body": "Cool! I will be contributing to the existing Python programming guide, which is not much detailed compared to Scala guide. If I submit PR for the Github's docs and programming guides, will they be automatically merged with official docs at (https://spark.incubator.apache.org/docs/latest/python-programming-guide.html)?", "created": "2014-02-10T23:12:28.382+0000"}, {"author": "Josh Rosen", "body": "No, they won't be automatically merged into the Spark website. The official docs reflect released versions of Spark, not snapshot builds. We might be able to modify the Jenkins master branch build to automatically publish snapshot version of the docs and Spark JARs.", "created": "2014-02-10T23:24:52.366+0000"}, {"author": "Reynold Xin", "body": "Actually that's a pretty good idea (to publish snapshot documentation). I am not sure how easy it is to setup though.", "created": "2014-02-10T23:29:27.354+0000"}, {"author": "Reynold Xin", "body": "This is expected.", "created": "2014-02-10T23:29:39.871+0000"}], "num_comments": 5, "text": "Issue: SPARK-1077\nSummary: Highlighted codes are not displayed properly in docs\nDescription: In the documentation and programming guides, the highlighted codes are not displayed properly. Something wrong with Jekyll's syntax highlighter? See here: https://github.com/jyotiska/incubator-spark/blob/master/docs/scala-programming-guide.md#initializing-spark\n\nComments (5):\n1. Josh Rosen: [The docs use Pygments for syntax highlighting|https://github.com/apache/incubator-spark/tree/master/docs#pygments], not Github-flavored markdown, so they don't display properly in GitHub. You can follow the instructions in the {{docs}} folder to compile the docs yourself, or you can view the compiled docs on the Spark website: https://spark.incubator.apache.org/docs/latest/scala-programming-guide.html#initializing-spark\n2. Jyotiska NK: Cool! I will be contributing to the existing Python programming guide, which is not much detailed compared to Scala guide. If I submit PR for the Github's docs and programming guides, will they be automatically merged with official docs at (https://spark.incubator.apache.org/docs/latest/python-programming-guide.html)?\n3. Josh Rosen: No, they won't be automatically merged into the Spark website. The official docs reflect released versions of Spark, not snapshot builds. We might be able to modify the Jenkins master branch build to automatically publish snapshot version of the docs and Spark JARs.\n4. Reynold Xin: Actually that's a pretty good idea (to publish snapshot documentation). I am not sure how easy it is to setup though.\n5. Reynold Xin: This is expected.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "b472427883abff07339a39fd15e84333", "issue_key": "SPARK-1078", "issue_type": "Task", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Replace lift-json with json4s-jackson", "description": "json4s-jackson is a Jackson-backed implementation of the Json4s common JSON API for Scala JSON libraries. (Evan Chan has a nice comparison of Scala JSON libraries here: http://engineering.ooyala.com/blog/comparing-scala-json-libraries) It is Apache-licensed, mostly API-compatible with lift-json, and easier for downstream operating system distributions to consume than lift-json. In terms of performance, json4s-jackson is slightly slower but comparable to lift-json on my machine when parsing very small JSON files (< 2kb and < ~30 objects), around 40% faster than lift-json on medium-sized files (~50kb), and significantly (~10x) faster on multi-megabyte files.", "reporter": "William Benton", "assignee": null, "created": "2014-02-11T12:03:45.000+0000", "updated": "2014-09-04T17:54:05.000+0000", "resolved": "2014-09-04T17:54:05.000+0000", "labels": [], "components": ["Deploy", "Web UI"], "comments": [{"author": "William Benton", "body": "(Here's the PR: https://github.com/apache/incubator-spark/pull/582 )", "created": "2014-02-12T09:05:08.122+0000"}, {"author": "Josh Rosen", "body": "It looks like this was fixed in SPARK-1132 / Spark 1.0.0, where we migrated to json4s.jackson.", "created": "2014-09-04T17:54:05.120+0000"}], "num_comments": 2, "text": "Issue: SPARK-1078\nSummary: Replace lift-json with json4s-jackson\nDescription: json4s-jackson is a Jackson-backed implementation of the Json4s common JSON API for Scala JSON libraries. (Evan Chan has a nice comparison of Scala JSON libraries here: http://engineering.ooyala.com/blog/comparing-scala-json-libraries) It is Apache-licensed, mostly API-compatible with lift-json, and easier for downstream operating system distributions to consume than lift-json. In terms of performance, json4s-jackson is slightly slower but comparable to lift-json on my machine when parsing very small JSON files (< 2kb and < ~30 objects), around 40% faster than lift-json on medium-sized files (~50kb), and significantly (~10x) faster on multi-megabyte files.\n\nComments (2):\n1. William Benton: (Here's the PR: https://github.com/apache/incubator-spark/pull/582 )\n2. Josh Rosen: It looks like this was fixed in SPARK-1132 / Spark 1.0.0, where we migrated to json4s.jackson.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "3704f6ad133e64714d573656480ec1aa", "issue_key": "SPARK-1079", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "EC2 scripts should allow mounting as XFS or EXT4", "description": "These offer much better performance when running benchmarks: I've done a hacked together implementation here, but it would be better if you could officially give a filesystem as an argument in the ec2 scripts: https://github.com/pwendell/spark-ec2/blob/c63995ce014df61ec1c61276687767e789eb79f7/setup-slave.sh#L21", "reporter": "Patrick Wendell", "assignee": null, "created": "2014-02-11T20:05:10.000+0000", "updated": "2016-01-27T10:10:39.000+0000", "resolved": "2016-01-27T10:10:39.000+0000", "labels": [], "components": ["EC2"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1079\nSummary: EC2 scripts should allow mounting as XFS or EXT4\nDescription: These offer much better performance when running benchmarks: I've done a hacked together implementation here, but it would be better if you could officially give a filesystem as an argument in the ec2 scripts: https://github.com/pwendell/spark-ec2/blob/c63995ce014df61ec1c61276687767e789eb79f7/setup-slave.sh#L21", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "a21f876ba758c7e54e1440c4b52f1a11", "issue_key": "SPARK-1080", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "ZK PersistenceEngine does not respect zookeeper dir", "description": "ZooKeeperPersistenceEngine is hard-coded to deserialize files from /spark/master_status, so the PersistenceEngine would fail to recover if the user specifies a custom spark.deploy.zookeeper.dir.", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2014-02-11T22:25:04.000+0000", "updated": "2014-02-11T23:25:54.000+0000", "resolved": "2014-02-11T22:51:43.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Mark Hamstra", "body": "Not strictly limited to this issue and related pull request, but something that I've been wondering about wrt ZooKeeperPersistenceEngine that maybe someone can clarify for me: Why is this code using the notoriously difficult ZooKeeper API directly instead of taking advantage of Curator?", "created": "2014-02-11T22:37:01.088+0000"}, {"author": "Aaron Davidson", "body": "https://github.com/apache/incubator-spark/pull/583 Thanks Raymond Liu!", "created": "2014-02-11T22:51:43.545+0000"}, {"author": "Aaron Davidson", "body": "@[~markhamstra] Mostly because I didn't know about Curator until 5 minutes ago! A rewrite would not be unwelcome if it's simpler and more likely to work correctly. Such a rewrite could almost certainly allow us to remove [SparkZooKeeperSession|https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/SparkZooKeeperSession.scala]. A new JIRA should probably be opened for that, though.", "created": "2014-02-11T22:56:00.752+0000"}, {"author": "Mark Hamstra", "body": "Okay, that answers that. Yes, a rewrite to use Curator does belong in a separate JIRA.", "created": "2014-02-11T23:01:59.213+0000"}, {"author": "Aaron Davidson", "body": "Created SPARK-1082 to track this.", "created": "2014-02-11T23:25:54.380+0000"}], "num_comments": 5, "text": "Issue: SPARK-1080\nSummary: ZK PersistenceEngine does not respect zookeeper dir\nDescription: ZooKeeperPersistenceEngine is hard-coded to deserialize files from /spark/master_status, so the PersistenceEngine would fail to recover if the user specifies a custom spark.deploy.zookeeper.dir.\n\nComments (5):\n1. Mark Hamstra: Not strictly limited to this issue and related pull request, but something that I've been wondering about wrt ZooKeeperPersistenceEngine that maybe someone can clarify for me: Why is this code using the notoriously difficult ZooKeeper API directly instead of taking advantage of Curator?\n2. Aaron Davidson: https://github.com/apache/incubator-spark/pull/583 Thanks Raymond Liu!\n3. Aaron Davidson: @[~markhamstra] Mostly because I didn't know about Curator until 5 minutes ago! A rewrite would not be unwelcome if it's simpler and more likely to work correctly. Such a rewrite could almost certainly allow us to remove [SparkZooKeeperSession|https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/SparkZooKeeperSession.scala]. A new JIRA should probably be opened for that, though.\n4. Mark Hamstra: Okay, that answers that. Yes, a rewrite to use Curator does belong in a separate JIRA.\n5. Aaron Davidson: Created SPARK-1082 to track this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "ab8b4a93d597dca1ef5caebe0731c3f0", "issue_key": "SPARK-1081", "issue_type": "Sub-task", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Annotate developer and experimental API's [Core]", "description": "We should annotate API's that are internal despite being java/scala private. An example is the internal listener interface. The main issue is figuring out the nicest way we can do this in scala and decide how we document it in the scala docs.", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "created": "2014-02-11T23:21:45.000+0000", "updated": "2014-04-09T08:15:45.000+0000", "resolved": "2014-04-09T08:15:45.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Michael Armbrust", "body": "For denoting experimental APIs in the doc, I suggest we follow the convention set by TypeSafe. They just added the following HTML to the top of the scala doc comment. This fragment inserts a small red \"Experimental\" badge at the top of the page. [Example|http://www.scala-lang.org/api/2.10.2/index.html#scala.reflect.api.Universe]. We can easily make this \"Unstable\" instead of \"Experimental\" based on how we decide to name things. {{<span class=\"badge badge-red\" style=\"float: right;\">EXPERIMENTAL</span>}} I think there are three ways we can attempt to add this information to the source code. h2. Annotation One option is to create an annotation that marks an API as unstable. Pros: Simple, other projects have done this. Cons: As far as I can tell there is no way to get the compiler to enforce this. We could maybe enforce it with scala macros in the future, but even I am not sure if that is a good idea. h2. Package Visibility Another option is to mark all internal components as {{protected[spark]}}. We would then specify that people who wish to extend spark's core functionality must place their code in {{org.apache.spark.contrib}}. This is close to what projects like Shark do already to modify internal APIs. Pros: Clearly denotes close coupling with spark. Cons: As far as I know there is no java equivalent of package visibility. We are also forcing people to put things in the spark namespace. h2. Implicits We could create an implicit object that when in scope denotes close coupling with Spark's internal APIs. Even nicer, when not present, we can have the compiler provide nice messages to the user explaining the problem. See the following REPL session for an example:  scala> import scala.annotation.implicitNotFound import scala.annotation.implicitNotFound scala> @implicitNotFound(\"You are attemping to use a SparkInternal API, which may be subject to change in future versions. If you are sure you want to couple your code closely with this internal API then add 'import org.apache.spark.SparkUnstableApis' to this file.\") trait SparkUnstableApi defined trait SparkUnstableApi scala> class SparkInternalClass(p1: Int)(implicit u: SparkUnstableApi) defined class SparkInternalClass scala> new SparkInternalClass(1) <console>:11: error: You are attemping to use a SparkInternal API, which may be subject to change in future versions. If you are sure you want to couple your code closely with this internal API then add 'import org.apache.spark.SparkUnstableApis' to this file. new SparkInternalClass(1) ^ scala> implicit object SparkUnstableApis extends SparkUnstableApi defined module SparkUnstableApis scala> new SparkInternalClass(1) res1: SparkInternalClass = SparkInternalClass@7aae3ed7  This is pretty close to what scala itself is already doing with imports that are now required for using advanced language features. Pros: Compiler enforced, nice error messages. Scala magic. Cons: Scala magic.", "created": "2014-02-24T15:06:41.163+0000"}, {"author": "Reynold Xin", "body": "I don't think 2 is an option (package visibility) because it forces user level code to be in a specific package. For 3, how does it work with Java?", "created": "2014-02-24T15:30:29.254+0000"}, {"author": "Michael Armbrust", "body": "Well, this isn't really \"user level code\", it is code that modifying the spark runtime by touching internal APIs. Requiring people to put such code in a package called contrib seems reasonable to me. If these are APIs that we intend user code to touch then I would argue we should provide stability and thus this is a non-issue. The second parameter list is just sugar on top of a combined parameter list, so java programmers would explicitly have to pass in a value here. Depending on what we do inside this could just be null, or could be an object of the correct type. So I think the answer is it would be usable but ugly (kinda like java :P).", "created": "2014-02-24T15:39:06.734+0000"}, {"author": "Reynold Xin", "body": "What about just adding a new RDD implementation? That doesn't modify Spark core at all.", "created": "2014-02-24T15:42:18.636+0000"}, {"author": "Michael Armbrust", "body": "There is now [an example of marking APIs as experimental|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SchemaRDD.scala] in the Spark SQL source code. This is based on the way that the scala compiler marks their APIs as experimental.", "created": "2014-03-21T15:47:52.214+0000"}], "num_comments": 5, "text": "Issue: SPARK-1081\nSummary: Annotate developer and experimental API's [Core]\nDescription: We should annotate API's that are internal despite being java/scala private. An example is the internal listener interface. The main issue is figuring out the nicest way we can do this in scala and decide how we document it in the scala docs.\n\nComments (5):\n1. Michael Armbrust: For denoting experimental APIs in the doc, I suggest we follow the convention set by TypeSafe. They just added the following HTML to the top of the scala doc comment. This fragment inserts a small red \"Experimental\" badge at the top of the page. [Example|http://www.scala-lang.org/api/2.10.2/index.html#scala.reflect.api.Universe]. We can easily make this \"Unstable\" instead of \"Experimental\" based on how we decide to name things. {{<span class=\"badge badge-red\" style=\"float: right;\">EXPERIMENTAL</span>}} I think there are three ways we can attempt to add this information to the source code. h2. Annotation One option is to create an annotation that marks an API as unstable. Pros: Simple, other projects have done this. Cons: As far as I can tell there is no way to get the compiler to enforce this. We could maybe enforce it with scala macros in the future, but even I am not sure if that is a good idea. h2. Package Visibility Another option is to mark all internal components as {{protected[spark]}}. We would then specify that people who wish to extend spark's core functionality must place their code in {{org.apache.spark.contrib}}. This is close to what projects like Shark do already to modify internal APIs. Pros: Clearly denotes close coupling with spark. Cons: As far as I know there is no java equivalent of package visibility. We are also forcing people to put things in the spark namespace. h2. Implicits We could create an implicit object that when in scope denotes close coupling with Spark's internal APIs. Even nicer, when not present, we can have the compiler provide nice messages to the user explaining the problem. See the following REPL session for an example:  scala> import scala.annotation.implicitNotFound import scala.annotation.implicitNotFound scala> @implicitNotFound(\"You are attemping to use a SparkInternal API, which may be subject to change in future versions. If you are sure you want to couple your code closely with this internal API then add 'import org.apache.spark.SparkUnstableApis' to this file.\") trait SparkUnstableApi defined trait SparkUnstableApi scala> class SparkInternalClass(p1: Int)(implicit u: SparkUnstableApi) defined class SparkInternalClass scala> new SparkInternalClass(1) <console>:11: error: You are attemping to use a SparkInternal API, which may be subject to change in future versions. If you are sure you want to couple your code closely with this internal API then add 'import org.apache.spark.SparkUnstableApis' to this file. new SparkInternalClass(1) ^ scala> implicit object SparkUnstableApis extends SparkUnstableApi defined module SparkUnstableApis scala> new SparkInternalClass(1) res1: SparkInternalClass = SparkInternalClass@7aae3ed7  This is pretty close to what scala itself is already doing with imports that are now required for using advanced language features. Pros: Compiler enforced, nice error messages. Scala magic. Cons: Scala magic.\n2. Reynold Xin: I don't think 2 is an option (package visibility) because it forces user level code to be in a specific package. For 3, how does it work with Java?\n3. Michael Armbrust: Well, this isn't really \"user level code\", it is code that modifying the spark runtime by touching internal APIs. Requiring people to put such code in a package called contrib seems reasonable to me. If these are APIs that we intend user code to touch then I would argue we should provide stability and thus this is a non-issue. The second parameter list is just sugar on top of a combined parameter list, so java programmers would explicitly have to pass in a value here. Depending on what we do inside this could just be null, or could be an object of the correct type. So I think the answer is it would be usable but ugly (kinda like java :P).\n4. Reynold Xin: What about just adding a new RDD implementation? That doesn't modify Spark core at all.\n5. Michael Armbrust: There is now [an example of marking APIs as experimental|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SchemaRDD.scala] in the Spark SQL source code. This is based on the way that the scala compiler marks their APIs as experimental.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "bfe42ebe01f44ad72a98abf27a5b2082", "issue_key": "SPARK-1082", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Use Curator for ZK interaction in standalone cluster", "description": "ZooKeeper's API is a veritable minefield of pitfalls. Apache Curator (http://curator.apache.org/) provides a cleaner and higher level API that helps avoid these problems. We should be able to replace almost all of [ZooKeeperLeaderElectionAgent|https://github.com/apache/incubator-spark/blob/e2c68642c64345434e2034082cf9b299491e9e9f/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala] and [SparkZooKeeperSession|https://github.com/apache/incubator-spark/blob/e2c68642c64345434e2034082cf9b299491e9e9f/core/src/main/scala/org/apache/spark/deploy/master/SparkZooKeeperSession.scala] with Curator equivalents.", "reporter": "Aaron Davidson", "assignee": null, "created": "2014-02-11T23:22:46.000+0000", "updated": "2015-07-20T12:54:24.000+0000", "resolved": "2015-07-20T12:54:23.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Raymond Valencia", "body": "@Aaron Are you going to take this? Or, if not in hurry, I can take this one", "created": "2014-02-12T16:35:23.215+0000"}, {"author": "Aaron Davidson", "body": "Feel free to take it on!", "created": "2014-02-12T16:39:21.066+0000"}, {"author": "Raymond Valencia", "body": "@Aaron : https://github.com/apache/incubator-spark/pull/611", "created": "2014-02-17T23:20:40.137+0000"}, {"author": "Sean R. Owen", "body": "I'm guessing this long since timed out", "created": "2015-07-20T12:54:24.019+0000"}], "num_comments": 4, "text": "Issue: SPARK-1082\nSummary: Use Curator for ZK interaction in standalone cluster\nDescription: ZooKeeper's API is a veritable minefield of pitfalls. Apache Curator (http://curator.apache.org/) provides a cleaner and higher level API that helps avoid these problems. We should be able to replace almost all of [ZooKeeperLeaderElectionAgent|https://github.com/apache/incubator-spark/blob/e2c68642c64345434e2034082cf9b299491e9e9f/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala] and [SparkZooKeeperSession|https://github.com/apache/incubator-spark/blob/e2c68642c64345434e2034082cf9b299491e9e9f/core/src/main/scala/org/apache/spark/deploy/master/SparkZooKeeperSession.scala] with Curator equivalents.\n\nComments (4):\n1. Raymond Valencia: @Aaron Are you going to take this? Or, if not in hurry, I can take this one\n2. Aaron Davidson: Feel free to take it on!\n3. Raymond Valencia: @Aaron : https://github.com/apache/incubator-spark/pull/611\n4. Sean R. Owen: I'm guessing this long since timed out", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "66ae860efcb7c7469e4a3049cf66d402", "issue_key": "SPARK-1083", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Build fail", "description": "Problem with building the latest version from github. [info] Loading project definition from C:\\Users\\Jan\\Documents\\GitHub\\incubator-s park\\project\\project [debug] [debug] Initial source changes: [debug] removed:Set() [debug] added: Set() [debug] modified: Set() [debug] Removed products: Set() [debug] Modified external sources: Set() [debug] Modified binary dependencies: Set() [debug] Initial directly invalidated sources: Set() [debug] [debug] Sources indirectly invalidated by: [debug] product: Set() [debug] binary dep: Set() [debug] external source: Set() [debug] All initially invalidated sources: Set() [debug] Copy resource mappings: [debug] java.lang.RuntimeException: Nonzero exit code (128): git clone https://github.co m/chenkelmann/junit_xml_listener.git C:\\Users\\Jan\\.sbt\\0.13\\staging\\5f76b43a3aca 87b5c013\\junit_xml_listener at scala.sys.package$.error(package.scala:27) at sbt.Resolvers$.run(Resolvers.scala:134) at sbt.Resolvers$.run(Resolvers.scala:123) at sbt.Resolvers$$anon$2.clone(Resolvers.scala:78) at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11$ $anonfun$apply$5.apply$mcV$sp(Resolvers.scala:104) at sbt.Resolvers$.creates(Resolvers.scala:141) at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11. apply(Resolvers.scala:103) at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11. apply(Resolvers.scala:103) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$3.apply(Bui ldLoader.scala:90) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$3.apply(Bui ldLoader.scala:89) at scala.Option.map(Option.scala:145) at sbt.BuildLoader$$anonfun$componentLoader$1.apply(BuildLoader.scala:89 ) at sbt.BuildLoader$$anonfun$componentLoader$1.apply(BuildLoader.scala:85 ) at sbt.MultiHandler.apply(BuildLoader.scala:16) at sbt.BuildLoader.apply(BuildLoader.scala:142) at sbt.Load$.loadAll(Load.scala:314) at sbt.Load$.loadURI(Load.scala:266) at sbt.Load$.load(Load.scala:262) at sbt.Load$.load(Load.scala:253) at sbt.Load$.apply(Load.scala:137) at sbt.Load$.buildPluginDefinition(Load.scala:597) at sbt.Load$.buildPlugins(Load.scala:563) at sbt.Load$.plugins(Load.scala:551) at sbt.Load$.loadUnit(Load.scala:412) at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:258) at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:258) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$ apply$5$$anonfun$apply$6.apply(BuildLoader.scala:93) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$ apply$5$$anonfun$apply$6.apply(BuildLoader.scala:92) at sbt.BuildLoader.apply(BuildLoader.scala:143) at sbt.Load$.loadAll(Load.scala:314) at sbt.Load$.loadURI(Load.scala:266) at sbt.Load$.load(Load.scala:262) at sbt.Load$.load(Load.scala:253) at sbt.Load$.apply(Load.scala:137) at sbt.Load$.defaultLoad(Load.scala:40) at sbt.BuiltinCommands$.doLoadProject(Main.scala:451) at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:445) at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:445) at sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.sca la:60) at sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.sca la:60) at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.sca la:62) at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.sca la:62) at sbt.Command$.process(Command.scala:95) at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100) at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100) at sbt.State$$anon$1.process(State.scala:179) at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100) at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100) at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18) at sbt.MainLoop$.next(MainLoop.scala:100) at sbt.MainLoop$.run(MainLoop.scala:93) at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71) at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66) at sbt.Using.apply(Using.scala:25) at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66) at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49) at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33) at sbt.MainLoop$.runLogged(MainLoop.scala:25) at sbt.StandardMain$.runManaged(Main.scala:57) at sbt.xMain.run(Main.scala:29) at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:57) at xsbt.boot.Launch$.withContextLoader(Launch.scala:77) at xsbt.boot.Launch$.run(Launch.scala:57) at xsbt.boot.Launch$$anonfun$explicit$1.apply(Launch.scala:45) at xsbt.boot.Launch$.launch(Launch.scala:65) at xsbt.boot.Launch$.apply(Launch.scala:16) at xsbt.boot.Boot$.runImpl(Boot.scala:32) at xsbt.boot.Boot$.main(Boot.scala:21) at xsbt.boot.Boot.main(Boot.scala) [error] Nonzero exit code (128): git clone https://github.com/chenkelmann/junit_ xml_listener.git C:\\Users\\Jan\\.sbt\\0.13\\staging\\5f76b43a3aca87b5c013\\junit_xml_l istener [error] Use 'last' for the full log. [debug] > load-failed [debug] > last  sbt version  [info] This is sbt 0.13.1 [info] No project is currently loaded [info] sbt, sbt plugins, and build definitions are using Scala 2.10.3  scala version  C:\\Users\\Jan\\Documents\\GitHub\\incubator-spark>scala -version Scala code runner version 2.10.2 -- Copyright 2002-2013, LAMP/EPFL  java version  C:\\Users\\Jan\\Documents\\GitHub\\incubator-spark>java -version java version \"1.7.0_07\" Java(TM) SE Runtime Environment (build 1.7.0_07-b11) Java HotSpot(TM) 64-Bit Server VM (build 23.3-b01, mixed mode)", "reporter": "Jan Paw", "assignee": null, "created": "2014-02-12T03:18:09.000+0000", "updated": "2014-10-13T18:08:13.000+0000", "resolved": "2014-10-13T18:08:13.000+0000", "labels": [], "components": ["Build", "Windows"], "comments": [{"author": "Sean R. Owen", "body": "This looks like a git error, and is ancient at this point. I presume that since we have evidence that Windows builds subsequently worked, this was either a local problem or fixed by something else.", "created": "2014-10-13T18:08:13.606+0000"}], "num_comments": 1, "text": "Issue: SPARK-1083\nSummary: Build fail\nDescription: Problem with building the latest version from github. [info] Loading project definition from C:\\Users\\Jan\\Documents\\GitHub\\incubator-s park\\project\\project [debug] [debug] Initial source changes: [debug] removed:Set() [debug] added: Set() [debug] modified: Set() [debug] Removed products: Set() [debug] Modified external sources: Set() [debug] Modified binary dependencies: Set() [debug] Initial directly invalidated sources: Set() [debug] [debug] Sources indirectly invalidated by: [debug] product: Set() [debug] binary dep: Set() [debug] external source: Set() [debug] All initially invalidated sources: Set() [debug] Copy resource mappings: [debug] java.lang.RuntimeException: Nonzero exit code (128): git clone https://github.co m/chenkelmann/junit_xml_listener.git C:\\Users\\Jan\\.sbt\\0.13\\staging\\5f76b43a3aca 87b5c013\\junit_xml_listener at scala.sys.package$.error(package.scala:27) at sbt.Resolvers$.run(Resolvers.scala:134) at sbt.Resolvers$.run(Resolvers.scala:123) at sbt.Resolvers$$anon$2.clone(Resolvers.scala:78) at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11$ $anonfun$apply$5.apply$mcV$sp(Resolvers.scala:104) at sbt.Resolvers$.creates(Resolvers.scala:141) at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11. apply(Resolvers.scala:103) at sbt.Resolvers$DistributedVCS$$anonfun$toResolver$1$$anonfun$apply$11. apply(Resolvers.scala:103) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$3.apply(Bui ldLoader.scala:90) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$3.apply(Bui ldLoader.scala:89) at scala.Option.map(Option.scala:145) at sbt.BuildLoader$$anonfun$componentLoader$1.apply(BuildLoader.scala:89 ) at sbt.BuildLoader$$anonfun$componentLoader$1.apply(BuildLoader.scala:85 ) at sbt.MultiHandler.apply(BuildLoader.scala:16) at sbt.BuildLoader.apply(BuildLoader.scala:142) at sbt.Load$.loadAll(Load.scala:314) at sbt.Load$.loadURI(Load.scala:266) at sbt.Load$.load(Load.scala:262) at sbt.Load$.load(Load.scala:253) at sbt.Load$.apply(Load.scala:137) at sbt.Load$.buildPluginDefinition(Load.scala:597) at sbt.Load$.buildPlugins(Load.scala:563) at sbt.Load$.plugins(Load.scala:551) at sbt.Load$.loadUnit(Load.scala:412) at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:258) at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:258) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$ apply$5$$anonfun$apply$6.apply(BuildLoader.scala:93) at sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$ apply$5$$anonfun$apply$6.apply(BuildLoader.scala:92) at sbt.BuildLoader.apply(BuildLoader.scala:143) at sbt.Load$.loadAll(Load.scala:314) at sbt.Load$.loadURI(Load.scala:266) at sbt.Load$.load(Load.scala:262) at sbt.Load$.load(Load.scala:253) at sbt.Load$.apply(Load.scala:137) at sbt.Load$.defaultLoad(Load.scala:40) at sbt.BuiltinCommands$.doLoadProject(Main.scala:451) at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:445) at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:445) at sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.sca la:60) at sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.sca la:60) at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.sca la:62) at sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.sca la:62) at sbt.Command$.process(Command.scala:95) at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100) at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100) at sbt.State$$anon$1.process(State.scala:179) at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100) at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100) at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18) at sbt.MainLoop$.next(MainLoop.scala:100) at sbt.MainLoop$.run(MainLoop.scala:93) at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71) at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66) at sbt.Using.apply(Using.scala:25) at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66) at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49) at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33) at sbt.MainLoop$.runLogged(MainLoop.scala:25) at sbt.StandardMain$.runManaged(Main.scala:57) at sbt.xMain.run(Main.scala:29) at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:57) at xsbt.boot.Launch$.withContextLoader(Launch.scala:77) at xsbt.boot.Launch$.run(Launch.scala:57) at xsbt.boot.Launch$$anonfun$explicit$1.apply(Launch.scala:45) at xsbt.boot.Launch$.launch(Launch.scala:65) at xsbt.boot.Launch$.apply(Launch.scala:16) at xsbt.boot.Boot$.runImpl(Boot.scala:32) at xsbt.boot.Boot$.main(Boot.scala:21) at xsbt.boot.Boot.main(Boot.scala) [error] Nonzero exit code (128): git clone https://github.com/chenkelmann/junit_ xml_listener.git C:\\Users\\Jan\\.sbt\\0.13\\staging\\5f76b43a3aca87b5c013\\junit_xml_l istener [error] Use 'last' for the full log. [debug] > load-failed [debug] > last  sbt version  [info] This is sbt 0.13.1 [info] No project is currently loaded [info] sbt, sbt plugins, and build definitions are using Scala 2.10.3  scala version  C:\\Users\\Jan\\Documents\\GitHub\\incubator-spark>scala -version Scala code runner version 2.10.2 -- Copyright 2002-2013, LAMP/EPFL  java version  C:\\Users\\Jan\\Documents\\GitHub\\incubator-spark>java -version java version \"1.7.0_07\" Java(TM) SE Runtime Environment (build 1.7.0_07-b11) Java HotSpot(TM) 64-Bit Server VM (build 23.3-b01, mixed mode)\n\nComments (1):\n1. Sean R. Owen: This looks like a git error, and is ancient at this point. I presume that since we have evidence that Windows builds subsequently worked, this was either a local problem or fixed by something else.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "9df2e8e35f7b08af737b0296433fbe30", "issue_key": "SPARK-1084", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Fix most build warnings", "description": "I hope another boring tidy-up JIRA might be welcome. I'd like to fix most of the warnings that appear during build, so that developers don't become accustomed to them. The accompanying pull request contains a number of commits to quash most warnings observed through the mvn and sbt builds, although not all of them. FIXED! [WARNING] Parameter tasks is deprecated, use target instead Just a matter of updating <tasks> -> <target> in inline Ant scripts. WARNING: -p has been deprecated and will be reused for a different (but still very cool) purpose in ScalaTest 2.0. Please change all uses of -p to -R. Goes away with updating scalatest plugin -> 1.0-RC2 [WARNING] Note: /Users/srowen/Documents/incubator-spark/core/src/test/scala/org/apache/spark/JavaAPISuite.java uses unchecked or unsafe operations. [WARNING] Note: Recompile with -Xlint:unchecked for details. Mostly @SuppressWarnings(\"unchecked\") but needed a few more things to reveal the warning source: <fork>true</fork> (also needd for <maxmem>) and version 3.1 of the plugin. In a few cases some declaration changes were appropriate to avoid warnings. /Users/srowen/Documents/incubator-spark/core/src/main/scala/org/apache/spark/util/IndestructibleActorSystem.scala:25: warning: Could not find any member to link for \"akka.actor.ActorSystem\". /** ^ Getting several scaladoc errors like this and I'm not clear why it can't find the type -- outside its module? Remove the links as they're evidently not linking anyway? /Users/srowen/Documents/incubator-spark/repl/src/main/scala/org/apache/spark/repl/SparkIMain.scala:86: warning: Variable eval undefined in comment for class SparkIMain in class SparkIMain $ has to be escaped as \\$ in scaladoc, apparently [WARNING] 'dependencyManagement.dependencies.dependency.exclusions.exclusion.artifactId' for org.apache.hadoop:hadoop-yarn-client:jar with value '*' does not match a valid id pattern. @ org.apache.spark:spark-parent:1.0.0-incubating-SNAPSHOT, /Users/srowen/Documents/incubator-spark/pom.xml, line 494, column 25 This one might need review. This is valid Maven syntax, but, Maven still warns on it. I wanted to see if we can do without it. These are trying to exclude: - org.codehaus.jackson - org.sonatype.sisu.inject - org.xerial.snappy org.sonatype.sisu.inject doesn't actually seem to be a dependency anyway. org.xerial.snappy is used by dependencies but the version seems to match anyway (1.0.5). org.codehaus.jackson was intended to exclude 1.8.8, since Spark streaming wants 1.9.11 directly. But the exclusion is in the wrong place if so, since Spark depends straight on Avro, which is what brings in 1.8.8, still. (hadoop-client 1.0.4 includes Jackson 1.0.1, so that needs an exclusion, but the other Hadoop modules don't.) HBase depends on 1.8.8 but figured it was intentional to leave that as it would not collide with Spark streaming. (?) (I understand this varies by Hadoop version but confirmed this is all the same for 1.0.4, 0.23.7, 2.2.0.) NOT FIXED. [warn] /Users/srowen/Documents/incubator-spark/streaming/src/test/scala/org/apache/spark/streaming/InputStreamsSuite.scala:305: method connect in class IOManager is deprecated: use the new implementation in package akka.io instead [warn] override def preStart = IOManager(context.system).connect(new InetSocketAddress(port)) Not confident enough to fix this. [WARNING] there were 6 feature warning(s); re-run with -feature for details Don't know enough Scala to address these, yet. [WARNING] We have a duplicate org/yaml/snakeyaml/scanner/ScannerImpl$Chomping.class in /Users/srowen/.m2/repository/org/yaml/snakeyaml/1.6/snakeyaml-1.6.jar Probably addressable by being more careful about how binaries are packed though this appear to be ignorable; two identical copies of the class are colliding. [WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile and [WARNING] JAR will be empty - no content was marked for inclusion! Apparently harmless warnings, but I don't know how to disable them.", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "created": "2014-02-12T05:25:54.000+0000", "updated": "2015-01-15T09:08:40.000+0000", "resolved": "2014-03-02T14:28:26.000+0000", "labels": ["mvn", "sbt", "warning"], "components": ["Build"], "comments": [{"author": "Sean Owen", "body": "There are two PRs associated with this -- per Aaron, first there's everything but dependency changes: https://github.com/apache/incubator-spark/pull/637 and then dependency-related changes https://github.com/apache/incubator-spark/pull/650", "created": "2014-02-25T02:34:28.089+0000"}], "num_comments": 1, "text": "Issue: SPARK-1084\nSummary: Fix most build warnings\nDescription: I hope another boring tidy-up JIRA might be welcome. I'd like to fix most of the warnings that appear during build, so that developers don't become accustomed to them. The accompanying pull request contains a number of commits to quash most warnings observed through the mvn and sbt builds, although not all of them. FIXED! [WARNING] Parameter tasks is deprecated, use target instead Just a matter of updating <tasks> -> <target> in inline Ant scripts. WARNING: -p has been deprecated and will be reused for a different (but still very cool) purpose in ScalaTest 2.0. Please change all uses of -p to -R. Goes away with updating scalatest plugin -> 1.0-RC2 [WARNING] Note: /Users/srowen/Documents/incubator-spark/core/src/test/scala/org/apache/spark/JavaAPISuite.java uses unchecked or unsafe operations. [WARNING] Note: Recompile with -Xlint:unchecked for details. Mostly @SuppressWarnings(\"unchecked\") but needed a few more things to reveal the warning source: <fork>true</fork> (also needd for <maxmem>) and version 3.1 of the plugin. In a few cases some declaration changes were appropriate to avoid warnings. /Users/srowen/Documents/incubator-spark/core/src/main/scala/org/apache/spark/util/IndestructibleActorSystem.scala:25: warning: Could not find any member to link for \"akka.actor.ActorSystem\". /** ^ Getting several scaladoc errors like this and I'm not clear why it can't find the type -- outside its module? Remove the links as they're evidently not linking anyway? /Users/srowen/Documents/incubator-spark/repl/src/main/scala/org/apache/spark/repl/SparkIMain.scala:86: warning: Variable eval undefined in comment for class SparkIMain in class SparkIMain $ has to be escaped as \\$ in scaladoc, apparently [WARNING] 'dependencyManagement.dependencies.dependency.exclusions.exclusion.artifactId' for org.apache.hadoop:hadoop-yarn-client:jar with value '*' does not match a valid id pattern. @ org.apache.spark:spark-parent:1.0.0-incubating-SNAPSHOT, /Users/srowen/Documents/incubator-spark/pom.xml, line 494, column 25 This one might need review. This is valid Maven syntax, but, Maven still warns on it. I wanted to see if we can do without it. These are trying to exclude: - org.codehaus.jackson - org.sonatype.sisu.inject - org.xerial.snappy org.sonatype.sisu.inject doesn't actually seem to be a dependency anyway. org.xerial.snappy is used by dependencies but the version seems to match anyway (1.0.5). org.codehaus.jackson was intended to exclude 1.8.8, since Spark streaming wants 1.9.11 directly. But the exclusion is in the wrong place if so, since Spark depends straight on Avro, which is what brings in 1.8.8, still. (hadoop-client 1.0.4 includes Jackson 1.0.1, so that needs an exclusion, but the other Hadoop modules don't.) HBase depends on 1.8.8 but figured it was intentional to leave that as it would not collide with Spark streaming. (?) (I understand this varies by Hadoop version but confirmed this is all the same for 1.0.4, 0.23.7, 2.2.0.) NOT FIXED. [warn] /Users/srowen/Documents/incubator-spark/streaming/src/test/scala/org/apache/spark/streaming/InputStreamsSuite.scala:305: method connect in class IOManager is deprecated: use the new implementation in package akka.io instead [warn] override def preStart = IOManager(context.system).connect(new InetSocketAddress(port)) Not confident enough to fix this. [WARNING] there were 6 feature warning(s); re-run with -feature for details Don't know enough Scala to address these, yet. [WARNING] We have a duplicate org/yaml/snakeyaml/scanner/ScannerImpl$Chomping.class in /Users/srowen/.m2/repository/org/yaml/snakeyaml/1.6/snakeyaml-1.6.jar Probably addressable by being more careful about how binaries are packed though this appear to be ignorable; two identical copies of the class are colliding. [WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile and [WARNING] JAR will be empty - no content was marked for inclusion! Apparently harmless warnings, but I don't know how to disable them.\n\nComments (1):\n1. Sean Owen: There are two PRs associated with this -- per Aaron, first there's everything but dependency changes: https://github.com/apache/incubator-spark/pull/637 and then dependency-related changes https://github.com/apache/incubator-spark/pull/650", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "0ee06dd3027caaa05aedc6f3707205ed", "issue_key": "SPARK-1085", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found)", "description": "", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2014-02-12T13:55:11.000+0000", "updated": "2014-02-12T20:50:52.000+0000", "resolved": "2014-02-12T20:50:52.000+0000", "labels": [], "components": [], "comments": [{"author": "Andy Konwinski", "body": "Any idea what needs to be done to make this work? E.g. will `yum install scalastyle` on all Jenkins worker nodes fix it (or even work)? On Feb 12, 2014 1:57 PM, \"Reynold Xin (JIRA)\" <", "created": "2014-02-12T14:08:09.743+0000"}, {"author": "Reynold Xin", "body": "The problem is the older versions of Spark doesn't have the scalastyle configuration in SBT. I submitted a PR to fix that. https://github.com/apache/incubator-spark/pull/590", "created": "2014-02-12T20:50:37.957+0000"}], "num_comments": 2, "text": "Issue: SPARK-1085\nSummary: Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found)\n\nComments (2):\n1. Andy Konwinski: Any idea what needs to be done to make this work? E.g. will `yum install scalastyle` on all Jenkins worker nodes fix it (or even work)? On Feb 12, 2014 1:57 PM, \"Reynold Xin (JIRA)\" <\n2. Reynold Xin: The problem is the older versions of Spark doesn't have the scalastyle configuration in SBT. I submitted a PR to fix that. https://github.com/apache/incubator-spark/pull/590", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "b701ae42004fd4062b66832d5094d0a9", "issue_key": "SPARK-1086", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Some corner case during HA master switching?", "description": "In spark standalone HA mode. It seems to me that when current master is down, and the new master is not elected yet, at this time if the worker is sending executor update or driver update message. this message will be lost. Though executor status is rebuild after new master is recovered, thus the status might be alright. While the driver might be relaunched even it actually finished successfully, this might be a problem? And then the message send to AppClientListener might be lost and never been regenerated thus lead to some problem?", "reporter": "Raymond Valencia", "assignee": null, "created": "2014-02-12T18:18:54.000+0000", "updated": "2015-10-19T23:07:03.000+0000", "resolved": "2015-03-21T15:43:40.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Sean R. Owen", "body": "I'm trying to figure out if this is still a problem, is a question, is something with more actionable info? it's over a year old so I am tempted to close this otherwise.", "created": "2015-03-07T16:36:53.251+0000"}, {"author": "Apache Spark", "body": "User 'JihongMA' has created a pull request for this issue: https://github.com/apache/spark/pull/9172", "created": "2015-10-19T23:07:03.604+0000"}], "num_comments": 2, "text": "Issue: SPARK-1086\nSummary: Some corner case during HA master switching?\nDescription: In spark standalone HA mode. It seems to me that when current master is down, and the new master is not elected yet, at this time if the worker is sending executor update or driver update message. this message will be lost. Though executor status is rebuild after new master is recovered, thus the status might be alright. While the driver might be relaunched even it actually finished successfully, this might be a problem? And then the message send to AppClientListener might be lost and never been regenerated thus lead to some problem?\n\nComments (2):\n1. Sean R. Owen: I'm trying to figure out if this is still a problem, is a question, is something with more actionable info? it's over a year old so I am tempted to close this otherwise.\n2. Apache Spark: User 'JihongMA' has created a pull request for this issue: https://github.com/apache/spark/pull/9172", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "1af517e8d42ab4523b47455a8aa8a45d", "issue_key": "SPARK-1087", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Separate file for traceback and callsite related functions", "description": "Right now, _extract_concise_traceback() is written inside rdd.py which provides the callsite information. But for [SPARK-972](https://spark-project.atlassian.net/browse/SPARK-972) in PR #581, we used the function from context.py. Also some issues were faced regarding the return string format. It would be a good idea to move the the traceback function from rdd and create a separate file for future developments.", "reporter": "Jyotiska NK", "assignee": null, "created": "2014-02-12T19:53:28.000+0000", "updated": "2014-09-16T02:28:30.000+0000", "resolved": "2014-09-16T02:28:30.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Jyotiska NK", "body": "Can one of the admins assign me to this issue and change the status to \"In Progress\"? I will submit a PR after PR #581 gets merged.", "created": "2014-02-12T19:54:46.236+0000"}, {"author": "Matthew Farrellee", "body": "[~jyotiska] PR 581 was merged (though it looked fairly trivial). is this still relevant?", "created": "2014-09-07T12:08:20.356+0000"}, {"author": "Jyotiska NK", "body": "We initially thought this would be a good feature to add going forward. But after the PR was merged, it was abandoned. Also, PR 581 was created in the incubator github repo. In the new one, it was [PR #34|https://github.com/apache/spark/pull/34]. If it is a relevant feature, I can submit a PR for this.", "created": "2014-09-07T18:32:01.121+0000"}, {"author": "Matthew Farrellee", "body": "[~jyotiska] please do!", "created": "2014-09-08T19:26:10.914+0000"}, {"author": "Apache Spark", "body": "User 'staple' has created a pull request for this issue: https://github.com/apache/spark/pull/2385", "created": "2014-09-14T05:35:27.425+0000"}, {"author": "Josh Rosen", "body": "Issue resolved by pull request 2385 [https://github.com/apache/spark/pull/2385]", "created": "2014-09-16T02:28:30.772+0000"}], "num_comments": 6, "text": "Issue: SPARK-1087\nSummary: Separate file for traceback and callsite related functions\nDescription: Right now, _extract_concise_traceback() is written inside rdd.py which provides the callsite information. But for [SPARK-972](https://spark-project.atlassian.net/browse/SPARK-972) in PR #581, we used the function from context.py. Also some issues were faced regarding the return string format. It would be a good idea to move the the traceback function from rdd and create a separate file for future developments.\n\nComments (6):\n1. Jyotiska NK: Can one of the admins assign me to this issue and change the status to \"In Progress\"? I will submit a PR after PR #581 gets merged.\n2. Matthew Farrellee: [~jyotiska] PR 581 was merged (though it looked fairly trivial). is this still relevant?\n3. Jyotiska NK: We initially thought this would be a good feature to add going forward. But after the PR was merged, it was abandoned. Also, PR 581 was created in the incubator github repo. In the new one, it was [PR #34|https://github.com/apache/spark/pull/34]. If it is a relevant feature, I can submit a PR for this.\n4. Matthew Farrellee: [~jyotiska] please do!\n5. Apache Spark: User 'staple' has created a pull request for this issue: https://github.com/apache/spark/pull/2385\n6. Josh Rosen: Issue resolved by pull request 2385 [https://github.com/apache/spark/pull/2385]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "96a8d3414c2ed765088776c189731c8e", "issue_key": "SPARK-1088", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Create a script for running tests", "description": "So we can run that in Jenkins and have version specific testing.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2014-02-12T20:51:40.000+0000", "updated": "2014-03-08T16:13:17.000+0000", "resolved": "2014-03-08T16:13:17.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "PR submitted: https://github.com/apache/incubator-spark/pull/592", "created": "2014-02-12T20:52:59.457+0000"}], "num_comments": 1, "text": "Issue: SPARK-1088\nSummary: Create a script for running tests\nDescription: So we can run that in Jenkins and have version specific testing.\n\nComments (1):\n1. Reynold Xin: PR submitted: https://github.com/apache/incubator-spark/pull/592", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "828ee11d61c4d5c2c0c338883c00ec44", "issue_key": "SPARK-1089", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "ADD_JARS regression in Spark 0.9.0", "description": "Using the ADD_JARS environment variable with spark-shell used to add the jar to both the shell and the various workers. Now it only adds to the workers and importing a custom class in the shell is broken. The workaround is to add custom jars to both ADD_JARS and SPARK_CLASSPATH. We should fix ADD_JARS so it works properly again. See various threads on the user list: https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201402.mbox/%3CCAJbo4neMLiTrnm1XbyqomWmp0m+EUcg4yE-txuRGSVKOb5KLeA@mail.gmail.com%3E (another one that doesn't appear in the archives yet titled \"ADD_JARS not working on 0.9\")", "reporter": "Andrew Ash", "assignee": "Nan Zhu", "created": "2014-02-13T14:25:55.000+0000", "updated": "2014-07-09T18:12:36.000+0000", "resolved": "2014-02-26T23:43:16.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "I'm interested in fixing this Can anyone assign it to me? Thank you!", "created": "2014-02-16T17:33:10.141+0000"}, {"author": "Nan Zhu", "body": "made a PR https://github.com/apache/incubator-spark/pull/614, the analysis on the reason of this bug is over there", "created": "2014-02-18T08:30:49.108+0000"}, {"author": "Nan Zhu", "body": "as a temporary work around, you don't need to set ADD_JARS and SPARK_CLASSPATH at the same time just SPARK_CLASSPATH is enough", "created": "2014-02-21T10:47:45.961+0000"}, {"author": "Nicholas Chammas", "body": "So going forward, is the correct procedure for adding external jars to set {{SPARK_CLASSPATH}}? Is there a doc somewhere that details this process? And what is the difference between setting these environment variables and calling {{sc.addJar()}} from within the shell?", "created": "2014-07-09T18:12:36.229+0000"}], "num_comments": 4, "text": "Issue: SPARK-1089\nSummary: ADD_JARS regression in Spark 0.9.0\nDescription: Using the ADD_JARS environment variable with spark-shell used to add the jar to both the shell and the various workers. Now it only adds to the workers and importing a custom class in the shell is broken. The workaround is to add custom jars to both ADD_JARS and SPARK_CLASSPATH. We should fix ADD_JARS so it works properly again. See various threads on the user list: https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201402.mbox/%3CCAJbo4neMLiTrnm1XbyqomWmp0m+EUcg4yE-txuRGSVKOb5KLeA@mail.gmail.com%3E (another one that doesn't appear in the archives yet titled \"ADD_JARS not working on 0.9\")\n\nComments (4):\n1. Nan Zhu: I'm interested in fixing this Can anyone assign it to me? Thank you!\n2. Nan Zhu: made a PR https://github.com/apache/incubator-spark/pull/614, the analysis on the reason of this bug is over there\n3. Nan Zhu: as a temporary work around, you don't need to set ADD_JARS and SPARK_CLASSPATH at the same time just SPARK_CLASSPATH is enough\n4. Nicholas Chammas: So going forward, is the correct procedure for adding external jars to set {{SPARK_CLASSPATH}}? Is there a doc somewhere that details this process? And what is the difference between setting these environment variables and calling {{sc.addJar()}} from within the shell?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "651b2c9aae08e94889f66580d88eeed3", "issue_key": "SPARK-1090", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark-shell should print help information about parameters and should allow user to configure exe memory", "description": "spark-shell should print help information about parameters and should allow user to configure exe memory there is no document about hot to set --cores/-c in spark-shell and also users should be able to set executor memory through command line options", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-02-13T18:03:03.000+0000", "updated": "2014-02-18T13:16:51.000+0000", "resolved": "2014-02-18T13:16:51.000+0000", "labels": [], "components": [], "comments": [{"author": "Nan Zhu", "body": "made a PR https://github.com/apache/incubator-spark/pull/599", "created": "2014-02-13T18:04:59.444+0000"}, {"author": "Nan Zhu", "body": "solved, can any administrator help to close this issue? Thank you", "created": "2014-02-18T10:54:37.092+0000"}], "num_comments": 2, "text": "Issue: SPARK-1090\nSummary: spark-shell should print help information about parameters and should allow user to configure exe memory\nDescription: spark-shell should print help information about parameters and should allow user to configure exe memory there is no document about hot to set --cores/-c in spark-shell and also users should be able to set executor memory through command line options\n\nComments (2):\n1. Nan Zhu: made a PR https://github.com/apache/incubator-spark/pull/599\n2. Nan Zhu: solved, can any administrator help to close this issue? Thank you", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "f972baffc02a33b3d09f326c47077f56", "issue_key": "SPARK-1091", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Cloudpickle does not work correctly for some methods that use a splat", "description": "Cloudpickle seems to have some issues with some methods that use splats  from operator import itemgetter test = {\"one\": 1, \"two\": 2} test_func = itemgetter(\"one\", \"two\") print test_func(test) #=> (1, 2) from pyspark import cloudpickle import pickle serialized = cloudpickle.dumps(test_func) deserialized = pickle.loads(serialized) print deserialized(test) #=> KeyError: (\"one\", \"two\")  As you can see, instead of getting \"one\" and \"two\" seperately and creating one tuple, it instead looks for the key (\"one\", \"two\") in the dict", "reporter": "Bouke van der Bijl", "assignee": null, "created": "2014-02-14T10:49:50.000+0000", "updated": "2014-02-14T14:07:39.000+0000", "resolved": "2014-02-14T14:07:39.000+0000", "labels": ["cloudpickle", "pyspark"], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "This looks like a duplicate of SPARK-791; I've got some notes in that issue, so let's continue any discussions over there.", "created": "2014-02-14T14:04:10.771+0000"}], "num_comments": 1, "text": "Issue: SPARK-1091\nSummary: Cloudpickle does not work correctly for some methods that use a splat\nDescription: Cloudpickle seems to have some issues with some methods that use splats  from operator import itemgetter test = {\"one\": 1, \"two\": 2} test_func = itemgetter(\"one\", \"two\") print test_func(test) #=> (1, 2) from pyspark import cloudpickle import pickle serialized = cloudpickle.dumps(test_func) deserialized = pickle.loads(serialized) print deserialized(test) #=> KeyError: (\"one\", \"two\")  As you can see, instead of getting \"one\" and \"two\" seperately and creating one tuple, it instead looks for the key (\"one\", \"two\") in the dict\n\nComments (1):\n1. Josh Rosen: This looks like a duplicate of SPARK-791; I've got some notes in that issue, so let's continue any discussions over there.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "c3dd8e89da807ae43209f2498204a994", "issue_key": "SPARK-1092", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "SparkContext should not read SPARK_MEM to set memory usage of executors", "description": "Currently, users will usually set SPARK_MEM to control the memory usage of driver programs, (in spark-class) 91 JAVA_OPTS=\"$OUR_JAVA_OPTS\" 92 JAVA_OPTS=\"$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH\" 93 JAVA_OPTS=\"$JAVA_OPTS -Xms$SPARK_MEM -Xmx$SPARK_MEM\" if they didn't set spark.executor.memory, the value in this environment variable will also affect the memory usage of executors, because the following lines in SparkContext private[spark] val executorMemory = conf.getOption(\"spark.executor.memory\") .orElse(Option(System.getenv(\"SPARK_MEM\"))) .map(Utils.memoryStringToMb) .getOrElse(512) also since SPARK_MEM has been (proposed to) deprecated in SPARK-929 (https://spark-project.atlassian.net/browse/SPARK-929) and the corresponding PR (https://github.com/apache/incubator-spark/pull/104) we should remove this line", "reporter": "Nan Zhu", "assignee": null, "created": "2014-02-14T15:32:37.000+0000", "updated": "2014-02-14T15:57:50.000+0000", "resolved": "2014-02-14T15:57:50.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Would regress existing behavior.", "created": "2014-02-14T15:57:50.879+0000"}], "num_comments": 1, "text": "Issue: SPARK-1092\nSummary: SparkContext should not read SPARK_MEM to set memory usage of executors\nDescription: Currently, users will usually set SPARK_MEM to control the memory usage of driver programs, (in spark-class) 91 JAVA_OPTS=\"$OUR_JAVA_OPTS\" 92 JAVA_OPTS=\"$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH\" 93 JAVA_OPTS=\"$JAVA_OPTS -Xms$SPARK_MEM -Xmx$SPARK_MEM\" if they didn't set spark.executor.memory, the value in this environment variable will also affect the memory usage of executors, because the following lines in SparkContext private[spark] val executorMemory = conf.getOption(\"spark.executor.memory\") .orElse(Option(System.getenv(\"SPARK_MEM\"))) .map(Utils.memoryStringToMb) .getOrElse(512) also since SPARK_MEM has been (proposed to) deprecated in SPARK-929 (https://spark-project.atlassian.net/browse/SPARK-929) and the corresponding PR (https://github.com/apache/incubator-spark/pull/104) we should remove this line\n\nComments (1):\n1. Patrick McFadin: Would regress existing behavior.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "cc80c70e123477692768d6bfb6eb64c1", "issue_key": "SPARK-1093", "issue_type": "New Feature", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "API Stability in Spark 1.X (Umbrella)", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "2014-02-14T15:44:46.000+0000", "updated": "2014-05-19T19:08:12.000+0000", "resolved": "2014-05-19T19:08:12.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1093\nSummary: API Stability in Spark 1.X (Umbrella)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "efd652b7a817dde0aabf85c3ac0c369b", "issue_key": "SPARK-1094", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Enforce Binary Compatibility in Spark Build", "description": "", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "created": "2014-02-14T15:46:00.000+0000", "updated": "2020-02-07T17:26:43.000+0000", "resolved": "2014-03-24T21:21:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Prashant Sharma", "body": "I think we can only enforce it once we have released 1.0 ?", "created": "2014-02-16T22:24:51.941+0000"}, {"author": "Patrick McFadin", "body": "We should start enforcing it now and we can add exceptions if necessary for 1.0. For instance, we may want to re-factor some code for Spark 1.0 based on what we learn from enforcing this with new pull requests.", "created": "2014-02-17T09:57:43.936+0000"}, {"author": "Prashant Sharma", "body": "Then I guess we would need to publish SNAPSHOT artifacts somewhere ? As it is required by MiMa. There should also be a way to ignore Mima for a particular PR ?", "created": "2014-02-17T20:58:56.903+0000"}, {"author": "Patrick McFadin", "body": "I mean this: From now on, we should compare new pull requests against Spark 0.9.0 release. If they break binary compatibility then they should have to add an exception to Mima in the pull request so that the build is happy. This way we can practice adding exceptions. Before we do this we need to (a) filter out false positives and add relevant exceptions in the Mima configuration. (b) if we have any true positives, e.g. we have actually broken this in master already vs 0.9.0, we should figure out what it was and decide to either add an exception or revert the change.", "created": "2014-02-17T21:02:18.131+0000"}, {"author": "Patrick McFadin", "body": "Hmm... one thing that might come up is that we might add exceptions we want to later remove... or see if somehow the exceptions can be scoped only for specific releases. We can also just document once we add the exception that it should be removed once 1.0 is released. I'm not sure there are going to be many of these.", "created": "2014-02-17T21:03:35.745+0000"}, {"author": "Prashant Sharma", "body": "But there are a lot \"b)\" reported at the moment. Will it be okay to add all of them as exceptions ?", "created": "2014-02-17T21:07:00.363+0000"}, {"author": "Patrick McFadin", "body": "Hm - could you show what they are? Are they legitimate API changes or just things that aren't considered public API's. I think many of the things are actually API's we will eventually mark semi-private and mark as exceptions in the build. Could you give examples of changes you think are legitimate API changes we've made in the last few weeks?", "created": "2014-02-17T21:17:09.499+0000"}, {"author": "Prashant Sharma", "body": "Here is a quick attempt https://gist.github.com/ScrapCodes/fde092bd2234d75efc37. There are things listed in (b) which may not actually belong there in a very general case. But they may, incase someone writes a library on top of spark.", "created": "2014-02-17T21:35:11.827+0000"}, {"author": "Prashant Sharma", "body": "From whatever I have understood, I guess they all belong to the excludes ! Haven't verified mllib though.", "created": "2014-02-17T22:25:28.559+0000"}, {"author": "Patrick McFadin", "body": "The main issue is that this gives a lot of false positives due to classes which are private\\[X\\]. I dug into MIMA a bit more and I because of the way it parses class files it may be diffficult to implement a rule in MIMA that ignores these. The issue is that these package private declaration end up inside of a special bytecode annotation called ScalaSig that is not easy to parse given the current architecture of MIMA which relies on a forked, fairly old, version of the Scala compiler's class parser. There is a workaround though - we can create a standalone program that goes through all of the Spark classes and uses runtime reflection to determine their visibility. The program can output a flat file with a list of excludes, and then we can set-up the MIMA build to read these excludes from the file. Here is an example from the Spark Shell:  scala> import scala.reflect.runtime.universe.runtimeMirror import scala.reflect.runtime.universe.runtimeMirror scala> val mirror = runtimeMirror(getClass.getClassLoader) mirror: reflect.runtime.universe.Mirror = JavaMirror with org.apache.spark.repl.SparkIMain$TranslatingClassLoader@3c6cf99b of type class org.apache.spark.repl.SparkIMain$TranslatingClassLoader with classpath [/tmp/spark-1d5fd4d3-97b5-4dde-915e-450ec53a6ee0] and parent being scala.tools.nsc.util.ScalaClassLoader$URLClassLoader@16774e1b of type class scala.tools.nsc.util.ScalaClassLoader$URLClassLoader with classpath [file:/usr/lib/jvm/jdk1.7.0_25/jre/lib/resources.jar,file:/usr/lib/jvm/jdk1.... scala> mirror.staticClass(\"org.apache.spark.rdd.RDD\").privateWithin res15: reflect.runtime.universe.Symbol = <none> scala> mirror.staticClass(\"org.apache.spark.storage.BlockInfo\").privateWithin res16: reflect.runtime.universe.Symbol = package storage", "created": "2014-03-16T22:00:06.100+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/20", "created": "2015-12-10T15:06:35.701+0000"}], "num_comments": 11, "text": "Issue: SPARK-1094\nSummary: Enforce Binary Compatibility in Spark Build\n\nComments (11):\n1. Prashant Sharma: I think we can only enforce it once we have released 1.0 ?\n2. Patrick McFadin: We should start enforcing it now and we can add exceptions if necessary for 1.0. For instance, we may want to re-factor some code for Spark 1.0 based on what we learn from enforcing this with new pull requests.\n3. Prashant Sharma: Then I guess we would need to publish SNAPSHOT artifacts somewhere ? As it is required by MiMa. There should also be a way to ignore Mima for a particular PR ?\n4. Patrick McFadin: I mean this: From now on, we should compare new pull requests against Spark 0.9.0 release. If they break binary compatibility then they should have to add an exception to Mima in the pull request so that the build is happy. This way we can practice adding exceptions. Before we do this we need to (a) filter out false positives and add relevant exceptions in the Mima configuration. (b) if we have any true positives, e.g. we have actually broken this in master already vs 0.9.0, we should figure out what it was and decide to either add an exception or revert the change.\n5. Patrick McFadin: Hmm... one thing that might come up is that we might add exceptions we want to later remove... or see if somehow the exceptions can be scoped only for specific releases. We can also just document once we add the exception that it should be removed once 1.0 is released. I'm not sure there are going to be many of these.\n6. Prashant Sharma: But there are a lot \"b)\" reported at the moment. Will it be okay to add all of them as exceptions ?\n7. Patrick McFadin: Hm - could you show what they are? Are they legitimate API changes or just things that aren't considered public API's. I think many of the things are actually API's we will eventually mark semi-private and mark as exceptions in the build. Could you give examples of changes you think are legitimate API changes we've made in the last few weeks?\n8. Prashant Sharma: Here is a quick attempt https://gist.github.com/ScrapCodes/fde092bd2234d75efc37. There are things listed in (b) which may not actually belong there in a very general case. But they may, incase someone writes a library on top of spark.\n9. Prashant Sharma: From whatever I have understood, I guess they all belong to the excludes ! Haven't verified mllib though.\n10. Patrick McFadin: The main issue is that this gives a lot of false positives due to classes which are private\\[X\\]. I dug into MIMA a bit more and I because of the way it parses class files it may be diffficult to implement a rule in MIMA that ignores these. The issue is that these package private declaration end up inside of a special bytecode annotation called ScalaSig that is not easy to parse given the current architecture of MIMA which relies on a forked, fairly old, version of the Scala compiler's class parser. There is a workaround though - we can create a standalone program that goes through all of the Spark classes and uses runtime reflection to determine their visibility. The program can output a flat file with a list of excludes, and then we can set-up the MIMA build to read these excludes from the file. Here is an example from the Spark Shell:  scala> import scala.reflect.runtime.universe.runtimeMirror import scala.reflect.runtime.universe.runtimeMirror scala> val mirror = runtimeMirror(getClass.getClassLoader) mirror: reflect.runtime.universe.Mirror = JavaMirror with org.apache.spark.repl.SparkIMain$TranslatingClassLoader@3c6cf99b of type class org.apache.spark.repl.SparkIMain$TranslatingClassLoader with classpath [/tmp/spark-1d5fd4d3-97b5-4dde-915e-450ec53a6ee0] and parent being scala.tools.nsc.util.ScalaClassLoader$URLClassLoader@16774e1b of type class scala.tools.nsc.util.ScalaClassLoader$URLClassLoader with classpath [file:/usr/lib/jvm/jdk1.7.0_25/jre/lib/resources.jar,file:/usr/lib/jvm/jdk1.... scala> mirror.staticClass(\"org.apache.spark.rdd.RDD\").privateWithin res15: reflect.runtime.universe.Symbol = <none> scala> mirror.staticClass(\"org.apache.spark.storage.BlockInfo\").privateWithin res16: reflect.runtime.universe.Symbol = package storage", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "439d2a426cbd1eaf2a1ff81c125d4593", "issue_key": "SPARK-1095", "issue_type": "Sub-task", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Ensure all public methods return explicit types", "description": "This talk explains some of the challenges around typing for binary compatibility: http://www.slideshare.net/mircodotta/managing-binary-compatibility-in-scala For public methods we should always declare the type. We've had this as a guideline in the past but we need to make sure we obey it in all public interfaces. Also, we should return the most general type possible.", "reporter": "Patrick Wendell", "assignee": "Reynold Xin", "created": "2014-02-14T15:53:10.000+0000", "updated": "2015-04-03T06:27:02.000+0000", "resolved": "2015-04-03T06:27:02.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "I accidentally closed this but it's not merged yet.", "created": "2014-03-18T14:58:25.823+0000"}], "num_comments": 1, "text": "Issue: SPARK-1095\nSummary: Ensure all public methods return explicit types\nDescription: This talk explains some of the challenges around typing for binary compatibility: http://www.slideshare.net/mircodotta/managing-binary-compatibility-in-scala For public methods we should always declare the type. We've had this as a guideline in the past but we need to make sure we obey it in all public interfaces. Also, we should return the most general type possible.\n\nComments (1):\n1. Patrick McFadin: I accidentally closed this but it's not merged yet.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "d36f482e01400b03e1533d1097d330ee", "issue_key": "SPARK-1096", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add Scalastyle Plug-in For Spaces after Comments", "description": "After line length (which is now checked) this is the number one thing people forget in pull requests. We should see whether a custom scalastyle rule can be used to check this. In general I'm guessing we'll want to add several custom rules, so this is a good time to look into how it works. The scala AST seems to have comments as recognized token so it might work, but not sure if they are gobbled before the plug-in gets to it. This is something to look into. I find this particular style issue so pervasive and annoying that I'd even settle for a bash command inside of our test script that greps throughout the codebase. Scalastyle is a much better way to do this though - if we can make it work!", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "created": "2014-02-14T17:51:40.000+0000", "updated": "2020-02-07T17:26:39.000+0000", "resolved": "2014-03-28T00:22:49.000+0000", "labels": [], "components": ["Project Infra"], "comments": [{"author": "Prashant Sharma", "body": "I hope you mean this  // correct  and  //wrong", "created": "2014-02-17T04:18:33.881+0000"}, {"author": "Patrick McFadin", "body": "Yes - that's the one!", "created": "2014-02-17T09:55:25.961+0000"}, {"author": "Prashant Sharma", "body": "I was adding a few more test cases.  package foobar object Foobar { //Incorrect // correct comment//not wrong//check val a = 10//Incorrect val b = 100 //Incorrect val c = 1// Correct val d = 2 // Correct val e = 3 }\"\"\"  Do you think they make sense ? Should we have similar thing for /* and /** ?", "created": "2014-02-20T22:21:24.520+0000"}, {"author": "Patrick McFadin", "body": "Absolutely - let's make this cover as many cases as possible.", "created": "2014-02-20T22:57:35.782+0000"}, {"author": "Prashant Sharma", "body": "Looks like they don't have this feature of letting us plugin checks yet, scalastyle/scalastyle#25", "created": "2014-02-26T19:58:47.847+0000"}, {"author": "Patrick McFadin", "body": "Hey [~prashant] if you have the time or inclination, it would be great to see if we could add that feature to scalastyle, then we can ask them to make another release. That would open the doors for us to write a bunch of our own style rules, which would be awesome and allow us to review patches more easily.", "created": "2014-02-27T00:26:19.074+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/124", "created": "2015-12-10T15:06:43.120+0000"}], "num_comments": 7, "text": "Issue: SPARK-1096\nSummary: Add Scalastyle Plug-in For Spaces after Comments\nDescription: After line length (which is now checked) this is the number one thing people forget in pull requests. We should see whether a custom scalastyle rule can be used to check this. In general I'm guessing we'll want to add several custom rules, so this is a good time to look into how it works. The scala AST seems to have comments as recognized token so it might work, but not sure if they are gobbled before the plug-in gets to it. This is something to look into. I find this particular style issue so pervasive and annoying that I'd even settle for a bash command inside of our test script that greps throughout the codebase. Scalastyle is a much better way to do this though - if we can make it work!\n\nComments (7):\n1. Prashant Sharma: I hope you mean this  // correct  and  //wrong\n2. Patrick McFadin: Yes - that's the one!\n3. Prashant Sharma: I was adding a few more test cases.  package foobar object Foobar { //Incorrect // correct comment//not wrong//check val a = 10//Incorrect val b = 100 //Incorrect val c = 1// Correct val d = 2 // Correct val e = 3 }\"\"\"  Do you think they make sense ? Should we have similar thing for /* and /** ?\n4. Patrick McFadin: Absolutely - let's make this cover as many cases as possible.\n5. Prashant Sharma: Looks like they don't have this feature of letting us plugin checks yet, scalastyle/scalastyle#25\n6. Patrick McFadin: Hey [~prashant] if you have the time or inclination, it would be great to see if we could add that feature to scalastyle, then we can ask them to make another release. That would open the doors for us to write a bunch of our own style rules, which would be awesome and allow us to review patches more easily.\n7. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/124", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "60a3b292f2d7967ce2cce9ada65f7403", "issue_key": "SPARK-1097", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ConcurrentModificationException", "description": "14/02/16 08:18:45 WARN TaskSetManager: Loss was due to java.util.ConcurrentModificationException java.util.ConcurrentModificationException at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926) at java.util.HashMap$KeyIterator.next(HashMap.java:960) at java.util.AbstractCollection.addAll(AbstractCollection.java:341) at java.util.HashSet.<init>(HashSet.java:117) at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:554) at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:439) at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:110) at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:154) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:32) at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:72) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102) at org.apache.spark.scheduler.Task.run(Task.scala:53) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)", "reporter": "Fabrizio Milo", "assignee": "Raymond Liu", "created": "2014-02-16T14:31:49.000+0000", "updated": "2014-10-06T23:06:02.000+0000", "resolved": "2014-06-13T17:53:43.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Karthik Kambatla", "body": "What version of Hadoop is this?", "created": "2014-02-27T23:18:16.883+0000"}, {"author": "Nishkam Ravi", "body": "Attached is a patch for this issue. Verified with mvn test/compile/install. The fix is to move HashSet initialization to the synchronized block right above it.", "created": "2014-04-02T01:29:37.161+0000"}, {"author": "Sean R. Owen", "body": "Standard procedure is to provide a pull request. But, you're suggesting a fix to Hadoop code, which belong in your Hadoop JIRA, yes. This can't fix the problem from the Spark end. (Eyeballing the Hadoop 2.2.0 code, I tend to agree with your patch. Mutation of finalParameters appears consistently synchronized, which means the constructor reading it to copy has to lock on the other Configuration or else exactly this can happen.) Is a workaround in Spark to synchronize on the Configuration object when calling this constructor? (I smell a deadlock risk.) Or something crazy like trying the constructor until it doesn't fail this way?", "created": "2014-04-02T04:52:02.143+0000"}, {"author": "Nishkam Ravi", "body": "The problem should be solved at the root. This issue can be exposed by other systems as well, in addition to Spark. The fix is straightforward and harmless. I can initiate a pull request as well.", "created": "2014-04-02T05:16:40.844+0000"}, {"author": "Sean R. Owen", "body": "I agree, but, we can't patch Hadoop from here. I'm just saying that for purposes of a SPARK-* issue, in anything like the short-term, one would have to propose a workaround within Spark code, if anything. While also trying to fix it at the root, separately, in a HADOOP-* issue. (Spark does not have a copy of Hadoop, yesterday's April Fools joke aside.)", "created": "2014-04-02T05:20:22.122+0000"}, {"author": "Nishkam Ravi", "body": "We can consider putting a workaround in Spark as well (for non-CDH users that may be running an older version of Hadoop and not updating it periodically). For now, this fix needs to go upstream, so we can backport it to CDH. The CDH-Spark bundle would then inherit this fix. The same issue has been noted in Hadoop-10456 as well.", "created": "2014-04-02T18:00:31.699+0000"}, {"author": "Nishkam Ravi", "body": "Patch submitted against Hadoop-10456.", "created": "2014-04-02T22:14:44.466+0000"}, {"author": "Tsuyoshi Ozawa", "body": "A patch by Nishkam on HADOOP-10456 has been already reviewed and will be committed in a few days against hadoop's trunk.", "created": "2014-04-03T09:43:07.743+0000"}, {"author": "Jim Blomo", "body": "FYI still seeing this on spark 1.0, Hadoop 2.4  java.util.ConcurrentModificationException (java.util.ConcurrentModificationException) java.util.HashMap$HashIterator.nextEntry(HashMap.java:922) java.util.HashMap$KeyIterator.next(HashMap.java:956) java.util.AbstractCollection.addAll(AbstractCollection.java:341) java.util.HashSet.<init>(HashSet.java:117) org.apache.hadoop.conf.Configuration.<init>(Configuration.java:671) com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:98) org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2402) org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89) org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2436) org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2418) org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373) org.apache.hadoop.fs.Path.getFileSystem(Path.java:296) org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:107) org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67) org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:190) org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:181) org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:200) org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175) org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175) org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160) org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:174)", "created": "2014-06-06T19:33:25.909+0000"}, {"author": "Tsuyoshi Ozawa", "body": "[~jblomo], thank you for reporting. This issue is fixed in next minor Hadoop release - 2.4.1. Note that 2.4.0 doesn't include the fix.", "created": "2014-06-06T20:39:36.510+0000"}, {"author": "Nishkam Ravi", "body": "Have initiated a PR for a workaround in Spark as well (for developers using < 2.4.1): https://github.com/apache/spark/pull/1000", "created": "2014-06-06T20:53:11.995+0000"}, {"author": "Raymond Liu", "body": "Summit another PR to actually(hopes) workaround this problem https://github.com/apache/spark/pull/1273", "created": "2014-07-01T08:47:50.934+0000"}, {"author": "Patrick Wendell", "body": "A follow up to this fix is in Spark 1.0.2: https://github.com/apache/spark/pull/1409/files", "created": "2014-07-26T01:58:41.719+0000"}], "num_comments": 13, "text": "Issue: SPARK-1097\nSummary: ConcurrentModificationException\nDescription: 14/02/16 08:18:45 WARN TaskSetManager: Loss was due to java.util.ConcurrentModificationException java.util.ConcurrentModificationException at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926) at java.util.HashMap$KeyIterator.next(HashMap.java:960) at java.util.AbstractCollection.addAll(AbstractCollection.java:341) at java.util.HashSet.<init>(HashSet.java:117) at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:554) at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:439) at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:110) at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:154) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149) at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.UnionPartition.iterator(UnionRDD.scala:32) at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:72) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102) at org.apache.spark.scheduler.Task.run(Task.scala:53) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)\n\nComments (13):\n1. Karthik Kambatla: What version of Hadoop is this?\n2. Nishkam Ravi: Attached is a patch for this issue. Verified with mvn test/compile/install. The fix is to move HashSet initialization to the synchronized block right above it.\n3. Sean R. Owen: Standard procedure is to provide a pull request. But, you're suggesting a fix to Hadoop code, which belong in your Hadoop JIRA, yes. This can't fix the problem from the Spark end. (Eyeballing the Hadoop 2.2.0 code, I tend to agree with your patch. Mutation of finalParameters appears consistently synchronized, which means the constructor reading it to copy has to lock on the other Configuration or else exactly this can happen.) Is a workaround in Spark to synchronize on the Configuration object when calling this constructor? (I smell a deadlock risk.) Or something crazy like trying the constructor until it doesn't fail this way?\n4. Nishkam Ravi: The problem should be solved at the root. This issue can be exposed by other systems as well, in addition to Spark. The fix is straightforward and harmless. I can initiate a pull request as well.\n5. Sean R. Owen: I agree, but, we can't patch Hadoop from here. I'm just saying that for purposes of a SPARK-* issue, in anything like the short-term, one would have to propose a workaround within Spark code, if anything. While also trying to fix it at the root, separately, in a HADOOP-* issue. (Spark does not have a copy of Hadoop, yesterday's April Fools joke aside.)\n6. Nishkam Ravi: We can consider putting a workaround in Spark as well (for non-CDH users that may be running an older version of Hadoop and not updating it periodically). For now, this fix needs to go upstream, so we can backport it to CDH. The CDH-Spark bundle would then inherit this fix. The same issue has been noted in Hadoop-10456 as well.\n7. Nishkam Ravi: Patch submitted against Hadoop-10456.\n8. Tsuyoshi Ozawa: A patch by Nishkam on HADOOP-10456 has been already reviewed and will be committed in a few days against hadoop's trunk.\n9. Jim Blomo: FYI still seeing this on spark 1.0, Hadoop 2.4  java.util.ConcurrentModificationException (java.util.ConcurrentModificationException) java.util.HashMap$HashIterator.nextEntry(HashMap.java:922) java.util.HashMap$KeyIterator.next(HashMap.java:956) java.util.AbstractCollection.addAll(AbstractCollection.java:341) java.util.HashSet.<init>(HashSet.java:117) org.apache.hadoop.conf.Configuration.<init>(Configuration.java:671) com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:98) org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2402) org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89) org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2436) org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2418) org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373) org.apache.hadoop.fs.Path.getFileSystem(Path.java:296) org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:107) org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67) org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:190) org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:181) org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:200) org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175) org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:175) org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160) org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:174)\n10. Tsuyoshi Ozawa: [~jblomo], thank you for reporting. This issue is fixed in next minor Hadoop release - 2.4.1. Note that 2.4.0 doesn't include the fix.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "5902e3d642cb583576bcc1ae70299d71", "issue_key": "SPARK-1098", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Cleanup and document ClassTag stuff in Java API", "description": "There is significant repeated and poorly styled code related to ClassTags in the Java API. Additionally, our usage of ClassTags is very unusual, and deserves some form of documentation.", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2014-02-16T16:21:00.000+0000", "updated": "2014-02-21T11:00:54.000+0000", "resolved": "2014-02-21T11:00:54.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1098\nSummary: Cleanup and document ClassTag stuff in Java API\nDescription: There is significant repeated and poorly styled code related to ClassTags in the Java API. Additionally, our usage of ClassTags is very unusual, and deserves some form of documentation.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "45d3caa176491a996a8cd3a7b7f856a7", "issue_key": "SPARK-1099", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Allow inferring number of cores with local[*]", "description": "It seems reasonable that the default number of cores used by spark's local mode (when no value is specified) is drawn from the spark.cores.max configuration parameter (which, conveniently, is now settable as a command-line option in spark-shell). For the sake of consistency, it's probable that this change would also entail making the default number of cores when spark.cores.max is NOT specified to be as many logical cores are on the machine (which is what standalone mode does). This too seems reasonable, as Spark is inherently a distributed system and I think it's expected that it should use multiple cores by default. However, it is a behavioral change, and thus requires caution.", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2014-02-16T19:25:54.000+0000", "updated": "2016-04-30T03:32:03.000+0000", "resolved": "2014-04-07T23:54:21.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "OuyangJin", "body": "@Araon I can work on this. Can anyone put as assignee. Thanks!", "created": "2014-02-22T00:44:10.583+0000"}, {"author": "OuyangJin", "body": "@Araon, I wonder that your goal is to add a new config property named \"spark.max.cores\" , not existed \"spark.cores.max\" for each applicatoin. And in spark local cluster mode, one can specify the number of workers , and the n workers and the master is running on the same jvm . And the cores specified by your \"spark.max.cores\" is the total number of cores for the n workers or cores for each worker? I think its the total num of cores for n workes. And how we should pass the coresPerWorker to Worker.startSystemAndActor with our \"spark.max.cores\"?", "created": "2014-02-22T02:24:21.959+0000"}, {"author": "Aaron Davidson", "body": "Hey, [~qqsun8819] -- I did mean spark.cores.max, that was a typo in my head :) I believe some of your questions stem from looking at LocalSparkCluster, which is actually used only for the \"MASTER=local-cluster\\[2,2,512\\]\" syntax. When using standard local mode, we create a LocalBackend, which simply takes the number of cores as an argument. There is no worker paradigm in play, just one big Executor with a bunch of cores. (Sorry for delayed response, was travelling!)", "created": "2014-03-06T09:16:31.653+0000"}, {"author": "OuyangJin", "body": "@Araon, PR is ready, and here I'll state what I do . This is really a behavioral change, so I want you to know what my patch is suitable . And I welcome any of your advice if you have a different opinion: 1 I change the \"MASTER=local\" pattern of create LocalBackEnd . In the past, we passed 1 core to it . now it use a default cores The reason here is that when someone use spark-shell to start local mode , Repl will use this \"MASTER=local\" pattern as default. So if one also specify cores in the spark-shell command line, it will all go in here. So here pass 1 core is not suitalbe reponding to our change here. 2 In the LocalBackEnd , the \"totalCores\" variable are fetched following a different rule(in the past it just take in a userd passed cores, like 1 in \"MASTER=local\" pattern, 2 in \"MASTER=local[2]\" pattern\" rules: a The second argument of LocalBackEnd 's constructor indicating cores have a default value which is Int.MaxValue. If user didn't pass it , its first default value is Int.MaxValue b In getMaxCores, we first compare the former value to Int.MaxValue. if it's not equal, we think that user has passed their desired value, so just use it c. If b is not satified, we then get cores from spark.cores.max, and we get real logical cores from Runtime. And if cores specified by spark.cores.max is bigger than logical cores, we use logical cores, otherwise we use spark.cores.max 3 In SparkContextSchedulerCreationSuite 's test(\"local\") case, assertion is modified from 1 to logical cores, because \"MASTER=local\" pattern use default vaules.", "created": "2014-03-08T23:42:06.964+0000"}, {"author": "OuyangJin", "body": "PR in https://github.com/apache/spark/pull/110", "created": "2014-03-08T23:54:28.033+0000"}, {"author": "Aaron Davidson", "body": "Reverted change due to build failure, reopened at https://github.com/apache/spark/pull/182", "created": "2014-03-19T18:10:43.195+0000"}, {"author": "Apache Spark", "body": "User 'qqsun8819' has created a pull request for this issue: https://github.com/apache/spark/pull/110", "created": "2016-04-30T03:32:03.245+0000"}], "num_comments": 7, "text": "Issue: SPARK-1099\nSummary: Allow inferring number of cores with local[*]\nDescription: It seems reasonable that the default number of cores used by spark's local mode (when no value is specified) is drawn from the spark.cores.max configuration parameter (which, conveniently, is now settable as a command-line option in spark-shell). For the sake of consistency, it's probable that this change would also entail making the default number of cores when spark.cores.max is NOT specified to be as many logical cores are on the machine (which is what standalone mode does). This too seems reasonable, as Spark is inherently a distributed system and I think it's expected that it should use multiple cores by default. However, it is a behavioral change, and thus requires caution.\n\nComments (7):\n1. OuyangJin: @Araon I can work on this. Can anyone put as assignee. Thanks!\n2. OuyangJin: @Araon, I wonder that your goal is to add a new config property named \"spark.max.cores\" , not existed \"spark.cores.max\" for each applicatoin. And in spark local cluster mode, one can specify the number of workers , and the n workers and the master is running on the same jvm . And the cores specified by your \"spark.max.cores\" is the total number of cores for the n workers or cores for each worker? I think its the total num of cores for n workes. And how we should pass the coresPerWorker to Worker.startSystemAndActor with our \"spark.max.cores\"?\n3. Aaron Davidson: Hey, [~qqsun8819] -- I did mean spark.cores.max, that was a typo in my head :) I believe some of your questions stem from looking at LocalSparkCluster, which is actually used only for the \"MASTER=local-cluster\\[2,2,512\\]\" syntax. When using standard local mode, we create a LocalBackend, which simply takes the number of cores as an argument. There is no worker paradigm in play, just one big Executor with a bunch of cores. (Sorry for delayed response, was travelling!)\n4. OuyangJin: @Araon, PR is ready, and here I'll state what I do . This is really a behavioral change, so I want you to know what my patch is suitable . And I welcome any of your advice if you have a different opinion: 1 I change the \"MASTER=local\" pattern of create LocalBackEnd . In the past, we passed 1 core to it . now it use a default cores The reason here is that when someone use spark-shell to start local mode , Repl will use this \"MASTER=local\" pattern as default. So if one also specify cores in the spark-shell command line, it will all go in here. So here pass 1 core is not suitalbe reponding to our change here. 2 In the LocalBackEnd , the \"totalCores\" variable are fetched following a different rule(in the past it just take in a userd passed cores, like 1 in \"MASTER=local\" pattern, 2 in \"MASTER=local[2]\" pattern\" rules: a The second argument of LocalBackEnd 's constructor indicating cores have a default value which is Int.MaxValue. If user didn't pass it , its first default value is Int.MaxValue b In getMaxCores, we first compare the former value to Int.MaxValue. if it's not equal, we think that user has passed their desired value, so just use it c. If b is not satified, we then get cores from spark.cores.max, and we get real logical cores from Runtime. And if cores specified by spark.cores.max is bigger than logical cores, we use logical cores, otherwise we use spark.cores.max 3 In SparkContextSchedulerCreationSuite 's test(\"local\") case, assertion is modified from 1 to logical cores, because \"MASTER=local\" pattern use default vaules.\n5. OuyangJin: PR in https://github.com/apache/spark/pull/110\n6. Aaron Davidson: Reverted change due to build failure, reopened at https://github.com/apache/spark/pull/182\n7. Apache Spark: User 'qqsun8819' has created a pull request for this issue: https://github.com/apache/spark/pull/110", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "a32f5f42f17b17feedc34ecacaec99bb", "issue_key": "SPARK-1100", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "saveAsTextFile shouldn't clobber by default", "description": "If I call rdd.saveAsTextFile with an existing directory, it will cheerfully and silently overwrite the files in there. This is bad enough if it means I've accidentally blown away the results of a job that might have taken minutes or hours to run. But it's worse if the second job happens to have fewer partitions than the first...in that case, my output directory now contains some \"part\" files from the earlier job, and some \"part\" files from the later job. The only way to know the difference is timestamp. I wonder if Spark's saveAsTextFile shouldn't work more like Hadoop MapReduce which insists that the output directory not exist before the job starts. Similarly HDFS won't override files by default. Perhaps there could be an optional argument for saveAsTextFile that indicates if it should delete the existing directory before starting. (I can't see any time I'd want to allow writing to an existing directory with data already in it. Would the mix of output from different tasks ever be desirable?)", "reporter": "Diana Carroll", "assignee": "Patrick Wendell", "created": "2014-02-18T09:45:05.000+0000", "updated": "2014-04-29T18:50:44.000+0000", "resolved": "2014-03-01T17:29:09.000+0000", "labels": [], "components": ["Input/Output"], "comments": [{"author": "Nan Zhu", "body": "indeed, overwriting silently is risky but some users have been used to this, e.g. their jobs are started periodically to update the data in a directory... I think we'd better make it as an option, if spark.overwrite = true (by default, current behaviour), then it is directly overwrite the directory, if spark.overwrite = false, it will reject the writing I would like to work on it, any admin can assign it to me?", "created": "2014-02-18T12:56:48.259+0000"}, {"author": "Reynold Xin", "body": "I added you [~CodingCat] to the developer list so you can assign tickets to yourself in the future.", "created": "2014-02-18T12:58:37.426+0000"}, {"author": "Nan Zhu", "body": "many thanks @Reynold Xin", "created": "2014-02-18T13:00:20.307+0000"}, {"author": "Diana Carroll", "body": "How about making it an option on the function call, that way it can be set by the developer for each individual write, rather than for a whole application or site? Also maybe a warning or at least info message indicating that files are being overwritten? I still think it's questionable to overwrite individual files in a directory rather than clear out the directory, even if, as you say, I want a job to \"periodically update the data\". If my first job's data has three partitions, it will save three files: part-00000, part-00001 and part-00002. If my second job has two partitions it will overwrite part-00000 and part-00001 from the first job, but leave the third file part-00003 hanging around. If I look in the directory, it's not easy to tell that my output data from the second job does NOT have three partitions/files, it only has two. If I build a report from all three files, or have a script that assembles them into a single file, two thirds of the data in the report would correctly be from the most recent job, and one third would incorrectly be from the older job. I'm having trouble imagining a use case where this would be desired behavior... Thanks for looking at this!", "created": "2014-02-18T13:12:54.995+0000"}, {"author": "Nan Zhu", "body": "Good point, so if the option is set to true, we will clear out the directory first, if false, we will reject the writing but I'm still concerning about that since Spark has been running for years, there would be someone is occasionally utilizing this \"feature\".... anyway, I would like to make a PR first (maybe in this week) and then revise it according to the feedbacks from others", "created": "2014-02-18T13:26:43.037+0000"}, {"author": "Nan Zhu", "body": "made a PR https://github.com/apache/incubator-spark/pull/626", "created": "2014-02-20T21:44:45.855+0000"}, {"author": "Nan Zhu", "body": "Hi, Diana Carroll, Can you confirm the left over old file problem happening on HDFS? mridulm said this should not happen I reproduced it in Local file system and S3, but I don't have a HDFS environment, See our discussion here:https://github.com/apache/incubator-spark/pull/626 Best, Nan", "created": "2014-02-21T11:16:04.970+0000"}, {"author": "Patrick McFadin", "body": "Ya I don't think we want Spark ever deleting HDFS directories. I'd propose doing something like Hadoop MR, except maybe just check if the directory already has output files in it rather than if it exists. This would avoid regressing behavior for users that, e.g., output data into a directory that contains other stuff.", "created": "2014-02-23T12:08:26.544+0000"}], "num_comments": 8, "text": "Issue: SPARK-1100\nSummary: saveAsTextFile shouldn't clobber by default\nDescription: If I call rdd.saveAsTextFile with an existing directory, it will cheerfully and silently overwrite the files in there. This is bad enough if it means I've accidentally blown away the results of a job that might have taken minutes or hours to run. But it's worse if the second job happens to have fewer partitions than the first...in that case, my output directory now contains some \"part\" files from the earlier job, and some \"part\" files from the later job. The only way to know the difference is timestamp. I wonder if Spark's saveAsTextFile shouldn't work more like Hadoop MapReduce which insists that the output directory not exist before the job starts. Similarly HDFS won't override files by default. Perhaps there could be an optional argument for saveAsTextFile that indicates if it should delete the existing directory before starting. (I can't see any time I'd want to allow writing to an existing directory with data already in it. Would the mix of output from different tasks ever be desirable?)\n\nComments (8):\n1. Nan Zhu: indeed, overwriting silently is risky but some users have been used to this, e.g. their jobs are started periodically to update the data in a directory... I think we'd better make it as an option, if spark.overwrite = true (by default, current behaviour), then it is directly overwrite the directory, if spark.overwrite = false, it will reject the writing I would like to work on it, any admin can assign it to me?\n2. Reynold Xin: I added you [~CodingCat] to the developer list so you can assign tickets to yourself in the future.\n3. Nan Zhu: many thanks @Reynold Xin\n4. Diana Carroll: How about making it an option on the function call, that way it can be set by the developer for each individual write, rather than for a whole application or site? Also maybe a warning or at least info message indicating that files are being overwritten? I still think it's questionable to overwrite individual files in a directory rather than clear out the directory, even if, as you say, I want a job to \"periodically update the data\". If my first job's data has three partitions, it will save three files: part-00000, part-00001 and part-00002. If my second job has two partitions it will overwrite part-00000 and part-00001 from the first job, but leave the third file part-00003 hanging around. If I look in the directory, it's not easy to tell that my output data from the second job does NOT have three partitions/files, it only has two. If I build a report from all three files, or have a script that assembles them into a single file, two thirds of the data in the report would correctly be from the most recent job, and one third would incorrectly be from the older job. I'm having trouble imagining a use case where this would be desired behavior... Thanks for looking at this!\n5. Nan Zhu: Good point, so if the option is set to true, we will clear out the directory first, if false, we will reject the writing but I'm still concerning about that since Spark has been running for years, there would be someone is occasionally utilizing this \"feature\".... anyway, I would like to make a PR first (maybe in this week) and then revise it according to the feedbacks from others\n6. Nan Zhu: made a PR https://github.com/apache/incubator-spark/pull/626\n7. Nan Zhu: Hi, Diana Carroll, Can you confirm the left over old file problem happening on HDFS? mridulm said this should not happen I reproduced it in Local file system and S3, but I don't have a HDFS environment, See our discussion here:https://github.com/apache/incubator-spark/pull/626 Best, Nan\n8. Patrick McFadin: Ya I don't think we want Spark ever deleting HDFS directories. I'd propose doing something like Hadoop MR, except maybe just check if the directory already has output files in it rather than if it exists. This would avoid regressing behavior for users that, e.g., output data into a directory that contains other stuff.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "6f01caffab72386565ae2dd7241cb250", "issue_key": "SPARK-1101", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Umbrella for hardening Spark on YARN", "description": "This is an umbrella JIRA to track near-term improvements for Spark on YARN. I don't think huge changes are required - just fixing some bugs, plugging usability gaps, and enhancing documentation.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-02-18T13:30:25.000+0000", "updated": "2014-05-07T01:15:39.000+0000", "resolved": "2014-05-07T01:15:39.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Sandy Ryza", "body": "Closing this as all its subtasks are completed.", "created": "2014-05-07T01:15:39.818+0000"}], "num_comments": 1, "text": "Issue: SPARK-1101\nSummary: Umbrella for hardening Spark on YARN\nDescription: This is an umbrella JIRA to track near-term improvements for Spark on YARN. I don't think huge changes are required - just fixing some bugs, plugging usability gaps, and enhancing documentation.\n\nComments (1):\n1. Sandy Ryza: Closing this as all its subtasks are completed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "d943d0851c3d24f91ce2a1188a94bd15", "issue_key": "SPARK-1102", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Create a saveAsNewAPIHadoopDataset method", "description": "Right now RDDs can only be saved as files using the new Hadoop API, not as \"datasets\" with no filename and just a JobConf. See http://codeforhire.com/2014/02/18/using-spark-with-mongodb/ for an example of how you have to give a bogus filename. For the old Hadoop API, we have saveAsHadoopDataset.", "reporter": "Matei Alexandru Zaharia", "assignee": "Nan Zhu", "created": "2014-02-18T13:50:27.000+0000", "updated": "2014-03-18T11:06:55.000+0000", "resolved": "2014-03-18T11:06:55.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Nan Zhu", "body": "I'm just working on the other JIRA (SPARK-1100) involving the same part of code, I would like to fix this also", "created": "2014-02-20T17:18:08.060+0000"}, {"author": "Nan Zhu", "body": "Due to the code conflict, I can only fix this after the PR for SPARK-1100 is merged or closed", "created": "2014-02-21T11:18:39.145+0000"}, {"author": "Nan Zhu", "body": "anyway, made a PR https://github.com/apache/incubator-spark/pull/636 I don't mind resolve the conflict manually", "created": "2014-02-22T23:32:00.837+0000"}], "num_comments": 3, "text": "Issue: SPARK-1102\nSummary: Create a saveAsNewAPIHadoopDataset method\nDescription: Right now RDDs can only be saved as files using the new Hadoop API, not as \"datasets\" with no filename and just a JobConf. See http://codeforhire.com/2014/02/18/using-spark-with-mongodb/ for an example of how you have to give a bogus filename. For the old Hadoop API, we have saveAsHadoopDataset.\n\nComments (3):\n1. Nan Zhu: I'm just working on the other JIRA (SPARK-1100) involving the same part of code, I would like to fix this also\n2. Nan Zhu: Due to the code conflict, I can only fix this after the PR for SPARK-1100 is merged or closed\n3. Nan Zhu: anyway, made a PR https://github.com/apache/incubator-spark/pull/636 I don't mind resolve the conflict manually", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "cf24a5cb558374ef3ef689732a85aae9", "issue_key": "SPARK-1103", "issue_type": "Improvement", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Garbage collect RDD information inside of Spark", "description": "When Spark jobs run for a long period of time, state accumulates. This is dealt with now using TTL-based cleaning. Instead we should do proper garbage collection using weak references.", "reporter": "Patrick Wendell", "assignee": "Tathagata Das", "created": "2014-02-18T15:52:59.000+0000", "updated": "2015-09-14T07:43:06.000+0000", "resolved": "2014-04-08T06:41:30.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Ash", "body": "https://github.com/apache/spark/pull/126", "created": "2014-05-27T18:45:07.764+0000"}, {"author": "Apache Spark", "body": "User 'tdas' has created a pull request for this issue: https://github.com/apache/spark/pull/126", "created": "2015-09-14T07:43:06.278+0000"}], "num_comments": 2, "text": "Issue: SPARK-1103\nSummary: Garbage collect RDD information inside of Spark\nDescription: When Spark jobs run for a long period of time, state accumulates. This is dealt with now using TTL-based cleaning. Instead we should do proper garbage collection using weak references.\n\nComments (2):\n1. Andrew Ash: https://github.com/apache/spark/pull/126\n2. Apache Spark: User 'tdas' has created a pull request for this issue: https://github.com/apache/spark/pull/126", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "8970cc2bd384157175104d2d7bf550e3", "issue_key": "SPARK-1104", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Worker should not block while killing executors", "description": "Sometimes due to large shuffles executors will take a long time shutting down. In particular this can happen if large numbers of shuffle files are around (this will be alleviated by SPARK-1103, but nonetheless...). The symptom is you have DEAD workers sitting around in the UI and the existing workers keep trying to re-register but can't because they've been assumed dead. If killing the executor happens in its own thread, or if the ExecutorRunner were an actor, this would not be a problem. For 0.9 I'd prefer the former approach since it minimizes code changes.", "reporter": "Patrick McFadin", "assignee": "Nan Zhu", "created": "2014-02-18T15:56:09.000+0000", "updated": "2014-04-24T22:57:12.000+0000", "resolved": "2014-04-24T22:57:12.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Nan Zhu", "body": "I went through the related code, if my understanding is correct, the difficulty is that how to call process.destroy() and waitFor() in a separate thread, my proposal is to have a cleanup thread to maintain the process together with workerThread, How do you think about that, Patrick?", "created": "2014-02-24T12:34:10.079+0000"}, {"author": "Nan Zhu", "body": "made the PR: https://github.com/apache/spark/pull/35", "created": "2014-02-27T10:02:20.435+0000"}, {"author": "Nan Zhu", "body": "any one would like to review the PR?", "created": "2014-03-10T04:25:51.618+0000"}], "num_comments": 3, "text": "Issue: SPARK-1104\nSummary: Worker should not block while killing executors\nDescription: Sometimes due to large shuffles executors will take a long time shutting down. In particular this can happen if large numbers of shuffle files are around (this will be alleviated by SPARK-1103, but nonetheless...). The symptom is you have DEAD workers sitting around in the UI and the existing workers keep trying to re-register but can't because they've been assumed dead. If killing the executor happens in its own thread, or if the ExecutorRunner were an actor, this would not be a problem. For 0.9 I'd prefer the former approach since it minimizes code changes.\n\nComments (3):\n1. Nan Zhu: I went through the related code, if my understanding is correct, the difficulty is that how to call process.destroy() and waitFor() in a separate thread, my proposal is to have a cleanup thread to maintain the process together with workerThread, How do you think about that, Patrick?\n2. Nan Zhu: made the PR: https://github.com/apache/spark/pull/35\n3. Nan Zhu: any one would like to review the PR?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "6143571e642cce395ad46bbc927cc664", "issue_key": "SPARK-1105", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "config.yml has an error version number", "description": "as reported by http://apache-spark-user-list.1001560.n3.nabble.com/question-about-compiling-SimpleApp-td1689.html the SCALA_VERSION in config.yml is wrong it should be 2.10.3", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-02-18T16:24:11.000+0000", "updated": "2014-02-20T11:13:06.000+0000", "resolved": "2014-02-20T11:13:06.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1105\nSummary: config.yml has an error version number\nDescription: as reported by http://apache-spark-user-list.1001560.n3.nabble.com/question-about-compiling-SimpleApp-td1689.html the SCALA_VERSION in config.yml is wrong it should be 2.10.3", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "4f03c3f1f5c796d0e79c4b0e9b76651d", "issue_key": "SPARK-1106", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "check key name and identity file before launch a cluster", "description": "I launched an EC2 cluster without providing a key name and an identity file. The error showed up after two minutes. It would be good to check those options before launch, given the fact that EC2 billing rounds up to hours. PR: https://github.com/apache/incubator-spark/pull/617", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-02-18T18:02:55.000+0000", "updated": "2014-02-18T18:54:10.000+0000", "resolved": "2014-02-18T18:54:10.000+0000", "labels": [], "components": ["EC2"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1106\nSummary: check key name and identity file before launch a cluster\nDescription: I launched an EC2 cluster without providing a key name and an identity file. The error showed up after two minutes. It would be good to check those options before launch, given the fact that EC2 billing rounds up to hours. PR: https://github.com/apache/incubator-spark/pull/617", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "82406622f4b3bc205df8aac62c165f45", "issue_key": "SPARK-1107", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add shutdown hook on executor stop to stop running tasks", "description": "Originally reported by aash: http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3CCA%2B-p3AHXYhpjXH9fr8jQ5%2B_gc%3DNHjLbOiJB9bHSahfEET5aHBQ%40mail.gmail.com%3E Latest in thread: http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3CCA+-p3AFi7vz=2Oty3CAA0G+5EKg+A84uVqrL9TgstVgwGYB_iw@mail.gmail.com%3E The most popular approach is to add a shutdown hook that stops running tasks in the executors.", "reporter": "Andrew Ash", "assignee": null, "created": "2014-02-18T20:40:57.000+0000", "updated": "2019-05-21T05:37:17.000+0000", "resolved": "2019-05-21T05:37:17.000+0000", "labels": ["bulk-closed"], "components": ["Spark Core"], "comments": [{"author": "jobs wang", "body": "according mails, i add threadPool shutdown when kill task, is it right, it's my patch?", "created": "2014-02-21T07:00:20.257+0000"}, {"author": "Sean R. Owen", "body": "We have a shutdown hook that stops the SparkContext, which is kind of related.", "created": "2015-05-15T13:49:04.988+0000"}, {"author": "Michael Schmeißer", "body": "I added a link to an issue which is about an executor setup hook, which i think is related and should be addressed together.", "created": "2015-09-28T09:19:32.636+0000"}], "num_comments": 3, "text": "Issue: SPARK-1107\nSummary: Add shutdown hook on executor stop to stop running tasks\nDescription: Originally reported by aash: http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3CCA%2B-p3AHXYhpjXH9fr8jQ5%2B_gc%3DNHjLbOiJB9bHSahfEET5aHBQ%40mail.gmail.com%3E Latest in thread: http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3CCA+-p3AFi7vz=2Oty3CAA0G+5EKg+A84uVqrL9TgstVgwGYB_iw@mail.gmail.com%3E The most popular approach is to add a shutdown hook that stops running tasks in the executors.\n\nComments (3):\n1. jobs wang: according mails, i add threadPool shutdown when kill task, is it right, it's my patch?\n2. Sean R. Owen: We have a shutdown hook that stops the SparkContext, which is kind of related.\n3. Michael Schmeißer: I added a link to an issue which is about an executor setup hook, which i think is related and should be addressed together.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "1a970fe6a251a80ad359386b9d8d24fe", "issue_key": "SPARK-1108", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "saveAsNewAPIHadoopFile throws NPE with TableOutputFormat", "description": "When using HBase's TableOutputFormat, saveAsNewAPIHadoopFile throws NPE because the destination table is not set.", "reporter": "Bryn Keller", "assignee": "Bryn Keller", "created": "2014-02-18T23:31:06.000+0000", "updated": "2014-03-09T17:52:46.000+0000", "resolved": "2014-03-09T17:52:46.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Bryn Keller", "body": "I'll create a pull request for this tomorrow.", "created": "2014-02-18T23:33:21.833+0000"}, {"author": "Bryn Keller", "body": "PR: https://github.com/apache/incubator-spark/pull/638 Merged in https://github.com/apache/spark/commit/4d880304867b55a4f2138617b30600b7fa013b14 Still needs to be merged to branch-0.9 AFAIK", "created": "2014-02-27T22:39:39.921+0000"}], "num_comments": 2, "text": "Issue: SPARK-1108\nSummary: saveAsNewAPIHadoopFile throws NPE with TableOutputFormat\nDescription: When using HBase's TableOutputFormat, saveAsNewAPIHadoopFile throws NPE because the destination table is not set.\n\nComments (2):\n1. Bryn Keller: I'll create a pull request for this tomorrow.\n2. Bryn Keller: PR: https://github.com/apache/incubator-spark/pull/638 Merged in https://github.com/apache/spark/commit/4d880304867b55a4f2138617b30600b7fa013b14 Still needs to be merged to branch-0.9 AFAIK", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "dcd46d8526f75660261c26ed01fa8e87", "issue_key": "SPARK-1109", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "wrong API docs for pyspark map function", "description": "The source code/API docs for the pyspark RDD map function says: def map(self, f, preservesPartitioning=False): \"\"\" Return a new RDD containing the distinct elements in this RDD. \"\"\" def func(split, iterator): return imap(f, iterator) return PipelinedRDD(self, func, preservesPartitioning) I think that was incorrectly cut-and-pasted from the distinct() function, and should actually say \"Return a new RDD by applying a function to each element of this RDD.\"", "reporter": "Diana Carroll", "assignee": "Prashant Sharma", "created": "2014-02-19T09:48:29.000+0000", "updated": "2020-02-07T17:26:39.000+0000", "resolved": "2014-03-04T15:34:35.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/73", "created": "2015-12-10T15:06:19.722+0000"}], "num_comments": 1, "text": "Issue: SPARK-1109\nSummary: wrong API docs for pyspark map function\nDescription: The source code/API docs for the pyspark RDD map function says: def map(self, f, preservesPartitioning=False): \"\"\" Return a new RDD containing the distinct elements in this RDD. \"\"\" def func(split, iterator): return imap(f, iterator) return PipelinedRDD(self, func, preservesPartitioning) I think that was incorrectly cut-and-pasted from the distinct() function, and should actually say \"Return a new RDD by applying a function to each element of this RDD.\"\n\nComments (1):\n1. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/73", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "64e643597cfdb4184e37cc196662323c", "issue_key": "SPARK-1110", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Clean up and clarify use of SPARK_HOME", "description": "In the spirit of SPARK-929 we should clean up the use of SPARK_HOME and, if possible, remove it entirely. We need to look through what this is used for. One use was allowing applications to run different versions of Spark in standalone mode. For instance, someone could submit an application with a custom SPARK_HOME and the Worker would launch an Executor using a different path for Spark. This use case is not widely used and maybe should just be removed. The existing constructors that take SPARK_HOME for this purpose should be deprecated and we should explain that SPARK_HOME is no longer used for this purpose. If there are other legitimate reasons for SPARK_HOME, we can keep it around... we need to audit the uses of it.", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2014-02-19T15:31:12.000+0000", "updated": "2014-05-15T18:20:03.000+0000", "resolved": "2014-05-15T18:20:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Josh Rosen", "body": "There's some relevant discussion in https://github.com/apache/incubator-spark/pull/192.", "created": "2014-02-19T15:45:59.876+0000"}, {"author": "Patrick Wendell", "body": "This was subsumed by the other configuration clean-up.", "created": "2014-05-15T18:20:03.993+0000"}], "num_comments": 2, "text": "Issue: SPARK-1110\nSummary: Clean up and clarify use of SPARK_HOME\nDescription: In the spirit of SPARK-929 we should clean up the use of SPARK_HOME and, if possible, remove it entirely. We need to look through what this is used for. One use was allowing applications to run different versions of Spark in standalone mode. For instance, someone could submit an application with a custom SPARK_HOME and the Worker would launch an Executor using a different path for Spark. This use case is not widely used and maybe should just be removed. The existing constructors that take SPARK_HOME for this purpose should be deprecated and we should explain that SPARK_HOME is no longer used for this purpose. If there are other legitimate reasons for SPARK_HOME, we can keep it around... we need to audit the uses of it.\n\nComments (2):\n1. Josh Rosen: There's some relevant discussion in https://github.com/apache/incubator-spark/pull/192.\n2. Patrick Wendell: This was subsumed by the other configuration clean-up.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "9ea3d85a57395592a84e125cca786edf", "issue_key": "SPARK-1111", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "URL Validation Throws Error for HDFS URL's", "description": "The validation method incorrectly assumes Java's URL parser is okay with hdfs urls.", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2014-02-19T22:26:27.000+0000", "updated": "2014-03-30T04:13:36.000+0000", "resolved": "2014-02-21T11:24:33.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "jobs wang", "body": "what error？", "created": "2014-02-21T05:57:54.089+0000"}, {"author": "Aaron Davidson", "body": "The issue was that we used new URL(userURL) as a way to verify that the URL was correct, but the URL constructor throws an Exception if the protocol is hdfs://, which is not good. With [PR #625|https://github.com/apache/incubator-spark/pull/625], we instead check manually for URL-like syntax.", "created": "2014-02-21T11:24:29.249+0000"}, {"author": "jobs wang", "body": "thanks！I know！:-)", "created": "2014-02-22T06:53:33.317+0000"}], "num_comments": 3, "text": "Issue: SPARK-1111\nSummary: URL Validation Throws Error for HDFS URL's\nDescription: The validation method incorrectly assumes Java's URL parser is okay with hdfs urls.\n\nComments (3):\n1. jobs wang: what error？\n2. Aaron Davidson: The issue was that we used new URL(userURL) as a way to verify that the URL was correct, but the URL constructor throws an Exception if the protocol is hdfs://, which is not good. With [PR #625|https://github.com/apache/incubator-spark/pull/625], we instead check manually for URL-like syntax.\n3. jobs wang: thanks！I know！:-)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "a518ae74727b71bab454311d50b3d4e9", "issue_key": "SPARK-1112", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "When spark.akka.frameSize > 10, task results bigger than 10MiB block execution", "description": "When I set the spark.akka.frameSize to something over 10, the messages sent from the executors to the driver completely block the execution if the message is bigger than 10MiB and smaller than the frameSize (if it's above the frameSize, it's ok) Workaround is to set the spark.akka.frameSize to 10. In this case, since 0.8.1, the blockManager deal with the data to be sent. It seems slower than akka direct message though. The configuration seems to be correctly read (see actorSystemConfig.txt), so I don't see where the 10MiB could come from", "reporter": "Guillaume Pitel", "assignee": "Xiangrui Meng", "created": "2014-02-20T08:34:26.000+0000", "updated": "2014-12-01T08:29:53.000+0000", "resolved": "2014-07-17T04:31:18.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Kay Ousterhout", "body": "Thanks for reporting this! Does Spark hang, or does the worker throw an exception? If the former, would you mind uploading the Spark worker log, and if the latter, can you add the stack trace?", "created": "2014-02-20T15:47:07.435+0000"}, {"author": "Guillaume Pitel", "body": "No Exception, and not \"hanging\" in the bad way the executors can sometime hang : if I kill the driver, the workers receive the shutdown signal and exit cleanly. Here are the logs : DRIVER : 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:13 as 2083 bytes in 0 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:14 as TID 2294 on executor 1: t4.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:14 as 2083 bytes in 0 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:15 as TID 2295 on executor 4: t3.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:15 as 2083 bytes in 1 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:16 as TID 2296 on executor 0: t0.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:16 as 2083 bytes in 2 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:17 as TID 2297 on executor 3: t1.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:17 as 2083 bytes in 1 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:18 as TID 2298 on executor 2: t5.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:18 as 2083 bytes in 1 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:19 as TID 2299 on executor 5: t6.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:19 as 2083 bytes in 1 ms EXECUTOR : 14/02/19 15:21:53 INFO Executor: Serialized size of result for 2287 is 17229427 14/02/19 15:21:53 INFO Executor: Sending result for 2287 directly to driver 14/02/19 15:21:53 INFO Executor: Serialized size of result for 2299 is 17229262 14/02/19 15:21:53 INFO Executor: Sending result for 2299 directly to driver 14/02/19 15:21:53 INFO Executor: Finished task ID 2299 14/02/19 15:21:53 INFO Executor: Finished task ID 2287 14/02/19 15:21:53 INFO Executor: Serialized size of result for 2281 is 17229426 14/02/19 15:21:53 INFO Executor: Sending result for 2281 directly to driver 14/02/19 15:21:53 INFO Executor: Finished task ID 2281 There is a timezone difference between driver & executor", "created": "2014-02-20T23:03:43.133+0000"}, {"author": "W.M. Roshan Weerasuriya", "body": "I have a similar issue, I'm on spark-0.9.0 compiled with cdh-4.2.1 For me, serialized tasks over 10 MB do not reach executors. I've tried this with spark.akka.frameSize set to 160 and 10. The workaround suggested (setting spark.akka.frameSize to 10) does not work for me. I've confirmed that even if serialized tasks are just under 10MB, the executors do get them and the task is completed. Spark hangs. There are no exceptions or unusual ERROR/WARN/DEBUG logs in the driver, master, executor or worker daemon logs. The executors just don't seem to have received the tasks. The application UI shows the task status as running, but never progresses. Here are the last few lines from my driver and one executor: DRIVER: 14/02/21 06:37:00 INFO TaskSetManager: Finished TID 797 in 53897 ms on spark-slave01 (progress: 78/80) 14/02/21 06:37:00 INFO DAGScheduler: Completed ResultTask(9, 58) 14/02/21 06:37:08 INFO TaskSetManager: Finished TID 768 in 75767 ms on spark-slave02 (progress: 79/80) 14/02/21 06:37:08 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool 14/02/21 06:37:08 INFO DAGScheduler: Completed ResultTask(9, 69) 14/02/21 06:37:08 INFO DAGScheduler: Stage 9 (reduceByKeyLocally at SKMeans.scala:174) finished in 99.048 s 14/02/21 06:37:08 INFO SparkContext: Job finished: reduceByKeyLocally at SKMeans.scala:174, took 99.359019444 s 14/02/21 06:37:09 INFO SparkContext: Starting job: reduceByKeyLocally at SKMeans.scala:174 14/02/21 06:37:09 INFO DAGScheduler: Got job 7 (reduceByKeyLocally at SKMeans.scala:174) with 80 output partitions (allowLocal=false) 14/02/21 06:37:09 INFO DAGScheduler: Final stage: Stage 10 (reduceByKeyLocally at SKMeans.scala:174) 14/02/21 06:37:09 INFO DAGScheduler: Parents of final stage: List() 14/02/21 06:37:09 INFO DAGScheduler: Missing parents: List() 14/02/21 06:37:09 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174), which has no missing parents 14/02/21 06:37:10 INFO DAGScheduler: Submitting 80 missing tasks from Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174) 14/02/21 06:37:10 INFO TaskSchedulerImpl: Adding task set 10.0 with 80 tasks 14/02/21 06:37:10 INFO TaskSetManager: Starting task 10.0:0 as TID 800 on executor 2: spark-slave01 (PROCESS_LOCAL) 14/02/21 06:37:10 INFO TaskSetManager: Serialized task 10.0:0 as 10700743 bytes in 19 ms ...<Starting task and Serialized task lines repeat for each of 40 tasks.> EXECUTOR: spark-slave01 14/02/21 06:36:08 DEBUG Executor: Task 798's epoch is 3 14/02/21 06:36:08 DEBUG CacheManager: Looking for partition rdd_4_60 14/02/21 06:36:08 DEBUG BlockManager: Getting local block rdd_4_60 14/02/21 06:36:08 DEBUG BlockManager: Level for block rdd_4_60 is StorageLevel(false, true, true, 1) 14/02/21 06:36:08 DEBUG BlockManager: Getting block rdd_4_60 from memory 14/02/21 06:36:08 INFO BlockManager: Found block rdd_4_60 locally 14/02/21 06:36:21 INFO Executor: Serialized size of result for 765 is 1224222 14/02/21 06:36:21 INFO Executor: Sending result for 765 directly to driver 14/02/21 06:36:21 INFO Executor: Finished task ID 765 14/02/21 06:36:34 INFO Executor: Serialized size of result for 790 is 1262463 14/02/21 06:36:34 INFO Executor: Sending result for 790 directly to driver 14/02/21 06:36:34 INFO Executor: Finished task ID 790 14/02/21 06:36:37 INFO Executor: Serialized size of result for 784 is 1394816 14/02/21 06:36:37 INFO Executor: Sending result for 784 directly to driver 14/02/21 06:36:37 INFO Executor: Finished task ID 784 14/02/21 06:36:38 INFO Executor: Serialized size of result for 787 is 1409571 14/02/21 06:36:38 INFO Executor: Sending result for 787 directly to driver 14/02/21 06:36:38 INFO Executor: Finished task ID 787 14/02/21 06:36:41 INFO Executor: Serialized size of result for 798 is 1270321 14/02/21 06:36:41 INFO Executor: Sending result for 798 directly to driver 14/02/21 06:36:41 INFO Executor: Finished task ID 798 14/02/21 06:36:50 INFO Executor: Serialized size of result for 792 is 1175064 14/02/21 06:36:50 INFO Executor: Sending result for 792 directly to driver 14/02/21 06:36:50 INFO Executor: Finished task ID 792 14/02/21 06:36:52 INFO Executor: Serialized size of result for 794 is 1485354 14/02/21 06:36:52 INFO Executor: Sending result for 794 directly to driver 14/02/21 06:36:52 INFO Executor: Finished task ID 794 14/02/21 06:37:00 INFO Executor: Serialized size of result for 797 is 1615486 14/02/21 06:37:00 INFO Executor: Sending result for 797 directly to driver 14/02/21 06:37:00 INFO Executor: Finished task ID 797 Roshan", "created": "2014-02-20T23:14:31.051+0000"}, {"author": "Kay Ousterhout", "body": "Roshan, the issue you're seeing is different -- Guillaume's issue is when task results are too large to be sent using Akka (in which case Spark should use a different code path to send task results to the executor); your issue is when the task itself is too large, in which case Spark (in theory!) gives up and throw an error. We should fix both problems, but would you mind opening a separate issue?", "created": "2014-02-20T23:54:06.696+0000"}, {"author": "Kay Ousterhout", "body": "Guillaume, just to clarify, the logs you pasted above are for when you set the maximum frame size to 16MiB? I'm asking because the task results seem to be just slightly larger than 16MiB, which isn't the failure case you mentioned in your description.", "created": "2014-02-20T23:56:32.547+0000"}, {"author": "Guillaume Pitel", "body": "Sorry, I should have specified it. The frameSize was set to 512 for those logs. I've also tried with 16, and it works when results are over 16MB", "created": "2014-02-21T00:04:27.711+0000"}, {"author": "Kay Ousterhout", "body": "Cool thanks for clarifying! Looking into this...", "created": "2014-02-21T00:06:49.687+0000"}, {"author": "Guillaume Pitel", "body": "When I set BOTH the property on driver with System.setProperty(\"spark.akka.frameSize\", 128) AND I pass the env parameter to SparkContext with SPARK_JAVA_OPTS = \"-Dspark.akka.frameSize=128\" Then it seems to works. So maybe the problem comes from properties not being passed correctly to workers when executors are instanciated ? Also, I'm using packaged binary distribution for CDH4 on a standalone cluster", "created": "2014-02-21T01:04:08.692+0000"}, {"author": "W.M. Roshan Weerasuriya", "body": "Hi, I just realized my driver wasn't picking up spark.akka.frameSize value, because of a problem in the way I was passing it in. However, my executor's were picking this value correctly from their conf/spark-env.sh files. Now, both sides, the driver and executors print the correct value for frameSize with spark.akka.logAkkaConfig=true. I also noticed that simply starting the driver with java -Dspark.akka.frameSize=200 does not propagate this automatically to the executors. Not that this is an issue. I guess I was just confused about the configuration. Guillaume, seems like you have the reverse situation as mine, ie. your drivers are correctly configured with the right frameSize, but the executors are still using the 10MB default? To conclude, after ensuring that the driver is correctly configured with the right frameSize, so far, serialized tasks larger than 10MB are being received by the executors and run successfully. Roshan", "created": "2014-02-21T01:17:49.349+0000"}, {"author": "Guillaume Pitel", "body": "You're right Roshan.I was expecting the akka properties set before SparkContext creation to be propagated to the Executors (and based on what the Spark code does, it should be the case). I think it should be enforced for the whole akka stuff (timeouts and so on), as well as for the rest of spark properties", "created": "2014-02-21T05:35:36.780+0000"}, {"author": "Patrick McFadin", "body": "Hey There, I spent some time playing with this and couldn't reproduce the issue. The driver should capture the options and pass them to executors. I just tested this with a local cluster and was able to verify the akka frame size is passed to executors even if it's not set in spark-env.sh where the executor launches. [~roshan] - what happens if you remove the setting from spark-env.sh on the executors and only set it at the driver. Does that work correctly?", "created": "2014-02-21T23:33:12.526+0000"}, {"author": "Guillaume Pitel", "body": "In my code I already had some options passed in SPARK_JAVA_OPTS in the env of the SparkContext, but nothing about the akka frameSize. Could it be related ? Since it was working correctly in 0.8.1, maybe it's related to the new SparkConf ?", "created": "2014-02-22T00:16:23.984+0000"}, {"author": "Patrick McFadin", "body": "[~guillaumepitel] If you aren't setting akka.frameSize in SPARK_JAVA_OPTS then where are you setting it? What I was saying is that if you do System.setProperty(spark.akka.frameSize, XX) before you create the SparkContext it should collect this and send it to the executors correctly. One thing is if you set it after you create the SparkContext it won't work... are you doing this by any chance?", "created": "2014-02-22T00:23:20.471+0000"}, {"author": "Guillaume Pitel", "body": "No, I create the SparkContext after setting the properties (and I've nothing on my nodes for configuring the frameSize, it's a per-process configuration). So before my workaround, I was just setting the property (and the environment in the UI was showing the right value) and passing a SPARK_JAVA_OPTS to the env of the SparkContext with  System.setProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") System.setProperty(\"spark.kryo.registrator\", registrator) System.setProperty(\"spark.kryo.referenceTracking\", \"false\") System.setProperty(\"spark.kryoserializer.buffer.mb\", bufferSize.toString) System.setProperty(\"spark.locality.wait\", \"10000\") System.setProperty(\"spark.hadoop.mapreduce.output.fileoutputformat.compress\", \"true\") System.setProperty(\"spark.hadoop.mapreduce.output.fileoutputformat.compress.codec\", codec) System.setProperty(\"spark.hadoop.mapreduce.output.fileoutputformat.compress.type\", \"BLOCK\") System.setProperty(\"spark.akka.frameSize\", akkaFrameSize.toString) val sb = new StringBuilder() sb.append(\"-Dspark.storage.memoryFraction=\" + sparkMemoryFraction()) sb.append(\" -Dspark.worker.timeout=\" + sparkWorkerTimeout()) sb.append(\" -Dspark.akka.askTimeout=\" + sparkAkkaAskTimeout()) sb.append(\" -Dspark.akka.timeout=\" + sparkAkkaTimeout()) sb.append(\" -Dspark.shuffle.consolidateFiles=\" + sparkShuffleConsolidateFiles()) var env = new HashMap[String, String]() env += \"SPARK_JAVA_OPTS\" -> sb.toString() val sc = new SparkContext(sparkMaster(), appName, sparkHome(), jars(), env)  That caused a problem (the akka frame size seemed to be passed to the executor, but only after the creation of the actorSystem, because it was taking the right code path, but the akka system didn't seem to be properly configured). Now if I add this to my code :  sb.append(\" -Dspark.akka.frameSize=\" + akkaFrameSize)  It works", "created": "2014-02-22T00:39:31.475+0000"}, {"author": "W.M. Roshan Weerasuriya", "body": "@Patrick Wendell - Case 1. DRIVER: I start the driver with java -Dspark.akka.frameSize=200 -D... -cp ..... EXECUTOR: spark-env.sh on workers with frameSize specified: export SPARK_JAVA_OPTS='-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200' Executor akka config log reports frameSize=160 Case 2. DRIVER: I start the driver with java -Dspark.akka.frameSize=200 -D... -cp .... EXECUTOR: spark-env.sh on workers with frameSize NOT specified: export SPARK_JAVA_OPTS='-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200' Executor akka config log on workers has no frameSize and executor process(by ps waux) has no frameSize. Here, I suppose it picks up the default 10MB. Case 3. DRIVER: This time I export SPARK_JAVA_OPTS=\"-Dspark.akka.frameSize=220\" on the dirver host, before running the driver with java -Dspark.akka.frameSize=200 -D... -cp .... I'm not reading SPARK_JAVA_OPTS in the driver code. Driver's Akka config log, says frameSize=200. EXECUTOR1 on HOST1: No frameSize specified in spark-env Executor akka config log says frameSize=220. EXECUTOR2 on HOST2: frameSize=160 in spark-env Executor akka config log says frameSize=220, ps waux shows the process was started with this cmd: /usr/java/default/bin/java -cp sparkJar.jar -Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200 -Dspark.akka.frameSize=220 -Xms3072M -Xmx3072M org.apache.spark.executor.CoarseGrainedExecutorBackend ..... So the SPARK_JAVA_OPTS from both the executor and the driver are appended, but since java overrides the first frameSize with the second, the executor runs with frameSize=220 Case 4. DRIVER: This time I export SPARK_JAVA_OPTS=\"-Dspark.akka.frameSize=220\" on the driver host, but don't specify frameSize in the launch command. Again, I don't SPARK_JAVA_OPTS in the driver code. No frameSize in driver's akka config log. I expect driver picks up the default 10MB frameSize. Executors are the same as in case3. What I'm seeing is that, you can specify the frameSize for a driver in the launch command as java -Dspark.akka.frameSize property, and for the executors in their respective spark-env.sh. If you specify SPARK_JAVA_OPTS on the driver side, then this will override the value for the executors, but not for the driver. I don't use spark-class.sh to launch my driver, because somewhere I read that its meant for spark internal classes and examples. I also currently build my SparkContext directly, without using SparkConf. This not an issue for me any longer. That being said, it would have saved me loads of time, if the logs had provided some indication that sending serialized tasks to the executors had failed because they were larger than 10MB. Also, the documentation could be a bit clearer about what is set where and which property overrides or is overridden. Thanks.", "created": "2014-02-22T03:58:52.640+0000"}, {"author": "Kevin (Sangwoo) Kim", "body": "Hi all, I'm very new to Spark and doing some tests, I've experienced similar issue. (tested with Spark Shell, 0.9.1, r3.8xlarge instance on EC2 - 32 core / 244GiB MEM) I was trying to broadcast 700MB of data and Spark hangs when I run collect() method for the data. Here's the strange things : 1) when I tried val userInfo = sc.textFile(\"file:///spark/logs/user_sign_up2.csv\").map{line => val split = line.split(\",\"); (split(1), split)} val userInfoMap = userInfo.collectAsMap  it runs well. 2) when I tried val userInfo = sc.textFile(\"file:///spark/logs/user_sign_up2.csv\").map{line => val split = line.split(\",\"); (split(1), split(5))} val userInfoMap = userInfo.collectAsMap  Spark hangs. 3) when I slightly control the data size using sample() method or cutting the data file, it runs well. Our team investigated logs from master and worker then we found worker finished all tasks but master couldn't retrieve the result from a task the result size larger than 10MB We tried to apply the workaround setting spark.akka.frameSize to 9, it works like a charm. I guess it might hard to reproduce the issue, please contact me if there's need of testing or getting logs. Thanks!", "created": "2014-05-29T01:59:32.583+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I'm curious, why did you want to make the frameSize this big -- are the tasks themselves also big or just the results? There might be other buffers in Akka that can't be made bigger than this. It's possible that this changed in a newer Akka version (because larger frame sizes used to work before).", "created": "2014-05-29T02:17:56.950+0000"}, {"author": "Kevin (Sangwoo) Kim", "body": "[~matei] I've found the default of spark.akka.frameSize is 10 from the config document, http://spark.apache.org/docs/0.9.1/configuration.html just tried to slightly larger and smaller (11 and 9) values. I did collect() method on the userInfo and it might contains large data. (edited the first comment.)", "created": "2014-05-29T02:49:45.128+0000"}, {"author": "Chen Jin", "body": "[~matei] Do you know which akka version we should use to be able to use big frame size.", "created": "2014-06-13T23:00:20.620+0000"}, {"author": "Chen Jin", "body": "To follow up this thread, I have done some experiments when the frameSize is around 10MB . 1) spark.akka.frameSize = 10 If one of the partition size is very close to 10MB, say 9.97MB, the execution blocks without any exception or warning. Worker finished the task to send the serialized result, and then throw exception saying hadoop IPC client connection stops (changing the logging to debug level). However, the master never receives the results and the program just hangs. But if sizes for all the partitions less than some number btw 9.96MB amd 9.97MB, the program works fine. 2) spark.akka.frameSize = 9 when the partition size is just a little bit smaller than 9MB, it fails as well. This bug behavior is not exactly what spark-1112 is about, could you please guide me how to open a separate bug when the serialization size is very close to 10MB. Thanks a lot", "created": "2014-06-16T20:09:57.797+0000"}, {"author": "Chen Jin", "body": "I have filed a bug https://issues.apache.org/jira/browse/SPARK-2156", "created": "2014-06-19T00:10:33.592+0000"}, {"author": "Patrick Wendell", "body": "We were able to reproduce this - thanks for reporting it.", "created": "2014-06-19T01:38:18.592+0000"}, {"author": "Chen Jin", "body": "Awesome, looking forward to the fix. At least better error or exception message would be helpful.", "created": "2014-06-19T02:06:31.905+0000"}, {"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/spark/pull/1124", "created": "2014-06-19T02:35:30.196+0000"}, {"author": "Patrick Wendell", "body": "This is fixed in the 1.0 branch via: https://github.com/apache/spark/pull/1172", "created": "2014-06-23T02:49:36.023+0000"}, {"author": "Patrick Wendell", "body": "Fixed in 1.1.0 via: https://github.com/apache/spark/pull/1132", "created": "2014-06-25T02:06:52.586+0000"}, {"author": "Bharath Ravi Kumar", "body": "Can a clear workaround be specified for this bug please? For those unable to upgrade to run on 1.0.1 or 1.1.0 in production, general instructions on the workaround are required. This is a huge blocker for current production deployments (even on 1.0.0) otherwise. For instance, running a saveAsTextFile() on an RDD (~400MB) causes execution to freeze with the last log statements seen on the driver being: 14/06/25 16:38:55 INFO spark.SparkContext: Starting job: saveAsTextFile at Test.java:99 14/06/25 16:38:55 INFO scheduler.DAGScheduler: Got job 6 (saveAsTextFile at Test.java:99) with 2 output partitions (allowLocal=false) 14/06/25 16:38:55 INFO scheduler.DAGScheduler: Final stage: Stage 6(saveAsTextFile at Test.java:99) 14/06/25 16:38:55 INFO scheduler.DAGScheduler: Parents of final stage: List() 14/06/25 16:38:55 INFO scheduler.DAGScheduler: Missing parents: List() 14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting Stage 6 (MappedRDD[558] at saveAsTextFile at Test.java:99), which has no missing parents 14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 6 (MappedRDD[558] at saveAsTextFile at Test.java:99) 14/06/25 16:38:55 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks 14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:0 as TID 5 on executor 1: somehost.corp (PROCESS_LOCAL) 14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:0 as 351777 bytes in 36 ms 14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:1 as TID 6 on executor 0: someotherhost.corp (PROCESS_LOCAL) 14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:1 as 186453 bytes in 16 ms The test setup for reproducing this issue has two slaves (each with 24G) running spark standalone. The driver runs with Xmx 4G. Thanks.", "created": "2014-06-25T16:48:06.251+0000"}, {"author": "Patrick Wendell", "body": "[~reachbach] If you are running on standalone mode, it might work if you go on every node in your cluster and add the following to spark-env.sh:  export SPARK_JAVA_OPTS=\"-Dspark.akka.frameSize=XXX\"  However, this work around will only work if every job in your cluster is using the same frame size (XXX). The main recommendation is to upgrade to 1.0.1. We are very conservative about what we merge into maintenance branches, so we recommend users upgrade immediately once we release them.", "created": "2014-06-25T16:57:54.687+0000"}, {"author": "Patrick Wendell", "body": "This is not resolved yet because it needs to be back ported into 0.9", "created": "2014-06-25T16:59:17.584+0000"}, {"author": "Xiangrui Meng", "body": "PR for branch-0.9: https://github.com/apache/spark/pull/1455", "created": "2014-07-17T04:01:52.734+0000"}, {"author": "Xiangrui Meng", "body": "Issue resolved by pull request 1455 [https://github.com/apache/spark/pull/1455]", "created": "2014-07-17T04:31:18.367+0000"}, {"author": "DjvuLee", "body": "Does anyone test in version0.9.2，I found it also failed , while v1.0.1 & v1.1.0 is ok.", "created": "2014-07-24T14:42:11.531+0000"}, {"author": "Josh Rosen", "body": "Looks like the \"Fix Versions\" accidentally got overwritten during a backport / cherry-pick, so I've restored them based on the issue history.", "created": "2014-12-01T08:29:53.087+0000"}], "num_comments": 33, "text": "Issue: SPARK-1112\nSummary: When spark.akka.frameSize > 10, task results bigger than 10MiB block execution\nDescription: When I set the spark.akka.frameSize to something over 10, the messages sent from the executors to the driver completely block the execution if the message is bigger than 10MiB and smaller than the frameSize (if it's above the frameSize, it's ok) Workaround is to set the spark.akka.frameSize to 10. In this case, since 0.8.1, the blockManager deal with the data to be sent. It seems slower than akka direct message though. The configuration seems to be correctly read (see actorSystemConfig.txt), so I don't see where the 10MiB could come from\n\nComments (33):\n1. Kay Ousterhout: Thanks for reporting this! Does Spark hang, or does the worker throw an exception? If the former, would you mind uploading the Spark worker log, and if the latter, can you add the stack trace?\n2. Guillaume Pitel: No Exception, and not \"hanging\" in the bad way the executors can sometime hang : if I kill the driver, the workers receive the shutdown signal and exit cleanly. Here are the logs : DRIVER : 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:13 as 2083 bytes in 0 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:14 as TID 2294 on executor 1: t4.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:14 as 2083 bytes in 0 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:15 as TID 2295 on executor 4: t3.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:15 as 2083 bytes in 1 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:16 as TID 2296 on executor 0: t0.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:16 as 2083 bytes in 2 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:17 as TID 2297 on executor 3: t1.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:17 as 2083 bytes in 1 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:18 as TID 2298 on executor 2: t5.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:18 as 2083 bytes in 1 ms 14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:19 as TID 2299 on executor 5: t6.exensa.loc (PROCESS_LOCAL) 14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:19 as 2083 bytes in 1 ms EXECUTOR : 14/02/19 15:21:53 INFO Executor: Serialized size of result for 2287 is 17229427 14/02/19 15:21:53 INFO Executor: Sending result for 2287 directly to driver 14/02/19 15:21:53 INFO Executor: Serialized size of result for 2299 is 17229262 14/02/19 15:21:53 INFO Executor: Sending result for 2299 directly to driver 14/02/19 15:21:53 INFO Executor: Finished task ID 2299 14/02/19 15:21:53 INFO Executor: Finished task ID 2287 14/02/19 15:21:53 INFO Executor: Serialized size of result for 2281 is 17229426 14/02/19 15:21:53 INFO Executor: Sending result for 2281 directly to driver 14/02/19 15:21:53 INFO Executor: Finished task ID 2281 There is a timezone difference between driver & executor\n3. W.M. Roshan Weerasuriya: I have a similar issue, I'm on spark-0.9.0 compiled with cdh-4.2.1 For me, serialized tasks over 10 MB do not reach executors. I've tried this with spark.akka.frameSize set to 160 and 10. The workaround suggested (setting spark.akka.frameSize to 10) does not work for me. I've confirmed that even if serialized tasks are just under 10MB, the executors do get them and the task is completed. Spark hangs. There are no exceptions or unusual ERROR/WARN/DEBUG logs in the driver, master, executor or worker daemon logs. The executors just don't seem to have received the tasks. The application UI shows the task status as running, but never progresses. Here are the last few lines from my driver and one executor: DRIVER: 14/02/21 06:37:00 INFO TaskSetManager: Finished TID 797 in 53897 ms on spark-slave01 (progress: 78/80) 14/02/21 06:37:00 INFO DAGScheduler: Completed ResultTask(9, 58) 14/02/21 06:37:08 INFO TaskSetManager: Finished TID 768 in 75767 ms on spark-slave02 (progress: 79/80) 14/02/21 06:37:08 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool 14/02/21 06:37:08 INFO DAGScheduler: Completed ResultTask(9, 69) 14/02/21 06:37:08 INFO DAGScheduler: Stage 9 (reduceByKeyLocally at SKMeans.scala:174) finished in 99.048 s 14/02/21 06:37:08 INFO SparkContext: Job finished: reduceByKeyLocally at SKMeans.scala:174, took 99.359019444 s 14/02/21 06:37:09 INFO SparkContext: Starting job: reduceByKeyLocally at SKMeans.scala:174 14/02/21 06:37:09 INFO DAGScheduler: Got job 7 (reduceByKeyLocally at SKMeans.scala:174) with 80 output partitions (allowLocal=false) 14/02/21 06:37:09 INFO DAGScheduler: Final stage: Stage 10 (reduceByKeyLocally at SKMeans.scala:174) 14/02/21 06:37:09 INFO DAGScheduler: Parents of final stage: List() 14/02/21 06:37:09 INFO DAGScheduler: Missing parents: List() 14/02/21 06:37:09 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174), which has no missing parents 14/02/21 06:37:10 INFO DAGScheduler: Submitting 80 missing tasks from Stage 10 (MapPartitionsRDD[28] at reduceByKeyLocally at SKMeans.scala:174) 14/02/21 06:37:10 INFO TaskSchedulerImpl: Adding task set 10.0 with 80 tasks 14/02/21 06:37:10 INFO TaskSetManager: Starting task 10.0:0 as TID 800 on executor 2: spark-slave01 (PROCESS_LOCAL) 14/02/21 06:37:10 INFO TaskSetManager: Serialized task 10.0:0 as 10700743 bytes in 19 ms ...<Starting task and Serialized task lines repeat for each of 40 tasks.> EXECUTOR: spark-slave01 14/02/21 06:36:08 DEBUG Executor: Task 798's epoch is 3 14/02/21 06:36:08 DEBUG CacheManager: Looking for partition rdd_4_60 14/02/21 06:36:08 DEBUG BlockManager: Getting local block rdd_4_60 14/02/21 06:36:08 DEBUG BlockManager: Level for block rdd_4_60 is StorageLevel(false, true, true, 1) 14/02/21 06:36:08 DEBUG BlockManager: Getting block rdd_4_60 from memory 14/02/21 06:36:08 INFO BlockManager: Found block rdd_4_60 locally 14/02/21 06:36:21 INFO Executor: Serialized size of result for 765 is 1224222 14/02/21 06:36:21 INFO Executor: Sending result for 765 directly to driver 14/02/21 06:36:21 INFO Executor: Finished task ID 765 14/02/21 06:36:34 INFO Executor: Serialized size of result for 790 is 1262463 14/02/21 06:36:34 INFO Executor: Sending result for 790 directly to driver 14/02/21 06:36:34 INFO Executor: Finished task ID 790 14/02/21 06:36:37 INFO Executor: Serialized size of result for 784 is 1394816 14/02/21 06:36:37 INFO Executor: Sending result for 784 directly to driver 14/02/21 06:36:37 INFO Executor: Finished task ID 784 14/02/21 06:36:38 INFO Executor: Serialized size of result for 787 is 1409571 14/02/21 06:36:38 INFO Executor: Sending result for 787 directly to driver 14/02/21 06:36:38 INFO Executor: Finished task ID 787 14/02/21 06:36:41 INFO Executor: Serialized size of result for 798 is 1270321 14/02/21 06:36:41 INFO Executor: Sending result for 798 directly to driver 14/02/21 06:36:41 INFO Executor: Finished task ID 798 14/02/21 06:36:50 INFO Executor: Serialized size of result for 792 is 1175064 14/02/21 06:36:50 INFO Executor: Sending result for 792 directly to driver 14/02/21 06:36:50 INFO Executor: Finished task ID 792 14/02/21 06:36:52 INFO Executor: Serialized size of result for 794 is 1485354 14/02/21 06:36:52 INFO Executor: Sending result for 794 directly to driver 14/02/21 06:36:52 INFO Executor: Finished task ID 794 14/02/21 06:37:00 INFO Executor: Serialized size of result for 797 is 1615486 14/02/21 06:37:00 INFO Executor: Sending result for 797 directly to driver 14/02/21 06:37:00 INFO Executor: Finished task ID 797 Roshan\n4. Kay Ousterhout: Roshan, the issue you're seeing is different -- Guillaume's issue is when task results are too large to be sent using Akka (in which case Spark should use a different code path to send task results to the executor); your issue is when the task itself is too large, in which case Spark (in theory!) gives up and throw an error. We should fix both problems, but would you mind opening a separate issue?\n5. Kay Ousterhout: Guillaume, just to clarify, the logs you pasted above are for when you set the maximum frame size to 16MiB? I'm asking because the task results seem to be just slightly larger than 16MiB, which isn't the failure case you mentioned in your description.\n6. Guillaume Pitel: Sorry, I should have specified it. The frameSize was set to 512 for those logs. I've also tried with 16, and it works when results are over 16MB\n7. Kay Ousterhout: Cool thanks for clarifying! Looking into this...\n8. Guillaume Pitel: When I set BOTH the property on driver with System.setProperty(\"spark.akka.frameSize\", 128) AND I pass the env parameter to SparkContext with SPARK_JAVA_OPTS = \"-Dspark.akka.frameSize=128\" Then it seems to works. So maybe the problem comes from properties not being passed correctly to workers when executors are instanciated ? Also, I'm using packaged binary distribution for CDH4 on a standalone cluster\n9. W.M. Roshan Weerasuriya: Hi, I just realized my driver wasn't picking up spark.akka.frameSize value, because of a problem in the way I was passing it in. However, my executor's were picking this value correctly from their conf/spark-env.sh files. Now, both sides, the driver and executors print the correct value for frameSize with spark.akka.logAkkaConfig=true. I also noticed that simply starting the driver with java -Dspark.akka.frameSize=200 does not propagate this automatically to the executors. Not that this is an issue. I guess I was just confused about the configuration. Guillaume, seems like you have the reverse situation as mine, ie. your drivers are correctly configured with the right frameSize, but the executors are still using the 10MB default? To conclude, after ensuring that the driver is correctly configured with the right frameSize, so far, serialized tasks larger than 10MB are being received by the executors and run successfully. Roshan\n10. Guillaume Pitel: You're right Roshan.I was expecting the akka properties set before SparkContext creation to be propagated to the Executors (and based on what the Spark code does, it should be the case). I think it should be enforced for the whole akka stuff (timeouts and so on), as well as for the rest of spark properties", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "13f36dd9cdc6d1cedb5717ffe255b166", "issue_key": "SPARK-1113", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "External Spilling Bug - hash collision causes NoSuchElementException", "description": "When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int.MAX_VALUE signifies that the corresponding StreamBuffer is empty. However, Int.MAX_VALUE is a perfectly legitimate hash value. If there exists a key with this value, then ExternalAppendOnlyMap does not differentiate between empty StreamBuffers and StreamBuffers that contain only this key. As a result, a NoSuchElementException is thrown - https://github.com/apache/incubator-spark/blob/95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L304. java.util.NoSuchElementException (java.util.NoSuchElementException) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:277) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:212) org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)", "reporter": "Andrew Or", "assignee": "Andrew Or", "created": "2014-02-20T15:53:19.000+0000", "updated": "2014-03-09T17:43:00.000+0000", "resolved": "2014-02-22T17:00:44.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Andrew Or", "body": "PR opened at https://github.com/apache/incubator-spark/pull/624", "created": "2014-02-20T19:21:02.700+0000"}], "num_comments": 1, "text": "Issue: SPARK-1113\nSummary: External Spilling Bug - hash collision causes NoSuchElementException\nDescription: When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int.MAX_VALUE signifies that the corresponding StreamBuffer is empty. However, Int.MAX_VALUE is a perfectly legitimate hash value. If there exists a key with this value, then ExternalAppendOnlyMap does not differentiate between empty StreamBuffers and StreamBuffers that contain only this key. As a result, a NoSuchElementException is thrown - https://github.com/apache/incubator-spark/blob/95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L304. java.util.NoSuchElementException (java.util.NoSuchElementException) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:277) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:212) org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)\n\nComments (1):\n1. Andrew Or: PR opened at https://github.com/apache/incubator-spark/pull/624", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "61a5980e037cb40d9449090d587d8964", "issue_key": "SPARK-1114", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Patch to allow PySpark to use existing JVM and Gateway", "description": "Changes to PySpark implementation of SparkConf to take existing SparkConf JVM handle. Change to PySpark SparkContext to allow subclass specific context initialization. https://github.com/apache/incubator-spark/pull/622", "reporter": "Ahir Reddy", "assignee": "Ahir Reddy", "created": "2014-02-20T18:04:15.000+0000", "updated": "2014-02-20T21:22:02.000+0000", "resolved": "2014-02-20T21:22:02.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1114\nSummary: Patch to allow PySpark to use existing JVM and Gateway\nDescription: Changes to PySpark implementation of SparkConf to take existing SparkConf JVM handle. Change to PySpark SparkContext to allow subclass specific context initialization. https://github.com/apache/incubator-spark/pull/622", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "366388089957fbb36b962c41ded27996", "issue_key": "SPARK-1115", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Better error message when python worker process dies", "description": "Right now all I see is:  java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:392) at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:177) at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:55) at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:42) at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:89) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:53) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109) at org.apache.spark.scheduler.Task.run(Task.scala:53) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:724)", "reporter": "Sanford Ryza", "assignee": "Bouke van der Bijl", "created": "2014-02-21T01:36:30.000+0000", "updated": "2014-02-26T14:58:34.000+0000", "resolved": "2014-02-26T14:58:34.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "Fixed by https://github.com/apache/incubator-spark/pull/644", "created": "2014-02-26T14:58:34.940+0000"}], "num_comments": 1, "text": "Issue: SPARK-1115\nSummary: Better error message when python worker process dies\nDescription: Right now all I see is:  java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:392) at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:177) at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:55) at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:42) at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:89) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:53) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109) at org.apache.spark.scheduler.Task.run(Task.scala:53) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:724)\n\nComments (1):\n1. Josh Rosen: Fixed by https://github.com/apache/incubator-spark/pull/644", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "81b083ce79bb6af9f55181cf3dcf861d", "issue_key": "SPARK-1116", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution.", "description": "The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution. Replicate by: 1. executing a ./make-distribution.sh 2. copy the generated tar.gz file to a directory and extract. 3. cd into the spark directory that was expanded. e.g. spark-0.9.0-incubating 4. execute ./bin/spark-shell. Cause: The ./bin/spark-shell is unable to find the Main class to start and has no reference to a CLASSPATH. The problem is generated by the spark-class shell that fails to setup a CLASSPATH if Spark is a RELEASE.", "reporter": "Bernardo Gomez Palacio", "assignee": null, "created": "2014-02-21T02:07:07.000+0000", "updated": "2014-02-22T23:04:37.000+0000", "resolved": "2014-02-22T23:04:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Bernardo Gomez Palacio", "body": "Fix provided under: https://github.com/apache/incubator-spark/pull/627", "created": "2014-02-21T02:08:38.034+0000"}], "num_comments": 1, "text": "Issue: SPARK-1116\nSummary: The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution.\nDescription: The spark-shell will fail to start when Spark is deployed using the tar.gz file built by ./make-distribution. Replicate by: 1. executing a ./make-distribution.sh 2. copy the generated tar.gz file to a directory and extract. 3. cd into the spark directory that was expanded. e.g. spark-0.9.0-incubating 4. execute ./bin/spark-shell. Cause: The ./bin/spark-shell is unable to find the Main class to start and has no reference to a CLASSPATH. The problem is generated by the spark-class shell that fails to setup a CLASSPATH if Spark is a RELEASE.\n\nComments (1):\n1. Bernardo Gomez Palacio: Fix provided under: https://github.com/apache/incubator-spark/pull/627", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "295beaad412b9ec9ab8e71d16bcc605b", "issue_key": "SPARK-1144", "issue_type": "Improvement", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Run Apache RAT In SBT Build to Catch License Errors", "description": "For reference look at how Kafka used to do this: https://github.com/apache/kafka/commit/4ad98872ad25b93a10c368edc77b59ed4032d3af", "reporter": "Patrick Wendell", "assignee": "Prashant Sharma", "created": "2014-02-21T09:48:07.000+0000", "updated": "2020-11-19T04:06:55.000+0000", "resolved": "2014-03-24T08:45:13.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Patrick McFadin", "body": "Also it might be good to e-mail the kafka list about this, because it looks like this isn't still in use in their code (maybe they came up with a better way).", "created": "2014-02-21T10:30:44.235+0000"}, {"author": "Prashant Sharma", "body": "Looks like they are still using it and just that they think having a special target for it is pointless, since it is callable simply as java -jar rat.jar. ------Message attached----- Hi Prashant , rat is still run as part of the release process https://cwiki.apache.org/confluence/display/KAFKA/Release+Process (this document is going to change in the next day or so for the 0.8.1 release now that we moved to Gradle and away from SBT, fyi).... Gradle also has a license mechanism too (btw) that we use now in the build script https://github.com/apache/kafka/blob/0.8.1/build.gradle#L30 We removed the bash script to run rat (easy enough to run it by command line)... It didn't make sense to have a bash script that is used infrequently by a small few for what is a one line of command to run anyways... I don't think we are going to need it anymore but I will still run it prior to release. /******************************************* Joe Stein Founder, Principal Consultant Big Data Open Source Security LLC http://www.stealth.ly Twitter: @allthingshadoop <http://www.twitter.com/allthingshadoop> ********************************************/ On Wed, Feb 26, 2014 at 5:23 AM, Prashant Sharma <scrapcodes@gmail.com>wrote: > Hi all, > > I am one of the contributors to Apache Spark and was looking into RAT for > catching licensing errors in the codebase. From the git log it is clear > KAFKA once had Apache RAT in the build and was later removed. It is not > clear what was the alternative taken. This information would be helpful, in > the sense we can learn from your experience. > > -- > Prashant >", "created": "2014-02-26T22:17:45.123+0000"}, {"author": "Prashant Sharma", "body": "we can use an equivalent of that gradle plugin. https://github.com/Banno/sbt-license-plugin (I doubt if they work properly !)", "created": "2014-02-26T23:27:18.219+0000"}, {"author": "Patrick McFadin", "body": "Hey [~prashant] I think for this one we want to use RAT because it has Apache-specific guidelines. I actually think it's fine to just run RAT from a script provided that we can configure it with excludes and such. I think RAT supports doing this using an XML file even if you aren't using a RAT plug-in for the build tool. It's fine if this is isolated from the build (actually maybe that's better since we don't need to worry about it separately from maven and sbt), we can just call it in the test script when merging pull requests.", "created": "2014-02-26T23:31:04.864+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/125", "created": "2015-12-10T15:07:04.839+0000"}], "num_comments": 5, "text": "Issue: SPARK-1144\nSummary: Run Apache RAT In SBT Build to Catch License Errors\nDescription: For reference look at how Kafka used to do this: https://github.com/apache/kafka/commit/4ad98872ad25b93a10c368edc77b59ed4032d3af\n\nComments (5):\n1. Patrick McFadin: Also it might be good to e-mail the kafka list about this, because it looks like this isn't still in use in their code (maybe they came up with a better way).\n2. Prashant Sharma: Looks like they are still using it and just that they think having a special target for it is pointless, since it is callable simply as java -jar rat.jar. ------Message attached----- Hi Prashant , rat is still run as part of the release process https://cwiki.apache.org/confluence/display/KAFKA/Release+Process (this document is going to change in the next day or so for the 0.8.1 release now that we moved to Gradle and away from SBT, fyi).... Gradle also has a license mechanism too (btw) that we use now in the build script https://github.com/apache/kafka/blob/0.8.1/build.gradle#L30 We removed the bash script to run rat (easy enough to run it by command line)... It didn't make sense to have a bash script that is used infrequently by a small few for what is a one line of command to run anyways... I don't think we are going to need it anymore but I will still run it prior to release. /******************************************* Joe Stein Founder, Principal Consultant Big Data Open Source Security LLC http://www.stealth.ly Twitter: @allthingshadoop <http://www.twitter.com/allthingshadoop> ********************************************/ On Wed, Feb 26, 2014 at 5:23 AM, Prashant Sharma <scrapcodes@gmail.com>wrote: > Hi all, > > I am one of the contributors to Apache Spark and was looking into RAT for > catching licensing errors in the codebase. From the git log it is clear > KAFKA once had Apache RAT in the build and was later removed. It is not > clear what was the alternative taken. This information would be helpful, in > the sense we can learn from your experience. > > -- > Prashant >\n3. Prashant Sharma: we can use an equivalent of that gradle plugin. https://github.com/Banno/sbt-license-plugin (I doubt if they work properly !)\n4. Patrick McFadin: Hey [~prashant] I think for this one we want to use RAT because it has Apache-specific guidelines. I actually think it's fine to just run RAT from a script provided that we can configure it with excludes and such. I think RAT supports doing this using an XML file even if you aren't using a RAT plug-in for the build tool. It's fine if this is isolated from the build (actually maybe that's better since we don't need to worry about it separately from maven and sbt), we can just call it in the test script when merging pull requests.\n5. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/125", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "e24b7fdc309c130b1df4c1c1cfa1c1a1", "issue_key": "SPARK-1117", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Update accumulator docs", "description": "The current doc hints spark doesn't support accumulators of type `Long`, which is wrong.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-02-21T14:23:21.000+0000", "updated": "2014-02-21T22:45:26.000+0000", "resolved": "2014-02-21T22:45:26.000+0000", "labels": [], "components": [], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/incubator-spark/pull/631", "created": "2014-02-21T14:24:29.694+0000"}], "num_comments": 1, "text": "Issue: SPARK-1117\nSummary: Update accumulator docs\nDescription: The current doc hints spark doesn't support accumulators of type `Long`, which is wrong.\n\nComments (1):\n1. Xiangrui Meng: PR: https://github.com/apache/incubator-spark/pull/631", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "32405b2d2b0dd0eefe09dc1b4f3d59c8", "issue_key": "SPARK-1118", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Executor state shows as KILLED even the application is finished normally", "description": "This seems weird, ExecutorState has no option of FINISHED, a terminated executor can only be KILLED, FAILED, LOST", "reporter": "Nan Zhu", "assignee": "Kan Zhang", "created": "2014-02-21T19:12:03.000+0000", "updated": "2014-06-05T21:33:01.000+0000", "resolved": "2014-05-06T19:10:40.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "just a hint: the key to resolve this is to find out the reason of generating DisassociatedEvent~ maybe we should add a new deploymessage to send out before the driver exits normally", "created": "2014-02-22T21:27:54.457+0000"}], "num_comments": 1, "text": "Issue: SPARK-1118\nSummary: Executor state shows as KILLED even the application is finished normally\nDescription: This seems weird, ExecutorState has no option of FINISHED, a terminated executor can only be KILLED, FAILED, LOST\n\nComments (1):\n1. Nan Zhu: just a hint: the key to resolve this is to find out the reason of generating DisassociatedEvent~ maybe we should add a new deploymessage to send out before the driver exits normally", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "e5ce44a1ab1a2b2ddf0533bdbdcdc07e", "issue_key": "SPARK-1119", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Make examples and assembly jar naming consistent between maven/sbt", "description": "Right now it's somewhat different. Also it should be consistent with what the classpath and example scripts calculate. This makes them consistent: spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar spark-examples-1.0.0-SNAPSHOT-hadoop1.0.4.jar", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "created": "2014-02-21T22:37:15.000+0000", "updated": "2014-04-23T17:21:30.000+0000", "resolved": "2014-04-23T17:20:20.000+0000", "labels": [], "components": ["Build"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1119\nSummary: Make examples and assembly jar naming consistent between maven/sbt\nDescription: Right now it's somewhat different. Also it should be consistent with what the classpath and example scripts calculate. This makes them consistent: spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar spark-examples-1.0.0-SNAPSHOT-hadoop1.0.4.jar", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "46064655d435f1320fc40fc4ac5b12bb", "issue_key": "SPARK-1120", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Send all dependency logging through slf4j", "description": "There are a few dependencies that pull in other logging frameworks which don't get routed correctly. We should include the relevant slf4j adapters and exclude those logging libraries.", "reporter": "Patrick McFadin", "assignee": "Sean R. Owen", "created": "2014-02-22T19:50:20.000+0000", "updated": "2015-01-15T09:08:40.000+0000", "resolved": "2014-02-22T19:51:04.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Oops there is already an issue for this", "created": "2014-02-22T19:51:04.192+0000"}], "num_comments": 1, "text": "Issue: SPARK-1120\nSummary: Send all dependency logging through slf4j\nDescription: There are a few dependencies that pull in other logging frameworks which don't get routed correctly. We should include the relevant slf4j adapters and exclude those logging libraries.\n\nComments (1):\n1. Patrick McFadin: Oops there is already an issue for this", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "96f2a32d2f4831c77fe1e26175f5a505", "issue_key": "SPARK-1121", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Only add avro if the build is for Hadoop 0.23.X and SPARK_YARN is set", "description": "The reason why this is needed is that in the 0.23.X versions of hadoop-client the avro dependency is fully excluded: http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/0.23.10/hadoop-client-0.23.10.pom In later versions 2.2.X the avro dependency is correctly inherited from hadoop-common: http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.pom So as a workaround Spark currently depends on Avro directly in the sbt and scala builds. This is a bit ugly so I'd like to propose the following: 1. In the Maven build just remove avro and make a note on the building-with-maven page that they will need to manually add avro for this build. 2. On sbt only add the avro dependency if the version is 0.23.X and SPARK_YARN is true. Also we only need to add avro not both {avro, avro-ipc} like is there now.", "reporter": "Patrick McFadin", "assignee": "Prashant Sharma", "created": "2014-02-22T21:01:15.000+0000", "updated": "2020-02-07T17:26:40.000+0000", "resolved": "2014-03-02T15:18:49.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Thomas Graves", "body": "The dependencies couldn't be added in maven through the profiles? This is really user unfriendly for anyone using maven with hadoop 0.23.", "created": "2014-02-28T06:55:57.565+0000"}, {"author": "Sanford Ryza", "body": "Was this change tested against maven with YARN on Hadoop 2.2? I'm running into: [ERROR] The project org.apache.spark:yarn-parent_2.10:1.0.0-incubating-SNAPSHOT (/home/sandy/spark/spark/yarn/pom.xml) has 2 errors [ERROR] 'dependencies.dependency.version' for org.apache.avro:avro:jar is missing. @ line 55, column 17 [ERROR] 'dependencies.dependency.version' for org.apache.avro:avro-ipc:jar is missing. @ line 59, column 17", "created": "2014-02-28T09:09:44.664+0000"}, {"author": "Patrick McFadin", "body": "I'll fix this today sorry about that. --- sent from my phone On Feb 28, 2014 9:11 AM, \"Sandy Ryza (JIRA)\" <", "created": "2014-02-28T09:37:21.796+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/6", "created": "2015-12-10T15:06:18.458+0000"}, {"author": "Apache Spark", "body": "User 'pwendell' has created a pull request for this issue: https://github.com/apache/spark/pull/37", "created": "2016-07-29T22:49:05.547+0000"}], "num_comments": 5, "text": "Issue: SPARK-1121\nSummary: Only add avro if the build is for Hadoop 0.23.X and SPARK_YARN is set\nDescription: The reason why this is needed is that in the 0.23.X versions of hadoop-client the avro dependency is fully excluded: http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/0.23.10/hadoop-client-0.23.10.pom In later versions 2.2.X the avro dependency is correctly inherited from hadoop-common: http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.pom So as a workaround Spark currently depends on Avro directly in the sbt and scala builds. This is a bit ugly so I'd like to propose the following: 1. In the Maven build just remove avro and make a note on the building-with-maven page that they will need to manually add avro for this build. 2. On sbt only add the avro dependency if the version is 0.23.X and SPARK_YARN is true. Also we only need to add avro not both {avro, avro-ipc} like is there now.\n\nComments (5):\n1. Thomas Graves: The dependencies couldn't be added in maven through the profiles? This is really user unfriendly for anyone using maven with hadoop 0.23.\n2. Sanford Ryza: Was this change tested against maven with YARN on Hadoop 2.2? I'm running into: [ERROR] The project org.apache.spark:yarn-parent_2.10:1.0.0-incubating-SNAPSHOT (/home/sandy/spark/spark/yarn/pom.xml) has 2 errors [ERROR] 'dependencies.dependency.version' for org.apache.avro:avro:jar is missing. @ line 55, column 17 [ERROR] 'dependencies.dependency.version' for org.apache.avro:avro-ipc:jar is missing. @ line 59, column 17\n3. Patrick McFadin: I'll fix this today sorry about that. --- sent from my phone On Feb 28, 2014 9:11 AM, \"Sandy Ryza (JIRA)\" <\n4. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/6\n5. Apache Spark: User 'pwendell' has created a pull request for this issue: https://github.com/apache/spark/pull/37", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "b6a5205fcf6e58b9eddf5833c40e3151", "issue_key": "SPARK-1122", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Collect the RDD and send to each partition to form a new RDD", "description": "Two methods (allCollect, allCollectBroadcast) are added to RDD[T], which output a new RDD[Array[T]] instance with each partition containing all of the records of the original RDD stored in a single Array[T] instance (the same as RDD.collect). This functionality can be useful in machine learning tasks that require sharing updated parameters across partitions.", "reporter": "Shuo Xiang", "assignee": null, "created": "2014-02-22T21:30:54.000+0000", "updated": "2015-03-01T10:58:18.000+0000", "resolved": "2015-03-01T10:58:18.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Shuo Xiang", "body": "PR: https://github.com/apache/incubator-spark/pull/635/", "created": "2014-02-22T21:35:20.067+0000"}, {"author": "Sean R. Owen", "body": "Is this still live? It seems like this doesn't need special operations. You can collect() an RDD and broadcast it, right?", "created": "2015-01-23T12:31:41.830+0000"}, {"author": "Sean R. Owen", "body": "You can also accomplish this with {{mapPartitions}} and simply convert the {{Iterator}} you get for each partition into an {{Array}}.", "created": "2015-03-01T10:58:18.621+0000"}], "num_comments": 3, "text": "Issue: SPARK-1122\nSummary: Collect the RDD and send to each partition to form a new RDD\nDescription: Two methods (allCollect, allCollectBroadcast) are added to RDD[T], which output a new RDD[Array[T]] instance with each partition containing all of the records of the original RDD stored in a single Array[T] instance (the same as RDD.collect). This functionality can be useful in machine learning tasks that require sharing updated parameters across partitions.\n\nComments (3):\n1. Shuo Xiang: PR: https://github.com/apache/incubator-spark/pull/635/\n2. Sean R. Owen: Is this still live? It seems like this doesn't need special operations. You can collect() an RDD and broadcast it, right?\n3. Sean R. Owen: You can also accomplish this with {{mapPartitions}} and simply convert the {{Iterator}} you get for each partition into an {{Array}}.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "c2e4d59fec0f9fe208376106d28b92a4", "issue_key": "SPARK-1123", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "saveAsNewAPIHadoopFile throws java.lang.InstantiationException all the time", "description": "scala> val a = sc.textFile(\"/Users/nanzhu/code/incubator-spark/LICENSE\", 2).map(line => (\"a\", \"b\")) scala> a.saveAsNewAPIHadoopFile(\"/Users/nanzhu/code/output_rdd\") java.lang.InstantiationException at sun.reflect.InstantiationExceptionConstructorAccessorImpl.newInstance(InstantiationExceptionConstructorAccessorImpl.java:48) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at java.lang.Class.newInstance(Class.java:374) at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:632) at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:590) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:15) at $iwC$$iwC$$iwC.<init>(<console>:20) at $iwC$$iwC.<init>(<console>:22) at $iwC.<init>(<console>:24) at <init>(<console>:26) at .<init>(<console>:30) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:774) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1042) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:611) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:642) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:606) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:790) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:835) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:747) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:595) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:602) at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:605) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:928) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:878) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:970) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) ------- I'm not sure about the reason, but a work-around is to discard using outputFormatClass in the parameter list, but get the class of outputFormat by job.getOutputFormatClass", "reporter": "Nan Zhu", "assignee": null, "created": "2014-02-23T00:37:14.000+0000", "updated": "2014-02-23T10:20:07.000+0000", "resolved": "2014-02-23T10:08:02.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "an example of the work-around mentioned above is in https://github.com/CodingCat/incubator-spark/commit/478abdc6dd88eaad644e3850666dc40bf9d1f1a9", "created": "2014-02-23T00:57:59.901+0000"}, {"author": "Nan Zhu", "body": "just forgot to pass class information explicitly", "created": "2014-02-23T10:20:07.682+0000"}], "num_comments": 2, "text": "Issue: SPARK-1123\nSummary: saveAsNewAPIHadoopFile throws java.lang.InstantiationException all the time\nDescription: scala> val a = sc.textFile(\"/Users/nanzhu/code/incubator-spark/LICENSE\", 2).map(line => (\"a\", \"b\")) scala> a.saveAsNewAPIHadoopFile(\"/Users/nanzhu/code/output_rdd\") java.lang.InstantiationException at sun.reflect.InstantiationExceptionConstructorAccessorImpl.newInstance(InstantiationExceptionConstructorAccessorImpl.java:48) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at java.lang.Class.newInstance(Class.java:374) at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:632) at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:590) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:15) at $iwC$$iwC$$iwC.<init>(<console>:20) at $iwC$$iwC.<init>(<console>:22) at $iwC.<init>(<console>:24) at <init>(<console>:26) at .<init>(<console>:30) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:774) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1042) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:611) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:642) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:606) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:790) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:835) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:747) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:595) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:602) at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:605) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:928) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:878) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:970) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) ------- I'm not sure about the reason, but a work-around is to discard using outputFormatClass in the parameter list, but get the class of outputFormat by job.getOutputFormatClass\n\nComments (2):\n1. Nan Zhu: an example of the work-around mentioned above is in https://github.com/CodingCat/incubator-spark/commit/478abdc6dd88eaad644e3850666dc40bf9d1f1a9\n2. Nan Zhu: just forgot to pass class information explicitly", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "f5cae65d3221966227af53ce5a11eebe", "issue_key": "SPARK-1124", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Infinite NullPointerException failures due to a null in map output locations", "description": "The following spark-shell code leads to an infinite retry of the last stage in Spark 0.9:  val data = sc.parallelize(1 to 100, 2).map(x => {throw new NullPointerException; (x, x)}).reduceByKey(_ + _) data.count() // This first one terminates correctly with just an NPE data.count() // This second one never terminates, it keeps failing over and over  The problem seems to be that when there's an NPE in the map stage, we erroneously add map output locations for it, so the next job on the RDD runs only the reduce stage. Those tasks keep failing but they count as a fetch failure, so it keeps retrying.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2014-02-23T19:51:11.000+0000", "updated": "2014-02-26T09:56:06.000+0000", "resolved": "2014-02-24T17:04:23.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Fixed in https://github.com/apache/incubator-spark/pull/641.", "created": "2014-02-25T16:51:01.918+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Test comment", "created": "2014-02-25T16:59:39.935+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Test comment for email integration", "created": "2014-02-26T09:25:37.717+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Another test comment for email", "created": "2014-02-26T09:37:33.705+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Another test comment", "created": "2014-02-26T09:56:06.775+0000"}], "num_comments": 5, "text": "Issue: SPARK-1124\nSummary: Infinite NullPointerException failures due to a null in map output locations\nDescription: The following spark-shell code leads to an infinite retry of the last stage in Spark 0.9:  val data = sc.parallelize(1 to 100, 2).map(x => {throw new NullPointerException; (x, x)}).reduceByKey(_ + _) data.count() // This first one terminates correctly with just an NPE data.count() // This second one never terminates, it keeps failing over and over  The problem seems to be that when there's an NPE in the map stage, we erroneously add map output locations for it, so the next job on the RDD runs only the reduce stage. Those tasks keep failing but they count as a fetch failure, so it keeps retrying.\n\nComments (5):\n1. Matei Alexandru Zaharia: Fixed in https://github.com/apache/incubator-spark/pull/641.\n2. Matei Alexandru Zaharia: Test comment\n3. Matei Alexandru Zaharia: Test comment for email integration\n4. Matei Alexandru Zaharia: Another test comment for email\n5. Matei Alexandru Zaharia: Another test comment", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "d0a2daaec686a5cb5eeecc03ef7275a1", "issue_key": "SPARK-1125", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "The maven build error for Spark Examples", "description": "mvn -v Apache Maven 3.1.1 (0728685237757ffbf44136acec0402957f723d9a; 2013-09-17 23:22:22+0800) Maven home: /usr/local/Cellar/maven/3.1.1/libexec Java version: 1.7.0_51, vendor: Oracle Corporation Java home: /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre Default locale: zh_CN, platform encoding: UTF-8 OS name: \"mac os x\", version: \"10.9.1\", arch: \"x86_64\", family: \"mac\"", "reporter": "Guoqiang Li", "assignee": null, "created": "2014-02-24T00:17:02.000+0000", "updated": "2014-08-20T08:21:49.000+0000", "resolved": "2014-08-20T08:21:49.000+0000", "labels": [], "components": [], "comments": [{"author": "Guoqiang Li", "body": "This error occurs when the maven uses a http proxy.", "created": "2014-02-24T02:34:53.739+0000"}, {"author": "Sean Owen", "body": "I see you reopened the pull request, but haven't you found that it's just your proxy interfering? then it's nothing to do with Spark, and you already found the Maven settings to make it work, which are not necessary for everyone else that is not on your network.", "created": "2014-02-24T05:59:00.893+0000"}, {"author": "Guoqiang Li", "body": "When someone adds a http proxy configuration maven, he'll get an error when compiling the Spark. We can not determine all of the configuration http proxy, but we can guarantee configure http proxy maven compile spark will not appear this error.", "created": "2014-02-24T06:15:23.574+0000"}, {"author": "Sean Owen", "body": "It's accurate to say that, if your proxy is breaking HTTPS connections, and you do not configure workarounds in Maven, you will get an error from any project that accesses a repo over HTTPS. Your change in the pull request does not fix this problem. This is not something for which some general configuration would resolve the issue.", "created": "2014-02-26T23:24:36.133+0000"}, {"author": "Guoqiang Li", "body": "maven settings.xml :  <proxy> <id>optional</id> <active>true</active> <protocol>http</protocol> <host>127.0.0.1</host> <port>8087</port> <nonProxyHosts>localhost|127.0.0.1|local</nonProxyHosts> </proxy>  do this  mvn -U -Pyarn -Dhadoop.version=2.3.0 -Dyarn.version=2.3.0 compile -X >> http_proxy.txt", "created": "2014-02-27T00:49:20.252+0000"}, {"author": "Sean Owen", "body": "If I put this in my settings.xml, my build completely fails. I am sure it makes your build work, so you should set this. But this is not something that is an issue with Spark that needs to be patched.", "created": "2014-02-27T01:00:27.134+0000"}, {"author": "Guoqiang Li", "body": "In China, no proxy or vpn is not connected *maven.twttr.com* , therefore there are a lot of people need this http proxy configuratio in settings.xml", "created": "2014-02-27T04:53:32.487+0000"}, {"author": "Guoqiang Li", "body": "Building the current master using Maven has the same compiler error  git checkout 5d98cfc1c8fb17fbbeacc7192ac21c0b038cbd16 mvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean install", "created": "2014-03-10T00:50:36.200+0000"}, {"author": "Guoqiang Li", "body": "Apache Maven 3.2.1 can work.", "created": "2014-03-12T08:04:50.396+0000"}], "num_comments": 9, "text": "Issue: SPARK-1125\nSummary: The maven build error for Spark Examples\nDescription: mvn -v Apache Maven 3.1.1 (0728685237757ffbf44136acec0402957f723d9a; 2013-09-17 23:22:22+0800) Maven home: /usr/local/Cellar/maven/3.1.1/libexec Java version: 1.7.0_51, vendor: Oracle Corporation Java home: /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre Default locale: zh_CN, platform encoding: UTF-8 OS name: \"mac os x\", version: \"10.9.1\", arch: \"x86_64\", family: \"mac\"\n\nComments (9):\n1. Guoqiang Li: This error occurs when the maven uses a http proxy.\n2. Sean Owen: I see you reopened the pull request, but haven't you found that it's just your proxy interfering? then it's nothing to do with Spark, and you already found the Maven settings to make it work, which are not necessary for everyone else that is not on your network.\n3. Guoqiang Li: When someone adds a http proxy configuration maven, he'll get an error when compiling the Spark. We can not determine all of the configuration http proxy, but we can guarantee configure http proxy maven compile spark will not appear this error.\n4. Sean Owen: It's accurate to say that, if your proxy is breaking HTTPS connections, and you do not configure workarounds in Maven, you will get an error from any project that accesses a repo over HTTPS. Your change in the pull request does not fix this problem. This is not something for which some general configuration would resolve the issue.\n5. Guoqiang Li: maven settings.xml :  <proxy> <id>optional</id> <active>true</active> <protocol>http</protocol> <host>127.0.0.1</host> <port>8087</port> <nonProxyHosts>localhost|127.0.0.1|local</nonProxyHosts> </proxy>  do this  mvn -U -Pyarn -Dhadoop.version=2.3.0 -Dyarn.version=2.3.0 compile -X >> http_proxy.txt\n6. Sean Owen: If I put this in my settings.xml, my build completely fails. I am sure it makes your build work, so you should set this. But this is not something that is an issue with Spark that needs to be patched.\n7. Guoqiang Li: In China, no proxy or vpn is not connected *maven.twttr.com* , therefore there are a lot of people need this http proxy configuratio in settings.xml\n8. Guoqiang Li: Building the current master using Maven has the same compiler error  git checkout 5d98cfc1c8fb17fbbeacc7192ac21c0b038cbd16 mvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean install\n9. Guoqiang Li: Apache Maven 3.2.1 can work.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "496425cdae0f271ecbc58830156f2838", "issue_key": "SPARK-1126", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "spark-submit script for running compiled binaries", "description": "It would be useful to have a script, roughly similar to the \"hadoop jar\" command that is used for running compiled binaries against Spark. The script would do two things: * Set up the Spark classpath on the client side, so that users don't need to know where Spark jars are installed or bundle all of Spark inside their app jar. * Provide a layer over the different modes that apps can be run, so that the same spark-jar invocation could run the driver inside a YARN application master or in the client process, depending on the cluster setup.", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "created": "2014-02-24T00:42:15.000+0000", "updated": "2014-03-30T05:40:55.000+0000", "resolved": "2014-03-30T05:40:55.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "Just as another requirement, we might want the same script to work for Python. It can be called spark-app or spark-submit or something.", "created": "2014-02-24T15:52:55.443+0000"}, {"author": "Sanford Ryza", "body": "That makes sense to me. So I'm thinking: spark-app <app jar> <main class> [<args>] where args are: worker-memory - Memory requested from scheduler per executor. worker-cores - Cores requested from scheduler per executor. num-workers - Number of executors. master-memory - Memory requested from scheduler for driver. Only applies on yarn-standalone mode. master-cores - Memory requested from scheduler for driver. Only applies on yarn-standalone mode. master-max-heap - Max heap size for the driver JVM. deploy-mode - yarn-client, yarn-standalone, standalone-standalone, or standalone-client (could maybe use better names here, the confusing thing is that \"standalone\" refers to both a cluster manager and a deploy mode) supervise - Whether to automatically restart driver on failure. Only works on standalone-standalone mode, though we should be able to add support for this in yarn-standalone as well. add-jars - Additional jars that should be on the driver and executor classpaths. files - Files to place next to all executors. Only works in yarn-standalone and yarn-client mode. archives - Archives to extract next to all executors. Only works in yarn-standalone and yarn-client mode. queue - Queue/pool to submit the application to. Only works in yarn-standalone and yarn-client mode. args - Arguments to pass to the driver. It would be nice for deploy-mode to be settable by an environment variable. Passing the option would override it. This would allow cluster operators to set up a default deploy mode for their cluster and not require users to think about it. A user could specify a particular deploy mode if it matters to them. Because many of these don't apply to every mode, it's also worth considering some sort of mechanism for delegating options down to particular modes. But I think this might be hairier and not add much.", "created": "2014-02-24T17:40:46.499+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Hey Sandy, a few comments here: - As I mentioned above, we probably want this to work for Python too in a complete design. In that case you'd probably take either a .py file or a .egg. - I think deploy-mode should be separated into two pieces: a cluster URL and a flag for whether to run the driver on the cluster versus locally. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here. - Supported deploy modes should include Mesos (maybe you don't allow running the driver in Mesos right now, but it can certainly be outside as a client). - Some of these flags overlap with settings you can put in your SparkConf object today, such as worker memory, cluster URL (if that becomes a flag) and to some extent JARs. How will these be passed through? One option is to set the Java system properties for them when you execute the user's app (e.g. -Dspark.executor.memory=2g), which are going to populate the Conf by default unless the user overwrites them in their program. I don't know how much of an issue the latter will be but we can create a workaround if it is. - The master-memory and master-cores should really be called driver-memory and driver-cores, and they apply to in-cluster submission on the standalone mode too. - If we want this to become the standard, the script should also work on Windows. This makes it considerably hairier, to the point where we might want this to be a Scala class, though in that case the JVM startup overhead for launching it is kind of painful. Anyway, I do think this would be a great feature to have, but before you implement it, it would be great to see a more detailed design that takes into account these points. In particular the main thing I'm worried about is creating inconsistency across languages, operating systems or deployment modes. If we make this change and update all the docs on deploying applications and such, it would be nice to only have to make it once.", "created": "2014-02-25T23:29:55.854+0000"}, {"author": "Sanford Ryza", "body": "Thanks for taking a look Matei. I attached a design doc with an amended version of what I posted above. bq. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here. In the YARN case, the cluster URL encapsulates both the cluster manager and the deploy mode (client vs. standalone), while in the standalone case, it includes only the former. The design splits these apart for spark-app's arguments, but I wanted to highlight this. Another thing I wanted to ask about was memory configuraton. In Hadoop, the memory requested as a cap from the cluster manager is controlled separately from the memory given as max heap opts to the JVM. While this is clunky for a lot of reasons, an advantage is that it allows accounting for a process using memory off-heap, either through direct buffers or by forking a subprocess. If Spark wants to handle these situations, it might make sense to eventually make the amount of padding between the JVM heap and requested memory configurable? Working on non-Linux platforms will be very difficult if the script is written in bash. Even across Linux and Macs, there appears to be no shared utility for parsing long-form options. A python script could avoid the JVM startup overhead. I suppose this adds a python dependency, but most platforms now include python by default. Scala also sounds reasonable to me if that makes the most sense to you.", "created": "2014-02-26T11:05:19.908+0000"}, {"author": "Patrick McFadin", "body": "[~sandy] Hey Sandy - is it not possible to just manually parse the options in bash rather than use a library? Second, what are the semantics of addJars, is this going to add a jar where the driver program is (and then jars are distributed through the normal path through Spark)? Just want to be clear because there is also addJar inside of Spark context...", "created": "2014-02-26T14:01:09.796+0000"}, {"author": "Sanford Ryza", "body": "I suppose bash is turing complete, but it would be very painful and error-prone to use it for this. It also wouldn't solve the Windows issue. yarn-standalone mode supports an addJars parameter, which will place local jars on the cluster and add them to the YARN distributed cache so that they can be localized for containers. I misunderstood and thought that other deploy modes had a similar way of specifying jars to add via command line. So we can just document that it only works for yarn-standalone mode. To promote consistency, it could also make sense to add environment variables that would allow other deploy modes to add jars outside of code. We could also possibly reform the functionality in yarn-standalone mode somehow.", "created": "2014-02-26T15:12:47.288+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Python isn't available by default on Windows, so we probably can't use that. Regarding the cluster URL, it's okay if we change URL formats slightly as part of this feature so that you use \"yarn\" for both in-cluster and out-of-cluster clients, but you have a separate flag for \"run the client in the cluster\". I don't think we should combine these two properties (what type of cluster is it and do I want the driver inside) into one flag, because you just end up with flags that are all possible combinations of the two features. Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs. It's okay if the option is only for YARN at first but it will be very confusing for users if they have to submit one way to YARN and another way to standalone clusters, so I'd look into adding that.", "created": "2014-02-26T16:14:41.085+0000"}, {"author": "Sanford Ryza", "body": "bq. Regarding the cluster URL, it's okay if we change URL formats slightly as part of this feature By this, do you mean changing the yarn URL formats in existing code or just as interpreted by the spark-app script? I.e. should this still work: \"MASTER=yarn-client ./bin/spark-shell\"? bq. Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs. Cool, I filed SPARK-1142 for this work.", "created": "2014-02-26T17:36:17.341+0000"}, {"author": "Sanford Ryza", "body": "Would it be best to just go with Scala? In non-python cases, we can avoid extra JVM startup time by running the user class in-process instead of forking a new JVM.", "created": "2014-02-28T16:02:04.968+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yes, I think it's fine to do it in Scala.", "created": "2014-03-02T20:23:46.660+0000"}, {"author": "Bernardo Gomez Palacio", "body": "Is there any feature-branch that is covering this work. Will like to contribute.", "created": "2014-03-06T14:06:15.539+0000"}, {"author": "Sanford Ryza", "body": "Here's the pull request: https://github.com/apache/spark/pull/86", "created": "2014-03-06T14:07:35.522+0000"}, {"author": "Bernardo Gomez Palacio", "body": "Thanks, will look into it too.", "created": "2014-03-06T17:13:34.296+0000"}, {"author": "ASF GitHub Bot", "body": "Github user pwendell commented on a diff in the pull request: https://github.com/apache/spark/pull/86#discussion_r11095306 --- Diff: docs/cluster-overview.md --- @@ -50,6 +50,47 @@ The system currently supports three cluster managers: In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone cluster on Amazon EC2. +# Launching Applications --- End diff -- Alright let's punt this to a broader doc clean-up for 1.0 which we can do during the QA phase. I think that ideally yes, we should replace all of the mentions of the other clients with this.", "created": "2014-03-29T21:30:06.850+0000"}, {"author": "ASF GitHub Bot", "body": "Github user pwendell commented on the pull request: https://github.com/apache/spark/pull/86#issuecomment-39009977 Hey @sryza I'm going to submit a PR with some suggested follow-on changes, but I think we can go ahead and merge this for now as a starting point. Thanks for your work on this!", "created": "2014-03-29T21:41:02.211+0000"}, {"author": "ASF GitHub Bot", "body": "Github user asfgit closed the pull request at: https://github.com/apache/spark/pull/86", "created": "2014-03-29T22:31:41.715+0000"}], "num_comments": 16, "text": "Issue: SPARK-1126\nSummary: spark-submit script for running compiled binaries\nDescription: It would be useful to have a script, roughly similar to the \"hadoop jar\" command that is used for running compiled binaries against Spark. The script would do two things: * Set up the Spark classpath on the client side, so that users don't need to know where Spark jars are installed or bundle all of Spark inside their app jar. * Provide a layer over the different modes that apps can be run, so that the same spark-jar invocation could run the driver inside a YARN application master or in the client process, depending on the cluster setup.\n\nComments (16):\n1. Matei Alexandru Zaharia: Just as another requirement, we might want the same script to work for Python. It can be called spark-app or spark-submit or something.\n2. Sanford Ryza: That makes sense to me. So I'm thinking: spark-app <app jar> <main class> [<args>] where args are: worker-memory - Memory requested from scheduler per executor. worker-cores - Cores requested from scheduler per executor. num-workers - Number of executors. master-memory - Memory requested from scheduler for driver. Only applies on yarn-standalone mode. master-cores - Memory requested from scheduler for driver. Only applies on yarn-standalone mode. master-max-heap - Max heap size for the driver JVM. deploy-mode - yarn-client, yarn-standalone, standalone-standalone, or standalone-client (could maybe use better names here, the confusing thing is that \"standalone\" refers to both a cluster manager and a deploy mode) supervise - Whether to automatically restart driver on failure. Only works on standalone-standalone mode, though we should be able to add support for this in yarn-standalone as well. add-jars - Additional jars that should be on the driver and executor classpaths. files - Files to place next to all executors. Only works in yarn-standalone and yarn-client mode. archives - Archives to extract next to all executors. Only works in yarn-standalone and yarn-client mode. queue - Queue/pool to submit the application to. Only works in yarn-standalone and yarn-client mode. args - Arguments to pass to the driver. It would be nice for deploy-mode to be settable by an environment variable. Passing the option would override it. This would allow cluster operators to set up a default deploy mode for their cluster and not require users to think about it. A user could specify a particular deploy mode if it matters to them. Because many of these don't apply to every mode, it's also worth considering some sort of mechanism for delegating options down to particular modes. But I think this might be hairier and not add much.\n3. Matei Alexandru Zaharia: Hey Sandy, a few comments here: - As I mentioned above, we probably want this to work for Python too in a complete design. In that case you'd probably take either a .py file or a .egg. - I think deploy-mode should be separated into two pieces: a cluster URL and a flag for whether to run the driver on the cluster versus locally. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here. - Supported deploy modes should include Mesos (maybe you don't allow running the driver in Mesos right now, but it can certainly be outside as a client). - Some of these flags overlap with settings you can put in your SparkConf object today, such as worker memory, cluster URL (if that becomes a flag) and to some extent JARs. How will these be passed through? One option is to set the Java system properties for them when you execute the user's app (e.g. -Dspark.executor.memory=2g), which are going to populate the Conf by default unless the user overwrites them in their program. I don't know how much of an issue the latter will be but we can create a workaround if it is. - The master-memory and master-cores should really be called driver-memory and driver-cores, and they apply to in-cluster submission on the standalone mode too. - If we want this to become the standard, the script should also work on Windows. This makes it considerably hairier, to the point where we might want this to be a Scala class, though in that case the JVM startup overhead for launching it is kind of painful. Anyway, I do think this would be a great feature to have, but before you implement it, it would be great to see a more detailed design that takes into account these points. In particular the main thing I'm worried about is creating inconsistency across languages, operating systems or deployment modes. If we make this change and update all the docs on deploying applications and such, it would be nice to only have to make it once.\n4. Sanford Ryza: Thanks for taking a look Matei. I attached a design doc with an amended version of what I posted above. bq. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here. In the YARN case, the cluster URL encapsulates both the cluster manager and the deploy mode (client vs. standalone), while in the standalone case, it includes only the former. The design splits these apart for spark-app's arguments, but I wanted to highlight this. Another thing I wanted to ask about was memory configuraton. In Hadoop, the memory requested as a cap from the cluster manager is controlled separately from the memory given as max heap opts to the JVM. While this is clunky for a lot of reasons, an advantage is that it allows accounting for a process using memory off-heap, either through direct buffers or by forking a subprocess. If Spark wants to handle these situations, it might make sense to eventually make the amount of padding between the JVM heap and requested memory configurable? Working on non-Linux platforms will be very difficult if the script is written in bash. Even across Linux and Macs, there appears to be no shared utility for parsing long-form options. A python script could avoid the JVM startup overhead. I suppose this adds a python dependency, but most platforms now include python by default. Scala also sounds reasonable to me if that makes the most sense to you.\n5. Patrick McFadin: [~sandy] Hey Sandy - is it not possible to just manually parse the options in bash rather than use a library? Second, what are the semantics of addJars, is this going to add a jar where the driver program is (and then jars are distributed through the normal path through Spark)? Just want to be clear because there is also addJar inside of Spark context...\n6. Sanford Ryza: I suppose bash is turing complete, but it would be very painful and error-prone to use it for this. It also wouldn't solve the Windows issue. yarn-standalone mode supports an addJars parameter, which will place local jars on the cluster and add them to the YARN distributed cache so that they can be localized for containers. I misunderstood and thought that other deploy modes had a similar way of specifying jars to add via command line. So we can just document that it only works for yarn-standalone mode. To promote consistency, it could also make sense to add environment variables that would allow other deploy modes to add jars outside of code. We could also possibly reform the functionality in yarn-standalone mode somehow.\n7. Matei Alexandru Zaharia: Python isn't available by default on Windows, so we probably can't use that. Regarding the cluster URL, it's okay if we change URL formats slightly as part of this feature so that you use \"yarn\" for both in-cluster and out-of-cluster clients, but you have a separate flag for \"run the client in the cluster\". I don't think we should combine these two properties (what type of cluster is it and do I want the driver inside) into one flag, because you just end up with flags that are all possible combinations of the two features. Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs. It's okay if the option is only for YARN at first but it will be very confusing for users if they have to submit one way to YARN and another way to standalone clusters, so I'd look into adding that.\n8. Sanford Ryza: bq. Regarding the cluster URL, it's okay if we change URL formats slightly as part of this feature By this, do you mean changing the yarn URL formats in existing code or just as interpreted by the spark-app script? I.e. should this still work: \"MASTER=yarn-client ./bin/spark-shell\"? bq. Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs. Cool, I filed SPARK-1142 for this work.\n9. Sanford Ryza: Would it be best to just go with Scala? In non-python cases, we can avoid extra JVM startup time by running the user class in-process instead of forking a new JVM.\n10. Matei Alexandru Zaharia: Yes, I think it's fine to do it in Scala.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "e21f1adeea27c5257496e77c84c5fd6c", "issue_key": "SPARK-1127", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add saveAsHBase to PairRDDFunctions", "description": "Support to save data in HBase.", "reporter": "Haosdent Huang", "assignee": "Haosdent Huang", "created": "2014-02-24T01:50:42.000+0000", "updated": "2014-12-10T15:52:27.000+0000", "resolved": "2014-12-10T15:52:27.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Haosdent Huang", "body": "I begin to do some HBase support works in Spark. It you have a better approach about this issue, give me advice. Thanks in advance.", "created": "2014-02-24T01:53:50.514+0000"}, {"author": "Haosdent Huang", "body": "The pull request: https://github.com/apache/spark/pull/123", "created": "2014-03-11T04:19:57.993+0000"}, {"author": "Mark Hamstra", "body": "In terms of a better approach, what I am beginning to wonder is whether we should be looking for a higher level of abstraction, with data structures, code, API, etc. that can be used by multiple NoSQL, key-value data stores. That way we may be able to avoid some of the duplicate development and maintenance costs to support HBase, Cassandra, and whatever broadly similar external datastore someone next wants to use with Spark.", "created": "2014-03-11T08:42:28.437+0000"}, {"author": "Haosdent Huang", "body": "But the data structures are quite different from different NoSQL database. Add an saveAsNoSQL method and configure which NoSQL through JobConf?", "created": "2014-03-11T09:26:02.205+0000"}, {"author": "Mark Hamstra", "body": "Yes, undoubtedly there are a lot of differences among these various external datastore options, and there can't be just one concrete implementation within Spark to satisfy all of them. All that I am saying is that before we go too far down the road of supporting completely independent implementations for each of these, at least one somebody should take the time to figure out whether there are common abstractions that can be usefully shared among Spark/NoSQL connectors. I'm certainly not that somebody up to this point. And all of this is more of a meta-discussion than a discussion of your specific Issue/Pull Request, so lack of resolution of this meta-issue shouldn't be seen as blocking your effort. It can just as easily (and equally inappropriately) be hung on Cassandra PRs or other external datastore PRs.", "created": "2014-03-11T09:44:00.995+0000"}, {"author": "Haosdent Huang", "body": "Thank you for your explaination. I am misunderstand what you say before. How about create a \"NoSQLRDDFunctions\" and move \"saveAsHBase\" to it?", "created": "2014-03-11T10:07:51.196+0000"}, {"author": "Mark Hamstra", "body": "That might work. You might also look at the various *SaveToCassandra methods in Calliope or at other Spark+Cassandra efforts to see whether there is commonality that might be abstracted to a common NoSQLRDDFunctions or something like that.", "created": "2014-03-11T10:34:10.991+0000"}, {"author": "Haosdent Huang", "body": "OK, let me have a try. Thank you for you advice. :-)", "created": "2014-03-11T10:40:09.636+0000"}, {"author": "Mark Hamstra", "body": "You're welcome. I'm not sure that you will end up anyplace useful, but it will be helpful for someone to at least have taken a serious look at and reported back on whether a higher-level abstraction makes sense.", "created": "2014-03-11T10:45:53.656+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I agree that this should be done through an extra library instead of PairRDDFunctions. I actually think the easiest way to do it is just a `HBaseUtils` class with a static method that takes an RDD and information on how to save it. While it's less magical for Scala users, it will also make it easy to call from Java. We used this approach to move external data sources to separate modules in Spark 0.9; for example see http://spark.incubator.apache.org/docs/latest/api/external/flume/index.html#org.apache.spark.streaming.flume.FlumeUtils$ . This class should go into a separate `spark-hbase` module located in the `external` folder in the code so that it doesn't bring the HBase dependencies into the default build. Users who want those can link to `spark-hbase`.", "created": "2014-03-11T19:16:26.450+0000"}, {"author": "Haosdent Huang", "body": "Thank you. I also consider add saveAsHBase to PairRDDFunctions would bring dependence problem before. But I don't have other idea to workaround that. Now I realize that I could make a new external module to achieve this. Thank you very much.", "created": "2014-03-12T04:40:13.056+0000"}, {"author": "Haosdent Huang", "body": "@matei @Matei Zaharia I have update the pull request. Could you help me to review it? https://github.com/apache/spark/pull/194 Thank you very much.", "created": "2014-03-21T02:14:20.429+0000"}, {"author": "haosdent", "body": "[~pwendell] I have update the code in https://github.com/apache/spark/pull/194. Could you review it again and give any advice? Thank you in advance.", "created": "2014-04-08T08:25:17.864+0000"}, {"author": "haosdent", "body": "ping [~pwendell] I saw 1.0 have been released, any plan about this issue? :-)", "created": "2014-06-08T08:30:44.744+0000"}, {"author": "Reynold Xin", "body": "I looked at this a little bit. It seems to me it is best to go through SchemaRDD in Spark SQL to provide this support. In the future, SchemaRDD should probably become the narrow waist for all structured data support.", "created": "2014-06-25T08:09:23.369+0000"}, {"author": "haosdent", "body": "[~rxin] I have added SchemaRDD support. Could you provide more details about your advice for this patch? Thank you very much.", "created": "2014-06-25T08:41:27.318+0000"}, {"author": "Ted Yu", "body": "According to Reynold, First half of the external data source API (for reading but not writing) is already in 1.2: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala", "created": "2014-12-02T22:09:36.875+0000"}, {"author": "Sean R. Owen", "body": "Given the discussion in both PRs, this looks like a WontFix, and consensus was it should proceed in a separate project. Questions about SchemaRDD and 1.2 sound like a new topic.", "created": "2014-12-10T15:52:27.672+0000"}], "num_comments": 18, "text": "Issue: SPARK-1127\nSummary: Add saveAsHBase to PairRDDFunctions\nDescription: Support to save data in HBase.\n\nComments (18):\n1. Haosdent Huang: I begin to do some HBase support works in Spark. It you have a better approach about this issue, give me advice. Thanks in advance.\n2. Haosdent Huang: The pull request: https://github.com/apache/spark/pull/123\n3. Mark Hamstra: In terms of a better approach, what I am beginning to wonder is whether we should be looking for a higher level of abstraction, with data structures, code, API, etc. that can be used by multiple NoSQL, key-value data stores. That way we may be able to avoid some of the duplicate development and maintenance costs to support HBase, Cassandra, and whatever broadly similar external datastore someone next wants to use with Spark.\n4. Haosdent Huang: But the data structures are quite different from different NoSQL database. Add an saveAsNoSQL method and configure which NoSQL through JobConf?\n5. Mark Hamstra: Yes, undoubtedly there are a lot of differences among these various external datastore options, and there can't be just one concrete implementation within Spark to satisfy all of them. All that I am saying is that before we go too far down the road of supporting completely independent implementations for each of these, at least one somebody should take the time to figure out whether there are common abstractions that can be usefully shared among Spark/NoSQL connectors. I'm certainly not that somebody up to this point. And all of this is more of a meta-discussion than a discussion of your specific Issue/Pull Request, so lack of resolution of this meta-issue shouldn't be seen as blocking your effort. It can just as easily (and equally inappropriately) be hung on Cassandra PRs or other external datastore PRs.\n6. Haosdent Huang: Thank you for your explaination. I am misunderstand what you say before. How about create a \"NoSQLRDDFunctions\" and move \"saveAsHBase\" to it?\n7. Mark Hamstra: That might work. You might also look at the various *SaveToCassandra methods in Calliope or at other Spark+Cassandra efforts to see whether there is commonality that might be abstracted to a common NoSQLRDDFunctions or something like that.\n8. Haosdent Huang: OK, let me have a try. Thank you for you advice. :-)\n9. Mark Hamstra: You're welcome. I'm not sure that you will end up anyplace useful, but it will be helpful for someone to at least have taken a serious look at and reported back on whether a higher-level abstraction makes sense.\n10. Matei Alexandru Zaharia: I agree that this should be done through an extra library instead of PairRDDFunctions. I actually think the easiest way to do it is just a `HBaseUtils` class with a static method that takes an RDD and information on how to save it. While it's less magical for Scala users, it will also make it easy to call from Java. We used this approach to move external data sources to separate modules in Spark 0.9; for example see http://spark.incubator.apache.org/docs/latest/api/external/flume/index.html#org.apache.spark.streaming.flume.FlumeUtils$ . This class should go into a separate `spark-hbase` module located in the `external` folder in the code so that it doesn't bring the HBase dependencies into the default build. Users who want those can link to `spark-hbase`.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.139365"}}
{"id": "bf0bb215effd802cd630f9ed01fc16b6", "issue_key": "SPARK-1128", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "hadoop task properties not set while using InputFormat", "description": "The task specific Hadoop properties, in particular `mapred.task.id` and `mapred.tip.id` (or `mapred.task.partition`) are not set when calling `InputFormat#getRecordReader`. Implementations that rely on such properties will fail at this point as no information about the current task environment is provided even though the job is 'theoretically' in progress. I've noticed `SparkHadoopWriter.scala` sets this properties - it would be nice to have the same thing applied for reading data as well. Thanks!", "reporter": "Costin Leau", "assignee": "Nan Zhu", "created": "2014-02-24T11:55:59.000+0000", "updated": "2014-03-24T21:56:02.000+0000", "resolved": "2014-03-24T21:56:02.000+0000", "labels": [], "components": ["Input/Output"], "comments": [{"author": "Nan Zhu", "body": "https://github.com/apache/spark/pull/101", "created": "2014-03-07T16:40:25.411+0000"}], "num_comments": 1, "text": "Issue: SPARK-1128\nSummary: hadoop task properties not set while using InputFormat\nDescription: The task specific Hadoop properties, in particular `mapred.task.id` and `mapred.tip.id` (or `mapred.task.partition`) are not set when calling `InputFormat#getRecordReader`. Implementations that rely on such properties will fail at this point as no information about the current task environment is provided even though the job is 'theoretically' in progress. I've noticed `SparkHadoopWriter.scala` sets this properties - it would be nice to have the same thing applied for reading data as well. Thanks!\n\nComments (1):\n1. Nan Zhu: https://github.com/apache/spark/pull/101", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "c3ca12d19960610e64bb858c883df88b", "issue_key": "SPARK-1129", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "use a predefined seed when seed is zero in XORShiftRandom", "description": "If the seed is zero, XORShift generates all zeros, which would create unexpected result.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-02-24T18:10:17.000+0000", "updated": "2014-03-18T19:30:08.000+0000", "resolved": "2014-03-18T19:30:08.000+0000", "labels": [], "components": [], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/incubator-spark/pull/645", "created": "2014-03-18T19:27:56.483+0000"}, {"author": "Xiangrui Meng", "body": "Merged.", "created": "2014-03-18T19:28:08.759+0000"}, {"author": "Xiangrui Meng", "body": "To mark resolved.", "created": "2014-03-18T19:30:02.879+0000"}, {"author": "Xiangrui Meng", "body": "Merged.", "created": "2014-03-18T19:30:08.485+0000"}], "num_comments": 4, "text": "Issue: SPARK-1129\nSummary: use a predefined seed when seed is zero in XORShiftRandom\nDescription: If the seed is zero, XORShift generates all zeros, which would create unexpected result.\n\nComments (4):\n1. Xiangrui Meng: PR: https://github.com/apache/incubator-spark/pull/645\n2. Xiangrui Meng: Merged.\n3. Xiangrui Meng: To mark resolved.\n4. Xiangrui Meng: Merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "00bb77f143e067cd5471e95ad46f5e40", "issue_key": "SPARK-1130", "issue_type": "Task", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Clean up project documentation navigation menu", "description": "1. Create a top level Configuration menu 2. Move most stuff in More into Deployment 3. Rename More to Development", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2014-02-24T18:44:39.000+0000", "updated": "2014-03-18T10:01:07.000+0000", "resolved": "2014-03-18T10:01:07.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1130\nSummary: Clean up project documentation navigation menu\nDescription: 1. Create a top level Configuration menu 2. Move most stuff in More into Deployment 3. Rename More to Development", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "a12419295249629be93125e2a2f55d06", "issue_key": "SPARK-1131", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Better document the --args option for yarn-standalone mode", "description": "It took me a while to figure out that the correct way to use it with multiple arguments was to include the option multiple times. I.e. --args arg1 --args arg2 instead of --args \"arg1 arg2\"", "reporter": "Sanford Ryza", "assignee": "Karthik Kambatla", "created": "2014-02-24T20:30:38.000+0000", "updated": "2014-09-12T21:53:39.000+0000", "resolved": "2014-09-12T21:53:39.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Marcelo Masiero Vanzin", "body": "This is probably obsolete now with spark-submit.", "created": "2014-07-31T21:24:45.817+0000"}, {"author": "Andrew Or", "body": "--args is now deprecated. We use --arg instead.", "created": "2014-09-12T21:53:39.390+0000"}], "num_comments": 2, "text": "Issue: SPARK-1131\nSummary: Better document the --args option for yarn-standalone mode\nDescription: It took me a while to figure out that the correct way to use it with multiple arguments was to include the option multiple times. I.e. --args arg1 --args arg2 instead of --args \"arg1 arg2\"\n\nComments (2):\n1. Marcelo Masiero Vanzin: This is probably obsolete now with spark-submit.\n2. Andrew Or: --args is now deprecated. We use --arg instead.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "ab64c47b333c1226ebe896eee09c8ebc", "issue_key": "SPARK-1132", "issue_type": "Improvement", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Persisting Web UI through refactoring the SparkListener interface", "description": "This issue is a spin-off from another issue - https://spark-project.atlassian.net/browse/SPARK-969 The main issue with the existing Spark Web UI is that its information is lost as soon as the application terminates. This is the direct result of the SparkUI being coupled with SparkContext, which is stopped when the application is finished. The attached document proposes to tackle this by logging SparkListenerEvents to persist information displayed on the Web UI. We take this opportunity to replace the existing format for storing this information, HTML, with one that is more flexible, JSON. This allows further post-hoc analysis of a particular Spark application beyond simply reviving the Web UI.", "reporter": "Andrew Or", "assignee": "Andrew Or", "created": "2014-02-24T20:46:18.000+0000", "updated": "2015-05-10T17:35:04.000+0000", "resolved": "2014-03-19T13:17:35.000+0000", "labels": [], "components": ["Spark Core", "Web UI"], "comments": [{"author": "Thomas Graves", "body": "Sorry I haven't had time to follow the PR for this. So this jira refactored the listener interface and added the ability to store events to a file as well as ability for Master to reload from those files, is that correct? If the Master is restarted does can it pick up those files and finished applications? Could you perhaps summarize what all was added?", "created": "2014-03-19T13:52:10.407+0000"}, {"author": "Andrew Or", "body": "Hi Thomas, Your understanding is correct. The bulk of the change is within components accessible from SparkContext. This mainly includes (1) adding a special purpose listener to log Spark events to persisted storage, (2) allowing SparkUI to render either from live events (from SparkContext) or from replayed events (from logs), and (3) refactoring events in such a way that makes this possible. Changes made to Master, on the other hand, only represent a use case of this new functionality. The main change here is simply that Master now renders an after-the-fact SparkUI from these event logs (if any) after the associated application finishes. With or without Master, the application can choose to log events for other purposes, e.g. parsing them in a script for post-hoc analysis. For this reason, it is certainly possible for Master to pick up these files after restarting, as long as it maintains information for the associated application. Andrew", "created": "2014-03-19T16:22:21.435+0000"}, {"author": "sagar", "body": "Hi Team, I see the issue is resolved and Fix Version/s: is -1.0.0. Is 1.0.0 is spark version ? Where i can get the spark version 1.0.0. Currently i am getting below error - 15/05/10 08:42:20 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id 15/05/10 08:42:20 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id 15/05/10 08:42:20 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap 15/05/10 08:42:20 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition 15/05/10 08:42:20 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id 15/05/10 08:42:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1750 bytes result sent to driver 15/05/10 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 207 ms on localhost (1/1) 15/05/10 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 15/05/10 08:42:20 INFO DAGScheduler: Stage 0 (count at SparkFilter.java:22) finished in 0.225 s 15/05/10 08:42:20 INFO DAGScheduler: Job 0 finished: count at SparkFilter.java:22, took 0.314437 s 0 15/05/10 08:42:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144) at org.apache.spark.scheduler.EventLoggingListener.onStageCompleted(EventLoggingListener.scala:165) at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:32) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31) at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53) at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617) at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60) Caused by: java.io.IOException: Filesystem closed at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:792) at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1998) at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959) at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130) ... 19 more", "created": "2015-05-10T17:35:04.396+0000"}], "num_comments": 3, "text": "Issue: SPARK-1132\nSummary: Persisting Web UI through refactoring the SparkListener interface\nDescription: This issue is a spin-off from another issue - https://spark-project.atlassian.net/browse/SPARK-969 The main issue with the existing Spark Web UI is that its information is lost as soon as the application terminates. This is the direct result of the SparkUI being coupled with SparkContext, which is stopped when the application is finished. The attached document proposes to tackle this by logging SparkListenerEvents to persist information displayed on the Web UI. We take this opportunity to replace the existing format for storing this information, HTML, with one that is more flexible, JSON. This allows further post-hoc analysis of a particular Spark application beyond simply reviving the Web UI.\n\nComments (3):\n1. Thomas Graves: Sorry I haven't had time to follow the PR for this. So this jira refactored the listener interface and added the ability to store events to a file as well as ability for Master to reload from those files, is that correct? If the Master is restarted does can it pick up those files and finished applications? Could you perhaps summarize what all was added?\n2. Andrew Or: Hi Thomas, Your understanding is correct. The bulk of the change is within components accessible from SparkContext. This mainly includes (1) adding a special purpose listener to log Spark events to persisted storage, (2) allowing SparkUI to render either from live events (from SparkContext) or from replayed events (from logs), and (3) refactoring events in such a way that makes this possible. Changes made to Master, on the other hand, only represent a use case of this new functionality. The main change here is simply that Master now renders an after-the-fact SparkUI from these event logs (if any) after the associated application finishes. With or without Master, the application can choose to log events for other purposes, e.g. parsing them in a script for post-hoc analysis. For this reason, it is certainly possible for Master to pick up these files after restarting, as long as it maintains information for the associated application. Andrew\n3. sagar: Hi Team, I see the issue is resolved and Fix Version/s: is -1.0.0. Is 1.0.0 is spark version ? Where i can get the spark version 1.0.0. Currently i am getting below error - 15/05/10 08:42:20 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id 15/05/10 08:42:20 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id 15/05/10 08:42:20 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap 15/05/10 08:42:20 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition 15/05/10 08:42:20 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id 15/05/10 08:42:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1750 bytes result sent to driver 15/05/10 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 207 ms on localhost (1/1) 15/05/10 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 15/05/10 08:42:20 INFO DAGScheduler: Stage 0 (count at SparkFilter.java:22) finished in 0.225 s 15/05/10 08:42:20 INFO DAGScheduler: Job 0 finished: count at SparkFilter.java:22, took 0.314437 s 0 15/05/10 08:42:20 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144) at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144) at org.apache.spark.scheduler.EventLoggingListener.onStageCompleted(EventLoggingListener.scala:165) at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:32) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31) at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31) at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53) at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61) at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617) at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60) Caused by: java.io.IOException: Filesystem closed at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:792) at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1998) at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1959) at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130) ... 19 more", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "0ba3b4171fb660224697841c0a7adb0e", "issue_key": "SPARK-1133", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add a new small files input for MLlib, which will return an RDD[(fileName, content)]", "description": "As I am moving forward to write a LDA (Latent Dirichlet Allocation) implementation to Spark MLlib, I find that a small files input API is useful, so I write a smallTextFiles() to support it. smallTextFiles() digests a directory of text files, then return an RDD\\[(String, String)\\], the former String is the file name, while the latter one is the contents of the text file. smallTextFiles() can be used for local disk I/O, or HDFS I/O, just like the textFiles() in SparkContext. In the scenario of LDA, there are 2 common uses: 1. smallTextFiles() is used to preprocess local disk files, i.e. combine those files into a huge one, then transfer it onto HDFS to do further process, such as LDA clustering. 2. It is also used to transfer the raw directory of small files onto HDFS (though it is not recommended, because it will cost too many namenode entries), then clustering it directly with LDA.", "reporter": "Xusen Yin", "assignee": "Xusen Yin", "created": "2014-02-25T06:16:09.000+0000", "updated": "2014-04-04T18:13:37.000+0000", "resolved": "2014-04-04T18:13:37.000+0000", "labels": ["IO", "MLLib,", "hadoop"], "components": ["Input/Output"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1133\nSummary: Add a new small files input for MLlib, which will return an RDD[(fileName, content)]\nDescription: As I am moving forward to write a LDA (Latent Dirichlet Allocation) implementation to Spark MLlib, I find that a small files input API is useful, so I write a smallTextFiles() to support it. smallTextFiles() digests a directory of text files, then return an RDD\\[(String, String)\\], the former String is the file name, while the latter one is the contents of the text file. smallTextFiles() can be used for local disk I/O, or HDFS I/O, just like the textFiles() in SparkContext. In the scenario of LDA, there are 2 common uses: 1. smallTextFiles() is used to preprocess local disk files, i.e. combine those files into a huge one, then transfer it onto HDFS to do further process, such as LDA clustering. 2. It is also used to transfer the raw directory of small files onto HDFS (though it is not recommended, because it will cost too many namenode entries), then clustering it directly with LDA.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "998ca9ec397818e4831d98245e582b36", "issue_key": "SPARK-1134", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ipython won't run standalone python script", "description": "Using Spark 0.9.0, python 2.6.6, and ipython 1.1.0. The problem: If I want to run a python script as a standalone app, the docs say I should execute the command \"pyspark myscript.py\". This works as long as IPYTHON=0. But if IPYTHON=1 this doesn't work. This problem arose for me because I tried to save myself typing by setting IPYTHON=1 in my shell profile script. Which then meant I was unable to execute pyspark standalone scripts. My analysis: in the pyspark script, command line arguments are simply ignored if ipython is used: if [[ \"$IPYTHON\" = \"1\" ]] ; then exec ipython $IPYTHON_OPTS else exec \"$PYSPARK_PYTHON\" \"$@\" fi I thought I could get around this by changing the script to pass $@. However, this doesn't work: doing so results in an error saying multiple spark contexts can't be run at once. This is because of a feature?/bug? of ipython related to the PYTHONSTARTUP environment variable. the pyspark script sets this variable to point to the python/shell.py script, which initializes the Spark Context. In regular python, the PYTHONSTARTUP script runs ONLY if python is invoked in interactive mode; if run with a script, it ignores the variable. iPython runs that script every time, regardless. Which means it will always execute Spark's shell.py script to initialize the spark context even when it was invoked with a script. Proposed solution: short term: add this information to the Spark docs regarding iPython. Something like \"Note, iPython can only be used interactively. Use regular Python to execute pyspark script files.\" long term: change the pyspark script to tell if arguments are passed in; if so, just call python instead of pyspark, or don't set the PYTHONSTARTUP variable? Or maybe fix shell.py to detect if it's being invoked in non-interactively and not initialize sc.", "reporter": "Diana Carroll", "assignee": "Diana Carroll", "created": "2014-02-25T07:39:34.000+0000", "updated": "2014-04-03T22:50:12.000+0000", "resolved": "2014-04-03T22:50:12.000+0000", "labels": ["pyspark"], "components": ["PySpark"], "comments": [{"author": "Diana Carroll", "body": "Just tested this change to pyspark script file and it seems to work: if [[ \"$IPYTHON\" = \"1\" && $# = 0 ]] ; then exec ipython $IPYTHON_OPTS else exec \"$PYSPARK_PYTHON\" \"$@\" fi", "created": "2014-02-25T08:18:47.952+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This sounds good, do you want to send in a pull request? Otherwise someone else can fix it as suggested.", "created": "2014-02-25T15:41:48.432+0000"}, {"author": "Diana Carroll", "body": "Well, I would if I could. I tried following the advice and tutorial here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark But when I attempted to push, I apparently lack the access rights to do that. Sorry. On Tue, Feb 25, 2014 at 6:42 PM, Matei Zaharia (JIRA) <", "created": "2014-02-27T14:29:21.881+0000"}, {"author": "Diana Carroll", "body": "I was able to submit a pull request: https://github.com/apache/spark/pull/227 (which makes this fix and also removes IPYTHONOPTS) (the original pull request was https://github.com/apache/spark/pull/83)", "created": "2014-03-25T09:27:10.456+0000"}], "num_comments": 4, "text": "Issue: SPARK-1134\nSummary: ipython won't run standalone python script\nDescription: Using Spark 0.9.0, python 2.6.6, and ipython 1.1.0. The problem: If I want to run a python script as a standalone app, the docs say I should execute the command \"pyspark myscript.py\". This works as long as IPYTHON=0. But if IPYTHON=1 this doesn't work. This problem arose for me because I tried to save myself typing by setting IPYTHON=1 in my shell profile script. Which then meant I was unable to execute pyspark standalone scripts. My analysis: in the pyspark script, command line arguments are simply ignored if ipython is used: if [[ \"$IPYTHON\" = \"1\" ]] ; then exec ipython $IPYTHON_OPTS else exec \"$PYSPARK_PYTHON\" \"$@\" fi I thought I could get around this by changing the script to pass $@. However, this doesn't work: doing so results in an error saying multiple spark contexts can't be run at once. This is because of a feature?/bug? of ipython related to the PYTHONSTARTUP environment variable. the pyspark script sets this variable to point to the python/shell.py script, which initializes the Spark Context. In regular python, the PYTHONSTARTUP script runs ONLY if python is invoked in interactive mode; if run with a script, it ignores the variable. iPython runs that script every time, regardless. Which means it will always execute Spark's shell.py script to initialize the spark context even when it was invoked with a script. Proposed solution: short term: add this information to the Spark docs regarding iPython. Something like \"Note, iPython can only be used interactively. Use regular Python to execute pyspark script files.\" long term: change the pyspark script to tell if arguments are passed in; if so, just call python instead of pyspark, or don't set the PYTHONSTARTUP variable? Or maybe fix shell.py to detect if it's being invoked in non-interactively and not initialize sc.\n\nComments (4):\n1. Diana Carroll: Just tested this change to pyspark script file and it seems to work: if [[ \"$IPYTHON\" = \"1\" && $# = 0 ]] ; then exec ipython $IPYTHON_OPTS else exec \"$PYSPARK_PYTHON\" \"$@\" fi\n2. Matei Alexandru Zaharia: This sounds good, do you want to send in a pull request? Otherwise someone else can fix it as suggested.\n3. Diana Carroll: Well, I would if I could. I tried following the advice and tutorial here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark But when I attempted to push, I apparently lack the access rights to do that. Sorry. On Tue, Feb 25, 2014 at 6:42 PM, Matei Zaharia (JIRA) <\n4. Diana Carroll: I was able to submit a pull request: https://github.com/apache/spark/pull/227 (which makes this fix and also removes IPYTHONOPTS) (the original pull request was https://github.com/apache/spark/pull/83)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "e0cd38c5026ceb8712ca4a1f9242cd62", "issue_key": "SPARK-1135", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Anchors broken in latest docs due to bad JavaScript code", "description": "A recent PR that added Java vs Scala tabs for streaming also inadvertently added some bad code to a document.ready handler, breaking our other handler that manages scrolling to anchors correctly with the floating top bar. As a result the section title ended up always being hidden below the top bar.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "2014-02-25T17:22:30.000+0000", "updated": "2014-03-21T00:12:30.000+0000", "resolved": "2014-03-21T00:12:30.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1135\nSummary: Anchors broken in latest docs due to bad JavaScript code\nDescription: A recent PR that added Java vs Scala tabs for streaming also inadvertently added some bad code to a document.ready handler, breaking our other handler that manages scrolling to anchors correctly with the floating top bar. As a result the section title ended up always being hidden below the top bar.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "96c3188bdf59b71d23711f5ad6d32020", "issue_key": "SPARK-1136", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Fix FaultToleranceTest for Docker 0.8.1", "description": "Several changes were made between Docker 0.6 (when our spark-test docker files were created) and the current version of Docker, 0.8.1. There are two relevant to the FaultToleranceTest that causes it to fail: 1) A random host name is assigned to Docker containers. This host name, unlike the IP address, is not reachable from outside the container, but by default we'll try to use it as the Worker's Akka host. This fails when a newly-elected Master attempts to recover a Worker, since the Worker is not actually reachable at the host address it connected from. 2) IP addresses are now reassigned immediately upon container recycling. This means that we can confuse \"old\" and \"new\" Workers or Masters that happened to be assigned the same IP address. The main obvious issue that arises is when a Worker gets a \"attempted to re-register\" exception when it takes on a previous Worker's IP address during Master recovery.", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2014-02-25T21:42:53.000+0000", "updated": "2014-04-15T03:32:48.000+0000", "resolved": "2014-04-15T03:32:48.000+0000", "labels": [], "components": ["Build"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1136\nSummary: Fix FaultToleranceTest for Docker 0.8.1\nDescription: Several changes were made between Docker 0.6 (when our spark-test docker files were created) and the current version of Docker, 0.8.1. There are two relevant to the FaultToleranceTest that causes it to fail: 1) A random host name is assigned to Docker containers. This host name, unlike the IP address, is not reachable from outside the container, but by default we'll try to use it as the Worker's Akka host. This fails when a newly-elected Master attempts to recover a Worker, since the Worker is not actually reachable at the host address it connected from. 2) IP addresses are now reassigned immediately upon container recycling. This means that we can confuse \"old\" and \"new\" Workers or Masters that happened to be assigned the same IP address. The main obvious issue that arises is when a Worker gets a \"attempted to re-register\" exception when it takes on a previous Worker's IP address during Master recovery.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "d05c782aa188c049258c82a4f018747c", "issue_key": "SPARK-1137", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ZK Persistence Engine crashes if stored data has wrong serialVersionUID", "description": "The ZooKeeperPersistenceEngine contains information about concurrently existing Masters and Workers. This information, as the name suggests, is persistent in the event of a Master failure/restart. If the Spark version is upgraded, the Master will crash with a Java serialization exception when trying to re-read the persisted data. Instead of crashing (indefinitely), the Master should probably just ignore the prior data.", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2014-02-25T21:47:12.000+0000", "updated": "2018-07-25T20:44:08.000+0000", "resolved": "2014-04-15T03:33:05.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Apache Spark", "body": "User 'aarondav' has created a pull request for this issue: https://github.com/apache/spark/pull/4", "created": "2018-07-25T20:44:08.627+0000"}], "num_comments": 1, "text": "Issue: SPARK-1137\nSummary: ZK Persistence Engine crashes if stored data has wrong serialVersionUID\nDescription: The ZooKeeperPersistenceEngine contains information about concurrently existing Masters and Workers. This information, as the name suggests, is persistent in the event of a Master failure/restart. If the Spark version is upgraded, the Master will crash with a Java serialization exception when trying to re-read the persisted data. Instead of crashing (indefinitely), the Master should probably just ignore the prior data.\n\nComments (1):\n1. Apache Spark: User 'aarondav' has created a pull request for this issue: https://github.com/apache/spark/pull/4", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "3ff5537700fd193cad2c24ab5804b2b6", "issue_key": "SPARK-1138", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark 0.9.0 does not work with Hadoop / HDFS", "description": "UPDATE: This problem is certainly related to trying to use Spark 0.9.0 and the latest cloudera Hadoop / HDFS in the same jar. It seems no matter how I fiddle with the deps, the do not play nice together. I'm getting a java.util.concurrent.TimeoutException when trying to create a spark context with 0.9. I cannot, whatever I do, change the timeout. I've tried using System.setProperty, the SparkConf mechanism of creating a SparkContext and the -D flags when executing my jar. I seem to be able to run simple jobs from the spark-shell OK, but my more complicated jobs require external libraries so I need to build jars and execute them. Some code that causes this: println(\"Creating config\") val conf = new SparkConf() .setMaster(clusterMaster) .setAppName(\"MyApp\") .setSparkHome(sparkHome) .set(\"spark.akka.askTimeout\", parsed.getOrElse(timeouts, \"100\")) .set(\"spark.akka.timeout\", parsed.getOrElse(timeouts, \"100\")) println(\"Creating sc\") implicit val sc = new SparkContext(conf) The output: Creating config Creating sc log4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jLogger). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. [ERROR] [02/26/2014 11:05:25.491] [main] [Remoting] Remoting error: [Startup timed out] [ akka.remote.RemoteTransportException: Startup timed out at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129) at akka.remote.Remoting.start(Remoting.scala:191) at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588) at akka.actor.ActorSystem$.apply(ActorSystem.scala:111) at akka.actor.ActorSystem$.apply(ActorSystem.scala:104) at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:96) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:126) at org.apache.spark.SparkContext.<init>(SparkContext.scala:139) at com.adbrain.accuracy.EvaluateAdtruthIDs$.main(EvaluateAdtruthIDs.scala:40) at com.adbrain.accuracy.EvaluateAdtruthIDs.main(EvaluateAdtruthIDs.scala) Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:107) at akka.remote.Remoting.start(Remoting.scala:173) ... 11 more ] Exception in thread \"main\" java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:107) at akka.remote.Remoting.start(Remoting.scala:173) at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588) at akka.actor.ActorSystem$.apply(ActorSystem.scala:111) at akka.actor.ActorSystem$.apply(ActorSystem.scala:104) at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:96) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:126) at org.apache.spark.SparkContext.<init>(SparkContext.scala:139) at com.adbrain.accuracy.EvaluateAdtruthIDs$.main(EvaluateAdtruthIDs.scala:40) at com.adbrain.accuracy.EvaluateAdtruthIDs.main(EvaluateAdtruthIDs.scala)", "reporter": "Sam Abeyratne", "assignee": null, "created": "2014-02-26T03:13:25.000+0000", "updated": "2014-10-13T13:01:25.000+0000", "resolved": "2014-06-22T07:03:50.000+0000", "labels": [], "components": [], "comments": [{"author": "Sam Abeyratne", "body": "I've narrowed it down to something to do with the hadoop version. If I include \"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-cdh4.5.0\" then this causes the timeout exception. But if I do NOT include it, I get the client protocol exception.", "created": "2014-02-26T07:53:12.190+0000"}, {"author": "Sean Owen", "body": "FWIW we also see this in integration tests for CDH5 (i.e. Hadoop 2-based). I don't have anything to add other than, yes, can't change the timeout, and I suspect that may be the issue, since other tests that initialize a Spark streaming context are fine. I tried setting akka.remote.startup-timeout on the command line, in AkkaUtils, in the construction of the SparkConf for the unit tests, etc. I always see it use a value of 10000.", "created": "2014-03-06T01:51:21.712+0000"}, {"author": "Sam Abeyratne", "body": "Sorry, I actually got further with this and forgot to update the ticket. This seems symptomatic of having the incorrect version of Scala on some nodes of the cluster, i.e. 2.9.3 instead of 2.10.3. My sysadm found this, and forced 2.10.3 across the cluster. He also mentioned resolvers in sbt build file might have been wrong, and finally we determined that \"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-mr1-cdh4.5.0\" works better. My sysadm claims some of your documentation for setting up cluster is out of date and this is why he accidentally used 2.9.3. I'll try to dig out exactly what part of the documentation needs updating. Thanks", "created": "2014-03-06T02:22:56.303+0000"}, {"author": "Sean Owen", "body": "This is helpful info. I think it's worth putting aside the timeout settings. It seems weird that changing the Hadoop version would affect Scala versions, since Hadoop 2.0.x has nothing to do with Scala. (Nor does 2.3.x as far as I can tell by looking through the dependency graph.) You're talking about a cluster but is this also how you solved a failure in the unit tests? I mean this also fails in the FlumeStreamSuite.scala test, when pointed at Hadoop 2.3. Is \"your\" = Cloudera or Spark here?", "created": "2014-03-06T03:47:38.753+0000"}, {"author": "Sam Abeyratne", "body": "The hadoop version didn't change the version of Scala, it just somehow made things hit an error that was caused by the wrong version of scala on the cluster. No, the failure didn't occur in unit tests. // Is \"your\" = Cloudera or Spark here? // Sorry, don't understand the question.", "created": "2014-03-06T04:54:51.194+0000"}, {"author": "Sean Owen", "body": "OK. Maybe a similar symptom but different cause. My failure is observed in the unit tests. I am still looking into this then. You mentioned CDH and Spark, and so did I. I am at Cloudera. I was asking about your comment that \"your documentation is out of date\" -- didn't know whether it was something I could try to fix on the CDH side if that's what you meant.", "created": "2014-03-06T07:23:26.202+0000"}, {"author": "Sam Abeyratne", "body": "Ah, I mean Spark deployment documentation, not CDH.", "created": "2014-03-06T07:31:07.623+0000"}, {"author": "Evan Chan", "body": "I also hit this, but underlying error seems different for me: By the way, this is the underlying error for me: java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;)Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:282) at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:239) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) I also have found a successful workaround, for me: Just add Akka 2.2.4 to your dependencies. I am pretty sure the problem is because Spark decided to shade Akka 2.2.3.", "created": "2014-03-13T10:36:40.133+0000"}, {"author": "Evan Chan", "body": "Actually, I believe the problem is related to Netty. I also observed that when I pull in Hadoop 2.2.x / CDH5, then I still get the timeouts. That's because Hadoop is pulling in its own version of Netty. What's more, Spark itself pulls in netty 3.6.6.Final (through akka, avro, and other deps) as well as netty-all 4.0.3.Final. The final magic incantation that seems to work is to force netty 3.6.6.Final as a dependency. \"org.apache.spark\" %% \"spark-core\" % \"0.9.0-incubating\" % \"provided\" exclude(\"io.netty\", \"netty-all\"), // Force netty version. This avoids some Spark netty dependency problem. \"io.netty\" % \"netty\" % \"3.6.6.Final\", \"org.apache.hadoop\" % \"hadoop-client\" % \"2.2.0-cdh5.0.0-beta-2\" excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib) This works for me regardless of using the default Hadoop client of 1.0.4, or 2.2.0.", "created": "2014-03-14T12:58:10.456+0000"}, {"author": "Sam Abeyratne", "body": "I'm also getting the same exception when I include any of the following (useful for reading hdfs directly) \"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-mr1-cdh4.5.0\" \"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-mr1-cdh4.6.0\" it seems to work in my unit tests with: \"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-cdh4.5.0\" But when I run it with \"hadoop jar\" it fails It seems it is not completely possible to use the latest Hadoop / HDFS with Spark 0.9.0, that or one needs to spend a few months writing a hideously complicated build file. I'd be very grateful if any of the Spark guys could, upon every release, provide SBT build file examples on how to get Spark working with various versions of Hadoop. Given that Spark is nearly always on a stack that includes Hadoop & HDFS, it seems pretty necessary to be able to use both together. I really enjoy using Spark, but it I need to use HDFS and Hadoop with it! Many thanks", "created": "2014-03-25T06:05:52.409+0000"}, {"author": "Evan Chan", "body": "Sam, did you try either of my solutions? On Tue, Mar 25, 2014 at 6:08 AM, sam (JIRA) -- -- Evan Chan Staff Engineer ev@ooyala.com |", "created": "2014-03-25T09:03:20.520+0000"}, {"author": "Sam Abeyratne", "body": "Sorry, forgot to say, yes, tried adding 2.2.4. Also tried excluding netty, but \"excludeNetty\" is not defined in my build file, so I tried something like: \"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\" exclude(\"io.netty\", \"netty-all\"), \"io.netty\" % \"netty\" % \"3.6.6.Final\", \"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-mr1-cdh4.5.0\" exclude(\"io.netty\", \"netty-all\"), \"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-cdh4.5.0\" exclude(\"io.netty\", \"netty-all\"), Eventually managed to interact with hdfs by using java -cp and specifying full URI's to the files. Trying with hadoop jar and just using the paths causes the timeout problem.", "created": "2014-03-25T09:22:16.554+0000"}, {"author": "Evan Chan", "body": "I believe the real underlying cause is just the overall ballooning of dependencies, for which I mostly blame Hadoop (and for me, Parquet.... though Parquet is merged into trunk now). There must be a real lightweight hadoop client jar that doesn't pull in the rest of the world! On Tue, Mar 25, 2014 at 9:22 AM, sam (JIRA) -- -- Evan Chan Staff Engineer ev@ooyala.com |", "created": "2014-03-25T09:31:20.602+0000"}, {"author": "Nathan Kronenfeld", "body": "diff of working and unworking dependency trees", "created": "2014-03-26T11:35:01.532+0000"}, {"author": "Nathan Kronenfeld", "body": "I don't know if it helps or not, but... I've just started to hit this error in one of my tests. I'm building with Maven, if two dependencies depend on different versions of a transitive dependency, the first listed seems to win. If I depend on spark before hbase, my test fails with the listed error. If I depend on hbase before spark, my test succeeds The two dependencies are: <dependency> <groupId>org.apache.hbase</groupId> <artifactId>hbase</artifactId> <version>0.94.15-cdh4.6.0</version> <exclusions> <exclusion> <artifactId>asm</artifactId> <groupId>asm</groupId> </exclusion> <exclusion> <artifactId>slf4j-api</artifactId> <groupId>org.slf4j</groupId> </exclusion> <exclusion> <artifactId>slf4j-log4j12</artifactId> <groupId>org.slf4j</groupId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.apache.spark</groupId> <artifactId>spark-core_2.10</artifactId> <version>0.9.0-incubating</version> </dependency> My previous comment has an attachment which is the diff between the resultant dependency:tree outputs, I hope this helps.", "created": "2014-03-26T11:35:29.038+0000"}, {"author": "Sam Abeyratne", "body": "^^ Yes, I think in SBT you can specify which order to include dependencies via merge strategies. I also have Scalding as a dep in my project and that is bound to pull in a lot too. < start SBT rant> What would be really cool, is if someone made a tool to auto generate SBT build files. You simply input the list of technologies you want to use and the versions installed, e.g. hbase, 0.94.15-cdh4.6.0, Spark 0.9.0, Log4j, testing frameworks, etc, then it generates an SBT build file that actually works (handles all the crazy dependency nonsense, resolvers, merge strategies, etc, etc). Even better, you run the script on the cluster master(s) and it works out what versions you have installed, so you just specify HBase, Spark, etc. Why is it that decades on from Makefiles building projects is just as hideous?! < end rant >", "created": "2014-03-27T09:24:10.644+0000"}, {"author": "Evan Chan", "body": "Sam: although SBT is complicated, I think the real problem is the lack of clean dependencies in much of the Java ecosystem, and this may partly be because of the lack of a real packaging system and way to delineate and contain deps. Project Jigsaw might solve this problem. For example, how much better would the Spark world be if there was a truly clean Hadoop dependency jar? On Thu, Mar 27, 2014 at 9:25 AM, sam (JIRA) < -- -- Evan Chan Staff Engineer ev@ooyala.com | <http://www.ooyala.com/> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>", "created": "2014-03-27T09:37:20.532+0000"}, {"author": "Reynold Xin", "body": "Just want to chime in that I also encountered this stack trace, and the problem was an older Netty (in particular, I have Netty 3.4 on my classpath). Once I included Netty 3.6.6, the problem went away.", "created": "2014-05-23T22:57:56.613+0000"}, {"author": "Sean R. Owen", "body": "This is no longer observed in the unit tests. The comments here say it was a Netty dependency problem, and I know that has since been cleaned up. Suggest this is resolved then?", "created": "2014-06-21T13:13:43.297+0000"}, {"author": "Patrick Wendell", "body": "Yeah let's resolve this until we can reproduce it in the current master branch.", "created": "2014-06-22T07:03:50.799+0000"}, {"author": "Russell Jurney", "body": "I built spark master with 'sbt/sbt assembly publish-local' and had issues with my hadoop version, which is CDH 4.4. Then I built with CDH 4.4, via: 'SPARK_HADOOP_VERSION=2.0.0-mr1-cdh4.4.0 sbt/sbt assembly publish-local'. Note: I did not clean. I saw this issue. This is with Spark trunk when the released version is 1.0.1. Then I cleaned and rebuilt. The issue persists. What should I do?", "created": "2014-07-29T01:53:07.528+0000"}, {"author": "Russell Jurney", "body": "See https://github.com/apache/spark/pull/455", "created": "2014-07-29T01:53:37.261+0000"}, {"author": "Sunil Prabhakara", "body": "I am using Cloudera Version 4.2.1, Spark 1.1.0 and Scala 2.10.4; Observing similar error ERROR Remoting: Remoting error: [Startup failed] [ akka.remote.RemoteTransportException: Startup failed at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129) at akka.remote.Remoting.start(Remoting.scala:194) at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588) at akka.actor.ActorSystem$.apply(ActorSystem.scala:111) at akka.actor.ActorSystem$.apply(ActorSystem.scala:104) at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:54) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1446) ... along with Exception in thread \"main\" org.jboss.netty.channel.ChannelException: Failed to bind to: <my-host-name>/10.65.42.145:0 at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:391) at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:388) at scala.util.Success$$anonfun$map$1.apply(Try.scala:206) at scala.util.Try$.apply(Try.scala:161) at scala.util.Success.map(Try.scala:206) at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ... For the second error I tried to update the /etc/hosts file with IP address of my host name and updated the spark-env.sh files with same IP address as suggested in other answer but still struck with the above issues. I tried adding Netty 3.6.6 to the dependency but still didn't get resolved.", "created": "2014-10-13T12:56:50.767+0000"}], "num_comments": 23, "text": "Issue: SPARK-1138\nSummary: Spark 0.9.0 does not work with Hadoop / HDFS\nDescription: UPDATE: This problem is certainly related to trying to use Spark 0.9.0 and the latest cloudera Hadoop / HDFS in the same jar. It seems no matter how I fiddle with the deps, the do not play nice together. I'm getting a java.util.concurrent.TimeoutException when trying to create a spark context with 0.9. I cannot, whatever I do, change the timeout. I've tried using System.setProperty, the SparkConf mechanism of creating a SparkContext and the -D flags when executing my jar. I seem to be able to run simple jobs from the spark-shell OK, but my more complicated jobs require external libraries so I need to build jars and execute them. Some code that causes this: println(\"Creating config\") val conf = new SparkConf() .setMaster(clusterMaster) .setAppName(\"MyApp\") .setSparkHome(sparkHome) .set(\"spark.akka.askTimeout\", parsed.getOrElse(timeouts, \"100\")) .set(\"spark.akka.timeout\", parsed.getOrElse(timeouts, \"100\")) println(\"Creating sc\") implicit val sc = new SparkContext(conf) The output: Creating config Creating sc log4j:WARN No appenders could be found for logger (akka.event.slf4j.Slf4jLogger). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. [ERROR] [02/26/2014 11:05:25.491] [main] [Remoting] Remoting error: [Startup timed out] [ akka.remote.RemoteTransportException: Startup timed out at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129) at akka.remote.Remoting.start(Remoting.scala:191) at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588) at akka.actor.ActorSystem$.apply(ActorSystem.scala:111) at akka.actor.ActorSystem$.apply(ActorSystem.scala:104) at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:96) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:126) at org.apache.spark.SparkContext.<init>(SparkContext.scala:139) at com.adbrain.accuracy.EvaluateAdtruthIDs$.main(EvaluateAdtruthIDs.scala:40) at com.adbrain.accuracy.EvaluateAdtruthIDs.main(EvaluateAdtruthIDs.scala) Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:107) at akka.remote.Remoting.start(Remoting.scala:173) ... 11 more ] Exception in thread \"main\" java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:107) at akka.remote.Remoting.start(Remoting.scala:173) at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588) at akka.actor.ActorSystem$.apply(ActorSystem.scala:111) at akka.actor.ActorSystem$.apply(ActorSystem.scala:104) at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:96) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:126) at org.apache.spark.SparkContext.<init>(SparkContext.scala:139) at com.adbrain.accuracy.EvaluateAdtruthIDs$.main(EvaluateAdtruthIDs.scala:40) at com.adbrain.accuracy.EvaluateAdtruthIDs.main(EvaluateAdtruthIDs.scala)\n\nComments (23):\n1. Sam Abeyratne: I've narrowed it down to something to do with the hadoop version. If I include \"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-cdh4.5.0\" then this causes the timeout exception. But if I do NOT include it, I get the client protocol exception.\n2. Sean Owen: FWIW we also see this in integration tests for CDH5 (i.e. Hadoop 2-based). I don't have anything to add other than, yes, can't change the timeout, and I suspect that may be the issue, since other tests that initialize a Spark streaming context are fine. I tried setting akka.remote.startup-timeout on the command line, in AkkaUtils, in the construction of the SparkConf for the unit tests, etc. I always see it use a value of 10000.\n3. Sam Abeyratne: Sorry, I actually got further with this and forgot to update the ticket. This seems symptomatic of having the incorrect version of Scala on some nodes of the cluster, i.e. 2.9.3 instead of 2.10.3. My sysadm found this, and forced 2.10.3 across the cluster. He also mentioned resolvers in sbt build file might have been wrong, and finally we determined that \"org.apache.hadoop\" % \"hadoop-client\" % \"2.0.0-mr1-cdh4.5.0\" works better. My sysadm claims some of your documentation for setting up cluster is out of date and this is why he accidentally used 2.9.3. I'll try to dig out exactly what part of the documentation needs updating. Thanks\n4. Sean Owen: This is helpful info. I think it's worth putting aside the timeout settings. It seems weird that changing the Hadoop version would affect Scala versions, since Hadoop 2.0.x has nothing to do with Scala. (Nor does 2.3.x as far as I can tell by looking through the dependency graph.) You're talking about a cluster but is this also how you solved a failure in the unit tests? I mean this also fails in the FlumeStreamSuite.scala test, when pointed at Hadoop 2.3. Is \"your\" = Cloudera or Spark here?\n5. Sam Abeyratne: The hadoop version didn't change the version of Scala, it just somehow made things hit an error that was caused by the wrong version of scala on the cluster. No, the failure didn't occur in unit tests. // Is \"your\" = Cloudera or Spark here? // Sorry, don't understand the question.\n6. Sean Owen: OK. Maybe a similar symptom but different cause. My failure is observed in the unit tests. I am still looking into this then. You mentioned CDH and Spark, and so did I. I am at Cloudera. I was asking about your comment that \"your documentation is out of date\" -- didn't know whether it was something I could try to fix on the CDH side if that's what you meant.\n7. Sam Abeyratne: Ah, I mean Spark deployment documentation, not CDH.\n8. Evan Chan: I also hit this, but underlying error seems different for me: By the way, this is the underlying error for me: java.lang.VerifyError: (class: org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker signature: (Ljava/util/concurrent/Executor;)Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;) Wrong return type in function at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:282) at akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:239) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) I also have found a successful workaround, for me: Just add Akka 2.2.4 to your dependencies. I am pretty sure the problem is because Spark decided to shade Akka 2.2.3.\n9. Evan Chan: Actually, I believe the problem is related to Netty. I also observed that when I pull in Hadoop 2.2.x / CDH5, then I still get the timeouts. That's because Hadoop is pulling in its own version of Netty. What's more, Spark itself pulls in netty 3.6.6.Final (through akka, avro, and other deps) as well as netty-all 4.0.3.Final. The final magic incantation that seems to work is to force netty 3.6.6.Final as a dependency. \"org.apache.spark\" %% \"spark-core\" % \"0.9.0-incubating\" % \"provided\" exclude(\"io.netty\", \"netty-all\"), // Force netty version. This avoids some Spark netty dependency problem. \"io.netty\" % \"netty\" % \"3.6.6.Final\", \"org.apache.hadoop\" % \"hadoop-client\" % \"2.2.0-cdh5.0.0-beta-2\" excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib) This works for me regardless of using the default Hadoop client of 1.0.4, or 2.2.0.\n10. Sam Abeyratne: I'm also getting the same exception when I include any of the following (useful for reading hdfs directly) \"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-mr1-cdh4.5.0\" \"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-mr1-cdh4.6.0\" it seems to work in my unit tests with: \"org.apache.hadoop\" % \"hadoop-core\" % \"2.0.0-cdh4.5.0\" But when I run it with \"hadoop jar\" it fails It seems it is not completely possible to use the latest Hadoop / HDFS with Spark 0.9.0, that or one needs to spend a few months writing a hideously complicated build file. I'd be very grateful if any of the Spark guys could, upon every release, provide SBT build file examples on how to get Spark working with various versions of Hadoop. Given that Spark is nearly always on a stack that includes Hadoop & HDFS, it seems pretty necessary to be able to use both together. I really enjoy using Spark, but it I need to use HDFS and Hadoop with it! Many thanks", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "a30b68ab06185812b368ba6403cd4d16", "issue_key": "SPARK-1139", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "APIs like saveAsNewAPIHadoopFile are actually a mixture of old and new Hadoop API", "description": "In the new Hadoop API , mapreduce.*, the JobConf are actually replaced by mapreduce.Job but in Spark, the APIs using new Hadoop APIs are still receiving conf as parameter, seems a bit weird...", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-02-26T05:13:01.000+0000", "updated": "2014-02-28T15:39:02.000+0000", "resolved": "2014-02-28T09:43:11.000+0000", "labels": ["hadoop"], "components": ["Input/Output"], "comments": [{"author": "Sean Owen", "body": "For what it's worth, I agree with this, as it will also get rid of some deprecation warnings. And to be clear this does not mean using Hadoop 2-only APIs. These new APIs existed in 1.0.4, the oldest version that Spark supports. Nan I volunteer to help if I can help make the patch or review.", "created": "2014-02-26T06:15:42.567+0000"}, {"author": "Nan Zhu", "body": "Sean, thank you very much for the comments, Actually these new APIs are included in Hadoop from 2009 (from the slides I shared in the mail list) and I found this issue with a Spark compiled with 1.0.4 compiled Thanks for your reminding, I will check if they made some changes in 2.x version Though I have started writing the patch after sending the email, it will be highly appreciated if you can help to review my PR when they are ready", "created": "2014-02-26T06:30:40.532+0000"}, {"author": "Sean Owen", "body": "Yes, I was saying the same thing. I wanted to make sure people understood that you are suggesting using \"new\" APIs that are also present in Hadoop 1.x. Sometimes people think \"new Hadoop APIs\" refers to things in Hadoop 2.x+, like YARN.", "created": "2014-02-26T06:41:13.177+0000"}, {"author": "Sanford Ryza", "body": "The current API looks reasonable to me. Are you suggesting that saveAsNewAPIHadoopFile should take a Job instead of a Configuration? JobConf was not exactly replaced by Job. It would be equally reasonable to say that it was replaced by Configuration (which the Job accepts as an argument). In fact, the trend in the new APIs is more towards setting configuration options via the Configuration object than via Job. Many of the setter methods that existed in JobConf were not transferred over to Job (e.g. setQueueName). Job is meant to encapsulate functionality for running a MapReduce job on the cluster. From a higher level, it makes sense to me that an API meant for saving a file would take configuration options, but it would be weird for it to take a \"Job\" that it's not going to run.", "created": "2014-02-28T01:49:46.198+0000"}, {"author": "Sean Owen", "body": "Agree with that -- Configuration is the more natural replacement, not Job. But in any event it should be replaced with something since JobConf is deprecated. Any of the .mapred. classes should not be used at this point.", "created": "2014-02-28T02:08:26.307+0000"}, {"author": "Nan Zhu", "body": "I agree with that Job contains more functionalities for running than for configuration but in the current implementation, due to the fact that methods like OutputFormat.setOutputPath only accepts Job as the parameter, the first thing after we receive a Configuration is to create a new Job object...(though we can set output path by configuration.set() ) the other example I have given in the mail list is input methods, (paste in the last part of the post), where we create a job object from a Configuration and then get its Configuration member for passing as the parameter, and inside NewHadoopRDD, we re-construct the jobcontext object again... I know that most of these are due to the design of Hadoop APIs, but if we accept Job directly, it can simplify the implementation, or say making it more reasonable (for the setters you mentioned, we assume users should set via Configuration before passing Job object as the parameter) Thank you for the comments -------- val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf) then we create a jobContext based on this configuration object NewHadoopRDD.scala (L74) val jobContext = newJobContext(conf, jobId) val rawSplits = inputFormat.getSplits(jobContext).toArray", "created": "2014-02-28T05:42:10.998+0000"}, {"author": "Sanford Ryza", "body": "bq. But in any event it should be replaced with something since JobConf is deprecated. Let me know if I'm missing your meaning here, but there's nothing in the saveAsNewAPIHadoopFile method signature that requires using any deprecated objects or methods. If there are specific deprecation warnings you have in mind, we can track them down to what in the implementation is causing them? bq. if we accept Job directly, it can simplify the implementation I think it's better to have an API that makes sense than a simple underlying implementation. Also, there's no reason the implementation needs to construct this Job object. We could equivalently call conf.set(FileInputFormat.OUT_DIR, path).", "created": "2014-02-28T09:26:29.507+0000"}, {"author": "Nan Zhu", "body": "I think Sean is talking about the future when JobConf is deprecated and in current implementation, it is using Configuration Yes, I was saying that we can call conf.set() to avoid new a Job object, but that can only solve the problem in output function (yes...the current implementation needs to be revised); static methods like InputFomat.getSplits() still needs JobContext, so in any way we have to pass Configuration with getConfiguration() of mapreduce.Job and reconstruct mapreduce.Job with new Job(Configuration conf).... Anyway, I would like to keep the APIs unchanged based on our discussions and reconsideration about the cost of changing APIs and the benefit brought by reducing two or three lines of code.....", "created": "2014-02-28T09:42:42.179+0000"}, {"author": "Sean Owen", "body": "`JobConf` itself is actually not deprecated; a lot of its API is. I misspoke sort of. I took this issue to really be about `.mapred.` vs `.mapreduce.` classes. As I understand, the former are not formally deprecated, but are certainly meant to be superseded by the latter. There has been a lot of back-and-forth on this, leading to the current pretty confusing situation, but that's my understanding of the intent. It seems better to not mix these two parallel but separate APIs. And if you pick one pick `.mapreduce.`, as I see increasingly the new Hadoop 2.x code using the new package and new property names. So, I don't think this is a big issue, but it's bigger than what you originally noted Nan. I do think it's worth touching up everywhere just for sanity and future-proofing. Leave `Job` if we must; I mean the migration to `.mapreduce.` What if I were so bold as to create a patch anyway?", "created": "2014-02-28T09:54:18.226+0000"}, {"author": "Nan Zhu", "body": "I think the newHadoopAPI functions are OK, since they are actually using Configuration...As Sandy said, we can revise the current implementation to reduce the chance of getConfiguration, reconstruct Job, and get Configuration and reconstruct again, Sean, do you mean you want to fix saveHadoopFile family functions? I think they are intended to accept JobConf to support those old APIs, I think in future when Hadoop community deprecates those APIs, we only need to add @deprecate there?", "created": "2014-02-28T10:03:02.092+0000"}, {"author": "Nan Zhu", "body": "To my surprise, Job in mapreduce package are still using JobConf in some setters,  protected final org.apache.hadoop.mapred.JobConf conf; public void setOutputFormatClass(Class<? extends OutputFormat> cls ) throws IllegalStateException { ensureState(JobState.DEFINE); conf.setClass(OUTPUT_FORMAT_CLASS_ATTR, cls, OutputFormat.class); }", "created": "2014-02-28T10:09:50.062+0000"}, {"author": "Sean Owen", "body": "Well, before I say much more, I should actually look at all these occurrences of .mapred. in more detail. From similar surgery on another project I'm aware that there are weird gotchas -- like new APIs using old ones, or some method not yet existing in the new one. I am not concerned about Job vs JobConf per se but about using .mapreduce. where there is no point in staying with a .mapred. class. You can leave this closed and leave it on me to come back at some point with a patch for consideration.", "created": "2014-02-28T10:14:54.901+0000"}, {"author": "Nan Zhu", "body": "sure", "created": "2014-02-28T10:20:35.867+0000"}, {"author": "Sean Owen", "body": "I take it back. After looking at the code, most of the usages are in contexts where there is explicitly a method for the old and new APIs. There are a few instances where we could avoid the old API classes altogether, sure. The JobConf situation is a little tricky. Overall I think there should probably be a later changelist that deletes the \"old API\" versions of methods and then at that time change these other things. Really, the scope of the API issue is small.", "created": "2014-02-28T14:48:15.962+0000"}, {"author": "Nan Zhu", "body": "yes, I think we can keep the code as this at the moment, when the old Hadoop API is explicitly deprecated, we just add annotations or sweep out all mapred.* from Spark", "created": "2014-02-28T15:39:02.012+0000"}], "num_comments": 15, "text": "Issue: SPARK-1139\nSummary: APIs like saveAsNewAPIHadoopFile are actually a mixture of old and new Hadoop API\nDescription: In the new Hadoop API , mapreduce.*, the JobConf are actually replaced by mapreduce.Job but in Spark, the APIs using new Hadoop APIs are still receiving conf as parameter, seems a bit weird...\n\nComments (15):\n1. Sean Owen: For what it's worth, I agree with this, as it will also get rid of some deprecation warnings. And to be clear this does not mean using Hadoop 2-only APIs. These new APIs existed in 1.0.4, the oldest version that Spark supports. Nan I volunteer to help if I can help make the patch or review.\n2. Nan Zhu: Sean, thank you very much for the comments, Actually these new APIs are included in Hadoop from 2009 (from the slides I shared in the mail list) and I found this issue with a Spark compiled with 1.0.4 compiled Thanks for your reminding, I will check if they made some changes in 2.x version Though I have started writing the patch after sending the email, it will be highly appreciated if you can help to review my PR when they are ready\n3. Sean Owen: Yes, I was saying the same thing. I wanted to make sure people understood that you are suggesting using \"new\" APIs that are also present in Hadoop 1.x. Sometimes people think \"new Hadoop APIs\" refers to things in Hadoop 2.x+, like YARN.\n4. Sanford Ryza: The current API looks reasonable to me. Are you suggesting that saveAsNewAPIHadoopFile should take a Job instead of a Configuration? JobConf was not exactly replaced by Job. It would be equally reasonable to say that it was replaced by Configuration (which the Job accepts as an argument). In fact, the trend in the new APIs is more towards setting configuration options via the Configuration object than via Job. Many of the setter methods that existed in JobConf were not transferred over to Job (e.g. setQueueName). Job is meant to encapsulate functionality for running a MapReduce job on the cluster. From a higher level, it makes sense to me that an API meant for saving a file would take configuration options, but it would be weird for it to take a \"Job\" that it's not going to run.\n5. Sean Owen: Agree with that -- Configuration is the more natural replacement, not Job. But in any event it should be replaced with something since JobConf is deprecated. Any of the .mapred. classes should not be used at this point.\n6. Nan Zhu: I agree with that Job contains more functionalities for running than for configuration but in the current implementation, due to the fact that methods like OutputFormat.setOutputPath only accepts Job as the parameter, the first thing after we receive a Configuration is to create a new Job object...(though we can set output path by configuration.set() ) the other example I have given in the mail list is input methods, (paste in the last part of the post), where we create a job object from a Configuration and then get its Configuration member for passing as the parameter, and inside NewHadoopRDD, we re-construct the jobcontext object again... I know that most of these are due to the design of Hadoop APIs, but if we accept Job directly, it can simplify the implementation, or say making it more reasonable (for the setters you mentioned, we assume users should set via Configuration before passing Job object as the parameter) Thank you for the comments -------- val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf) then we create a jobContext based on this configuration object NewHadoopRDD.scala (L74) val jobContext = newJobContext(conf, jobId) val rawSplits = inputFormat.getSplits(jobContext).toArray\n7. Sanford Ryza: bq. But in any event it should be replaced with something since JobConf is deprecated. Let me know if I'm missing your meaning here, but there's nothing in the saveAsNewAPIHadoopFile method signature that requires using any deprecated objects or methods. If there are specific deprecation warnings you have in mind, we can track them down to what in the implementation is causing them? bq. if we accept Job directly, it can simplify the implementation I think it's better to have an API that makes sense than a simple underlying implementation. Also, there's no reason the implementation needs to construct this Job object. We could equivalently call conf.set(FileInputFormat.OUT_DIR, path).\n8. Nan Zhu: I think Sean is talking about the future when JobConf is deprecated and in current implementation, it is using Configuration Yes, I was saying that we can call conf.set() to avoid new a Job object, but that can only solve the problem in output function (yes...the current implementation needs to be revised); static methods like InputFomat.getSplits() still needs JobContext, so in any way we have to pass Configuration with getConfiguration() of mapreduce.Job and reconstruct mapreduce.Job with new Job(Configuration conf).... Anyway, I would like to keep the APIs unchanged based on our discussions and reconsideration about the cost of changing APIs and the benefit brought by reducing two or three lines of code.....\n9. Sean Owen: `JobConf` itself is actually not deprecated; a lot of its API is. I misspoke sort of. I took this issue to really be about `.mapred.` vs `.mapreduce.` classes. As I understand, the former are not formally deprecated, but are certainly meant to be superseded by the latter. There has been a lot of back-and-forth on this, leading to the current pretty confusing situation, but that's my understanding of the intent. It seems better to not mix these two parallel but separate APIs. And if you pick one pick `.mapreduce.`, as I see increasingly the new Hadoop 2.x code using the new package and new property names. So, I don't think this is a big issue, but it's bigger than what you originally noted Nan. I do think it's worth touching up everywhere just for sanity and future-proofing. Leave `Job` if we must; I mean the migration to `.mapreduce.` What if I were so bold as to create a patch anyway?\n10. Nan Zhu: I think the newHadoopAPI functions are OK, since they are actually using Configuration...As Sandy said, we can revise the current implementation to reduce the chance of getConfiguration, reconstruct Job, and get Configuration and reconstruct again, Sean, do you mean you want to fix saveHadoopFile family functions? I think they are intended to accept JobConf to support those old APIs, I think in future when Hadoop community deprecates those APIs, we only need to add @deprecate there?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "712abd5a9ee9fd00c61b47fa71aa0715", "issue_key": "SPARK-1140", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Remove references to ClusterScheduler", "description": "There are a bunch of references to ClusterScheduler in comments and in the name of tests. ClusterScheduler has been replaced by TaskSchedulerImpl, so the references should be updated accordingly.", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "created": "2014-02-26T16:00:20.000+0000", "updated": "2014-02-26T22:53:27.000+0000", "resolved": "2014-02-26T22:53:27.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1140\nSummary: Remove references to ClusterScheduler\nDescription: There are a bunch of references to ClusterScheduler in comments and in the name of tests. ClusterScheduler has been replaced by TaskSchedulerImpl, so the references should be updated accordingly.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "03ad550e181657c49586583b034471fd", "issue_key": "SPARK-1141", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Parallelize Task Serialization", "description": "When task closures are large, task serialization becomes the bottleneck in scheduling. We should parallelize task serialization to alleviate this problem.", "reporter": "Kay Ousterhout", "assignee": "OuyangJin", "created": "2014-02-26T16:12:11.000+0000", "updated": "2017-02-07T06:45:04.000+0000", "resolved": "2014-11-06T07:46:00.000+0000", "labels": [], "components": [], "comments": [{"author": "OuyangJin", "body": "I can work on this But @Kay, when you mean Parallelize, you mean use for a single task to paralize its serlization or for a taskSet use multiple threads and each thread working on a single task", "created": "2014-03-02T05:28:44.374+0000"}, {"author": "Kay Ousterhout", "body": "The latter -- have multiple threads that each serialize a particular task.", "created": "2014-03-02T09:41:43.554+0000"}, {"author": "Kay Ousterhout", "body": "Also, I think the first step for this should probably be a short design doc explaining how to do this, because I think it will be a somewhat invasive patch.", "created": "2014-03-02T10:32:20.037+0000"}, {"author": "OuyangJin", "body": "@Kay Thanks for your advice. I'll write a design doc for this and attach it on JIRA. And I can start coding after you think the design is good to go", "created": "2014-03-02T22:00:59.628+0000"}, {"author": "OuyangJin", "body": "Add a initial design doc for it.@Kay. Any advice is welcomed", "created": "2014-03-09T09:27:48.498+0000"}, {"author": "Kay Ousterhout", "body": "Thanks for writing this up! I won't be able to look at this until Wednesday but will definitely take a look then.", "created": "2014-03-10T15:05:36.456+0000"}, {"author": "OuyangJin", "body": "@Kay, it doesn't matter. Looking forward to your advice", "created": "2014-03-10T23:25:40.380+0000"}, {"author": "Kay Ousterhout", "body": "I took a look at the design doc -- thanks for writing that up! You propose adding a thread pool in each TaskSetManager for doing the serialization; did you think about adding the thread pool in TaskSchedulerImpl instead / what was your reasoning behind having the thread pool in TaskSetManager? One use case to consider, if you haven't already, is that when the cluster is very busy (so all or most of the workers are in use), TaskSchedulerImpl.resourceOffer() will typically only get called with an offer for a single worker (because that worker just finished a task, and all of the other workers are busy). We should make sure that, even in this case, the task serialization can be parallelized.", "created": "2014-03-12T14:27:47.518+0000"}, {"author": "OuyangJin", "body": "@Kay.Thanks very much for your review advice Of course we can put this thread pool inside TaskSchedulerImpl, and I think this solution is more or less neatly than putting it inside TaskSetManager. I really didn't consider your use case. Actually my original solution wants to create a asynchronized status between TaskSetManager and TaskSchedulerImpl, that is , it that loop , everytime we want serialize a task, we add it to TaskSetManager's own threadpool, and when the loop is finished, this threadpool has all wanted \" to be serialized \"task. and we have to wait for all serialization worker thread finish and record some states. I have to admit that my original solution is somewhat complicated. And put a threadpool inside TaskSchedulerImpl is simpler and neatly. Sorry for that. So if we put threadpool inside TaskSchedulerImpl, my implementation will be like this: TaskSetManager.resourceOffer will return a TaskDescWithoutSerializeTask object , this object will be a half-copy of TaskDescrption exception _serializedTask ByteBffer, instead, it will contain a Task object and seriailze part inside TaskSetManager.resourceOffer will be moved to TaskSchedulerImpl's \"Runnable\" working thread which will be placed inside threadpool. @Kay,do you think this design make sense. Today I implmented mainly part of dev code, and I can send this PR which is WIP. And I think we should add test case for this change Thanks!", "created": "2014-03-14T06:22:06.510+0000"}, {"author": "Kay Ousterhout", "body": "This sounds like a good plan and submitting the WIP PR sounds great! Thanks!", "created": "2014-03-16T10:29:29.512+0000"}, {"author": "Kay Ousterhout", "body": "Any update on this? I'd like this to get into 1.0; if you don't have time to work on this more I'll have some time to work on it Wednesday and can submit a patch.", "created": "2014-03-23T22:56:29.408+0000"}, {"author": "OuyangJin", "body": "sorry for late reply .I work on this the whole last weekend ,and one test case just failed, and I 'm not sure if it's related to my patch becuause lack of time. I can send my PR and you can review it first. And I 'll look into the fail case later today", "created": "2014-03-23T23:10:30.868+0000"}, {"author": "OuyangJin", "body": "PR is https://github.com/apache/spark/pull/214", "created": "2014-03-23T23:33:57.415+0000"}, {"author": "OuyangJin", "body": "Sorry again for late respons(just thinking of post the PR when all test case passed)", "created": "2014-03-23T23:34:41.943+0000"}, {"author": "OuyangJin", "body": "PR updated, failed case DriverSuite has already pass", "created": "2014-03-24T09:11:11.773+0000"}, {"author": "Kay Ousterhout", "body": "Because Spark now broadcasts the task binary, task serialization is no longer a bottleneck in the scheduler, and implementing this feature adds significant scheduler complexity.", "created": "2014-11-06T07:46:00.199+0000"}], "num_comments": 16, "text": "Issue: SPARK-1141\nSummary: Parallelize Task Serialization\nDescription: When task closures are large, task serialization becomes the bottleneck in scheduling. We should parallelize task serialization to alleviate this problem.\n\nComments (16):\n1. OuyangJin: I can work on this But @Kay, when you mean Parallelize, you mean use for a single task to paralize its serlization or for a taskSet use multiple threads and each thread working on a single task\n2. Kay Ousterhout: The latter -- have multiple threads that each serialize a particular task.\n3. Kay Ousterhout: Also, I think the first step for this should probably be a short design doc explaining how to do this, because I think it will be a somewhat invasive patch.\n4. OuyangJin: @Kay Thanks for your advice. I'll write a design doc for this and attach it on JIRA. And I can start coding after you think the design is good to go\n5. OuyangJin: Add a initial design doc for it.@Kay. Any advice is welcomed\n6. Kay Ousterhout: Thanks for writing this up! I won't be able to look at this until Wednesday but will definitely take a look then.\n7. OuyangJin: @Kay, it doesn't matter. Looking forward to your advice\n8. Kay Ousterhout: I took a look at the design doc -- thanks for writing that up! You propose adding a thread pool in each TaskSetManager for doing the serialization; did you think about adding the thread pool in TaskSchedulerImpl instead / what was your reasoning behind having the thread pool in TaskSetManager? One use case to consider, if you haven't already, is that when the cluster is very busy (so all or most of the workers are in use), TaskSchedulerImpl.resourceOffer() will typically only get called with an offer for a single worker (because that worker just finished a task, and all of the other workers are busy). We should make sure that, even in this case, the task serialization can be parallelized.\n9. OuyangJin: @Kay.Thanks very much for your review advice Of course we can put this thread pool inside TaskSchedulerImpl, and I think this solution is more or less neatly than putting it inside TaskSetManager. I really didn't consider your use case. Actually my original solution wants to create a asynchronized status between TaskSetManager and TaskSchedulerImpl, that is , it that loop , everytime we want serialize a task, we add it to TaskSetManager's own threadpool, and when the loop is finished, this threadpool has all wanted \" to be serialized \"task. and we have to wait for all serialization worker thread finish and record some states. I have to admit that my original solution is somewhat complicated. And put a threadpool inside TaskSchedulerImpl is simpler and neatly. Sorry for that. So if we put threadpool inside TaskSchedulerImpl, my implementation will be like this: TaskSetManager.resourceOffer will return a TaskDescWithoutSerializeTask object , this object will be a half-copy of TaskDescrption exception _serializedTask ByteBffer, instead, it will contain a Task object and seriailze part inside TaskSetManager.resourceOffer will be moved to TaskSchedulerImpl's \"Runnable\" working thread which will be placed inside threadpool. @Kay,do you think this design make sense. Today I implmented mainly part of dev code, and I can send this PR which is WIP. And I think we should add test case for this change Thanks!\n10. Kay Ousterhout: This sounds like a good plan and submitting the WIP PR sounds great! Thanks!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "e6df862f9cfe0545b6a72d46b1179bc2", "issue_key": "SPARK-1142", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Allow adding jars on app submission, outside of code", "description": "yarn-standalone mode supports an option that allows adding jars that will be distributed on the cluster with job submission. Providing similar functionality for other app submission modes will allow the spark-app script proposed in SPARK-1126 to support an add-jars option that works for every submit mode.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-02-26T17:09:55.000+0000", "updated": "2015-02-09T18:45:58.000+0000", "resolved": "2015-02-09T18:45:58.000+0000", "labels": [], "components": ["Spark Submit"], "comments": [{"author": "Patrick Wendell", "body": "This already exists - you can use the --jars flag to spark-submit or set 'spark.jars' manually.", "created": "2015-02-09T18:45:52.230+0000"}], "num_comments": 1, "text": "Issue: SPARK-1142\nSummary: Allow adding jars on app submission, outside of code\nDescription: yarn-standalone mode supports an option that allows adding jars that will be distributed on the cluster with job submission. Providing similar functionality for other app submission modes will allow the spark-app script proposed in SPARK-1126 to support an add-jars option that works for every submit mode.\n\nComments (1):\n1. Patrick Wendell: This already exists - you can use the --jars flag to spark-submit or set 'spark.jars' manually.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "69a82737c2978949371d2704fab4c24c", "issue_key": "SPARK-1143", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "ClusterSchedulerSuite (soon to be TaskSchedulerImplSuite) does not actually test the ClusterScheduler/TaskSchedulerImpl", "description": "This test should probably be both refactored and renamed -- it really tests the Pool / fair scheduling mechanisms and completely bypasses the scheduling code in TaskSchedulerImpl and TaskSetManager.", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "created": "2014-02-26T17:15:59.000+0000", "updated": "2015-01-09T17:47:33.000+0000", "resolved": "2015-01-09T17:47:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Nan Zhu", "body": "made a PR: https://github.com/apache/spark/pull/339", "created": "2014-04-06T22:24:22.452+0000"}, {"author": "Apache Spark", "body": "User 'kayousterhout' has created a pull request for this issue: https://github.com/apache/spark/pull/3967", "created": "2015-01-09T06:12:30.129+0000"}], "num_comments": 2, "text": "Issue: SPARK-1143\nSummary: ClusterSchedulerSuite (soon to be TaskSchedulerImplSuite) does not actually test the ClusterScheduler/TaskSchedulerImpl\nDescription: This test should probably be both refactored and renamed -- it really tests the Pool / fair scheduling mechanisms and completely bypasses the scheduling code in TaskSchedulerImpl and TaskSetManager.\n\nComments (2):\n1. Nan Zhu: made a PR: https://github.com/apache/spark/pull/339\n2. Apache Spark: User 'kayousterhout' has created a pull request for this issue: https://github.com/apache/spark/pull/3967", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "9e11f59b1e01e6657cc0166761bc729d", "issue_key": "SPARK-1145", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Memory mapping with many small blocks can cause JVM allocation failures", "description": "During a shuffle each block or block segment is memory mapped to a file. When the segments are very small and there are a large number of them, the memory maps can start failing and eventually the JVM will terminate. It's not clear exactly what's happening but it appears that when the JVM terminates about 265MB of virtual address space is used by memory mapped files. This doesn't seem affected at all by `-XXmaxdirectmemorysize` - AFAIK that option is just to give the JVM its own self imposed limit rather than allow it to run into OS limits. At the time of JVM failure it appears the overall OS memory becomes scarce, so it's possible there are overheads for each memory mapped file that are adding up here. One overhead is that the memory mapping occurs at the granularity of pages, so if blocks are really small there is natural overhead required to pad to the page boundary. In the particular case where I saw this, the JVM was running 4 reducers, each of which was trying to access about 30,000 blocks for a total of 120,000 concurrent reads. At about 65,000 open files it crapped out. In this case each file was about 1000 bytes. User should really be coalescing or using fewer reducers if they have 1000 byte shuffle files, but I expect this to happen nonetheless. My proposal was that if the file is smaller than a few pages, we should just read it into a java buffer and not bother to memory map it. Memory mapping huge numbers of small files in the JVM is neither recommended or good for performance, AFAIK. Below is the stack trace:  14/02/27 08:32:35 ERROR storage.BlockManagerWorker: Exception handling buffer message java.io.IOException: Map failed at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888) at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:89) at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:285) at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90) at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69) at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44) at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28) at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44) at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34) at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34) at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:512) at org.apache.spark.network.ConnectionManager$$anon$8.run(ConnectionManager.scala:478) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  And the JVM error log had a bunch of entries like this:  7f4b48f89000-7f4b48f8a000 r--s 00000000 ca:30 1622077901 /mnt4/spark/spark-local-20140227020022-227c/26/shuffle_0_22312_38 7f4b48f8a000-7f4b48f8b000 r--s 00000000 ca:20 545892715 /mnt3/spark/spark-local-20140227020022-5ef5/3a/shuffle_0_26808_20 7f4b48f8b000-7f4b48f8c000 r--s 00000000 ca:50 1622480741 /mnt2/spark/spark-local-20140227020022-315b/1c/shuffle_0_29013_19 7f4b48f8c000-7f4b48f8d000 r--s 00000000 ca:30 10082610 /mnt4/spark/spark-local-20140227020022-227c/3b/shuffle_0_28002_9 7f4b48f8d000-7f4b48f8e000 r--s 00000000 ca:50 1622268539 /mnt2/spark/spark-local-20140227020022-315b/3e/shuffle_0_23983_17 7f4b48f8e000-7f4b48f8f000 r--s 00000000 ca:50 1083068239 /mnt2/spark/spark-local-20140227020022-315b/37/shuffle_0_25505_22 7f4b48f8f000-7f4b48f90000 r--s 00000000 ca:30 9921006 /mnt4/spark/spark-local-20140227020022-227c/31/shuffle_0_24072_95 7f4b48f90000-7f4b48f91000 r--s 00000000 ca:50 10441349 /mnt2/spark/spark-local-20140227020022-315b/20/shuffle_0_27409_47 7f4b48f91000-7f4b48f92000 r--s 00000000 ca:50 10406042 /mnt2/spark/spark-local-20140227020022-315b/0e/shuffle_0_26481_84 7f4b48f92000-7f4b48f93000 r--s 00000000 ca:50 1622268192 /mnt2/spark/spark-local-20140227020022-315b/14/shuffle_0_23818_92 7f4b48f93000-7f4b48f94000 r--s 00000000 ca:50 1082957628 /mnt2/spark/spark-local-20140227020022-315b/09/shuffle_0_22824_45 7f4b48f94000-7f4b48f95000 r--s 00000000 ca:20 1082199965 /mnt3/spark/spark-local-20140227020022-5ef5/00/shuffle_0_1429_13 7f4b48f95000-7f4b48f96000 r--s 00000000 ca:20 10940995 /mnt3/spark/spark-local-20140227020022-5ef5/38/shuffle_0_28705_44 7f4b48f96000-7f4b48f97000 r--s 00000000 ca:10 17456971 /mnt/spark/spark-local-20140227020022-b372/28/shuffle_0_23055_72 7f4b48f97000-7f4b48f98000 r--s 00000000 ca:30 9853895 /mnt4/spark/spark-local-20140227020022-227c/08/shuffle_0_22797_42 7f4b48f98000-7f4b48f99000 r--s 00000000 ca:20 1622089728 /mnt3/spark/spark-local-20140227020022-5ef5/27/shuffle_0_24017_97 7f4b48f99000-7f4b48f9a000 r--s 00000000 ca:50 1082937570 /mnt2/spark/spark-local-20140227020022-315b/24/shuffle_0_22291_38 7f4b48f9a000-7f4b48f9b000 r--s 00000000 ca:30 10056604 /mnt4/spark/spark-local-20140227020022-227c/2f/shuffle_0_27408_59", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "created": "2014-02-27T00:20:03.000+0000", "updated": "2017-12-11T12:59:19.000+0000", "resolved": "2014-04-28T00:41:22.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick Wendell", "body": "Issue resolved by pull request 43 [https://github.com/apache/spark/pull/43]", "created": "2014-04-28T00:41:22.226+0000"}], "num_comments": 1, "text": "Issue: SPARK-1145\nSummary: Memory mapping with many small blocks can cause JVM allocation failures\nDescription: During a shuffle each block or block segment is memory mapped to a file. When the segments are very small and there are a large number of them, the memory maps can start failing and eventually the JVM will terminate. It's not clear exactly what's happening but it appears that when the JVM terminates about 265MB of virtual address space is used by memory mapped files. This doesn't seem affected at all by `-XXmaxdirectmemorysize` - AFAIK that option is just to give the JVM its own self imposed limit rather than allow it to run into OS limits. At the time of JVM failure it appears the overall OS memory becomes scarce, so it's possible there are overheads for each memory mapped file that are adding up here. One overhead is that the memory mapping occurs at the granularity of pages, so if blocks are really small there is natural overhead required to pad to the page boundary. In the particular case where I saw this, the JVM was running 4 reducers, each of which was trying to access about 30,000 blocks for a total of 120,000 concurrent reads. At about 65,000 open files it crapped out. In this case each file was about 1000 bytes. User should really be coalescing or using fewer reducers if they have 1000 byte shuffle files, but I expect this to happen nonetheless. My proposal was that if the file is smaller than a few pages, we should just read it into a java buffer and not bother to memory map it. Memory mapping huge numbers of small files in the JVM is neither recommended or good for performance, AFAIK. Below is the stack trace:  14/02/27 08:32:35 ERROR storage.BlockManagerWorker: Exception handling buffer message java.io.IOException: Map failed at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888) at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:89) at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:285) at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90) at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69) at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44) at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28) at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44) at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34) at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34) at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:512) at org.apache.spark.network.ConnectionManager$$anon$8.run(ConnectionManager.scala:478) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  And the JVM error log had a bunch of entries like this:  7f4b48f89000-7f4b48f8a000 r--s 00000000 ca:30 1622077901 /mnt4/spark/spark-local-20140227020022-227c/26/shuffle_0_22312_38 7f4b48f8a000-7f4b48f8b000 r--s 00000000 ca:20 545892715 /mnt3/spark/spark-local-20140227020022-5ef5/3a/shuffle_0_26808_20 7f4b48f8b000-7f4b48f8c000 r--s 00000000 ca:50 1622480741 /mnt2/spark/spark-local-20140227020022-315b/1c/shuffle_0_29013_19 7f4b48f8c000-7f4b48f8d000 r--s 00000000 ca:30 10082610 /mnt4/spark/spark-local-20140227020022-227c/3b/shuffle_0_28002_9 7f4b48f8d000-7f4b48f8e000 r--s 00000000 ca:50 1622268539 /mnt2/spark/spark-local-20140227020022-315b/3e/shuffle_0_23983_17 7f4b48f8e000-7f4b48f8f000 r--s 00000000 ca:50 1083068239 /mnt2/spark/spark-local-20140227020022-315b/37/shuffle_0_25505_22 7f4b48f8f000-7f4b48f90000 r--s 00000000 ca:30 9921006 /mnt4/spark/spark-local-20140227020022-227c/31/shuffle_0_24072_95 7f4b48f90000-7f4b48f91000 r--s 00000000 ca:50 10441349 /mnt2/spark/spark-local-20140227020022-315b/20/shuffle_0_27409_47 7f4b48f91000-7f4b48f92000 r--s 00000000 ca:50 10406042 /mnt2/spark/spark-local-20140227020022-315b/0e/shuffle_0_26481_84 7f4b48f92000-7f4b48f93000 r--s 00000000 ca:50 1622268192 /mnt2/spark/spark-local-20140227020022-315b/14/shuffle_0_23818_92 7f4b48f93000-7f4b48f94000 r--s 00000000 ca:50 1082957628 /mnt2/spark/spark-local-20140227020022-315b/09/shuffle_0_22824_45 7f4b48f94000-7f4b48f95000 r--s 00000000 ca:20 1082199965 /mnt3/spark/spark-local-20140227020022-5ef5/00/shuffle_0_1429_13 7f4b48f95000-7f4b48f96000 r--s 00000000 ca:20 10940995 /mnt3/spark/spark-local-20140227020022-5ef5/38/shuffle_0_28705_44 7f4b48f96000-7f4b48f97000 r--s 00000000 ca:10 17456971 /mnt/spark/spark-local-20140227020022-b372/28/shuffle_0_23055_72 7f4b48f97000-7f4b48f98000 r--s 00000000 ca:30 9853895 /mnt4/spark/spark-local-20140227020022-227c/08/shuffle_0_22797_42 7f4b48f98000-7f4b48f99000 r--s 00000000 ca:20 1622089728 /mnt3/spark/spark-local-20140227020022-5ef5/27/shuffle_0_24017_97 7f4b48f99000-7f4b48f9a000 r--s 00000000 ca:50 1082937570 /mnt2/spark/spark-local-20140227020022-315b/24/shuffle_0_22291_38 7f4b48f9a000-7f4b48f9b000 r--s 00000000 ca:30 10056604 /mnt4/spark/spark-local-20140227020022-227c/2f/shuffle_0_27408_59\n\nComments (1):\n1. Patrick Wendell: Issue resolved by pull request 43 [https://github.com/apache/spark/pull/43]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "ddcca7af621915090d849c0450ba988f", "issue_key": "SPARK-1146", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Vagrant to setup Spark cluster locally", "description": "We should use Vagrant to create a local clusters of VMs. It will allow developers run and test Spark Cluster on their dev machines. It could be expanded to YARN and Mesos cluster mode but initial focus will be on standalone.", "reporter": "Binh Nguyen", "assignee": null, "created": "2014-02-27T00:43:16.000+0000", "updated": "2014-12-10T15:50:02.000+0000", "resolved": "2014-12-10T15:50:02.000+0000", "labels": ["script"], "components": ["Deploy", "Project Infra"], "comments": [{"author": "Binh Nguyen", "body": "ticket for this PR: https://github.com/apache/spark/pull/26", "created": "2014-02-27T00:44:02.775+0000"}, {"author": "Sean R. Owen", "body": "(Warning: I've developed a little script to help find JIRAs whose PRs are resolved one way or the other, and so should be resolved. There may be a number of these coming in the next day or two.) The discussion in the PR indicates this will be a separate project if anything, currently hosted at https://github.com/ngbinh/spark-vagrant", "created": "2014-12-10T15:50:02.660+0000"}], "num_comments": 2, "text": "Issue: SPARK-1146\nSummary: Vagrant to setup Spark cluster locally\nDescription: We should use Vagrant to create a local clusters of VMs. It will allow developers run and test Spark Cluster on their dev machines. It could be expanded to YARN and Mesos cluster mode but initial focus will be on standalone.\n\nComments (2):\n1. Binh Nguyen: ticket for this PR: https://github.com/apache/spark/pull/26\n2. Sean R. Owen: (Warning: I've developed a little script to help find JIRAs whose PRs are resolved one way or the other, and so should be resolved. There may be a number of these coming in the next day or two.) The discussion in the PR indicates this will be a separate project if anything, currently hosted at https://github.com/ngbinh/spark-vagrant", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "ec029c09a7c4c067bcb6a767caf54527", "issue_key": "SPARK-1147", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark-project.org still goes to http://spark.incubator.apache.org/", "description": "http://spark-project.org/ still leads to http://spark.incubator.apache.org/, forwarding should be updated to http://spark.apache.org/", "reporter": "Jyotiska NK", "assignee": "Andy Konwinski", "created": "2014-02-27T01:56:04.000+0000", "updated": "2014-03-04T14:08:23.000+0000", "resolved": "2014-03-04T14:08:23.000+0000", "labels": [], "components": ["Project Infra"], "comments": [{"author": "Andy Konwinski", "body": "This is fixed now any traffic going to spark-project.org/PATH goes to spark.apache.org/PATH.", "created": "2014-03-04T14:08:23.427+0000"}], "num_comments": 1, "text": "Issue: SPARK-1147\nSummary: spark-project.org still goes to http://spark.incubator.apache.org/\nDescription: http://spark-project.org/ still leads to http://spark.incubator.apache.org/, forwarding should be updated to http://spark.apache.org/\n\nComments (1):\n1. Andy Konwinski: This is fixed now any traffic going to spark-project.org/PATH goes to spark.apache.org/PATH.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "2c785277d2cff186e0a5dbfefc699fd7", "issue_key": "SPARK-1148", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Suggestions for exception handling (avoid potential bugs)", "description": "Hi Spark developers, We are a group of researchers on software reliability. Recently we did a study and found that majority of the most severe failures in data-analytic systems are caused by bugs in exception handlers – that it is hard to anticipate all the possible real-world error scenarios. Therefore we built a simple checking tool that automatically detects some bug patterns that have caused some very severe real-world failures. I am reporting a few cases here. Any feedback is much appreciated! Ding ========================= Case 1: Line: 1249, File: \"org/apache/spark/SparkContext.scala\"  1244: val scheduler = try { 1245: val clazz = Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterScheduler\") 1246: val cons = clazz.getConstructor(classOf[SparkContext]) 1247: cons.newInstance(sc).asInstanceOf[TaskSchedulerImpl] 1248: } catch { 1249: // TODO: Enumerate the exact reasons why it can fail 1250: // But irrespective of it, it means we cannot proceed ! 1251: case th: Throwable => { 1252: throw new SparkException(\"YARN mode not available ?\", th) 1253: }  The comment suggests the specific exceptions should be enumerated here. The try block could throw the following exceptions: ClassNotFoundException NegativeArraySizeException NoSuchMethodException SecurityException InstantiationException IllegalAccessException IllegalArgumentException InvocationTargetException ClassCastException ========================================== ========================= Case 2: Line: 282, File: \"org/apache/spark/executor/Executor.scala\"  265: case t: Throwable => { 266: val serviceTime = (System.currentTimeMillis() - taskStart).toInt 267: val metrics = attemptedTask.flatMap(t => t.metrics) 268: for (m <- metrics) { 269: m.executorRunTime = serviceTime 270: m.jvmGCTime = gcTime - startGCTime 271: } 272: val reason = ExceptionFailure(t.getClass.getName, t.toString, t.getStackTrace, metrics) 273: execBackend.statusUpdate(taskId, TaskState.FAILED, ser.serialize(reason)) 274: 275: // TODO: Should we exit the whole executor here? On the one hand, the failed task may 276: // have left some weird state around depending on when the exception was thrown, but on 277: // the other hand, maybe we could detect that when future tasks fail and exit then. 278: logError(\"Exception in task ID \" + taskId, t) 279: //System.exit(1) 280: } 281: } finally { 282: // TODO: Unregister shuffle memory only for ResultTask 283: val shuffleMemoryMap = env.shuffleMemoryMap 284: shuffleMemoryMap.synchronized { 285: shuffleMemoryMap.remove(Thread.currentThread().getId) 286: } 287: runningTasks.remove(taskId) 288: }  From the comment in this Throwable exception handler it seems to suggest that the system should just exit? ========================================== ========================= Case 3: Line: 70, File: \"org/apache/spark/network/netty/FileServerHandler.java\"  66: try { 67: ctx.write(new DefaultFileRegion(new FileInputStream(file) 68: .getChannel(), fileSegment.offset(), fileSegment.length())); 69: } catch (Exception e) { 70: LOG.error(\"Exception: \", e); 71: }  \"Exception\" is too general. The try block only throws \"FileNotFoundException\". Although there is nothing wrong with it now, but later if code evolves this might cause some other exceptions to be swallowed. ==========================================", "reporter": "Ding Yuan", "assignee": null, "created": "2014-02-27T17:23:27.000+0000", "updated": "2014-12-16T22:10:05.000+0000", "resolved": "2014-12-16T22:10:05.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "OuyangJin", "body": "@Ding Yuan, First of all, nice work I can track this JIRA if any admins is willing to do this , and can put me as Assignee There are a few details I'm curious about the analysis result from your rearch team @Ding 1 All 3 cases you metioned are about Exception handling. I wonder the code analysis tool your team buildl to analysis spark code is concentrate on analying Exception handling? And how many kind of exception handling your tool can analysis?(what I can see in the case is over catching , detailed catching and unexceped exit in handling exception) 2 Your tool is whether static analysis(not running spark when analying) or runtime analysis(running spark when analying). If in runtime analying, what 's your runtime env 3 The coverage range for your tool. Only in core module or all spark modules Thanks for your work!", "created": "2014-03-02T03:16:36.513+0000"}, {"author": "Ding Yuan", "body": "Hey thanks for the response [~qqsun8819], it would be great if you could help to fix these. Re your questions: 1. Yes, our tool focus on some trivial buggy patterns in exception handling code where we found caused some catastrophic failures. The patterns you mentioned are exactly the ones our tool tries to detect. 2. It's a static analysis tool on byte code. 3. It covers all jar files, and i believe i ran it on all spark modules. In fact, we are planning to open-source it soon after some packaging. if you're interested I will let you know once we have done this. cheers,", "created": "2014-03-02T09:07:08.721+0000"}, {"author": "OuyangJin", "body": "@Ding Great.Looking forward to your further plan. And you can attach a complete report generate by your tool in this JIRA(if not violate any copyright for your research team) so that I can get a complete graph of this kind of problem in spark code. And I'm very pleased to be notified by your further progress of your tool , at any time . Thanks!", "created": "2014-03-03T05:03:32.679+0000"}, {"author": "Ding Yuan", "body": "In fact, the above mentioned cases are all we have found. In the software projects we scanned so far, Spark actually has the fewest warnings.", "created": "2014-03-05T20:00:39.320+0000"}, {"author": "OuyangJin", "body": "Good.I'll check current case you reported", "created": "2014-03-05T21:09:15.600+0000"}, {"author": "Sean R. Owen", "body": "[~d.yuan] Several problems like this have been fixed since. Can you open a PR to resolve any others that your tool has found or is it all fixed now?", "created": "2014-11-25T09:55:48.807+0000"}, {"author": "Ding Yuan", "body": "Glad to know these cases are addressed! As for version 0.9.0, I have reported all the cases (only three of them). I have opened another JIRA: https://issues.apache.org/jira/browse/SPARK-4863 to report the warnings on spark-1.1.1.", "created": "2014-12-16T19:06:59.293+0000"}, {"author": "Sean R. Owen", "body": "[~d.yuan] I don't know that these have been addressed; some issues like it have been, if I recall correctly. Whatever the status, I think it's useful to focus on master and/or later branches. Would it be OK to close this in favor of SPARK-4863?", "created": "2014-12-16T21:10:17.436+0000"}, {"author": "Ding Yuan", "body": "of course, please just close this one then. Thanks!", "created": "2014-12-16T21:53:25.374+0000"}], "num_comments": 9, "text": "Issue: SPARK-1148\nSummary: Suggestions for exception handling (avoid potential bugs)\nDescription: Hi Spark developers, We are a group of researchers on software reliability. Recently we did a study and found that majority of the most severe failures in data-analytic systems are caused by bugs in exception handlers – that it is hard to anticipate all the possible real-world error scenarios. Therefore we built a simple checking tool that automatically detects some bug patterns that have caused some very severe real-world failures. I am reporting a few cases here. Any feedback is much appreciated! Ding ========================= Case 1: Line: 1249, File: \"org/apache/spark/SparkContext.scala\"  1244: val scheduler = try { 1245: val clazz = Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterScheduler\") 1246: val cons = clazz.getConstructor(classOf[SparkContext]) 1247: cons.newInstance(sc).asInstanceOf[TaskSchedulerImpl] 1248: } catch { 1249: // TODO: Enumerate the exact reasons why it can fail 1250: // But irrespective of it, it means we cannot proceed ! 1251: case th: Throwable => { 1252: throw new SparkException(\"YARN mode not available ?\", th) 1253: }  The comment suggests the specific exceptions should be enumerated here. The try block could throw the following exceptions: ClassNotFoundException NegativeArraySizeException NoSuchMethodException SecurityException InstantiationException IllegalAccessException IllegalArgumentException InvocationTargetException ClassCastException ========================================== ========================= Case 2: Line: 282, File: \"org/apache/spark/executor/Executor.scala\"  265: case t: Throwable => { 266: val serviceTime = (System.currentTimeMillis() - taskStart).toInt 267: val metrics = attemptedTask.flatMap(t => t.metrics) 268: for (m <- metrics) { 269: m.executorRunTime = serviceTime 270: m.jvmGCTime = gcTime - startGCTime 271: } 272: val reason = ExceptionFailure(t.getClass.getName, t.toString, t.getStackTrace, metrics) 273: execBackend.statusUpdate(taskId, TaskState.FAILED, ser.serialize(reason)) 274: 275: // TODO: Should we exit the whole executor here? On the one hand, the failed task may 276: // have left some weird state around depending on when the exception was thrown, but on 277: // the other hand, maybe we could detect that when future tasks fail and exit then. 278: logError(\"Exception in task ID \" + taskId, t) 279: //System.exit(1) 280: } 281: } finally { 282: // TODO: Unregister shuffle memory only for ResultTask 283: val shuffleMemoryMap = env.shuffleMemoryMap 284: shuffleMemoryMap.synchronized { 285: shuffleMemoryMap.remove(Thread.currentThread().getId) 286: } 287: runningTasks.remove(taskId) 288: }  From the comment in this Throwable exception handler it seems to suggest that the system should just exit? ========================================== ========================= Case 3: Line: 70, File: \"org/apache/spark/network/netty/FileServerHandler.java\"  66: try { 67: ctx.write(new DefaultFileRegion(new FileInputStream(file) 68: .getChannel(), fileSegment.offset(), fileSegment.length())); 69: } catch (Exception e) { 70: LOG.error(\"Exception: \", e); 71: }  \"Exception\" is too general. The try block only throws \"FileNotFoundException\". Although there is nothing wrong with it now, but later if code evolves this might cause some other exceptions to be swallowed. ==========================================\n\nComments (9):\n1. OuyangJin: @Ding Yuan, First of all, nice work I can track this JIRA if any admins is willing to do this , and can put me as Assignee There are a few details I'm curious about the analysis result from your rearch team @Ding 1 All 3 cases you metioned are about Exception handling. I wonder the code analysis tool your team buildl to analysis spark code is concentrate on analying Exception handling? And how many kind of exception handling your tool can analysis?(what I can see in the case is over catching , detailed catching and unexceped exit in handling exception) 2 Your tool is whether static analysis(not running spark when analying) or runtime analysis(running spark when analying). If in runtime analying, what 's your runtime env 3 The coverage range for your tool. Only in core module or all spark modules Thanks for your work!\n2. Ding Yuan: Hey thanks for the response [~qqsun8819], it would be great if you could help to fix these. Re your questions: 1. Yes, our tool focus on some trivial buggy patterns in exception handling code where we found caused some catastrophic failures. The patterns you mentioned are exactly the ones our tool tries to detect. 2. It's a static analysis tool on byte code. 3. It covers all jar files, and i believe i ran it on all spark modules. In fact, we are planning to open-source it soon after some packaging. if you're interested I will let you know once we have done this. cheers,\n3. OuyangJin: @Ding Great.Looking forward to your further plan. And you can attach a complete report generate by your tool in this JIRA(if not violate any copyright for your research team) so that I can get a complete graph of this kind of problem in spark code. And I'm very pleased to be notified by your further progress of your tool , at any time . Thanks!\n4. Ding Yuan: In fact, the above mentioned cases are all we have found. In the software projects we scanned so far, Spark actually has the fewest warnings.\n5. OuyangJin: Good.I'll check current case you reported\n6. Sean R. Owen: [~d.yuan] Several problems like this have been fixed since. Can you open a PR to resolve any others that your tool has found or is it all fixed now?\n7. Ding Yuan: Glad to know these cases are addressed! As for version 0.9.0, I have reported all the cases (only three of them). I have opened another JIRA: https://issues.apache.org/jira/browse/SPARK-4863 to report the warnings on spark-1.1.1.\n8. Sean R. Owen: [~d.yuan] I don't know that these have been addressed; some issues like it have been, if I recall correctly. Whatever the status, I think it's useful to focus on master and/or later branches. Would it be OK to close this in favor of SPARK-4863?\n9. Ding Yuan: of course, please just close this one then. Thanks!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "382b5be446050d9e9136c9d68ac8f8dc", "issue_key": "SPARK-1149", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Bad partitioners can cause Spark to hang", "description": "While implementing a unit test for lookup, I accidentally created a situation where a partitioner returned a partition number that was outside its range. It should have returned 0 or 1, but in the last case, it returned a -1. Rather than reporting the problem via an exception, Spark simply hangs during the unit test run. We should catch this bad behavior by partitioners and throw an exception. test(\"lookup with bad partitioner\") { val pairs = sc.parallelize(Array((1,2), (3,4), (5,6), (5,7))) val p = new Partitioner { def numPartitions: Int = 2 def getPartition(key: Any): Int = key.hashCode() % 2 } val shuffled = pairs.partitionBy(p) assert(shuffled.partitioner === Some(p)) assert(shuffled.lookup(1) === Seq(2)) assert(shuffled.lookup(5) === Seq(6,7)) assert(shuffled.lookup(-1) === Seq()) }", "reporter": "Bryn Keller", "assignee": null, "created": "2014-02-27T21:00:03.000+0000", "updated": "2014-10-13T18:05:14.000+0000", "resolved": "2014-10-13T18:05:14.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Guoqiang Li", "body": "I opened a pull request:[#44|https://github.com/apache/spark/pull/44] . Spark should not to hang.", "created": "2014-02-28T01:18:42.141+0000"}, {"author": "Sean R. Owen", "body": "Looks like Patrick merged this into master in March. It might have been fixed for ... 1.0?", "created": "2014-10-13T18:05:14.854+0000"}], "num_comments": 2, "text": "Issue: SPARK-1149\nSummary: Bad partitioners can cause Spark to hang\nDescription: While implementing a unit test for lookup, I accidentally created a situation where a partitioner returned a partition number that was outside its range. It should have returned 0 or 1, but in the last case, it returned a -1. Rather than reporting the problem via an exception, Spark simply hangs during the unit test run. We should catch this bad behavior by partitioners and throw an exception. test(\"lookup with bad partitioner\") { val pairs = sc.parallelize(Array((1,2), (3,4), (5,6), (5,7))) val p = new Partitioner { def numPartitions: Int = 2 def getPartition(key: Any): Int = key.hashCode() % 2 } val shuffled = pairs.partitionBy(p) assert(shuffled.partitioner === Some(p)) assert(shuffled.lookup(1) === Seq(2)) assert(shuffled.lookup(5) === Seq(6,7)) assert(shuffled.lookup(-1) === Seq()) }\n\nComments (2):\n1. Guoqiang Li: I opened a pull request:[#44|https://github.com/apache/spark/pull/44] . Spark should not to hang.\n2. Sean R. Owen: Looks like Patrick merged this into master in March. It might have been fixed for ... 1.0?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "30ecf91bd5f5d1f34ef71dce468e4d55", "issue_key": "SPARK-1150", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "repo location in create_release script out of date", "description": "repo location in create_release script out of date", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-02-28T07:56:02.000+0000", "updated": "2014-03-01T17:27:31.000+0000", "resolved": "2014-03-01T17:27:31.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Nan Zhu", "body": "https://github.com/apache/spark/pull/48", "created": "2014-02-28T09:44:05.091+0000"}], "num_comments": 1, "text": "Issue: SPARK-1150\nSummary: repo location in create_release script out of date\nDescription: repo location in create_release script out of date\n\nComments (1):\n1. Nan Zhu: https://github.com/apache/spark/pull/48", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "725782b9773c69aa0b0e3c85965ff50e", "issue_key": "SPARK-1151", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "dev merge_spark_pr.py still references incubator-spark", "description": "We need to update the script to use spark.git instead of incubator-spark.git", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-02-28T09:04:11.000+0000", "updated": "2014-02-28T18:28:54.000+0000", "resolved": "2014-02-28T18:28:54.000+0000", "labels": [], "components": ["Project Infra"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1151\nSummary: dev merge_spark_pr.py still references incubator-spark\nDescription: We need to update the script to use spark.git instead of incubator-spark.git", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "8c6b564ab17751cfc885550ca77645c4", "issue_key": "SPARK-1152", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "ArrayStoreException on mapping RDD on cluster", "description": "With this code:  import org.apache.spark.{SparkConf, SparkContext, Partitioner} import org.apache.spark.SparkContext._ object twitterAggregation extends App { val conf = new SparkConf() .setMaster(\"spark://ec2-x-x-x-x.compute-1.amazonaws.com:7077\") //.setMaster(\"local\") .setAppName(\"foo\") .setJars(List(\"target/scala-2.10/foo_2.10-0.0.1.jar\")) .setSparkHome(\"/root/spark/\") val sc = new SparkContext(conf) sc.parallelize(Seq(\"b\")).map(identity).collect }  I get this:  14/02/28 18:41:10 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:1) 14/02/28 18:41:10 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0) 14/02/28 18:41:10 WARN scheduler.TaskSetManager: Loss was due to java.lang.ArrayStoreException java.lang.ArrayStoreException: [Ljava.lang.String; at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88) at scala.Array$.slowcopy(Array.scala:81) at scala.Array$.copy(Array.scala:107) at scala.collection.mutable.ResizableArray$class.copyToArray(ResizableArray.scala:77) at scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:47) at scala.collection.TraversableOnce$class.copyToArray(TraversableOnce.scala:241) at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:105) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:249) at scala.collection.AbstractTraversable.toArray(Traversable.scala:105) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) at org.apache.spark.rdd.RDD$$anonfun$4.apply(RDD.scala:602) at org.apache.spark.rdd.RDD$$anonfun$4.apply(RDD.scala:602) at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884) at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109) at org.apache.spark.scheduler.Task.run(Task.scala:53) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)  This only happens when running against a cluster as an app (sbt package && sbt play). With a master of \"local\", or running on the spark shell on a cluster, code runs without error.", "reporter": "Andrew Kerr", "assignee": null, "created": "2014-02-28T10:50:16.000+0000", "updated": "2014-03-28T10:31:21.000+0000", "resolved": "2014-03-28T10:23:27.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "cannot reproduce it...  object SparkTest { def main(args: Array[String]) { val conf = new SparkConf() //.setMaster(\"spark://127.0.0.1:7077\") .setMaster(\"local\") .setAppName(\"foo\") .setJars(List(\"target/scala-2.10/spark_test_2.10-1.0.jar\")) .setSparkHome(\"/root/spark/\") val sc = new SparkContext(conf) sc.parallelize(Seq(\"b\")).map(identity).collect } }   Nans-MacBook-Pro:spark_test nanzhu$ sbt run Loading /Users/nanzhu/Downloads/sbt/bin/sbt-launch-lib.bash [info] Loading global plugins from /Users/nanzhu/.sbt/0.13/plugins [info] Set current project to spark_test (in build file:/Users/nanzhu/code/spark_test/spark_test/) [info] Compiling 1 Scala source to /Users/nanzhu/code/spark_test/spark_test/target/scala-2.10/classes... [info] Running SparkTest 14/02/28 14:24:09 INFO slf4j.Slf4jLogger: Slf4jLogger started 14/02/28 14:24:09 INFO Remoting: Starting remoting 14/02/28 14:24:09 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@192.168.2.15:52451] 14/02/28 14:24:09 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@192.168.2.15:52451] 14/02/28 14:24:09 INFO spark.SparkEnv: Registering BlockManagerMaster 14/02/28 14:24:09 INFO storage.DiskBlockManager: Created local directory at /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-local-20140228142409-a829 14/02/28 14:24:09 INFO storage.MemoryStore: MemoryStore started with capacity 589.2 MB. 14/02/28 14:24:09 INFO network.ConnectionManager: Bound socket to port 52452 with id = ConnectionManagerId(192.168.2.15,52452) 14/02/28 14:24:09 INFO storage.BlockManagerMaster: Trying to register BlockManager 14/02/28 14:24:09 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.2.15:52452 with 589.2 MB RAM 14/02/28 14:24:09 INFO storage.BlockManagerMaster: Registered BlockManager 14/02/28 14:24:09 INFO spark.HttpServer: Starting HTTP Server 14/02/28 14:24:09 INFO server.Server: jetty-7.6.8.v20121106 14/02/28 14:24:09 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:52453 14/02/28 14:24:09 INFO broadcast.HttpBroadcast: Broadcast server started at http://192.168.2.15:52453 14/02/28 14:24:09 INFO spark.SparkEnv: Registering MapOutputTracker 14/02/28 14:24:09 INFO spark.HttpFileServer: HTTP File server directory is /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-ae2ccb3e-f492-446e-8769-b38345233e87 14/02/28 14:24:09 INFO spark.HttpServer: Starting HTTP Server 14/02/28 14:24:09 INFO server.Server: jetty-7.6.8.v20121106 14/02/28 14:24:09 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:52454 14/02/28 14:24:10 INFO server.Server: jetty-7.6.8.v20121106 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage/rdd,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/stage,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/pool,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/environment,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/executors,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/static,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/,null} 14/02/28 14:24:10 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040 14/02/28 14:24:10 INFO ui.SparkUI: Started Spark Web UI at http://192.168.2.15:4040 2014-02-28 14:24:10.219 java[14706:5f07] Unable to load realm info from SCDynamicStore 14/02/28 14:24:25 INFO spark.SparkContext: Added JAR target/scala-2.10/spark_test_2.10-1.0.jar at http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar with timestamp 1393615465324 14/02/28 14:24:25 INFO spark.SparkContext: Starting job: collect at SparkTest.scala:12 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Got job 0 (collect at SparkTest.scala:12) with 1 output partitions (allowLocal=false) 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Final stage: Stage 0 (collect at SparkTest.scala:12) 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Parents of final stage: List() 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Missing parents: List() 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at SparkTest.scala:12), which has no missing parents 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[1] at map at SparkTest.scala:12) 14/02/28 14:24:25 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks 14/02/28 14:24:25 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL) 14/02/28 14:24:25 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as 1389 bytes in 19 ms 14/02/28 14:24:25 INFO executor.Executor: Running task ID 0 14/02/28 14:24:25 INFO executor.Executor: Fetching http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar with timestamp 1393615465324 14/02/28 14:24:25 INFO util.Utils: Fetching http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar to /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/fetchFileTemp1181057927078676157.tmp 14/02/28 14:24:26 INFO executor.Executor: Adding file:/var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-1b6e6349-ef69-4b02-a9da-4ba05beb2371/spark_test_2.10-1.0.jar to class loader 14/02/28 14:24:26 INFO executor.Executor: Serialized size of result for 0 is 529 14/02/28 14:24:26 INFO executor.Executor: Sending result for 0 directly to driver 14/02/28 14:24:26 INFO executor.Executor: Finished task ID 0 14/02/28 14:24:26 INFO scheduler.TaskSetManager: Finished TID 0 in 400 ms on localhost (progress: 0/1) 14/02/28 14:24:26 INFO scheduler.TaskSchedulerImpl: Remove TaskSet 0.0 from pool 14/02/28 14:24:26 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0) 14/02/28 14:24:26 INFO scheduler.DAGScheduler: Stage 0 (collect at SparkTest.scala:12) finished in 0.419 s 14/02/28 14:24:26 INFO spark.SparkContext: Job finished: collect at SparkTest.scala:12, took 0.54146 s Not interrupting system thread Thread[Keep-Alive-Timer,8,system] 14/02/28 14:24:26 INFO network.ConnectionManager: Selector thread was interrupted! [success] Total time: 22 s, completed 28-Feb-2014 2:24:26 PM", "created": "2014-02-28T11:21:43.262+0000"}, {"author": "Andrew Kerr", "body": "Let me reiterate: this code works just fine with a master of \"local\". The exception occurs when the master is the master of a cluster (in my case 4 slaves, built using Spark's ec2 scripts).", "created": "2014-02-28T16:55:19.005+0000"}, {"author": "Patrick McFadin", "body": "I just tried to reproduce this on master using a local cluster but no luck:  $ ./bin/spark-class org.apache.spark.deploy.master.Master $ ./bin/spark-class org.apache.spark.deploy.worker.Worker <spark cluster url> $ MASTER=<spark cluster url> ./bin/spark-shell scala> sc.parallelize(Seq(\"b\")).map(identity).collect res3: Array[String] = Array(b)", "created": "2014-03-01T16:52:52.741+0000"}, {"author": "Nan Zhu", "body": "run in a ec2 cluster with 5 slaves, no luck..........:-)", "created": "2014-03-01T16:59:38.373+0000"}, {"author": "Patrick McFadin", "body": "[~andrewkerr] - I wonder if maybe there is some dependency conflict or issue with the code you are keeping inside of your jar that is causing this to fail. Are there any helpful error messages in the executor logs (in spark/work on the slave)?", "created": "2014-03-01T17:04:25.964+0000"}, {"author": "Andrew Kerr", "body": "I'm sorry, I just realised I hadn't specified that I have not had this problem when using the Spark shell. I seem to have thought \"shell\" and typed \"cluster\". I've had this problem only an using app and only pointing at a cluster, not local. I'll update the report with more details soon.", "created": "2014-03-04T09:59:39.110+0000"}, {"author": "Andrew Kerr", "body": "And now I can't reproduce this myself.", "created": "2014-03-06T07:35:08.668+0000"}, {"author": "Andrew Kerr", "body": "I'm wondering if somehow the slaves received a corrupted, old or inconsistent (set of) jar(s). This would explain why there is no problem in local mode. Come to think of it, line numbers in exceptions didn't necessarily match up with those in the source.", "created": "2014-03-07T05:11:41.633+0000"}, {"author": "Evan Chan", "body": "I reproduced it last night with master of local[4]. It doesn't happen all the time. See: https://github.com/ooyala/spark-jobserver Branch: velvia/fix-classloading-bug Commit: e58c789 Run: sbt job-server/test Failing test is in JobManagerSpec:181, \"should properly serialize case classes and other job jar classes\" See the test output log in job-server-test.log for the ArrayStoreException. -Evan", "created": "2014-03-28T10:17:59.092+0000"}, {"author": "Patrick McFadin", "body": "This was fixed by a patch that went in to master recently. I'll mark as duplicate.", "created": "2014-03-28T10:22:19.484+0000"}, {"author": "Evan Chan", "body": "Here is the exception stack trace from the unit test output: [2014-03-28 10:21:08,608] INFO .apache.spark.SparkContext [] [] - Successfully stopped SparkContext [2014-03-28 10:21:08,609] INFO rovider$RemotingTerminator [] [akka://spark/system/remoting-terminator] - Shutting down remote daemon. java.lang.ArrayStoreException: [Lspark.jobserver.Animal; at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:93) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870) at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56) at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859) at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616) at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:385) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) This might be related to serialization. I don't think Scala case classes by default extend Serializable...", "created": "2014-03-28T10:29:09.673+0000"}, {"author": "Evan Chan", "body": "Patrick, do you have the patch / PR #, and will this be backported to 0.9.1?", "created": "2014-03-28T10:29:47.109+0000"}, {"author": "Patrick McFadin", "body": "I linked the JIRA here - we'll probably wait until 0.9.2 because it's an invasive change involving classloaders.", "created": "2014-03-28T10:30:51.801+0000"}], "num_comments": 13, "text": "Issue: SPARK-1152\nSummary: ArrayStoreException on mapping RDD on cluster\nDescription: With this code:  import org.apache.spark.{SparkConf, SparkContext, Partitioner} import org.apache.spark.SparkContext._ object twitterAggregation extends App { val conf = new SparkConf() .setMaster(\"spark://ec2-x-x-x-x.compute-1.amazonaws.com:7077\") //.setMaster(\"local\") .setAppName(\"foo\") .setJars(List(\"target/scala-2.10/foo_2.10-0.0.1.jar\")) .setSparkHome(\"/root/spark/\") val sc = new SparkContext(conf) sc.parallelize(Seq(\"b\")).map(identity).collect }  I get this:  14/02/28 18:41:10 WARN scheduler.TaskSetManager: Lost TID 1 (task 0.0:1) 14/02/28 18:41:10 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0) 14/02/28 18:41:10 WARN scheduler.TaskSetManager: Loss was due to java.lang.ArrayStoreException java.lang.ArrayStoreException: [Ljava.lang.String; at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88) at scala.Array$.slowcopy(Array.scala:81) at scala.Array$.copy(Array.scala:107) at scala.collection.mutable.ResizableArray$class.copyToArray(ResizableArray.scala:77) at scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:47) at scala.collection.TraversableOnce$class.copyToArray(TraversableOnce.scala:241) at scala.collection.AbstractTraversable.copyToArray(Traversable.scala:105) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:249) at scala.collection.AbstractTraversable.toArray(Traversable.scala:105) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) at org.apache.spark.rdd.RDD$$anonfun$4.apply(RDD.scala:602) at org.apache.spark.rdd.RDD$$anonfun$4.apply(RDD.scala:602) at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884) at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109) at org.apache.spark.scheduler.Task.run(Task.scala:53) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)  This only happens when running against a cluster as an app (sbt package && sbt play). With a master of \"local\", or running on the spark shell on a cluster, code runs without error.\n\nComments (13):\n1. Nan Zhu: cannot reproduce it...  object SparkTest { def main(args: Array[String]) { val conf = new SparkConf() //.setMaster(\"spark://127.0.0.1:7077\") .setMaster(\"local\") .setAppName(\"foo\") .setJars(List(\"target/scala-2.10/spark_test_2.10-1.0.jar\")) .setSparkHome(\"/root/spark/\") val sc = new SparkContext(conf) sc.parallelize(Seq(\"b\")).map(identity).collect } }   Nans-MacBook-Pro:spark_test nanzhu$ sbt run Loading /Users/nanzhu/Downloads/sbt/bin/sbt-launch-lib.bash [info] Loading global plugins from /Users/nanzhu/.sbt/0.13/plugins [info] Set current project to spark_test (in build file:/Users/nanzhu/code/spark_test/spark_test/) [info] Compiling 1 Scala source to /Users/nanzhu/code/spark_test/spark_test/target/scala-2.10/classes... [info] Running SparkTest 14/02/28 14:24:09 INFO slf4j.Slf4jLogger: Slf4jLogger started 14/02/28 14:24:09 INFO Remoting: Starting remoting 14/02/28 14:24:09 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@192.168.2.15:52451] 14/02/28 14:24:09 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@192.168.2.15:52451] 14/02/28 14:24:09 INFO spark.SparkEnv: Registering BlockManagerMaster 14/02/28 14:24:09 INFO storage.DiskBlockManager: Created local directory at /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-local-20140228142409-a829 14/02/28 14:24:09 INFO storage.MemoryStore: MemoryStore started with capacity 589.2 MB. 14/02/28 14:24:09 INFO network.ConnectionManager: Bound socket to port 52452 with id = ConnectionManagerId(192.168.2.15,52452) 14/02/28 14:24:09 INFO storage.BlockManagerMaster: Trying to register BlockManager 14/02/28 14:24:09 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.2.15:52452 with 589.2 MB RAM 14/02/28 14:24:09 INFO storage.BlockManagerMaster: Registered BlockManager 14/02/28 14:24:09 INFO spark.HttpServer: Starting HTTP Server 14/02/28 14:24:09 INFO server.Server: jetty-7.6.8.v20121106 14/02/28 14:24:09 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:52453 14/02/28 14:24:09 INFO broadcast.HttpBroadcast: Broadcast server started at http://192.168.2.15:52453 14/02/28 14:24:09 INFO spark.SparkEnv: Registering MapOutputTracker 14/02/28 14:24:09 INFO spark.HttpFileServer: HTTP File server directory is /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-ae2ccb3e-f492-446e-8769-b38345233e87 14/02/28 14:24:09 INFO spark.HttpServer: Starting HTTP Server 14/02/28 14:24:09 INFO server.Server: jetty-7.6.8.v20121106 14/02/28 14:24:09 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:52454 14/02/28 14:24:10 INFO server.Server: jetty-7.6.8.v20121106 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage/rdd,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/stage,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/pool,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/environment,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/executors,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/static,null} 14/02/28 14:24:10 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/,null} 14/02/28 14:24:10 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040 14/02/28 14:24:10 INFO ui.SparkUI: Started Spark Web UI at http://192.168.2.15:4040 2014-02-28 14:24:10.219 java[14706:5f07] Unable to load realm info from SCDynamicStore 14/02/28 14:24:25 INFO spark.SparkContext: Added JAR target/scala-2.10/spark_test_2.10-1.0.jar at http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar with timestamp 1393615465324 14/02/28 14:24:25 INFO spark.SparkContext: Starting job: collect at SparkTest.scala:12 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Got job 0 (collect at SparkTest.scala:12) with 1 output partitions (allowLocal=false) 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Final stage: Stage 0 (collect at SparkTest.scala:12) 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Parents of final stage: List() 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Missing parents: List() 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at SparkTest.scala:12), which has no missing parents 14/02/28 14:24:25 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[1] at map at SparkTest.scala:12) 14/02/28 14:24:25 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks 14/02/28 14:24:25 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL) 14/02/28 14:24:25 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as 1389 bytes in 19 ms 14/02/28 14:24:25 INFO executor.Executor: Running task ID 0 14/02/28 14:24:25 INFO executor.Executor: Fetching http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar with timestamp 1393615465324 14/02/28 14:24:25 INFO util.Utils: Fetching http://192.168.2.15:52454/jars/spark_test_2.10-1.0.jar to /var/folders/hs/28p73t310w931t4n21d954_80000gn/T/fetchFileTemp1181057927078676157.tmp 14/02/28 14:24:26 INFO executor.Executor: Adding file:/var/folders/hs/28p73t310w931t4n21d954_80000gn/T/spark-1b6e6349-ef69-4b02-a9da-4ba05beb2371/spark_test_2.10-1.0.jar to class loader 14/02/28 14:24:26 INFO executor.Executor: Serialized size of result for 0 is 529 14/02/28 14:24:26 INFO executor.Executor: Sending result for 0 directly to driver 14/02/28 14:24:26 INFO executor.Executor: Finished task ID 0 14/02/28 14:24:26 INFO scheduler.TaskSetManager: Finished TID 0 in 400 ms on localhost (progress: 0/1) 14/02/28 14:24:26 INFO scheduler.TaskSchedulerImpl: Remove TaskSet 0.0 from pool 14/02/28 14:24:26 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0) 14/02/28 14:24:26 INFO scheduler.DAGScheduler: Stage 0 (collect at SparkTest.scala:12) finished in 0.419 s 14/02/28 14:24:26 INFO spark.SparkContext: Job finished: collect at SparkTest.scala:12, took 0.54146 s Not interrupting system thread Thread[Keep-Alive-Timer,8,system] 14/02/28 14:24:26 INFO network.ConnectionManager: Selector thread was interrupted! [success] Total time: 22 s, completed 28-Feb-2014 2:24:26 PM\n2. Andrew Kerr: Let me reiterate: this code works just fine with a master of \"local\". The exception occurs when the master is the master of a cluster (in my case 4 slaves, built using Spark's ec2 scripts).\n3. Patrick McFadin: I just tried to reproduce this on master using a local cluster but no luck:  $ ./bin/spark-class org.apache.spark.deploy.master.Master $ ./bin/spark-class org.apache.spark.deploy.worker.Worker <spark cluster url> $ MASTER=<spark cluster url> ./bin/spark-shell scala> sc.parallelize(Seq(\"b\")).map(identity).collect res3: Array[String] = Array(b)\n4. Nan Zhu: run in a ec2 cluster with 5 slaves, no luck..........:-)\n5. Patrick McFadin: [~andrewkerr] - I wonder if maybe there is some dependency conflict or issue with the code you are keeping inside of your jar that is causing this to fail. Are there any helpful error messages in the executor logs (in spark/work on the slave)?\n6. Andrew Kerr: I'm sorry, I just realised I hadn't specified that I have not had this problem when using the Spark shell. I seem to have thought \"shell\" and typed \"cluster\". I've had this problem only an using app and only pointing at a cluster, not local. I'll update the report with more details soon.\n7. Andrew Kerr: And now I can't reproduce this myself.\n8. Andrew Kerr: I'm wondering if somehow the slaves received a corrupted, old or inconsistent (set of) jar(s). This would explain why there is no problem in local mode. Come to think of it, line numbers in exceptions didn't necessarily match up with those in the source.\n9. Evan Chan: I reproduced it last night with master of local[4]. It doesn't happen all the time. See: https://github.com/ooyala/spark-jobserver Branch: velvia/fix-classloading-bug Commit: e58c789 Run: sbt job-server/test Failing test is in JobManagerSpec:181, \"should properly serialize case classes and other job jar classes\" See the test output log in job-server-test.log for the ArrayStoreException. -Evan\n10. Patrick McFadin: This was fixed by a patch that went in to master recently. I'll mark as duplicate.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "291eef05fa0aa73feaf78edec71cb931", "issue_key": "SPARK-1153", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Generalize VertexId in GraphX so that UUIDs can be used as vertex IDs.", "description": "Currently, {{VertexId}} is a type-synonym for {{Long}}. I would like to be able to use {{UUID}} as the vertex ID type because the data I want to process with GraphX uses that type for its primay-keys. Others might have a different type for their primary-keys. Generalizing {{VertexId}} (with a type class) will help in such cases.", "reporter": "Deepak Nulu", "assignee": null, "created": "2014-02-28T10:51:24.000+0000", "updated": "2017-12-07T03:12:09.000+0000", "resolved": "2016-01-16T13:30:43.000+0000", "labels": [], "components": ["GraphX"], "comments": [{"author": "Deepak Nulu", "body": "The following description of {{VertexId}}:  /** * A 64-bit vertex identifier that uniquely identifies a vertex within a graph. It does not need * to follow any ordering or any constraints other than uniqueness. */ type VertexId = Long  made me hopeful that I would be able to make {{VertexId}} a type-class, change method/function signatures and fix all the compile errors, without needing to understand GraphX code. To test this, I changed {{VertexId}} to:  type VertexId = java.util.UUID  Unfortunately, the compile errors seem to indicate that it is not just a matter of fixing compile errors:  [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:31: type mismatch; [error] found : Int(0) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] var srcId: VertexId = 0, [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:32: type mismatch; [error] found : Int(0) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] var dstId: VertexId = 0, [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:60: value - is not a member of org.apache.spark.graphx.VertexId [error] (if (a.srcId != b.srcId) a.srcId - b.srcId else a.dstId - b.dstId).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:60: value - is not a member of org.apache.spark.graphx.VertexId [error] (if (a.srcId != b.srcId) a.srcId - b.srcId else a.dstId - b.dstId).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala:74: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] builder.add(dstId, srcId, 1) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:78: type mismatch; [error] found : Long(1125899906842597L) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val mixingPrime: VertexId = 1125899906842597L [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:79: overloaded method value abs with alternatives: [error] (x: Double)Double <and> [error] (x: Float)Float <and> [error] (x: Long)Long <and> [error] (x: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId) [error] val col: PartitionID = ((math.abs(src) * mixingPrime) % ceilSqrtNumParts).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:80: overloaded method value abs with alternatives: [error] (x: Double)Double <and> [error] (x: Float)Float <and> [error] (x: Long)Long <and> [error] (x: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId) [error] val row: PartitionID = ((math.abs(dst) * mixingPrime) % ceilSqrtNumParts).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:91: type mismatch; [error] found : Long(1125899906842597L) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val mixingPrime: VertexId = 1125899906842597L [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:92: overloaded method value abs with alternatives: [error] (x: Double)Double <and> [error] (x: Float)Float <and> [error] (x: Long)Long <and> [error] (x: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId) [error] (math.abs(src) * mixingPrime).toInt % numParts [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:115: overloaded method value min with alternatives: [error] (x: Double,y: Double)Double <and> [error] (x: Float,y: Float)Float <and> [error] (x: Long,y: Long)Long <and> [error] (x: Int,y: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId) [error] val lower = math.min(src, dst) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:116: overloaded method value max with alternatives: [error] (x: Double,y: Double)Double <and> [error] (x: Float,y: Float)Float <and> [error] (x: Long,y: Long)Long <and> [error] (x: Int,y: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId) [error] val higher = math.max(src, dst) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartition.scala:165: value < is not a member of org.apache.spark.graphx.VertexId [error] while (j < other.size && other.srcIds(j) < srcId) { j += 1 } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartition.scala:167: value < is not a member of org.apache.spark.graphx.VertexId [error] while (j < other.size && other.srcIds(j) == srcId && other.dstIds(j) < dstId) { j += 1 } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:34: type mismatch; [error] found : java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:55: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg.vid, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:65: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] new VertexBroadcastMsg[Int](0, a, b).asInstanceOf[T] [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:79: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg.vid, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:89: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] new VertexBroadcastMsg[Long](0, a, b).asInstanceOf[T] [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:103: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg.vid, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:113: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] new VertexBroadcastMsg[Double](0, a, b).asInstanceOf[T] [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:127: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:151: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:175: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:41: value < is not a member of org.apache.spark.graphx.VertexId [error] if (edge.srcAttr < edge.dstAttr) { [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:43: value > is not a member of org.apache.spark.graphx.VertexId [error] } else if (edge.srcAttr > edge.dstAttr) { [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:51: overloaded method value min with alternatives: [error] (x: Double,y: Double)Double <and> [error] (x: Float,y: Float)Float <and> [error] (x: Long,y: Long)Long <and> [error] (x: Int,y: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId, Long) [error] vprog = (id, attr, msg) => math.min(attr, msg), [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:52: type mismatch; [error] found : Iterator[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] [error] required: Iterator[(org.apache.spark.graphx.VertexId, Long)] [error] sendMsg = sendMessage, [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/StronglyConnectedComponents.scala:75: type mismatch; [error] found : Long(9223372036854775807L) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] sccWorkGraph, Long.MaxValue, activeDirection = EdgeDirection.Out)( [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:54: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] src => (src, sampleLogNormal(mu, sigma, numVertices)) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:57: value toInt is not a member of org.apache.spark.graphx.VertexId [error] generateRandomEdges(v._1.toInt, v._2, numVertices) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:59: type mismatch; [error] found : org.apache.spark.rdd.RDD[Nothing] [error] required: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] [error] Note: Nothing <: org.apache.spark.graphx.Edge[Int], but class RDD is invariant in type T. [error] You may wish to define T as +T instead. (SLS 4.5) [error] Graph(vertices, edges, 0) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:64: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Array.fill(maxVertexId) { Edge[Int](src, rand.nextInt(maxVertexId), 1) } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:64: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Array.fill(maxVertexId) { Edge[Int](src, rand.nextInt(maxVertexId), 1) } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:129: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Edge[Int](src, dst, 1) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:129: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Edge[Int](src, dst, 1) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:209: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] def sub2ind(r: Int, c: Int): VertexId = r * cols + c [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:231: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val edges: RDD[(VertexId, VertexId)] = sc.parallelize(1 until nverts).map(vid => (vid, 0)) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:231: type mismatch; [error] found : Int(0) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val edges: RDD[(VertexId, VertexId)] = sc.parallelize(1 until nverts).map(vid => (vid, 0)) [error] ^ [error] 39 errors found", "created": "2014-02-28T11:22:10.349+0000"}, {"author": "Reynold Xin", "body": "This is a good feedback - my concern is it will have a pretty significant drop in performance if strings are used (we rely on hashing integers pretty heavily). Let us think about it more. If the demand is huge, we can prioritize this and make it optional.", "created": "2014-03-17T18:13:15.121+0000"}, {"author": "Deepak Nulu", "body": "With the change being requested, Longs can still be used by those who can use Longs and want that performance. The overhead (if any) will come from the type-class mechanism (or whatever mechanism is chosen for making the vertex-ID a customizable type), and I don't know how much that overhead will be.", "created": "2014-03-17T18:24:03.015+0000"}, {"author": "npanj", "body": "An alternative approach, that I have been using: 1 Use a preprocessing step that maps UUID to an Long. 2. Build graph based on Longs For Mapping in step 1: - Rank your uuids. - some kind of has function? For 1, graphx can provide a tool to generate map. I will like to hear how others are building graphs out of non-Long node types.", "created": "2014-05-28T06:41:15.613+0000"}, {"author": "Di Xiao", "body": "I like npanj's approach. It's universal. You treat UUID as attribute. Like the procedure from http://spark.apache.org/docs/latest/graphx-programming-guide.html // Connect to the Spark cluster == Build Graph (build VertexID if necessary) // Load my user data and parse into tuples of user id and attribute list // Parse the edge data which is already in userId -> userId format // Attach the user attributes == Clean Graph // Some users may not have attributes so we set them as empty, Restrict the graph to users with usernames and names == Compute // Compute the PageRank == Get Result // Get the attributes of the top pagerank users", "created": "2014-08-05T02:14:46.901+0000"}, {"author": "Dan Osipov", "body": "FWIW, UUID.getMostSignificantBits() or getLeastSignificantBits() can be used to generate a Long, with low collision probability. Using any type for the ID is still preferred.", "created": "2014-10-30T20:26:55.903+0000"}, {"author": "Carlos Balduz", "body": "I am currently using zipWithUniqueId() to get a VertexID for my data, but that means that after getting the VertexIDs, I have to go to the edges data to look for each of those strings and assign the Id I got from the previous step. I agree it would be nice to be able to choose a different tipe of ID, leaving the user to decide whether he prefers performance or usability.", "created": "2015-04-21T15:00:37.020+0000"}, {"author": "JJ Zhang", "body": "We would also really like a general customized ID available for Vertex. We've been using zipwithIndex to create IDs for now, however, it is a hassle process-wise because we never have a stable ID: any update to a new version of Graph with incremental input data requires a total rebuild of vertex/edges, or we will need another infrastructure to serve as an ID service: additional cost/maintenance. We already have unique IDs for all of our data entities. It would make processing/maintenance much easier if our stable IDs can be used directly", "created": "2015-08-21T15:34:40.813+0000"}, {"author": "Sean R. Owen", "body": "No activity in 2 years", "created": "2016-01-16T13:30:43.867+0000"}, {"author": "Nicholas Tietz", "body": "We are also running into this issue and would like this feature. For our use case and our data size, even a low risk of hash collisions is not acceptable, so we have to have a reliable way to form unique ids from our current unique string ids. I'm going to work on a patch next week. Since this is marked \"won't fix\" due to inactivity, what's the process if a PR is submitted? (Sorry, new to the Apache contribution process.)", "created": "2016-03-25T15:35:43.928+0000"}, {"author": "Reynold Xin", "body": "[~ntietz] changing this will very likely make performance regress for long ids, due to the lack of specialization. You might want to look into graphframes for more general graph functionalities too: https://github.com/graphframes/graphframes", "created": "2016-03-26T00:47:00.839+0000"}, {"author": "Nicholas Tietz", "body": "Thanks for the reply. I think that GraphFrames is not quite sufficient to meet our needs here but I will dive in further. My focus this week is on addressing our problem with hash collisions in forming graph vertex ids, so you may hear more from me. Could you say some more about where it will likely make performance regress? I am diving into the source this week, but pointers toward specific things to watch out for would be helpful.", "created": "2016-03-28T21:41:02.767+0000"}, {"author": "Reynold Xin", "body": "The main thing is that we encode the data assuming integer ids, and are using specialized data structure for int ids. If we change to generic types, the memory footprint will increase, and the performance will decrease too.", "created": "2016-03-28T21:49:36.415+0000"}, {"author": "Guillem LEFAIT", "body": "Hi Nicholas, we got the same needs here, and we delayed a fix until today where we found that collisions reach an arbitrary (low) level. As JJ Zhang said, I'm not confortable with a solution that produces everyday a new order (and consequently a new ID) but keeping a dictionnary of key/value seems costly given the number of data we're dealing with. Have you got a chance to make some experiments on the best way to solve this problem ?", "created": "2016-11-09T16:33:33.585+0000"}, {"author": "Nicholas Tietz", "body": "The decision we eventually made was to migrate as much of our code out of GraphX as we could (moving to writing more directly in Spark). We were running into other potential performance issues with GraphX and we could not do the kind of checkpointing we wanted to, so it was a workable solution for us. We wound up with minimal GraphX code (and when I left the company we were close to being able to remove ALL of it). At the end, we just dealt with the pain of managing consistent IDs ourselves and joining them in. It was not ideal, but it worked and the performance hit was made up for in other areas where we were able to migrate off of GraphX.", "created": "2016-11-09T17:03:31.106+0000"}, {"author": "Brain", "body": ":) why do not add string or uuid support?", "created": "2017-12-07T03:12:09.945+0000"}], "num_comments": 16, "text": "Issue: SPARK-1153\nSummary: Generalize VertexId in GraphX so that UUIDs can be used as vertex IDs.\nDescription: Currently, {{VertexId}} is a type-synonym for {{Long}}. I would like to be able to use {{UUID}} as the vertex ID type because the data I want to process with GraphX uses that type for its primay-keys. Others might have a different type for their primary-keys. Generalizing {{VertexId}} (with a type class) will help in such cases.\n\nComments (16):\n1. Deepak Nulu: The following description of {{VertexId}}:  /** * A 64-bit vertex identifier that uniquely identifies a vertex within a graph. It does not need * to follow any ordering or any constraints other than uniqueness. */ type VertexId = Long  made me hopeful that I would be able to make {{VertexId}} a type-class, change method/function signatures and fix all the compile errors, without needing to understand GraphX code. To test this, I changed {{VertexId}} to:  type VertexId = java.util.UUID  Unfortunately, the compile errors seem to indicate that it is not just a matter of fixing compile errors:  [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:31: type mismatch; [error] found : Int(0) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] var srcId: VertexId = 0, [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:32: type mismatch; [error] found : Int(0) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] var dstId: VertexId = 0, [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:60: value - is not a member of org.apache.spark.graphx.VertexId [error] (if (a.srcId != b.srcId) a.srcId - b.srcId else a.dstId - b.dstId).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/Edge.scala:60: value - is not a member of org.apache.spark.graphx.VertexId [error] (if (a.srcId != b.srcId) a.srcId - b.srcId else a.dstId - b.dstId).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala:74: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] builder.add(dstId, srcId, 1) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:78: type mismatch; [error] found : Long(1125899906842597L) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val mixingPrime: VertexId = 1125899906842597L [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:79: overloaded method value abs with alternatives: [error] (x: Double)Double <and> [error] (x: Float)Float <and> [error] (x: Long)Long <and> [error] (x: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId) [error] val col: PartitionID = ((math.abs(src) * mixingPrime) % ceilSqrtNumParts).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:80: overloaded method value abs with alternatives: [error] (x: Double)Double <and> [error] (x: Float)Float <and> [error] (x: Long)Long <and> [error] (x: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId) [error] val row: PartitionID = ((math.abs(dst) * mixingPrime) % ceilSqrtNumParts).toInt [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:91: type mismatch; [error] found : Long(1125899906842597L) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val mixingPrime: VertexId = 1125899906842597L [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:92: overloaded method value abs with alternatives: [error] (x: Double)Double <and> [error] (x: Float)Float <and> [error] (x: Long)Long <and> [error] (x: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId) [error] (math.abs(src) * mixingPrime).toInt % numParts [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:115: overloaded method value min with alternatives: [error] (x: Double,y: Double)Double <and> [error] (x: Float,y: Float)Float <and> [error] (x: Long,y: Long)Long <and> [error] (x: Int,y: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId) [error] val lower = math.min(src, dst) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/PartitionStrategy.scala:116: overloaded method value max with alternatives: [error] (x: Double,y: Double)Double <and> [error] (x: Float,y: Float)Float <and> [error] (x: Long,y: Long)Long <and> [error] (x: Int,y: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId) [error] val higher = math.max(src, dst) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartition.scala:165: value < is not a member of org.apache.spark.graphx.VertexId [error] while (j < other.size && other.srcIds(j) < srcId) { j += 1 } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartition.scala:167: value < is not a member of org.apache.spark.graphx.VertexId [error] while (j < other.size && other.srcIds(j) == srcId && other.dstIds(j) < dstId) { j += 1 } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:34: type mismatch; [error] found : java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:55: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg.vid, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:65: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] new VertexBroadcastMsg[Int](0, a, b).asInstanceOf[T] [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:79: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg.vid, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:89: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] new VertexBroadcastMsg[Long](0, a, b).asInstanceOf[T] [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:103: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg.vid, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:113: type mismatch; [error] found : Long [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] new VertexBroadcastMsg[Double](0, a, b).asInstanceOf[T] [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:127: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:151: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/impl/Serializers.scala:175: type mismatch; [error] found : org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] required: Long [error] writeVarLong(msg._1, optimizePositive = false) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:41: value < is not a member of org.apache.spark.graphx.VertexId [error] if (edge.srcAttr < edge.dstAttr) { [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:43: value > is not a member of org.apache.spark.graphx.VertexId [error] } else if (edge.srcAttr > edge.dstAttr) { [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:51: overloaded method value min with alternatives: [error] (x: Double,y: Double)Double <and> [error] (x: Float,y: Float)Float <and> [error] (x: Long,y: Long)Long <and> [error] (x: Int,y: Int)Int [error] cannot be applied to (org.apache.spark.graphx.VertexId, Long) [error] vprog = (id, attr, msg) => math.min(attr, msg), [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/ConnectedComponents.scala:52: type mismatch; [error] found : Iterator[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] [error] required: Iterator[(org.apache.spark.graphx.VertexId, Long)] [error] sendMsg = sendMessage, [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/lib/StronglyConnectedComponents.scala:75: type mismatch; [error] found : Long(9223372036854775807L) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] sccWorkGraph, Long.MaxValue, activeDirection = EdgeDirection.Out)( [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:54: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] src => (src, sampleLogNormal(mu, sigma, numVertices)) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:57: value toInt is not a member of org.apache.spark.graphx.VertexId [error] generateRandomEdges(v._1.toInt, v._2, numVertices) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:59: type mismatch; [error] found : org.apache.spark.rdd.RDD[Nothing] [error] required: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] [error] Note: Nothing <: org.apache.spark.graphx.Edge[Int], but class RDD is invariant in type T. [error] You may wish to define T as +T instead. (SLS 4.5) [error] Graph(vertices, edges, 0) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:64: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Array.fill(maxVertexId) { Edge[Int](src, rand.nextInt(maxVertexId), 1) } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:64: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Array.fill(maxVertexId) { Edge[Int](src, rand.nextInt(maxVertexId), 1) } [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:129: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Edge[Int](src, dst, 1) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:129: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] Edge[Int](src, dst, 1) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:209: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] def sub2ind(r: Int, c: Int): VertexId = r * cols + c [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:231: type mismatch; [error] found : Int [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val edges: RDD[(VertexId, VertexId)] = sc.parallelize(1 until nverts).map(vid => (vid, 0)) [error] ^ [error] /Users/dnulu/Downloads/spark-0.9.0-incubating/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala:231: type mismatch; [error] found : Int(0) [error] required: org.apache.spark.graphx.VertexId [error] (which expands to) java.util.UUID [error] val edges: RDD[(VertexId, VertexId)] = sc.parallelize(1 until nverts).map(vid => (vid, 0)) [error] ^ [error] 39 errors found\n2. Reynold Xin: This is a good feedback - my concern is it will have a pretty significant drop in performance if strings are used (we rely on hashing integers pretty heavily). Let us think about it more. If the demand is huge, we can prioritize this and make it optional.\n3. Deepak Nulu: With the change being requested, Longs can still be used by those who can use Longs and want that performance. The overhead (if any) will come from the type-class mechanism (or whatever mechanism is chosen for making the vertex-ID a customizable type), and I don't know how much that overhead will be.\n4. npanj: An alternative approach, that I have been using: 1 Use a preprocessing step that maps UUID to an Long. 2. Build graph based on Longs For Mapping in step 1: - Rank your uuids. - some kind of has function? For 1, graphx can provide a tool to generate map. I will like to hear how others are building graphs out of non-Long node types.\n5. Di Xiao: I like npanj's approach. It's universal. You treat UUID as attribute. Like the procedure from http://spark.apache.org/docs/latest/graphx-programming-guide.html // Connect to the Spark cluster == Build Graph (build VertexID if necessary) // Load my user data and parse into tuples of user id and attribute list // Parse the edge data which is already in userId -> userId format // Attach the user attributes == Clean Graph // Some users may not have attributes so we set them as empty, Restrict the graph to users with usernames and names == Compute // Compute the PageRank == Get Result // Get the attributes of the top pagerank users\n6. Dan Osipov: FWIW, UUID.getMostSignificantBits() or getLeastSignificantBits() can be used to generate a Long, with low collision probability. Using any type for the ID is still preferred.\n7. Carlos Balduz: I am currently using zipWithUniqueId() to get a VertexID for my data, but that means that after getting the VertexIDs, I have to go to the edges data to look for each of those strings and assign the Id I got from the previous step. I agree it would be nice to be able to choose a different tipe of ID, leaving the user to decide whether he prefers performance or usability.\n8. JJ Zhang: We would also really like a general customized ID available for Vertex. We've been using zipwithIndex to create IDs for now, however, it is a hassle process-wise because we never have a stable ID: any update to a new version of Graph with incremental input data requires a total rebuild of vertex/edges, or we will need another infrastructure to serve as an ID service: additional cost/maintenance. We already have unique IDs for all of our data entities. It would make processing/maintenance much easier if our stable IDs can be used directly\n9. Sean R. Owen: No activity in 2 years\n10. Nicholas Tietz: We are also running into this issue and would like this feature. For our use case and our data size, even a low risk of hash collisions is not acceptable, so we have to have a reliable way to form unique ids from our current unique string ids. I'm going to work on a patch next week. Since this is marked \"won't fix\" due to inactivity, what's the process if a PR is submitted? (Sorry, new to the Apache contribution process.)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "a49106a6433c050de4512eab55420e1d", "issue_key": "SPARK-1154", "issue_type": "Improvement", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Spark fills up disk with app-* folders", "description": "Current version of Spark fills up the disk with many app-* folders: $ ls /var/lib/spark app-20140210022347-0597 app-20140212173327-0627 app-20140218154110-0657 app-20140225232537-0017 app-20140225233548-0047 app-20140210022407-0598 app-20140212173347-0628 app-20140218154130-0658 app-20140225232551-0018 app-20140225233556-0048 app-20140210022427-0599 app-20140212173754-0629 app-20140218164232-0659 app-20140225232611-0019 app-20140225233603-0049 app-20140210022447-0600 app-20140212182235-0630 app-20140218165133-0660 app-20140225232802-0020 app-20140225233610-0050 app-20140210022508-0601 app-20140212182256-0631 app-20140218165148-0661 app-20140225232822-0021 app-20140225233617-0051 app-20140210022528-0602 app-20140213000014-0632 app-20140218165225-0662 app-20140225232940-0022 app-20140225233624-0052 app-20140211024356-0603 app-20140213002026-0633 app-20140218165249-0663 app-20140225233002-0023 app-20140225233631-0053 app-20140211024417-0604 app-20140213154948-0634 app-20140218172030-0664 app-20140225233056-0024 app-20140225233725-0054 app-20140211024437-0605 app-20140213171810-0635 app-20140218193853-0665 app-20140225233108-0025 app-20140225233731-0055 app-20140211024457-0606 app-20140213193637-0636 app-20140218194442-0666 app-20140225233124-0026 app-20140225233733-0056 app-20140211024517-0607 app-20140214011513-0637 app-20140218194746-0667 app-20140225233133-0027 app-20140225233734-0057 app-20140211024538-0608 app-20140214012151-0638 app-20140218194822-0668 app-20140225233147-0028 app-20140225233749-0058 app-20140211193443-0609 app-20140214013134-0639 app-20140218212317-0669 app-20140225233208-0029 app-20140225233759-0059 app-20140211195210-0610 app-20140214013332-0640 app-20140225180142-0000 app-20140225233215-0030 app-20140225233809-0060 app-20140211213935-0611 app-20140214013642-0641 app-20140225180411-0001 app-20140225233224-0031 app-20140225233828-0061 app-20140211214227-0612 app-20140214014246-0642 app-20140225180431-0002 app-20140225233232-0032 app-20140225234719-0062 app-20140211215317-0613 app-20140214014607-0643 app-20140225180452-0003 app-20140225233239-0033 app-20140226032845-0063 app-20140211224601-0614 app-20140214184943-0644 app-20140225180512-0004 app-20140225233320-0034 app-20140226033004-0064 app-20140212022206-0615 app-20140214185118-0645 app-20140225180533-0005 app-20140225233328-0035 app-20140226033119-0065 app-20140212022226-0616 app-20140214185851-0646 app-20140225180553-0006 app-20140225233354-0036 app-20140226033334-0066 app-20140212022246-0617 app-20140214222856-0647 app-20140225181115-0007 app-20140225233402-0037 app-20140226033354-0067 app-20140212043704-0618 app-20140214231312-0648 app-20140225181244-0008 app-20140225233409-0038 app-20140226033538-0068 app-20140212043724-0619 app-20140214231434-0649 app-20140225182051-0009 app-20140225233416-0039 app-20140226033826-0069 app-20140212043745-0620 app-20140214231542-0650 app-20140225183009-0010 app-20140225233426-0040 app-20140226034002-0070 app-20140212044016-0621 app-20140214231616-0651 app-20140225184133-0011 app-20140225233432-0041 app-20140226034053-0071 app-20140212044203-0622 app-20140214233016-0652 app-20140225184318-0012 app-20140225233439-0042 app-20140226034234-0072 app-20140212044224-0623 app-20140214233037-0653 app-20140225184709-0013 app-20140225233447-0043 app-20140226034426-0073 app-20140212045034-0624 app-20140218153242-0654 app-20140225184844-0014 app-20140225233526-0044 app-20140226034447-0074 app-20140212045119-0625 app-20140218153341-0655 app-20140225190051-0015 app-20140225233534-0045 app-20140212173310-0626 app-20140218153442-0656 app-20140225232516-0016 app-20140225233540-0046 This problem is particularly bad if you have a whole bunch of fast jobs. Also what makes the problem worse is that any jars for jobs is downloaded into the app-* folder, so that fills up the disk particularly fast. I would like to propose two things: 1) Spark should have a cleanup thread (or actor) which periodically removes old app-* folders; This should not be the responsibility of people deploying Spark. 2) The download of jars should not go to each app-* folder. This wastes a huge amount of space because most jobs use the same jars. Maybe I can open a separate ticket for this.", "reporter": "Evan Chan", "assignee": "Mingyu Kim", "created": "2014-02-28T11:40:28.000+0000", "updated": "2014-07-24T18:55:15.000+0000", "resolved": "2014-04-07T02:23:26.000+0000", "labels": ["starter"], "components": ["Deploy"], "comments": [{"author": "Sean Owen", "body": "FWIW Jeff H. here had observed the same thing and had the same request. I'm only sorry I'm not qualified to propose the actual change here but do agree this would help some intense deployments.", "created": "2014-02-28T11:53:26.285+0000"}, {"author": "Andrew Ash", "body": "+1 I've observed this too", "created": "2014-02-28T13:13:46.725+0000"}, {"author": "Patrick McFadin", "body": "The issue with having a clean-up is that these are user logs which some people might want around so it's sort of dangerous to just go around deleting stuff. In the past the assumption has been that the cluster administrator would be responsible for cleaning the logs (as with other long lived services which generate logs). One idea would be to have an optional TTL value where logs are deleted after that time - would that help here?", "created": "2014-03-01T17:03:13.726+0000"}, {"author": "Piyush Kansal", "body": "How about zipping the old log files after a certain amount of time(configurable) and then delete them afterwards(again configurable, as Patrick has already pointed out)? Advantages: - will save good amout of disk space - without losing log files too soon - will give enough time to the cluster administrator to delete the log files either on his own or as per the the configured time", "created": "2014-03-04T23:17:58.352+0000"}, {"author": "Piyush Kansal", "body": "Also, as Evan pointed out for the second issue, instead of saving the same jars in each folder, we can save them in a single common folder. This can drastically reduce the disk usage.", "created": "2014-03-04T23:23:12.839+0000"}, {"author": "Evan Chan", "body": "It sounds like there are two configuration knobs: - spark.old.logs.delete.ttl - spark.old.logs.archive.ttl Personally I'm actually more keen to work on the jar issue as I think it'll save the most space for us, but the TTL should be easy to implement too.", "created": "2014-03-04T23:43:54.682+0000"}, {"author": "Andrew Ash", "body": "I'm also interested in the cleanup of old jars. I have dozens of copies of old jars that are all identical throughout these work directories and the disk space starts to add up over time. Sent from my mobile phone On Mar 4, 2014 11:44 PM, \"Evan Chan (JIRA)\" <", "created": "2014-03-05T07:48:28.474+0000"}, {"author": "Patrick McFadin", "body": "This all seems reasonable. Storing all the jars in the same directory will require some extra effort because if two applications add a jar with the same name but different content you want to have two distinct copies. It's definitely do-able but will require a bit of effort. Anyways I think consolidating the jars and/or having a TTL are both reasonable strategies.", "created": "2014-03-05T21:10:15.774+0000"}, {"author": "Piyush Kansal", "body": "I am interested in working on this issue. Please assign it to me. Or can I send a pull request rightaway?", "created": "2014-03-05T23:36:51.776+0000"}, {"author": "Andrew Ash", "body": "Feel free to send one right away! Please put \"SPARK-1154\" at the beginning of the title and send the pull request to the new GitHub project (no longer incubator) at http://github.com/apache/spark Thanks Piyush! On Wed, Mar 5, 2014 at 11:37 PM, Piyush Kansal (JIRA) <", "created": "2014-03-05T23:46:28.426+0000"}, {"author": "Piyush Kansal", "body": "Hey Andrew, I am a newbie. Looks like pull request can only be sent once the patch is ready. In the meantime, does this issue needs to be assigned to me to ensure that no one else ends up wasting his efforts? Thanks. On Wed, Mar 5, 2014 at 11:48 PM, Andrew Ash (JIRA) < -- Best, Piyush Kansal http://about.me/piyushkansal", "created": "2014-03-06T00:14:28.413+0000"}, {"author": "Andrew Ash", "body": "I just assigned it to you. Looking forward to seeing your patch when it's ready!", "created": "2014-03-06T01:56:38.687+0000"}, {"author": "Piyush Kansal", "body": "Thanks! On Mar 6, 2014 1:57 AM, \"Andrew Ash (JIRA)\" <", "created": "2014-03-06T15:14:28.373+0000"}, {"author": "Evan Chan", "body": "Piyush, Which part of this ticket are you working on, the cleaning up of app folders, the relocation of jar saving to a central dir, or both?", "created": "2014-03-13T10:27:28.627+0000"}, {"author": "Evan Chan", "body": "Yeah, I'm going to do this next week. This is pretty important for how we use Spark, and I imagine others too. We only have about a week left :( Some guidance on how to test this would be good. On Thu, Mar 27, 2014 at 6:05 AM, Piyush Kansal (JIRA) < -- -- Evan Chan Staff Engineer ev@ooyala.com | <http://www.ooyala.com/> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>", "created": "2014-03-27T10:17:20.533+0000"}, {"author": "Mingyu Kim", "body": "I looked at the commit, and it seems like it wipes out app-* based on the last modification time of the directory itself. Because the modification time of a directory only changes when a child is added or removed, this may wipe out the app-* directory of a running Spark application if it has been running for more than TTL (unless new files/jars are added to the app directory once every while). I believe it should check the latest modification time of all the descendents of the app-* directory to decide whether to delete it or not. Am I mistaken?", "created": "2014-05-16T03:24:41.714+0000"}, {"author": "Patrick Wendell", "body": "[~mkim] yes you are correct - this is broken. Checkout SPARK-1860. Are you interested in fixing this?", "created": "2014-05-16T04:58:57.891+0000"}, {"author": "Mingyu Kim", "body": "Yes, I'd be happy to take it. Please feel free to assign it to me.", "created": "2014-05-17T22:18:43.221+0000"}, {"author": "Andrew Ash", "body": "For the record, this is Evan's PR that closed this ticket: https://github.com/apache/spark/pull/288", "created": "2014-07-24T18:55:15.832+0000"}], "num_comments": 19, "text": "Issue: SPARK-1154\nSummary: Spark fills up disk with app-* folders\nDescription: Current version of Spark fills up the disk with many app-* folders: $ ls /var/lib/spark app-20140210022347-0597 app-20140212173327-0627 app-20140218154110-0657 app-20140225232537-0017 app-20140225233548-0047 app-20140210022407-0598 app-20140212173347-0628 app-20140218154130-0658 app-20140225232551-0018 app-20140225233556-0048 app-20140210022427-0599 app-20140212173754-0629 app-20140218164232-0659 app-20140225232611-0019 app-20140225233603-0049 app-20140210022447-0600 app-20140212182235-0630 app-20140218165133-0660 app-20140225232802-0020 app-20140225233610-0050 app-20140210022508-0601 app-20140212182256-0631 app-20140218165148-0661 app-20140225232822-0021 app-20140225233617-0051 app-20140210022528-0602 app-20140213000014-0632 app-20140218165225-0662 app-20140225232940-0022 app-20140225233624-0052 app-20140211024356-0603 app-20140213002026-0633 app-20140218165249-0663 app-20140225233002-0023 app-20140225233631-0053 app-20140211024417-0604 app-20140213154948-0634 app-20140218172030-0664 app-20140225233056-0024 app-20140225233725-0054 app-20140211024437-0605 app-20140213171810-0635 app-20140218193853-0665 app-20140225233108-0025 app-20140225233731-0055 app-20140211024457-0606 app-20140213193637-0636 app-20140218194442-0666 app-20140225233124-0026 app-20140225233733-0056 app-20140211024517-0607 app-20140214011513-0637 app-20140218194746-0667 app-20140225233133-0027 app-20140225233734-0057 app-20140211024538-0608 app-20140214012151-0638 app-20140218194822-0668 app-20140225233147-0028 app-20140225233749-0058 app-20140211193443-0609 app-20140214013134-0639 app-20140218212317-0669 app-20140225233208-0029 app-20140225233759-0059 app-20140211195210-0610 app-20140214013332-0640 app-20140225180142-0000 app-20140225233215-0030 app-20140225233809-0060 app-20140211213935-0611 app-20140214013642-0641 app-20140225180411-0001 app-20140225233224-0031 app-20140225233828-0061 app-20140211214227-0612 app-20140214014246-0642 app-20140225180431-0002 app-20140225233232-0032 app-20140225234719-0062 app-20140211215317-0613 app-20140214014607-0643 app-20140225180452-0003 app-20140225233239-0033 app-20140226032845-0063 app-20140211224601-0614 app-20140214184943-0644 app-20140225180512-0004 app-20140225233320-0034 app-20140226033004-0064 app-20140212022206-0615 app-20140214185118-0645 app-20140225180533-0005 app-20140225233328-0035 app-20140226033119-0065 app-20140212022226-0616 app-20140214185851-0646 app-20140225180553-0006 app-20140225233354-0036 app-20140226033334-0066 app-20140212022246-0617 app-20140214222856-0647 app-20140225181115-0007 app-20140225233402-0037 app-20140226033354-0067 app-20140212043704-0618 app-20140214231312-0648 app-20140225181244-0008 app-20140225233409-0038 app-20140226033538-0068 app-20140212043724-0619 app-20140214231434-0649 app-20140225182051-0009 app-20140225233416-0039 app-20140226033826-0069 app-20140212043745-0620 app-20140214231542-0650 app-20140225183009-0010 app-20140225233426-0040 app-20140226034002-0070 app-20140212044016-0621 app-20140214231616-0651 app-20140225184133-0011 app-20140225233432-0041 app-20140226034053-0071 app-20140212044203-0622 app-20140214233016-0652 app-20140225184318-0012 app-20140225233439-0042 app-20140226034234-0072 app-20140212044224-0623 app-20140214233037-0653 app-20140225184709-0013 app-20140225233447-0043 app-20140226034426-0073 app-20140212045034-0624 app-20140218153242-0654 app-20140225184844-0014 app-20140225233526-0044 app-20140226034447-0074 app-20140212045119-0625 app-20140218153341-0655 app-20140225190051-0015 app-20140225233534-0045 app-20140212173310-0626 app-20140218153442-0656 app-20140225232516-0016 app-20140225233540-0046 This problem is particularly bad if you have a whole bunch of fast jobs. Also what makes the problem worse is that any jars for jobs is downloaded into the app-* folder, so that fills up the disk particularly fast. I would like to propose two things: 1) Spark should have a cleanup thread (or actor) which periodically removes old app-* folders; This should not be the responsibility of people deploying Spark. 2) The download of jars should not go to each app-* folder. This wastes a huge amount of space because most jobs use the same jars. Maybe I can open a separate ticket for this.\n\nComments (19):\n1. Sean Owen: FWIW Jeff H. here had observed the same thing and had the same request. I'm only sorry I'm not qualified to propose the actual change here but do agree this would help some intense deployments.\n2. Andrew Ash: +1 I've observed this too\n3. Patrick McFadin: The issue with having a clean-up is that these are user logs which some people might want around so it's sort of dangerous to just go around deleting stuff. In the past the assumption has been that the cluster administrator would be responsible for cleaning the logs (as with other long lived services which generate logs). One idea would be to have an optional TTL value where logs are deleted after that time - would that help here?\n4. Piyush Kansal: How about zipping the old log files after a certain amount of time(configurable) and then delete them afterwards(again configurable, as Patrick has already pointed out)? Advantages: - will save good amout of disk space - without losing log files too soon - will give enough time to the cluster administrator to delete the log files either on his own or as per the the configured time\n5. Piyush Kansal: Also, as Evan pointed out for the second issue, instead of saving the same jars in each folder, we can save them in a single common folder. This can drastically reduce the disk usage.\n6. Evan Chan: It sounds like there are two configuration knobs: - spark.old.logs.delete.ttl - spark.old.logs.archive.ttl Personally I'm actually more keen to work on the jar issue as I think it'll save the most space for us, but the TTL should be easy to implement too.\n7. Andrew Ash: I'm also interested in the cleanup of old jars. I have dozens of copies of old jars that are all identical throughout these work directories and the disk space starts to add up over time. Sent from my mobile phone On Mar 4, 2014 11:44 PM, \"Evan Chan (JIRA)\" <\n8. Patrick McFadin: This all seems reasonable. Storing all the jars in the same directory will require some extra effort because if two applications add a jar with the same name but different content you want to have two distinct copies. It's definitely do-able but will require a bit of effort. Anyways I think consolidating the jars and/or having a TTL are both reasonable strategies.\n9. Piyush Kansal: I am interested in working on this issue. Please assign it to me. Or can I send a pull request rightaway?\n10. Andrew Ash: Feel free to send one right away! Please put \"SPARK-1154\" at the beginning of the title and send the pull request to the new GitHub project (no longer incubator) at http://github.com/apache/spark Thanks Piyush! On Wed, Mar 5, 2014 at 11:37 PM, Piyush Kansal (JIRA) <", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "586404afd056ee35744ae131659b50a9", "issue_key": "SPARK-1155", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Clean up and document use of SparkEnv", "description": "We should provide better documentation explaining what SparkEnv is and why it needs to be thread local (basically, to allow it to be accessed inside of closures on executors). Also, in cases where SparkEnv is being accessed on the driver we should access it through the associated SparkContext rather than through the thread local. Finally, we should see if it's possible to just remove this as a thread local and instead make it a static singleton that the exeucutor sets once. This last thing might not be possible if, under certain code paths, this is used on the driver.", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "created": "2014-02-28T15:36:54.000+0000", "updated": "2016-01-05T21:18:42.000+0000", "resolved": "2016-01-05T21:18:42.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "This issue is no longer relevant now that SparkEnv is no longer a thread-local.", "created": "2016-01-05T21:18:42.297+0000"}], "num_comments": 1, "text": "Issue: SPARK-1155\nSummary: Clean up and document use of SparkEnv\nDescription: We should provide better documentation explaining what SparkEnv is and why it needs to be thread local (basically, to allow it to be accessed inside of closures on executors). Also, in cases where SparkEnv is being accessed on the driver we should access it through the associated SparkContext rather than through the thread local. Finally, we should see if it's possible to just remove this as a thread local and instead make it a static singleton that the exeucutor sets once. This last thing might not be possible if, under certain code paths, this is used on the driver.\n\nComments (1):\n1. Josh Rosen: This issue is no longer relevant now that SparkEnv is no longer a thread-local.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "598c4d94790e25263027f9340f74bc78", "issue_key": "SPARK-1156", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Allow spark-ec2 to login to a cluster with 0 slaves", "description": "{{spark-ec2}} allows you to launch a cluster with no slaves. However, if you try to login to such a cluster, you get the following error:  Searching for existing cluster <cluster-name>... Found 1 master(s), 0 slaves ERROR: Could not find slaves in group <cluster-name>-slaves  {{spark-ec2}} should allow you to connect to a cluster with no slaves.", "reporter": "Nicholas Chammas", "assignee": "Nan Zhu", "created": "2014-03-01T21:14:24.000+0000", "updated": "2014-03-07T07:23:20.000+0000", "resolved": "2014-03-07T07:23:20.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Nan Zhu", "body": "PR: https://github.com/apache/spark/pull/58", "created": "2014-03-02T18:31:45.012+0000"}], "num_comments": 1, "text": "Issue: SPARK-1156\nSummary: Allow spark-ec2 to login to a cluster with 0 slaves\nDescription: {{spark-ec2}} allows you to launch a cluster with no slaves. However, if you try to login to such a cluster, you get the following error:  Searching for existing cluster <cluster-name>... Found 1 master(s), 0 slaves ERROR: Could not find slaves in group <cluster-name>-slaves  {{spark-ec2}} should allow you to connect to a cluster with no slaves.\n\nComments (1):\n1. Nan Zhu: PR: https://github.com/apache/spark/pull/58", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "5d47f731f1418e001ecdcb0779135fa1", "issue_key": "SPARK-1157", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "L-BFGS Optimizer", "description": "L-BFGS (Limited-memory BFGS) is an optimization algorithm like BFGS which uses an approximation to the inverse of Hessian matrix to steer its search through the variable space, but where BFGS stores a dense nxn approximation to the inverse Hessian, L-BFGS only stores a few vectors to represent the approximation. For high dimensional optimization problems, the Newton method or BFGS is not applicable since the amount of memory needed to store the Hessian will grow exponentially, while L-BFGS only stores couple vectors. One of the use case can be training large-scale logistic regression with so many features. We'll use breeze implementation of L-BFGS.", "reporter": "DB Tsai", "assignee": "DB Tsai", "created": "2014-03-01T23:37:59.000+0000", "updated": "2014-04-15T18:14:01.000+0000", "resolved": "2014-04-15T18:14:01.000+0000", "labels": [], "components": [], "comments": [{"author": "DB Tsai", "body": "PR: https://github.com/apache/spark/pull/353", "created": "2014-04-09T04:54:42.431+0000"}], "num_comments": 1, "text": "Issue: SPARK-1157\nSummary: L-BFGS Optimizer\nDescription: L-BFGS (Limited-memory BFGS) is an optimization algorithm like BFGS which uses an approximation to the inverse of Hessian matrix to steer its search through the variable space, but where BFGS stores a dense nxn approximation to the inverse Hessian, L-BFGS only stores a few vectors to represent the approximation. For high dimensional optimization problems, the Newton method or BFGS is not applicable since the amount of memory needed to store the Hessian will grow exponentially, while L-BFGS only stores couple vectors. One of the use case can be training large-scale logistic regression with so many features. We'll use breeze implementation of L-BFGS.\n\nComments (1):\n1. DB Tsai: PR: https://github.com/apache/spark/pull/353", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "b16c32b649929e88897a4ff5fa89d5e4", "issue_key": "SPARK-1158", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fix flaky RateLimitedOutputStreamSuite", "description": "", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2014-03-02T16:25:45.000+0000", "updated": "2014-12-19T16:16:31.000+0000", "resolved": "2014-03-03T21:25:10.000+0000", "labels": ["flaky-test"], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1158\nSummary: Fix flaky RateLimitedOutputStreamSuite", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "680822a683c25e6efa0f39d334806818", "issue_key": "SPARK-1159", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add Shortest-path computations to graphx.lib", "description": "Add a landmark-based shortest-path computation to org.apache.spark.graphx.lib, to gather the lengths of shortest paths to a given set of nodes across the whole graph. See PR: https://github.com/apache/spark/pull/10", "reporter": "Andres Perez", "assignee": null, "created": "2014-03-02T16:30:05.000+0000", "updated": "2015-09-07T08:29:22.000+0000", "resolved": "2015-09-07T08:29:22.000+0000", "labels": [], "components": ["GraphX"], "comments": [{"author": "Andres Perez", "body": "The pull-request no longer adds any dependencies.", "created": "2014-04-23T17:36:48.285+0000"}], "num_comments": 1, "text": "Issue: SPARK-1159\nSummary: Add Shortest-path computations to graphx.lib\nDescription: Add a landmark-based shortest-path computation to org.apache.spark.graphx.lib, to gather the lengths of shortest paths to a given set of nodes across the whole graph. See PR: https://github.com/apache/spark/pull/10\n\nComments (1):\n1. Andres Perez: The pull-request no longer adds any dependencies.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "8ba51b8524db44666b9cab84646e9f31", "issue_key": "SPARK-1160", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Deprecate RDD.toArray", "description": "It's redundant with collect() and the name doesn't make sense in Java, where we return a List (we can't return an array due to the way Java generics work). It's also missing in Python.", "reporter": "Matei Alexandru Zaharia", "assignee": "Nan Zhu", "created": "2014-03-02T17:42:41.000+0000", "updated": "2014-03-12T19:50:01.000+0000", "resolved": "2014-03-12T19:50:01.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "https://github.com/apache/spark/pull/105", "created": "2014-03-08T06:33:28.434+0000"}], "num_comments": 1, "text": "Issue: SPARK-1160\nSummary: Deprecate RDD.toArray\nDescription: It's redundant with collect() and the name doesn't make sense in Java, where we return a List (we can't return an array due to the way Java generics work). It's also missing in Python.\n\nComments (1):\n1. Nan Zhu: https://github.com/apache/spark/pull/105", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "5548b7e3d29b75634145c209862f6abe", "issue_key": "SPARK-1161", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add saveAsObjectFile and SparkContext.objectFile in Python", "description": "It can use pickling for serialization and a SequenceFile on disk similar to the JVM versions of these.", "reporter": "Matei Alexandru Zaharia", "assignee": "Kan Zhang", "created": "2014-03-02T17:45:12.000+0000", "updated": "2014-06-04T01:18:59.000+0000", "resolved": "2014-06-04T01:18:59.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Kan Zhang", "body": "PR: https://github.com/apache/spark/pull/755", "created": "2014-05-13T05:39:08.290+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Merged this in -- thanks Kan!", "created": "2014-06-04T01:18:59.617+0000"}], "num_comments": 2, "text": "Issue: SPARK-1161\nSummary: Add saveAsObjectFile and SparkContext.objectFile in Python\nDescription: It can use pickling for serialization and a SequenceFile on disk similar to the JVM versions of these.\n\nComments (2):\n1. Kan Zhang: PR: https://github.com/apache/spark/pull/755\n2. Matei Alexandru Zaharia: Merged this in -- thanks Kan!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "335e92cad4037348851a0c256b4f9488", "issue_key": "SPARK-1162", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add top() and takeOrdered() to PySpark", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "created": "2014-03-02T17:48:33.000+0000", "updated": "2015-12-10T15:06:16.000+0000", "resolved": "2014-04-03T22:45:07.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/93", "created": "2015-12-10T15:06:16.936+0000"}], "num_comments": 1, "text": "Issue: SPARK-1162\nSummary: Add top() and takeOrdered() to PySpark\n\nComments (1):\n1. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/93", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "d613127f04eeda8c62d7c7e970d1afde", "issue_key": "SPARK-1163", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Miscellaneous missing PySpark methods", "description": "The following utility / debugging methods on RDD are missing: - name - setName - generator - setGenerator - id - toDebugString - getStorageLevel Would be nice to add them in.", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "created": "2014-03-02T17:51:04.000+0000", "updated": "2020-02-07T17:26:44.000+0000", "resolved": "2014-03-11T23:58:00.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1163\nSummary: Miscellaneous missing PySpark methods\nDescription: The following utility / debugging methods on RDD are missing: - name - setName - generator - setGenerator - id - toDebugString - getStorageLevel Would be nice to add them in.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "1b63607fe3d2544653c7b280bc405ca7", "issue_key": "SPARK-1164", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Deprecate RDD.reduceByKeyToDriver", "description": "It's missing in Java and Python and it's just an alias for reduceByKeyLocally.", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "created": "2014-03-02T17:54:24.000+0000", "updated": "2020-02-07T17:26:41.000+0000", "resolved": "2014-03-11T23:56:47.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/72", "created": "2015-12-10T15:06:13.909+0000"}], "num_comments": 1, "text": "Issue: SPARK-1164\nSummary: Deprecate RDD.reduceByKeyToDriver\nDescription: It's missing in Java and Python and it's just an alias for reduceByKeyLocally.\n\nComments (1):\n1. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/72", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "2c1ced6d8c05c70a63ef255148fd7933", "issue_key": "SPARK-1165", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add RDD.intersection() to Java and Python", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "created": "2014-03-02T17:55:25.000+0000", "updated": "2020-02-07T17:26:41.000+0000", "resolved": "2014-03-11T23:56:20.000+0000", "labels": [], "components": ["Java API", "PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1165\nSummary: Add RDD.intersection() to Java and Python", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "f6f4821fa6d0e6f21ef1a98990c6dd63", "issue_key": "SPARK-1166", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "leftover vpc_id may block the creation of new ec2 cluster", "description": "When I run the spark-ec2 script to build ec2 cluster in EC2, for some reason, I always received errors as following:  Setting up security groups... ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'null' for protocol. VPC security group rules must specify protocols explicitly.</Message></Error></Errors><RequestID>fc56f0ba-915a-45b6-8555-05d4dd0f14ee</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 813, in <module> main() File \"./spark_ec2.py\", line 806, in main real_main() File \"./spark_ec2.py\", line 689, in real_main conn, opts, cluster_name) File \"./spark_ec2.py\", line 244, in launch_cluster slave_group.authorize(src_group=master_group) File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2181, in authorize_security_group File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'null' for protocol. VPC security group rules must specify protocols explicitly.</Message></Error></Errors><RequestID>fc56f0ba-915a-45b6-8555-05d4dd0f14ee</RequestID></Response>", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-03-02T18:55:17.000+0000", "updated": "2014-07-22T20:30:59.000+0000", "resolved": "2014-04-16T21:09:15.000+0000", "labels": [], "components": [], "comments": [{"author": "Nan Zhu", "body": "I looked at the boto implementation, it turns out that the new created master_group and slave_group has a valid vpc_id, so that when the following code execute, boto thinks that we should pass the protocol type explicitly  group_name = None if not self.vpc_id: group_name = self.name group_id = None if self.vpc_id: group_id = self.id src_group_name = None src_group_owner_id = None src_group_group_id = None if src_group: cidr_ip = None src_group_owner_id = src_group.owner_id if not self.vpc_id: src_group_name = src_group.name else: if hasattr(src_group, 'group_id'): src_group_group_id = src_group.group_id else: src_group_group_id = src_group.id status = self.connection.authorize_security_group(group_name, src_group_name, src_group_owner_id, ip_protocol, from_port, to_port, cidr_ip, group_id, src_group_group_id)", "created": "2014-03-02T18:57:51.677+0000"}, {"author": "Nan Zhu", "body": "cannot reproduce after several weeks....", "created": "2014-04-16T21:09:15.921+0000"}, {"author": "Anass BENSRHIR", "body": "Actually i've got the same error today , and i can't laucnh a cluster : here is the output : ./spark-ec2 -i ~/amazonhdp.pem -k amazonhdp -s 4 -t m1.small launch hadoopi Setting up security groups... Creating security group hadoopi-master Creating security group hadoopi-slaves ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-2ec1b84b' does not exist</Message></Error></Errors><RequestID>6554c7a8-f68a-4032-ad63-65106e2de9b3</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 856, in <module> main() File \"./spark_ec2.py\", line 848, in main real_main() File \"./spark_ec2.py\", line 731, in real_main conn, opts, cluster_name) File \"./spark_ec2.py\", line 252, in launch_cluster master_group.authorize('tcp', 50030, 50030, '0.0.0.0/0') File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2181, in authorize_security_group File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-2ec1b84b' does not exist</Message></Error></Errors><RequestID>6554c7a8-f68a-4032-ad63-65106e2de9b3</RequestID></Response>", "created": "2014-07-22T03:29:07.533+0000"}, {"author": "bruce szalwinski", "body": "I've been able to reproduce, but not consistently. On this occasion, I had previously started an instance, didn't use it for anything and soon there after shut it down. Don't know if that means anything. Using spark 1.0 from spark-ec2 -k mykeypair -i ~/.aws/mykeypair.pem -s 7 -r \"us-west-2\" -t r3.large launch \"sparck-cluster\" Setting up security groups... Creating security group sparck-cluster-master Creating security group sparck-cluster-slaves ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-185fe07d' does not exist</Message></Error></Errors><RequestID>80f1e1e3-e340-4cd2-ba64-53c13525ab2b</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 909, in <module> main() File \"./spark_ec2.py\", line 901, in main real_main() File \"./spark_ec2.py\", line 779, in real_main (master_nodes, slave_nodes) = launch_cluster(conn, opts, cluster_name) File \"./spark_ec2.py\", line 279, in launch_cluster master_group.authorize(src_group=slave_group) File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2150, in authorize_security_group File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2093, in authorize_security_group_deprecated File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-185fe07d' does not exist</Message></Error></Errors><RequestID>80f1e1e3-e340-4cd2-ba64-53c13525ab2b</RequestID></Response>", "created": "2014-07-22T20:23:00.518+0000"}, {"author": "bruce szalwinski", "body": "To resolve, I go to https://console.aws.amazon.com/vpc/home?region=us-west-2#securityGroups: and manually delete the security groups and then I'm able to start up a cluster.", "created": "2014-07-22T20:30:59.836+0000"}], "num_comments": 5, "text": "Issue: SPARK-1166\nSummary: leftover vpc_id may block the creation of new ec2 cluster\nDescription: When I run the spark-ec2 script to build ec2 cluster in EC2, for some reason, I always received errors as following:  Setting up security groups... ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'null' for protocol. VPC security group rules must specify protocols explicitly.</Message></Error></Errors><RequestID>fc56f0ba-915a-45b6-8555-05d4dd0f14ee</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 813, in <module> main() File \"./spark_ec2.py\", line 806, in main real_main() File \"./spark_ec2.py\", line 689, in real_main conn, opts, cluster_name) File \"./spark_ec2.py\", line 244, in launch_cluster slave_group.authorize(src_group=master_group) File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2181, in authorize_security_group File \"/Users/nanzhu/code/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'null' for protocol. VPC security group rules must specify protocols explicitly.</Message></Error></Errors><RequestID>fc56f0ba-915a-45b6-8555-05d4dd0f14ee</RequestID></Response>\n\nComments (5):\n1. Nan Zhu: I looked at the boto implementation, it turns out that the new created master_group and slave_group has a valid vpc_id, so that when the following code execute, boto thinks that we should pass the protocol type explicitly  group_name = None if not self.vpc_id: group_name = self.name group_id = None if self.vpc_id: group_id = self.id src_group_name = None src_group_owner_id = None src_group_group_id = None if src_group: cidr_ip = None src_group_owner_id = src_group.owner_id if not self.vpc_id: src_group_name = src_group.name else: if hasattr(src_group, 'group_id'): src_group_group_id = src_group.group_id else: src_group_group_id = src_group.id status = self.connection.authorize_security_group(group_name, src_group_name, src_group_owner_id, ip_protocol, from_port, to_port, cidr_ip, group_id, src_group_group_id)\n2. Nan Zhu: cannot reproduce after several weeks....\n3. Anass BENSRHIR: Actually i've got the same error today , and i can't laucnh a cluster : here is the output : ./spark-ec2 -i ~/amazonhdp.pem -k amazonhdp -s 4 -t m1.small launch hadoopi Setting up security groups... Creating security group hadoopi-master Creating security group hadoopi-slaves ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-2ec1b84b' does not exist</Message></Error></Errors><RequestID>6554c7a8-f68a-4032-ad63-65106e2de9b3</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 856, in <module> main() File \"./spark_ec2.py\", line 848, in main real_main() File \"./spark_ec2.py\", line 731, in real_main conn, opts, cluster_name) File \"./spark_ec2.py\", line 252, in launch_cluster master_group.authorize('tcp', 50030, 50030, '0.0.0.0/0') File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2181, in authorize_security_group File \"/opt/spark-1.0.1-bin-hadoop2/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-2ec1b84b' does not exist</Message></Error></Errors><RequestID>6554c7a8-f68a-4032-ad63-65106e2de9b3</RequestID></Response>\n4. bruce szalwinski: I've been able to reproduce, but not consistently. On this occasion, I had previously started an instance, didn't use it for anything and soon there after shut it down. Don't know if that means anything. Using spark 1.0 from spark-ec2 -k mykeypair -i ~/.aws/mykeypair.pem -s 7 -r \"us-west-2\" -t r3.large launch \"sparck-cluster\" Setting up security groups... Creating security group sparck-cluster-master Creating security group sparck-cluster-slaves ERROR:boto:400 Bad Request ERROR:boto:<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-185fe07d' does not exist</Message></Error></Errors><RequestID>80f1e1e3-e340-4cd2-ba64-53c13525ab2b</RequestID></Response> Traceback (most recent call last): File \"./spark_ec2.py\", line 909, in <module> main() File \"./spark_ec2.py\", line 901, in main real_main() File \"./spark_ec2.py\", line 779, in real_main (master_nodes, slave_nodes) = launch_cluster(conn, opts, cluster_name) File \"./spark_ec2.py\", line 279, in launch_cluster master_group.authorize(src_group=slave_group) File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/securitygroup.py\", line 184, in authorize File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2150, in authorize_security_group File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/ec2/connection.py\", line 2093, in authorize_security_group_deprecated File \"/Users/szalwinb/playpen/spark/ec2/third_party/boto-2.4.1.zip/boto-2.4.1/boto/connection.py\", line 944, in get_status boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Response><Errors><Error><Code>InvalidGroup.NotFound</Code><Message>The security group 'sg-185fe07d' does not exist</Message></Error></Errors><RequestID>80f1e1e3-e340-4cd2-ba64-53c13525ab2b</RequestID></Response>\n5. bruce szalwinski: To resolve, I go to https://console.aws.amazon.com/vpc/home?region=us-west-2#securityGroups: and manually delete the security groups and then I'm able to start up a cluster.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "8da4f87a4d11b3b322bce63da2b0b4be", "issue_key": "SPARK-1167", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Remove metrics-ganglia from default build due to LGPL issue", "description": "The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here: https://groups.google.com/forum/#!searchin/metrics-user/lgpl/metrics-user/1tQd_qZHQNE/TqAfXYwh7OUJ We should isolate this code in a separate module inside of an `/extras` folder and have a build flag in Maven/SBT (e.g. `-Pganglia`) that will pull this in if desired. That way users can still use it if they do a special build.", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2014-03-02T20:16:28.000+0000", "updated": "2014-03-30T04:13:39.000+0000", "resolved": "2014-03-11T11:55:05.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1167\nSummary: Remove metrics-ganglia from default build due to LGPL issue\nDescription: The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here: https://groups.google.com/forum/#!searchin/metrics-user/lgpl/metrics-user/1tQd_qZHQNE/TqAfXYwh7OUJ We should isolate this code in a separate module inside of an `/extras` folder and have a build flag in Maven/SBT (e.g. `-Pganglia`) that will pull this in if desired. That way users can still use it if they do a special build.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "c0839d47c5ff08f154f97cf9097b84fe", "issue_key": "SPARK-1168", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add foldByKey to PySpark", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Prashant Sharma", "created": "2014-03-02T20:18:06.000+0000", "updated": "2020-02-07T17:26:40.000+0000", "resolved": "2014-03-10T13:38:11.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "William Benton", "body": "I'd like to take this issue; can you assign it to me?", "created": "2014-03-10T06:23:03.812+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Prashant actually just did it (https://github.com/apache/spark/pull/115), but thanks for offering!", "created": "2014-03-10T13:38:06.396+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/115", "created": "2015-12-10T15:06:11.146+0000"}], "num_comments": 3, "text": "Issue: SPARK-1168\nSummary: Add foldByKey to PySpark\n\nComments (3):\n1. William Benton: I'd like to take this issue; can you assign it to me?\n2. Matei Alexandru Zaharia: Prashant actually just did it (https://github.com/apache/spark/pull/115), but thanks for offering!\n3. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/115", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "83957f5113da95a9ab36dfc8a586a677", "issue_key": "SPARK-1169", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add countApproxDistinctByKey to PySpark", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "2014-03-02T20:18:52.000+0000", "updated": "2016-01-18T13:51:07.000+0000", "resolved": "2016-01-18T13:51:07.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Dan McClary", "body": "Should this implement approximate jobs, or is a linear probabilistic approach using a standard job acceptable?", "created": "2014-03-12T13:19:03.889+0000"}, {"author": "Andrew Ash", "body": "On current master (1.2) I see that rdd.py now has a countApproxDistinct() method, but I don't see one for countApproxDistinctByKey() so this ticket is half completed.", "created": "2014-11-14T10:49:28.945+0000"}, {"author": "Josh Rosen", "body": "Does anyone still want this feature? I'd like to close out old JIRAs that we won't fix in order to clear the Python backlog. If nobody chimes in in favor of this feature, I'll close this.", "created": "2015-05-31T21:33:27.053+0000"}, {"author": "William Cox", "body": "I would like this feature.", "created": "2015-09-09T21:31:15.477+0000"}], "num_comments": 4, "text": "Issue: SPARK-1169\nSummary: Add countApproxDistinctByKey to PySpark\n\nComments (4):\n1. Dan McClary: Should this implement approximate jobs, or is a linear probabilistic approach using a standard job acceptable?\n2. Andrew Ash: On current master (1.2) I see that rdd.py now has a countApproxDistinct() method, but I don't see one for countApproxDistinctByKey() so this ticket is half completed.\n3. Josh Rosen: Does anyone still want this feature? I'd like to close out old JIRAs that we won't fix in order to clear the Python backlog. If nobody chimes in in favor of this feature, I'll close this.\n4. William Cox: I would like this feature.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "122350050777f09c3f368120ac2dbced", "issue_key": "SPARK-1170", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add histogram() to PySpark", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Josh Rosen", "created": "2014-03-02T20:19:09.000+0000", "updated": "2020-02-07T17:23:29.000+0000", "resolved": "2014-08-14T06:45:13.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Dan McClary", "body": "PR available here https://github.com/apache/spark/pull/122", "created": "2014-04-15T14:13:56.704+0000"}, {"author": "Josh Rosen", "body": "Hi [~dwmclary] and [~prashant], It looks like your pull requests implement slightly different versions of histogram, one that uses evenly-spaced buckets and another that uses a user-provided array of buckets. Since the Scala API supports both, it would be nice for Python to do the same. Does one of you want to merge both of these pull requests together into a single patch that supports both styles of histogram?", "created": "2014-07-27T01:43:50.862+0000"}, {"author": "Apache Spark", "body": "User 'nrchandan' has created a pull request for this issue: https://github.com/apache/spark/pull/1783", "created": "2014-08-05T08:22:13.290+0000"}, {"author": "Chandan Kumar", "body": "Davies is working on this.", "created": "2014-08-14T06:45:14.022+0000"}], "num_comments": 4, "text": "Issue: SPARK-1170\nSummary: Add histogram() to PySpark\n\nComments (4):\n1. Dan McClary: PR available here https://github.com/apache/spark/pull/122\n2. Josh Rosen: Hi [~dwmclary] and [~prashant], It looks like your pull requests implement slightly different versions of histogram, one that uses evenly-spaced buckets and another that uses a user-provided array of buckets. Since the Scala API supports both, it would be nice for Python to do the same. Does one of you want to merge both of these pull requests together into a single patch that supports both styles of histogram?\n3. Apache Spark: User 'nrchandan' has created a pull request for this issue: https://github.com/apache/spark/pull/1783\n4. Chandan Kumar: Davies is working on this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "1ee19c09c02d64980fb3523e469c7f80", "issue_key": "SPARK-1171", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "when executor is removed, we should reduce totalCores instead of just freeCores on that executor", "description": "When the executor is removed, the current implementation will only reduce the freeCores on that executor. Actually we should reduce the totalCores... The impact of this bug is that the default parallelism of a job may be set incorrectly after an executor is removed (so an RDD may get split into more partitions than the amount of parallelism in the cluster). In other words, the bug leads to performance degredation but no correctness problem.", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-03-02T20:32:49.000+0000", "updated": "2014-03-05T14:03:31.000+0000", "resolved": "2014-03-05T14:01:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Nan Zhu", "body": "proposed a PR: https://github.com/apache/spark/pull/63", "created": "2014-03-02T20:40:42.725+0000"}], "num_comments": 1, "text": "Issue: SPARK-1171\nSummary: when executor is removed, we should reduce totalCores instead of just freeCores on that executor\nDescription: When the executor is removed, the current implementation will only reduce the freeCores on that executor. Actually we should reduce the totalCores... The impact of this bug is that the default parallelism of a job may be set incorrectly after an executor is removed (so an RDD may get split into more partitions than the amount of parallelism in the cluster). In other words, the bug leads to performance degredation but no correctness problem.\n\nComments (1):\n1. Nan Zhu: proposed a PR: https://github.com/apache/spark/pull/63", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "b68f90b261b00605e1b7b32ff20c3103", "issue_key": "SPARK-1172", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Improve naming of the BlockManager classes", "description": "There is something called BlockManagerMaster that doesn't run on the driver (only on the Executors). There is a thing called BlockManagerWorker that is not really a \"worker\" component but a bridge between the connection manager and the block manager. I found these names confusing on my first read of this code and in the last week two other people have asked me to explain it to them. So perhaps we can find more intuitive names here... :)", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "created": "2014-03-02T22:10:09.000+0000", "updated": "2020-05-17T18:21:44.000+0000", "resolved": "2014-09-16T16:13:50.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Nan Zhu", "body": "Totally agree with this, the name is confusing..... but, why BlockMaster is not running on Driver, To my understanding, BlockMaster is a wrapper of BlockMasterActor...in executor side, BlockMasterActor is actually a reference to the driver-end real actor.... Did I misunderstand something?", "created": "2014-03-04T17:58:50.182+0000"}, {"author": "Andrew Or", "body": "Here's a quick clarification of the current naming scheme: BlockMaster runs on both executors the driver. On the driver, it's only used in the local mode. BlockManagerMaster runs on both executors and the driver. It handles the communication from each BlockManager to the BlockManagerMasterActor. On the driver, this is a local inter-thread communication. BlockManagerMasterActor exists only on the driver. It maintains an overview of which blocks are where and other useful information. It does not necessarily know about all blocks, however, because there is an option in the BlockManager to update a block without telling master (i.e. BlockManagerMasterActor). BlockManagerSlaveActor exists on both executors and the driver. It handles the communication from the BlockManagerMasterActor to each BlockManager. As before, this is a local inter-thread communication on the driver.", "created": "2014-03-30T18:37:11.023+0000"}, {"author": "Andrew Or", "body": "TD and I discussed more about this. I think this issue is part of a broader refactor of the BlockManager* interface. We should really maintain the abstraction that each actor should only be for sending/receiving messages, rather than maintaining state, which should be done in other classes (e.g. BlockManagerMaster). A bigger issue is that currently the only Akka interface between the driver and the executors is through the BlockManager* interface. This has not been a problem in the past, until we started to use this for other things (e.g. cleaning up shuffles in the MapOutputTracker in the automatic cleanup patch), which becomes quite ugly as BlockManager* classes begin to take in more and more things as parameters.", "created": "2014-03-30T18:42:55.933+0000"}, {"author": "Patrick Wendell", "body": "Okay let's close this until we do a braoder refactoring of the block manger.", "created": "2014-09-16T16:13:50.722+0000"}], "num_comments": 4, "text": "Issue: SPARK-1172\nSummary: Improve naming of the BlockManager classes\nDescription: There is something called BlockManagerMaster that doesn't run on the driver (only on the Executors). There is a thing called BlockManagerWorker that is not really a \"worker\" component but a bridge between the connection manager and the block manager. I found these names confusing on my first read of this code and in the last week two other people have asked me to explain it to them. So perhaps we can find more intuitive names here... :)\n\nComments (4):\n1. Nan Zhu: Totally agree with this, the name is confusing..... but, why BlockMaster is not running on Driver, To my understanding, BlockMaster is a wrapper of BlockMasterActor...in executor side, BlockMasterActor is actually a reference to the driver-end real actor.... Did I misunderstand something?\n2. Andrew Or: Here's a quick clarification of the current naming scheme: BlockMaster runs on both executors the driver. On the driver, it's only used in the local mode. BlockManagerMaster runs on both executors and the driver. It handles the communication from each BlockManager to the BlockManagerMasterActor. On the driver, this is a local inter-thread communication. BlockManagerMasterActor exists only on the driver. It maintains an overview of which blocks are where and other useful information. It does not necessarily know about all blocks, however, because there is an option in the BlockManager to update a block without telling master (i.e. BlockManagerMasterActor). BlockManagerSlaveActor exists on both executors and the driver. It handles the communication from the BlockManagerMasterActor to each BlockManager. As before, this is a local inter-thread communication on the driver.\n3. Andrew Or: TD and I discussed more about this. I think this issue is part of a broader refactor of the BlockManager* interface. We should really maintain the abstraction that each actor should only be for sending/receiving messages, rather than maintaining state, which should be done in other classes (e.g. BlockManagerMaster). A bigger issue is that currently the only Akka interface between the driver and the executors is through the BlockManager* interface. This has not been a problem in the past, until we started to use this for other things (e.g. cleaning up shuffles in the MapOutputTracker in the automatic cleanup patch), which becomes quite ugly as BlockManager* classes begin to take in more and more things as parameters.\n4. Patrick Wendell: Okay let's close this until we do a braoder refactoring of the block manger.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "b078e0eaeace6f1146b1036cb7506737", "issue_key": "SPARK-1173", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Improve scala streaming docs", "description": "Clarify imports to add implicit conversions to DStream and fix other small typos in the streaming intro documentation.", "reporter": "Aaron Kimball", "assignee": "Aaron Kimball", "created": "2014-03-02T23:12:32.000+0000", "updated": "2014-03-02T23:30:57.000+0000", "resolved": "2014-03-02T23:30:57.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Aaron Kimball", "body": "I added a small patch for this and sent a pull req. at https://github.com/apache/spark/pull/64", "created": "2014-03-02T23:18:41.399+0000"}], "num_comments": 1, "text": "Issue: SPARK-1173\nSummary: Improve scala streaming docs\nDescription: Clarify imports to add implicit conversions to DStream and fix other small typos in the streaming intro documentation.\n\nComments (1):\n1. Aaron Kimball: I added a small patch for this and sent a pull req. at https://github.com/apache/spark/pull/64", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "bb2a7f08cbfe0928cb652928acdca51b", "issue_key": "SPARK-1174", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Adding port configuration for HttpFileServer", "description": "I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time. I need ability to configure this port.", "reporter": "Egor Pakhomov", "assignee": "Egor Pahomov", "created": "2014-03-03T04:15:29.000+0000", "updated": "2014-09-02T01:18:36.000+0000", "resolved": "2014-09-02T01:18:36.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "I think it is a good feature to have, I will try to implement this", "created": "2014-03-03T09:06:11.318+0000"}, {"author": "Egor Pakhomov", "body": "Sorry, that I haven't attached pull request right away https://github.com/apache/incubator-spark/pull/657", "created": "2014-03-04T02:32:46.194+0000"}, {"author": "Nan Zhu", "body": "do you mind closing that and reopen in the new repo? https://github.com/apache/spark", "created": "2014-03-04T04:56:14.073+0000"}, {"author": "Egor Pakhomov", "body": "Sure, but can you me explain difference between these two repositories?", "created": "2014-03-04T10:00:19.687+0000"}, {"author": "Nan Zhu", "body": "incubator-spark is for the incubation stage of the project, now it is not used any more", "created": "2014-03-04T10:02:36.772+0000"}, {"author": "Egor Pakhomov", "body": "https://github.com/apache/spark/pull/81/", "created": "2014-03-05T00:46:37.566+0000"}], "num_comments": 6, "text": "Issue: SPARK-1174\nSummary: Adding port configuration for HttpFileServer\nDescription: I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time. I need ability to configure this port.\n\nComments (6):\n1. Nan Zhu: I think it is a good feature to have, I will try to implement this\n2. Egor Pakhomov: Sorry, that I haven't attached pull request right away https://github.com/apache/incubator-spark/pull/657\n3. Nan Zhu: do you mind closing that and reopen in the new repo? https://github.com/apache/spark\n4. Egor Pakhomov: Sure, but can you me explain difference between these two repositories?\n5. Nan Zhu: incubator-spark is for the incubation stage of the project, now it is not used any more\n6. Egor Pakhomov: https://github.com/apache/spark/pull/81/", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "c12d6f39f1648212fcacbd5ed9bdccc8", "issue_key": "SPARK-1175", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "on shutting down a long running job, the cluster does not accept new jobs and gets hung", "description": "When shutting down a long processing job (24+ hours) that runs periodically on the same context and generates a lot of shuffles (many hundreds of GB) the spark workers get hung for a long while and the cluster does not accept new jobs. The only way to proceed is to kill -9 the workers. This is a big problem because when multiple contexts run on the same cluster, one mast stop them all for a simple restart. The context is stopped using sc.stop() This happens both in standalone mode and under mesos. We suspect this is caused by the \"delete Spark local dirs\" thread. Attached a thread dump of the worker. Also, the relevant part may be: \"SIGTERM handler\" - Thread t@41040 java.lang.Thread.State: BLOCKED at java.lang.Shutdown.exit(Shutdown.java:168) - waiting to lock <69eab6a3> (a java.lang.Class) owned by \"SIGTERM handler\" t@41038 at java.lang.Terminator$1.handle(Terminator.java:35) at sun.misc.Signal$1.run(Signal.java:195) at java.lang.Thread.run(Thread.java:662) Locked ownable synchronizers: - None \"delete Spark local dirs\" - Thread t@40 java.lang.Thread.State: RUNNABLE at java.io.UnixFileSystem.delete0(Native Method) at java.io.UnixFileSystem.delete(UnixFileSystem.java:251) at java.io.File.delete(File.java:904) at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:482) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34) at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34) at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478) at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:141) at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:139) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) at org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139) Locked ownable synchronizers: - None \"SIGTERM handler\" - Thread t@41038 java.lang.Thread.State: WAITING at java.lang.Object.wait(Native Method) - waiting on <355c6c8d> (a org.apache.spark.storage.DiskBlockManager$$anon$1) at java.lang.Thread.join(Thread.java:1186) at java.lang.Thread.join(Thread.java:1239) at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:79) at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:24) at java.lang.Shutdown.runHooks(Shutdown.java:79) at java.lang.Shutdown.sequence(Shutdown.java:123) at java.lang.Shutdown.exit(Shutdown.java:168) - locked <69eab6a3> (a java.lang.Class) at java.lang.Terminator$1.handle(Terminator.java:35) at sun.misc.Signal$1.run(Signal.java:195) at java.lang.Thread.run(Thread.java:662) Locked ownable synchronizers: - None", "reporter": "Tal Sliwowicz", "assignee": "Nan Zhu", "created": "2014-03-03T22:27:35.000+0000", "updated": "2014-05-04T08:12:00.000+0000", "resolved": "2014-05-04T08:12:00.000+0000", "labels": ["shutdown", "worker"], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "it is related to https://spark-project.atlassian.net/browse/SPARK-1104?", "created": "2014-03-04T04:58:07.837+0000"}, {"author": "Tal Sliwowicz", "body": "Seems related, but not sure - symptoms are the same though.", "created": "2014-03-04T05:07:09.406+0000"}, {"author": "Nan Zhu", "body": "I spent some time to look at the code, here is my basic analysis, I think these two issues are closely related BlockManager and many other components are started for each Executor, while the Executor object is created by CoarseExecuterBackend. Many shutdown hooks are registered for all of them... CoarseExecutorBackend is actually started in ExecutorRunner, where a workerThread is created to start CoarseExecutorBackend process through command line (specified in appDesc) (the process is handled by a process object inside ExecutorRunner) However, once the process is killed, i.e. process.destroy() is called, a lot of cleanup work is triggered. The current implementation calls ExecutorRunner.kill() in Worker thread, causing the blocking of worker on shutdown of some application. (what confuses me is since the shutdownhooks are started in separate thread, why the worker is still blocked? release buffer in stdout and stderr takes time?)", "created": "2014-03-04T16:42:32.741+0000"}, {"author": "Nan Zhu", "body": "or, process.destroy() is a blocking method, it only returns after shutdown thread returns..... In my PR, I moved calling of that method to the workerThread, I think it should resolve the issue", "created": "2014-03-04T16:51:15.599+0000"}, {"author": "Tal Sliwowicz", "body": "Great! Can you point me to the PR? Is it - https://github.com/apache/spark/pull/35 ?", "created": "2014-03-04T22:36:12.215+0000"}, {"author": "Nan Zhu", "body": "yes, that's it", "created": "2014-03-05T05:24:31.193+0000"}, {"author": "Tal Sliwowicz", "body": "Do you think it will be included in 1.0?", "created": "2014-04-16T09:02:38.695+0000"}, {"author": "Nan Zhu", "body": "It has been there for a long time...I'm not sure...and obviously, I need to refresh it......", "created": "2014-04-16T10:17:58.633+0000"}, {"author": "Tal Sliwowicz", "body": "This prevents us from having a real automated solution for fail over when the driver fails. We cannot automatically start a new driver because spark is stuck on cleanup.", "created": "2014-04-16T10:20:05.151+0000"}, {"author": "Nan Zhu", "body": "BTW, did you see workers as DEAD in the UI?", "created": "2014-04-16T10:22:00.487+0000"}, {"author": "Tal Sliwowicz", "body": "Yes", "created": "2014-04-16T15:04:24.921+0000"}, {"author": "Nan Zhu", "body": "this should have been fixed in https://github.com/apache/spark/commit/f99af8529b6969986f0c3e03f6ff9b7bb9d53ece", "created": "2014-05-04T08:11:43.126+0000"}], "num_comments": 12, "text": "Issue: SPARK-1175\nSummary: on shutting down a long running job, the cluster does not accept new jobs and gets hung\nDescription: When shutting down a long processing job (24+ hours) that runs periodically on the same context and generates a lot of shuffles (many hundreds of GB) the spark workers get hung for a long while and the cluster does not accept new jobs. The only way to proceed is to kill -9 the workers. This is a big problem because when multiple contexts run on the same cluster, one mast stop them all for a simple restart. The context is stopped using sc.stop() This happens both in standalone mode and under mesos. We suspect this is caused by the \"delete Spark local dirs\" thread. Attached a thread dump of the worker. Also, the relevant part may be: \"SIGTERM handler\" - Thread t@41040 java.lang.Thread.State: BLOCKED at java.lang.Shutdown.exit(Shutdown.java:168) - waiting to lock <69eab6a3> (a java.lang.Class) owned by \"SIGTERM handler\" t@41038 at java.lang.Terminator$1.handle(Terminator.java:35) at sun.misc.Signal$1.run(Signal.java:195) at java.lang.Thread.run(Thread.java:662) Locked ownable synchronizers: - None \"delete Spark local dirs\" - Thread t@40 java.lang.Thread.State: RUNNABLE at java.io.UnixFileSystem.delete0(Native Method) at java.io.UnixFileSystem.delete(UnixFileSystem.java:251) at java.io.File.delete(File.java:904) at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:482) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34) at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:479) at org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(Utils.scala:478) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34) at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:478) at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:141) at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(DiskBlockManager.scala:139) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) at org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139) Locked ownable synchronizers: - None \"SIGTERM handler\" - Thread t@41038 java.lang.Thread.State: WAITING at java.lang.Object.wait(Native Method) - waiting on <355c6c8d> (a org.apache.spark.storage.DiskBlockManager$$anon$1) at java.lang.Thread.join(Thread.java:1186) at java.lang.Thread.join(Thread.java:1239) at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:79) at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:24) at java.lang.Shutdown.runHooks(Shutdown.java:79) at java.lang.Shutdown.sequence(Shutdown.java:123) at java.lang.Shutdown.exit(Shutdown.java:168) - locked <69eab6a3> (a java.lang.Class) at java.lang.Terminator$1.handle(Terminator.java:35) at sun.misc.Signal$1.run(Signal.java:195) at java.lang.Thread.run(Thread.java:662) Locked ownable synchronizers: - None\n\nComments (12):\n1. Nan Zhu: it is related to https://spark-project.atlassian.net/browse/SPARK-1104?\n2. Tal Sliwowicz: Seems related, but not sure - symptoms are the same though.\n3. Nan Zhu: I spent some time to look at the code, here is my basic analysis, I think these two issues are closely related BlockManager and many other components are started for each Executor, while the Executor object is created by CoarseExecuterBackend. Many shutdown hooks are registered for all of them... CoarseExecutorBackend is actually started in ExecutorRunner, where a workerThread is created to start CoarseExecutorBackend process through command line (specified in appDesc) (the process is handled by a process object inside ExecutorRunner) However, once the process is killed, i.e. process.destroy() is called, a lot of cleanup work is triggered. The current implementation calls ExecutorRunner.kill() in Worker thread, causing the blocking of worker on shutdown of some application. (what confuses me is since the shutdownhooks are started in separate thread, why the worker is still blocked? release buffer in stdout and stderr takes time?)\n4. Nan Zhu: or, process.destroy() is a blocking method, it only returns after shutdown thread returns..... In my PR, I moved calling of that method to the workerThread, I think it should resolve the issue\n5. Tal Sliwowicz: Great! Can you point me to the PR? Is it - https://github.com/apache/spark/pull/35 ?\n6. Nan Zhu: yes, that's it\n7. Tal Sliwowicz: Do you think it will be included in 1.0?\n8. Nan Zhu: It has been there for a long time...I'm not sure...and obviously, I need to refresh it......\n9. Tal Sliwowicz: This prevents us from having a real automated solution for fail over when the driver fails. We cannot automatically start a new driver because spark is stuck on cleanup.\n10. Nan Zhu: BTW, did you see workers as DEAD in the UI?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "400c3f35f5b7afa30fd5dac65732bc85", "issue_key": "SPARK-1176", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Adding port configuration for HttpBroadcast", "description": "I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time. I need ability to configure this port.", "reporter": "Egor Pakhomov", "assignee": null, "created": "2014-03-04T02:34:12.000+0000", "updated": "2014-09-21T14:13:43.000+0000", "resolved": "2014-09-21T14:13:43.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Egor Pakhomov", "body": "https://github.com/apache/incubator-spark/pull/658/", "created": "2014-03-04T02:49:55.955+0000"}, {"author": "Matthew Farrellee", "body": "[~epakhomov] it looks like this was resolved by SPARK-2157. i'm going to close this, but please feel free to re-open if it is still an issue for you.", "created": "2014-09-21T14:12:53.689+0000"}], "num_comments": 2, "text": "Issue: SPARK-1176\nSummary: Adding port configuration for HttpBroadcast\nDescription: I run spark in big organization, where to open port accessible to other computers in network, I need to create a ticket on DevOps and it executes for days. I can't have port for some spark service to be changed all the time. I need ability to configure this port.\n\nComments (2):\n1. Egor Pakhomov: https://github.com/apache/incubator-spark/pull/658/\n2. Matthew Farrellee: [~epakhomov] it looks like this was resolved by SPARK-2157. i'm going to close this, but please feel free to re-open if it is still an issue for you.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "c3f1d7cb39ba1b3c7024afd01b945bcc", "issue_key": "SPARK-1177", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Allow SPARK_JAR to be set in system properties", "description": "I'd like to be able to do from my scala code: System.setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.jarOfClass(this.getClass).head) System.setProperty(\"SPARK_JAR\", SparkContext.jarOfClass(SparkContext.getClass).head) And do nothing on OS level.", "reporter": "Egor Pakhomov", "assignee": null, "created": "2014-03-04T02:55:52.000+0000", "updated": "2014-09-21T14:03:42.000+0000", "resolved": "2014-09-21T14:03:42.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Egor Pakhomov", "body": "I'll create pull request in about a minute", "created": "2014-03-04T02:56:13.120+0000"}, {"author": "Egor Pakhomov", "body": "correct pull request https://github.com/apache/spark/pull/82", "created": "2014-03-05T01:19:58.147+0000"}, {"author": "Matthew Farrellee", "body": "[~epakhomov] it looks like this has been resolved in other change, for instance being able to use spark.yarn.jar. i'm going to close this, but feel free to re-open if you think it is still important.", "created": "2014-09-21T14:03:22.360+0000"}], "num_comments": 3, "text": "Issue: SPARK-1177\nSummary: Allow SPARK_JAR to be set in system properties\nDescription: I'd like to be able to do from my scala code: System.setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.jarOfClass(this.getClass).head) System.setProperty(\"SPARK_JAR\", SparkContext.jarOfClass(SparkContext.getClass).head) And do nothing on OS level.\n\nComments (3):\n1. Egor Pakhomov: I'll create pull request in about a minute\n2. Egor Pakhomov: correct pull request https://github.com/apache/spark/pull/82\n3. Matthew Farrellee: [~epakhomov] it looks like this has been resolved in other change, for instance being able to use spark.yarn.jar. i'm going to close this, but feel free to re-open if you think it is still important.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "a0d9c4a4e97a941d27ca10ec33bf1558", "issue_key": "SPARK-1178", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "missing document about spark.scheduler.revive.interval", "description": "The configuration on spark.scheduler.revive.interval is undocumented but actually used https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L64", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-03-04T05:08:11.000+0000", "updated": "2014-03-04T11:20:08.000+0000", "resolved": "2014-03-04T11:20:08.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1178\nSummary: missing document about spark.scheduler.revive.interval\nDescription: The configuration on spark.scheduler.revive.interval is undocumented but actually used https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L64", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "f6e9591e2a322f3bf6942d343adb9183", "issue_key": "SPARK-1179", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Allow SPARK_YARN_APP_JAR to be set in system properties", "description": "I'd like to be able to do from my scala code: System.setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.jarOfClass(this.getClass).head) System.setProperty(\"SPARK_JAR\", SparkContext.jarOfClass(SparkContext.getClass).head) And do nothing on OS level.", "reporter": "Egor Pakhomov", "assignee": null, "created": "2014-03-04T05:14:57.000+0000", "updated": "2014-03-05T01:22:14.000+0000", "resolved": "2014-03-05T01:22:14.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Egor Pakhomov", "body": "https://github.com/apache/incubator-spark/pull/660", "created": "2014-03-04T05:21:03.445+0000"}, {"author": "Nan Zhu", "body": "also, do you mind closing your PR and open in the new repo, https://github.com/apache/spark ?", "created": "2014-03-04T05:25:22.779+0000"}, {"author": "Thomas Graves", "body": "SPARK_YARN_APP_JAR has been removed with https://github.com/apache/incubator-spark/pull/553", "created": "2014-03-04T06:37:56.318+0000"}], "num_comments": 3, "text": "Issue: SPARK-1179\nSummary: Allow SPARK_YARN_APP_JAR to be set in system properties\nDescription: I'd like to be able to do from my scala code: System.setProperty(\"SPARK_YARN_APP_JAR\", SparkContext.jarOfClass(this.getClass).head) System.setProperty(\"SPARK_JAR\", SparkContext.jarOfClass(SparkContext.getClass).head) And do nothing on OS level.\n\nComments (3):\n1. Egor Pakhomov: https://github.com/apache/incubator-spark/pull/660\n2. Nan Zhu: also, do you mind closing your PR and open in the new repo, https://github.com/apache/spark ?\n3. Thomas Graves: SPARK_YARN_APP_JAR has been removed with https://github.com/apache/incubator-spark/pull/553", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "d62022d8a0195fa3bdf88b1f80756398", "issue_key": "SPARK-1180", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Allow to provide a custom persistence engine", "description": "Currently Spark supports only predefined ZOOKEEPER and FILESYSTEM persistence engines. It would be nice to give a possibility to provide custom persistence engine by specifying a class name in {{spark.deploy.recoveryMode}}.", "reporter": "Jacek Lewandowski", "assignee": "Prashant Sharma", "created": "2014-03-04T06:15:06.000+0000", "updated": "2015-02-01T21:39:22.000+0000", "resolved": "2015-02-01T21:39:22.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1180\nSummary: Allow to provide a custom persistence engine\nDescription: Currently Spark supports only predefined ZOOKEEPER and FILESYSTEM persistence engines. It would be nice to give a possibility to provide custom persistence engine by specifying a class name in {{spark.deploy.recoveryMode}}.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "7de791d2e0a22f225f5aa1fe21a90cdb", "issue_key": "SPARK-1181", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "'mvn test' fails out of the box since sbt assembly does not necessarily exist", "description": "The test suite requires that \"sbt assembly\" has been run in order for some tests (like DriverSuite) to pass. The tests themselves say as much. This means that a \"mvn test\" from a fresh clone fails. There's a pretty simple fix, to have Maven's test-compile phase invoke \"sbt assembly\". I suppose the only downside is re-invoking \"sbt assembly\" each time tests are run. I'm open to ideas about how to set this up more intelligently but it would be a generally good thing if the Maven build's tests passed out of the box.", "reporter": "Sean R. Owen", "assignee": null, "created": "2014-03-04T10:36:27.000+0000", "updated": "2015-01-15T09:07:31.000+0000", "resolved": "2014-03-04T14:27:13.000+0000", "labels": ["assembly", "maven", "sbt", "test"], "components": ["Build"], "comments": [{"author": "Sean Owen", "body": "Per discussion on the pull request, this can be made to work by manually invoking 'package' first. See https://github.com/apache/spark/pull/77", "created": "2014-03-04T14:27:13.336+0000"}], "num_comments": 1, "text": "Issue: SPARK-1181\nSummary: 'mvn test' fails out of the box since sbt assembly does not necessarily exist\nDescription: The test suite requires that \"sbt assembly\" has been run in order for some tests (like DriverSuite) to pass. The tests themselves say as much. This means that a \"mvn test\" from a fresh clone fails. There's a pretty simple fix, to have Maven's test-compile phase invoke \"sbt assembly\". I suppose the only downside is re-invoking \"sbt assembly\" each time tests are run. I'm open to ideas about how to set this up more intelligently but it would be a generally good thing if the Maven build's tests passed out of the box.\n\nComments (1):\n1. Sean Owen: Per discussion on the pull request, this can be made to work by manually invoking 'package' first. See https://github.com/apache/spark/pull/77", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "92dedc56993b38954265665d65759c5d", "issue_key": "SPARK-1182", "issue_type": "Task", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Sort the configuration parameters in configuration.md", "description": "It is a little bit confusing right now since the config options are all over the place in some arbitrarily sorted order. https://github.com/apache/spark/blob/master/docs/configuration.md", "reporter": "Reynold Xin", "assignee": "Brennon York", "created": "2014-03-04T10:55:27.000+0000", "updated": "2020-02-07T17:20:54.000+0000", "resolved": "2015-02-26T00:26:14.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Nan Zhu", "body": "I'm working on the other issue on the parameters, so just sorting the properties conveniently. https://github.com/apache/spark/pull/85/files", "created": "2014-03-05T20:57:21.622+0000"}, {"author": "Prashant Sharma", "body": "This can be assigned to Nan Zhu, I hope.", "created": "2014-03-12T00:00:41.274+0000"}, {"author": "Sean R. Owen", "body": "The current PR seems to be https://github.com/apache/spark/pull/2312 but its purpose is not just sorting. Is this JIRA resolved then? The current ordering is not alphabetical but does logically group \"spark.foo.*\" props together, and puts them in order of importance, sort of. (With maybe one exception or two.)", "created": "2014-11-25T10:17:01.311+0000"}, {"author": "Reynold Xin", "body": "I think the ticket is to make it alphabetical so it preserves both logical grouping as well as making it easier to find.", "created": "2014-11-25T23:30:41.963+0000"}, {"author": "Apache Spark", "body": "User 'brennonyork' has created a pull request for this issue: https://github.com/apache/spark/pull/3863", "created": "2014-12-31T22:11:34.345+0000"}, {"author": "Brennon York", "body": "All, given the PR that [~srowen] pointed out doesn't resolve sorting of all the options I went ahead and manually sorted it this once with the hope that, as it gets updated, we can preserve the logical and grouped sorting moving forward.", "created": "2014-12-31T22:15:22.302+0000"}, {"author": "Brennon York", "body": "Given [~joshrosen]'s comments on the PR making merge-conflict hell, would it be better just to scratch this as an issue and close everything out? Its either that or deal with all the merge conflicts for any / all backports moving forward. Thoughts?", "created": "2015-02-23T22:44:40.629+0000"}, {"author": "Reynold Xin", "body": "cc [~pwendell] what do you think? I think it is probably still worth it to do that, since the config page is sort of a mess. The backporting documentation fix thing should be small. I don't think we backport documentation fixes that often.", "created": "2015-02-24T03:24:21.498+0000"}, {"author": "Reynold Xin", "body": "Actually when I filed this ticket, I don't think there was extensive high level grouping of config options (there was maybe two or three). I took a look at the latest grouping and they looked reasonable. A few problems with the existing grouping: 1. Networking section actually contains some shuffle settings. Those should be moved into shuffle. 2. Application Properties contains some serialization settings. Those should be moved into Compression and Serialization. 3. Runtime Environment contains some Mesos specific setting. All the way down on the page we link to each resource manager's own page for resource manager specific settings. 4. Scheduling section contains a mesos specific setting (spark.mesos.coarse). 5. It is sort of arbitrary that \"Dynamic allocation\" has its own section (at the very least, \"a\" should be upper case.) 6. \"spark.ui.view.acls\" is in security, but it also belongs in UI. Where do we put stuff like this? Within each section, config options should probably be sorted alphabetically.", "created": "2015-02-24T04:01:29.815+0000"}, {"author": "Brennon York", "body": "[~rxin] I've incorporated all the changes you mentioned (save for #6) and updated all merge conflicts thus far (not fun haha). If we want to move forward with this I'd ask that we move with a bit of brevity on this one before I need to fix a ton of merge conflicts again :/ Thanks!", "created": "2015-02-25T22:03:29.768+0000"}, {"author": "Reynold Xin", "body": "Merged. Thanks for doing this, [~boyork]. Couple minor things that would be great to be addressed as a followup PR: 1. Link to the Mesos configuration table rather than just the Mesos page. 2. If possible, make Mesos/YARN/Standalone appear in the table of contents on top.", "created": "2015-02-26T00:26:04.908+0000"}], "num_comments": 11, "text": "Issue: SPARK-1182\nSummary: Sort the configuration parameters in configuration.md\nDescription: It is a little bit confusing right now since the config options are all over the place in some arbitrarily sorted order. https://github.com/apache/spark/blob/master/docs/configuration.md\n\nComments (11):\n1. Nan Zhu: I'm working on the other issue on the parameters, so just sorting the properties conveniently. https://github.com/apache/spark/pull/85/files\n2. Prashant Sharma: This can be assigned to Nan Zhu, I hope.\n3. Sean R. Owen: The current PR seems to be https://github.com/apache/spark/pull/2312 but its purpose is not just sorting. Is this JIRA resolved then? The current ordering is not alphabetical but does logically group \"spark.foo.*\" props together, and puts them in order of importance, sort of. (With maybe one exception or two.)\n4. Reynold Xin: I think the ticket is to make it alphabetical so it preserves both logical grouping as well as making it easier to find.\n5. Apache Spark: User 'brennonyork' has created a pull request for this issue: https://github.com/apache/spark/pull/3863\n6. Brennon York: All, given the PR that [~srowen] pointed out doesn't resolve sorting of all the options I went ahead and manually sorted it this once with the hope that, as it gets updated, we can preserve the logical and grouped sorting moving forward.\n7. Brennon York: Given [~joshrosen]'s comments on the PR making merge-conflict hell, would it be better just to scratch this as an issue and close everything out? Its either that or deal with all the merge conflicts for any / all backports moving forward. Thoughts?\n8. Reynold Xin: cc [~pwendell] what do you think? I think it is probably still worth it to do that, since the config page is sort of a mess. The backporting documentation fix thing should be small. I don't think we backport documentation fixes that often.\n9. Reynold Xin: Actually when I filed this ticket, I don't think there was extensive high level grouping of config options (there was maybe two or three). I took a look at the latest grouping and they looked reasonable. A few problems with the existing grouping: 1. Networking section actually contains some shuffle settings. Those should be moved into shuffle. 2. Application Properties contains some serialization settings. Those should be moved into Compression and Serialization. 3. Runtime Environment contains some Mesos specific setting. All the way down on the page we link to each resource manager's own page for resource manager specific settings. 4. Scheduling section contains a mesos specific setting (spark.mesos.coarse). 5. It is sort of arbitrary that \"Dynamic allocation\" has its own section (at the very least, \"a\" should be upper case.) 6. \"spark.ui.view.acls\" is in security, but it also belongs in UI. Where do we put stuff like this? Within each section, config options should probably be sorted alphabetically.\n10. Brennon York: [~rxin] I've incorporated all the changes you mentioned (save for #6) and updated all merge conflicts thus far (not fun haha). If we want to move forward with this I'd ask that we move with a bit of brevity on this one before I need to fix a ton of merge conflicts again :/ Thanks!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "fd9ea4d92c01994384a38f3ce2e1722b", "issue_key": "SPARK-1183", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Inconsistent meaning of \"worker\" in docs", "description": "In the Spark Standalone Mode docs, \"worker\" refers to the long-running slave processes that start executor processes. In most other docs, \"worker\" refers to the executor processes themselves.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-03-04T12:13:58.000+0000", "updated": "2014-04-04T20:50:07.000+0000", "resolved": "2014-03-13T12:42:39.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Sanford Ryza", "body": "The best thing to me would be to remove the word worker wherever possible. I.e. refer to \"executor\" or \"slave\". If master/slave isn't PC, we could use lord/vassal instead?", "created": "2014-03-04T12:19:28.304+0000"}], "num_comments": 1, "text": "Issue: SPARK-1183\nSummary: Inconsistent meaning of \"worker\" in docs\nDescription: In the Spark Standalone Mode docs, \"worker\" refers to the long-running slave processes that start executor processes. In most other docs, \"worker\" refers to the executor processes themselves.\n\nComments (1):\n1. Sanford Ryza: The best thing to me would be to remove the word worker wherever possible. I.e. refer to \"executor\" or \"slave\". If master/slave isn't PC, we could use lord/vassal instead?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "af64a35f1ed1b6427f53c6416ef359e9", "issue_key": "SPARK-1184", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Update the distribution tar.gz to include spark-assembly jar", "description": "This JIRA tracks 2 things: 1. There seems to be something going on in our assembly generation logic because of which are two assembly jars. Something like: spark-assembly_2.10-1.0.0-SNAPSHOT.jar and spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.0.5-alpha.jar The former is pretty bogus and doesn't contain any class files and should be gotten rid of. The latter contains all the good stuff. It essentially is the uber jar generated by the maven-shade-plugin 2. The current bigtop-dist profile that builds the maven assembly (a .tar.gz file) using the maven-assembly-plugin includes the bogus jar and not the legit spark-assembly jar. We should get rid of the first one from this assembly (which would happen when we fix #1) and put the legit uber jar in it. 3. Also, the bigtop-dist profile is meant to exclude the hadoop related jars from the distribution. It does a good job of doing so for org.apache.hadoop jars but misses the avro and zookeeper jars that are also provided by hadoop land.", "reporter": "Mark Grover", "assignee": "Mark Grover", "created": "2014-03-04T14:30:28.000+0000", "updated": "2014-04-18T21:07:05.000+0000", "resolved": "2014-04-18T21:07:05.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Mark Grover", "body": "Committed quite a while ago: https://github.com/apache/spark/commit/cda381f88cc03340fdf7b2d681699babbae2a56e Resolving", "created": "2014-04-18T21:07:01.292+0000"}], "num_comments": 1, "text": "Issue: SPARK-1184\nSummary: Update the distribution tar.gz to include spark-assembly jar\nDescription: This JIRA tracks 2 things: 1. There seems to be something going on in our assembly generation logic because of which are two assembly jars. Something like: spark-assembly_2.10-1.0.0-SNAPSHOT.jar and spark-assembly_2.10-1.0.0-SNAPSHOT-hadoop2.0.5-alpha.jar The former is pretty bogus and doesn't contain any class files and should be gotten rid of. The latter contains all the good stuff. It essentially is the uber jar generated by the maven-shade-plugin 2. The current bigtop-dist profile that builds the maven assembly (a .tar.gz file) using the maven-assembly-plugin includes the bogus jar and not the legit spark-assembly jar. We should get rid of the first one from this assembly (which would happen when we fix #1) and put the legit uber jar in it. 3. Also, the bigtop-dist profile is meant to exclude the hadoop related jars from the distribution. It does a good job of doing so for org.apache.hadoop jars but misses the avro and zookeeper jars that are also provided by hadoop land.\n\nComments (1):\n1. Mark Grover: Committed quite a while ago: https://github.com/apache/spark/commit/cda381f88cc03340fdf7b2d681699babbae2a56e Resolving", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "78ca4d33118960357a1133cc363a80a9", "issue_key": "SPARK-1185", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "In Spark Programming Guide, \"Master URLs\" should mention yarn-client", "description": "It would also be helpful to mention that the reason a host:post isn't required for YARN mode is that it comes from the Hadoop configuration.", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "created": "2014-03-04T14:42:07.000+0000", "updated": "2014-11-06T17:42:54.000+0000", "resolved": "2014-11-06T17:42:54.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1185\nSummary: In Spark Programming Guide, \"Master URLs\" should mention yarn-client\nDescription: It would also be helpful to mention that the reason a host:post isn't required for YARN mode is that it comes from the Hadoop configuration.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "9327a7a02d0dd7a187ad281fc245037a", "issue_key": "SPARK-1186", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Enrich the Spark Shell to support additional arguments.", "description": "Enrich the Spark Shell functionality to support the following options.  Usage: spark-shell [OPTIONS] OPTIONS: -h --help : Print this help information. -c --cores : The maximum number of cores to be used by the Spark Shell. -em --executor-memory : The memory used by each executor of the Spark Shell, the number is followed by m for megabytes or g for gigabytes, e.g. \"1g\". -dm --driver-memory : The memory used by the Spark Shell, the number is followed by m for megabytes or g for gigabytes, e.g. \"1g\". -m --master : A full string that describes the Spark Master, defaults to \"local\" e.g. \"spark://localhost:7077\". --log-conf : Enables logging of the supplied SparkConf as INFO at start of the Spark Context. e.g. spark-shell -m spark://localhost:7077 -c 4 -dm 512m -em 2g  **Note**: the options described above are not visually aligned due JIRA's rendering, in the bash CLI they are.", "reporter": "Bernardo Gomez Palacio", "assignee": null, "created": "2014-03-04T15:59:00.000+0000", "updated": "2014-04-19T07:15:20.000+0000", "resolved": "2014-04-19T07:15:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Bernardo Gomez Palacio", "body": "Pull request provided https://github.com/apache/incubator-spark/pull/661", "created": "2014-03-04T16:11:39.530+0000"}, {"author": "Bernardo Gomez Palacio", "body": "Updated the options to be in-line with [SPARK-1126]", "created": "2014-03-05T10:59:51.530+0000"}, {"author": "Bernardo Gomez Palacio", "body": "Migrated the pull-request to the Spark Github Repository https://github.com/apache/spark/pull/84", "created": "2014-03-05T15:44:06.193+0000"}, {"author": "Bernardo Gomez Palacio", "body": "After a rebase based on apache/master and a squash commit I was unable to kill the previous pull-request (84) and had to open a new one (116). https://github.com/apache/spark/pull/116", "created": "2014-03-10T10:56:04.101+0000"}, {"author": "ASF GitHub Bot", "body": "Github user asfgit closed the pull request at: https://github.com/apache/spark/pull/116", "created": "2014-03-30T03:26:37.528+0000"}], "num_comments": 5, "text": "Issue: SPARK-1186\nSummary: Enrich the Spark Shell to support additional arguments.\nDescription: Enrich the Spark Shell functionality to support the following options.  Usage: spark-shell [OPTIONS] OPTIONS: -h --help : Print this help information. -c --cores : The maximum number of cores to be used by the Spark Shell. -em --executor-memory : The memory used by each executor of the Spark Shell, the number is followed by m for megabytes or g for gigabytes, e.g. \"1g\". -dm --driver-memory : The memory used by the Spark Shell, the number is followed by m for megabytes or g for gigabytes, e.g. \"1g\". -m --master : A full string that describes the Spark Master, defaults to \"local\" e.g. \"spark://localhost:7077\". --log-conf : Enables logging of the supplied SparkConf as INFO at start of the Spark Context. e.g. spark-shell -m spark://localhost:7077 -c 4 -dm 512m -em 2g  **Note**: the options described above are not visually aligned due JIRA's rendering, in the bash CLI they are.\n\nComments (5):\n1. Bernardo Gomez Palacio: Pull request provided https://github.com/apache/incubator-spark/pull/661\n2. Bernardo Gomez Palacio: Updated the options to be in-line with [SPARK-1126]\n3. Bernardo Gomez Palacio: Migrated the pull-request to the Spark Github Repository https://github.com/apache/spark/pull/84\n4. Bernardo Gomez Palacio: After a rebase based on apache/master and a squash commit I was unable to kill the previous pull-request (84) and had to open a new one (116). https://github.com/apache/spark/pull/116\n5. ASF GitHub Bot: Github user asfgit closed the pull request at: https://github.com/apache/spark/pull/116", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "14c9ee38a0da250974aa5f93d5fee31c", "issue_key": "SPARK-1187", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Missing Pyspark methods", "description": "Following methods in SparkContext are missing, 1) setJobGroup() 2) setLocalProperty() 3) getLocalProperty() 4) sparkUser()", "reporter": "Prabin Banka", "assignee": "Prabin Banka", "created": "2014-03-04T21:47:42.000+0000", "updated": "2014-03-06T12:46:33.000+0000", "resolved": "2014-03-06T12:46:33.000+0000", "labels": [], "components": ["PySpark"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1187\nSummary: Missing Pyspark methods\nDescription: Following methods in SparkContext are missing, 1) setJobGroup() 2) setLocalProperty() 3) getLocalProperty() 4) sparkUser()", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "c25ebc45906400af808547b14fd210f4", "issue_key": "SPARK-1188", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "GraphX triplets not working properly", "description": "I followed the GraphX tutorial at http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html on a local stand-alone cluster (Spark version 0.9.0) with two workers. Somehow, the graph.triplets is not returning what it should -- only Eds and Frans. ``` scala> graph.edges.toArray 14/03/04 16:15:57 INFO SparkContext: Starting job: collect at EdgeRDD.scala:51 14/03/04 16:15:57 INFO DAGScheduler: Got job 5 (collect at EdgeRDD.scala:51) with 1 output partitions (allowLocal=false) 14/03/04 16:15:57 INFO DAGScheduler: Final stage: Stage 27 (collect at EdgeRDD.scala:51) 14/03/04 16:15:57 INFO DAGScheduler: Parents of final stage: List() 14/03/04 16:15:57 INFO DAGScheduler: Missing parents: List() 14/03/04 16:15:57 INFO DAGScheduler: Submitting Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51), which has no missing parents 14/03/04 16:15:57 INFO DAGScheduler: Submitting 1 missing tasks from Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51) 14/03/04 16:15:57 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks 14/03/04 16:15:57 INFO TaskSetManager: Starting task 27.0:0 as TID 11 on executor localhost: localhost (PROCESS_LOCAL) 14/03/04 16:15:57 INFO TaskSetManager: Serialized task 27.0:0 as 2068 bytes in 1 ms 14/03/04 16:15:57 INFO Executor: Running task ID 11 14/03/04 16:15:57 INFO BlockManager: Found block rdd_2_0 locally 14/03/04 16:15:57 INFO Executor: Serialized size of result for 11 is 936 14/03/04 16:15:57 INFO Executor: Sending result for 11 directly to driver 14/03/04 16:15:57 INFO Executor: Finished task ID 11 14/03/04 16:15:57 INFO TaskSetManager: Finished TID 11 in 13 ms on localhost (progress: 0/1) 14/03/04 16:15:57 INFO DAGScheduler: Completed ResultTask(27, 0) 14/03/04 16:15:57 INFO TaskSchedulerImpl: Remove TaskSet 27.0 from pool 14/03/04 16:15:57 INFO DAGScheduler: Stage 27 (collect at EdgeRDD.scala:51) finished in 0.015 s 14/03/04 16:15:57 INFO SparkContext: Job finished: collect at EdgeRDD.scala:51, took 0.023602266 s res7: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3)) scala> graph.vertices.toArray 14/03/04 16:16:18 INFO SparkContext: Starting job: toArray at <console>:27 14/03/04 16:16:18 INFO DAGScheduler: Got job 6 (toArray at <console>:27) with 1 output partitions (allowLocal=false) 14/03/04 16:16:18 INFO DAGScheduler: Final stage: Stage 28 (toArray at <console>:27) 14/03/04 16:16:18 INFO DAGScheduler: Parents of final stage: List(Stage 32, Stage 29) 14/03/04 16:16:18 INFO DAGScheduler: Missing parents: List() 14/03/04 16:16:18 INFO DAGScheduler: Submitting Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52), which has no missing parents 14/03/04 16:16:18 INFO DAGScheduler: Submitting 1 missing tasks from Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52) 14/03/04 16:16:18 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks 14/03/04 16:16:18 INFO TaskSetManager: Starting task 28.0:0 as TID 12 on executor localhost: localhost (PROCESS_LOCAL) 14/03/04 16:16:18 INFO TaskSetManager: Serialized task 28.0:0 as 2426 bytes in 0 ms 14/03/04 16:16:18 INFO Executor: Running task ID 12 14/03/04 16:16:18 INFO BlockManager: Found block rdd_14_0 locally 14/03/04 16:16:18 INFO Executor: Serialized size of result for 12 is 947 14/03/04 16:16:18 INFO Executor: Sending result for 12 directly to driver 14/03/04 16:16:18 INFO Executor: Finished task ID 12 14/03/04 16:16:18 INFO TaskSetManager: Finished TID 12 in 13 ms on localhost (progress: 0/1) 14/03/04 16:16:18 INFO DAGScheduler: Completed ResultTask(28, 0) 14/03/04 16:16:18 INFO TaskSchedulerImpl: Remove TaskSet 28.0 from pool 14/03/04 16:16:18 INFO DAGScheduler: Stage 28 (toArray at <console>:27) finished in 0.015 s 14/03/04 16:16:18 INFO SparkContext: Job finished: toArray at <console>:27, took 0.027839851 s res9: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((4,(David,42)), (2,(Bob,27)), (6,(Fran,50)), (5,(Ed,55)), (3,(Charlie,65)), (1,(Alice,28))) scala> graph.triplets.toArray 14/03/04 16:16:30 INFO SparkContext: Starting job: toArray at <console>:27 14/03/04 16:16:30 INFO DAGScheduler: Got job 7 (toArray at <console>:27) with 1 output partitions (allowLocal=false) 14/03/04 16:16:31 INFO DAGScheduler: Final stage: Stage 33 (toArray at <console>:27) 14/03/04 16:16:31 INFO DAGScheduler: Parents of final stage: List(Stage 34) 14/03/04 16:16:31 INFO DAGScheduler: Missing parents: List() 14/03/04 16:16:31 INFO DAGScheduler: Submitting Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60), which has no missing parents 14/03/04 16:16:31 INFO DAGScheduler: Submitting 1 missing tasks from Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60) 14/03/04 16:16:31 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks 14/03/04 16:16:31 INFO TaskSetManager: Starting task 33.0:0 as TID 13 on executor localhost: localhost (PROCESS_LOCAL) 14/03/04 16:16:31 INFO TaskSetManager: Serialized task 33.0:0 as 3322 bytes in 1 ms 14/03/04 16:16:31 INFO Executor: Running task ID 13 14/03/04 16:16:31 INFO BlockManager: Found block rdd_2_0 locally 14/03/04 16:16:31 INFO BlockManager: Found block rdd_31_0 locally 14/03/04 16:16:31 INFO Executor: Serialized size of result for 13 is 931 14/03/04 16:16:31 INFO Executor: Sending result for 13 directly to driver 14/03/04 16:16:31 INFO Executor: Finished task ID 13 14/03/04 16:16:31 INFO TaskSetManager: Finished TID 13 in 17 ms on localhost (progress: 0/1) 14/03/04 16:16:31 INFO DAGScheduler: Completed ResultTask(33, 0) 14/03/04 16:16:31 INFO TaskSchedulerImpl: Remove TaskSet 33.0 from pool 14/03/04 16:16:31 INFO DAGScheduler: Stage 33 (toArray at <console>:27) finished in 0.019 s 14/03/04 16:16:31 INFO SparkContext: Job finished: toArray at <console>:27, took 0.037909394 s res10: Array[org.apache.spark.graphx.EdgeTriplet[(String, Int),Int]] = Array(((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3)) ```", "reporter": "Kev Alan", "assignee": "Daniel Darabos", "created": "2014-03-05T00:13:56.000+0000", "updated": "2014-05-29T08:20:34.000+0000", "resolved": "2014-04-23T18:41:51.000+0000", "labels": [], "components": ["GraphX"], "comments": [{"author": "Reynold Xin", "body": "The problem is that we are reusing an EdgeTriplet object. Try force a copy before you do the collect. e.g. triplets.map(_.copy).collect()", "created": "2014-03-05T00:25:59.537+0000"}, {"author": "Daniel Darabos", "body": "This has bitten us as well. I am no expert, but my impression is that re-using an object in these iterators was a terrible idea. Consider this example: scala> val g = graphx.util.GraphGenerators.starGraph(sc, 5) scala> g.edges.collect Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1)) scala> g.edges.map(x => x).collect Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1)) It can and does go very wrong in practice too. Consider this: scala> g.edges.saveAsObjectFile(\"edges\") scala> sc.objectFile[graphx.Edge[Int]](\"edges\").collect Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1)) Indeed map(_.copy) is a good workaround. But seems like this is a nasty trap laid out for your users. Did you measure the performance gain? If not, or if it's insignificant, I would suggest not re-using the object in the iterators. It would simplify the GraphX source too. However, if it is significant, one idea is to offer separate mapFast() methods that do re-use, while the more commonly used map() would offer the more commonly expected (copying) semantics. Also I wish GraphX offered graph save/load functionality. If it did, I would only have discovered this bug a few weeks from now :).", "created": "2014-03-13T17:42:24.478+0000"}, {"author": "Daniel Darabos", "body": "Sorry, I forgot to test your workaround before commenting. It does not work. scala> g.triplets.map(_.copy).collect <console>:16: error: missing arguments for method copy in class Edge; follow this method with `_' if you want to treat it as a partially applied function g.triplets.map(_.copy).collect ^ You can of course write a copy function yourself. I'd be happy to work on this. Can I just get rid of the re-use, or would you prefer another approach?", "created": "2014-03-19T06:32:34.677+0000"}, {"author": "Daniel Darabos", "body": "Okay, it was just missing the parentheses: scala> g.triplets.map(_.copy()).collect res1: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1)) (Scala novice here :).)", "created": "2014-03-28T08:19:47.545+0000"}, {"author": "Daniel Darabos", "body": "I've sent a pull request to eliminate the re-use (https://github.com/apache/spark/pull/276). The change has no performance impact and simplifies the code.", "created": "2014-03-31T11:13:19.772+0000"}, {"author": "Daniel Darabos", "body": "The changes are in the master branch now. I can't figure out how to close a JIRA ticket :).", "created": "2014-04-23T09:29:58.471+0000"}, {"author": "Reynold Xin", "body": "I added you to contributor list so you should be able to edit in the future. Cheers.", "created": "2014-04-23T18:41:44.889+0000"}, {"author": "Reynold Xin", "body": "Adding a link to the commit: https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e", "created": "2014-05-19T20:28:45.033+0000"}], "num_comments": 8, "text": "Issue: SPARK-1188\nSummary: GraphX triplets not working properly\nDescription: I followed the GraphX tutorial at http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html on a local stand-alone cluster (Spark version 0.9.0) with two workers. Somehow, the graph.triplets is not returning what it should -- only Eds and Frans. ``` scala> graph.edges.toArray 14/03/04 16:15:57 INFO SparkContext: Starting job: collect at EdgeRDD.scala:51 14/03/04 16:15:57 INFO DAGScheduler: Got job 5 (collect at EdgeRDD.scala:51) with 1 output partitions (allowLocal=false) 14/03/04 16:15:57 INFO DAGScheduler: Final stage: Stage 27 (collect at EdgeRDD.scala:51) 14/03/04 16:15:57 INFO DAGScheduler: Parents of final stage: List() 14/03/04 16:15:57 INFO DAGScheduler: Missing parents: List() 14/03/04 16:15:57 INFO DAGScheduler: Submitting Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51), which has no missing parents 14/03/04 16:15:57 INFO DAGScheduler: Submitting 1 missing tasks from Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51) 14/03/04 16:15:57 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks 14/03/04 16:15:57 INFO TaskSetManager: Starting task 27.0:0 as TID 11 on executor localhost: localhost (PROCESS_LOCAL) 14/03/04 16:15:57 INFO TaskSetManager: Serialized task 27.0:0 as 2068 bytes in 1 ms 14/03/04 16:15:57 INFO Executor: Running task ID 11 14/03/04 16:15:57 INFO BlockManager: Found block rdd_2_0 locally 14/03/04 16:15:57 INFO Executor: Serialized size of result for 11 is 936 14/03/04 16:15:57 INFO Executor: Sending result for 11 directly to driver 14/03/04 16:15:57 INFO Executor: Finished task ID 11 14/03/04 16:15:57 INFO TaskSetManager: Finished TID 11 in 13 ms on localhost (progress: 0/1) 14/03/04 16:15:57 INFO DAGScheduler: Completed ResultTask(27, 0) 14/03/04 16:15:57 INFO TaskSchedulerImpl: Remove TaskSet 27.0 from pool 14/03/04 16:15:57 INFO DAGScheduler: Stage 27 (collect at EdgeRDD.scala:51) finished in 0.015 s 14/03/04 16:15:57 INFO SparkContext: Job finished: collect at EdgeRDD.scala:51, took 0.023602266 s res7: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3)) scala> graph.vertices.toArray 14/03/04 16:16:18 INFO SparkContext: Starting job: toArray at <console>:27 14/03/04 16:16:18 INFO DAGScheduler: Got job 6 (toArray at <console>:27) with 1 output partitions (allowLocal=false) 14/03/04 16:16:18 INFO DAGScheduler: Final stage: Stage 28 (toArray at <console>:27) 14/03/04 16:16:18 INFO DAGScheduler: Parents of final stage: List(Stage 32, Stage 29) 14/03/04 16:16:18 INFO DAGScheduler: Missing parents: List() 14/03/04 16:16:18 INFO DAGScheduler: Submitting Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52), which has no missing parents 14/03/04 16:16:18 INFO DAGScheduler: Submitting 1 missing tasks from Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52) 14/03/04 16:16:18 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks 14/03/04 16:16:18 INFO TaskSetManager: Starting task 28.0:0 as TID 12 on executor localhost: localhost (PROCESS_LOCAL) 14/03/04 16:16:18 INFO TaskSetManager: Serialized task 28.0:0 as 2426 bytes in 0 ms 14/03/04 16:16:18 INFO Executor: Running task ID 12 14/03/04 16:16:18 INFO BlockManager: Found block rdd_14_0 locally 14/03/04 16:16:18 INFO Executor: Serialized size of result for 12 is 947 14/03/04 16:16:18 INFO Executor: Sending result for 12 directly to driver 14/03/04 16:16:18 INFO Executor: Finished task ID 12 14/03/04 16:16:18 INFO TaskSetManager: Finished TID 12 in 13 ms on localhost (progress: 0/1) 14/03/04 16:16:18 INFO DAGScheduler: Completed ResultTask(28, 0) 14/03/04 16:16:18 INFO TaskSchedulerImpl: Remove TaskSet 28.0 from pool 14/03/04 16:16:18 INFO DAGScheduler: Stage 28 (toArray at <console>:27) finished in 0.015 s 14/03/04 16:16:18 INFO SparkContext: Job finished: toArray at <console>:27, took 0.027839851 s res9: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((4,(David,42)), (2,(Bob,27)), (6,(Fran,50)), (5,(Ed,55)), (3,(Charlie,65)), (1,(Alice,28))) scala> graph.triplets.toArray 14/03/04 16:16:30 INFO SparkContext: Starting job: toArray at <console>:27 14/03/04 16:16:30 INFO DAGScheduler: Got job 7 (toArray at <console>:27) with 1 output partitions (allowLocal=false) 14/03/04 16:16:31 INFO DAGScheduler: Final stage: Stage 33 (toArray at <console>:27) 14/03/04 16:16:31 INFO DAGScheduler: Parents of final stage: List(Stage 34) 14/03/04 16:16:31 INFO DAGScheduler: Missing parents: List() 14/03/04 16:16:31 INFO DAGScheduler: Submitting Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60), which has no missing parents 14/03/04 16:16:31 INFO DAGScheduler: Submitting 1 missing tasks from Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60) 14/03/04 16:16:31 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks 14/03/04 16:16:31 INFO TaskSetManager: Starting task 33.0:0 as TID 13 on executor localhost: localhost (PROCESS_LOCAL) 14/03/04 16:16:31 INFO TaskSetManager: Serialized task 33.0:0 as 3322 bytes in 1 ms 14/03/04 16:16:31 INFO Executor: Running task ID 13 14/03/04 16:16:31 INFO BlockManager: Found block rdd_2_0 locally 14/03/04 16:16:31 INFO BlockManager: Found block rdd_31_0 locally 14/03/04 16:16:31 INFO Executor: Serialized size of result for 13 is 931 14/03/04 16:16:31 INFO Executor: Sending result for 13 directly to driver 14/03/04 16:16:31 INFO Executor: Finished task ID 13 14/03/04 16:16:31 INFO TaskSetManager: Finished TID 13 in 17 ms on localhost (progress: 0/1) 14/03/04 16:16:31 INFO DAGScheduler: Completed ResultTask(33, 0) 14/03/04 16:16:31 INFO TaskSchedulerImpl: Remove TaskSet 33.0 from pool 14/03/04 16:16:31 INFO DAGScheduler: Stage 33 (toArray at <console>:27) finished in 0.019 s 14/03/04 16:16:31 INFO SparkContext: Job finished: toArray at <console>:27, took 0.037909394 s res10: Array[org.apache.spark.graphx.EdgeTriplet[(String, Int),Int]] = Array(((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3)) ```\n\nComments (8):\n1. Reynold Xin: The problem is that we are reusing an EdgeTriplet object. Try force a copy before you do the collect. e.g. triplets.map(_.copy).collect()\n2. Daniel Darabos: This has bitten us as well. I am no expert, but my impression is that re-using an object in these iterators was a terrible idea. Consider this example: scala> val g = graphx.util.GraphGenerators.starGraph(sc, 5) scala> g.edges.collect Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1)) scala> g.edges.map(x => x).collect Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1)) It can and does go very wrong in practice too. Consider this: scala> g.edges.saveAsObjectFile(\"edges\") scala> sc.objectFile[graphx.Edge[Int]](\"edges\").collect Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1)) Indeed map(_.copy) is a good workaround. But seems like this is a nasty trap laid out for your users. Did you measure the performance gain? If not, or if it's insignificant, I would suggest not re-using the object in the iterators. It would simplify the GraphX source too. However, if it is significant, one idea is to offer separate mapFast() methods that do re-use, while the more commonly used map() would offer the more commonly expected (copying) semantics. Also I wish GraphX offered graph save/load functionality. If it did, I would only have discovered this bug a few weeks from now :).\n3. Daniel Darabos: Sorry, I forgot to test your workaround before commenting. It does not work. scala> g.triplets.map(_.copy).collect <console>:16: error: missing arguments for method copy in class Edge; follow this method with `_' if you want to treat it as a partially applied function g.triplets.map(_.copy).collect ^ You can of course write a copy function yourself. I'd be happy to work on this. Can I just get rid of the re-use, or would you prefer another approach?\n4. Daniel Darabos: Okay, it was just missing the parentheses: scala> g.triplets.map(_.copy()).collect res1: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1)) (Scala novice here :).)\n5. Daniel Darabos: I've sent a pull request to eliminate the re-use (https://github.com/apache/spark/pull/276). The change has no performance impact and simplifies the code.\n6. Daniel Darabos: The changes are in the master branch now. I can't figure out how to close a JIRA ticket :).\n7. Reynold Xin: I added you to contributor list so you should be able to edit in the future. Cheers.\n8. Reynold Xin: Adding a link to the commit: https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "f4b7b9fc7bff3f9083725c534d2f6bee", "issue_key": "SPARK-1189", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support authentication between Spark components", "description": "Add Security to Spark - Akka, Http, ConnectionManager, UI use servlets. see PR https://github.com/apache/spark/pull/33. The design doc is written in the javadoc for the SecurityManager. This is to add basic authentication to the spark communication channels using a shared secret. The UI can also be protected by adding Filters. The main focus of this jira is for Spark on Yarn. For spark on Yarn the Hadoop UGI can be used to distributed the shared secret when the application is deployed and there will be a separate shared secret for each application. The authentication mechanisms added here can also be used in other deploys( standalone, mesos). But the distribution method will be from a simply config that needs to be shared amongst all the master/workers and application.", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-03-05T06:01:26.000+0000", "updated": "2014-03-06T17:09:24.000+0000", "resolved": "2014-03-06T16:30:58.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Hey Tom just renamed this to be more clear about the scope. Hope that's alright, just want people to understand that this patch is focused on authentication.", "created": "2014-03-06T17:09:24.024+0000"}], "num_comments": 1, "text": "Issue: SPARK-1189\nSummary: Support authentication between Spark components\nDescription: Add Security to Spark - Akka, Http, ConnectionManager, UI use servlets. see PR https://github.com/apache/spark/pull/33. The design doc is written in the javadoc for the SecurityManager. This is to add basic authentication to the spark communication channels using a shared secret. The UI can also be protected by adding Filters. The main focus of this jira is for Spark on Yarn. For spark on Yarn the Hadoop UGI can be used to distributed the shared secret when the application is deployed and there will be a separate shared secret for each application. The authentication mechanisms added here can also be used in other deploys( standalone, mesos). But the distribution method will be from a simply config that needs to be shared amongst all the master/workers and application.\n\nComments (1):\n1. Patrick McFadin: Hey Tom just renamed this to be more clear about the scope. Hope that's alright, just want people to understand that this patch is focused on authentication.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "4d0627b0dbc51bde105b389df1229334", "issue_key": "SPARK-1190", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Do not initialize log4j if slf4j log4j backend is not being used", "description": "I already have a patch here just need to test it and commit. IIRC there were some issues with the maven build. https://github.com/apache/incubator-spark/pull/573 https://github.com/pwendell/incubator-spark/commit/66594e88e5be50fca073a7ef38fa62db4082b3c8 initialization with: java.lang.StackOverflowError at java.lang.ThreadLocal.access$400(ThreadLocal.java:72) at java.lang.ThreadLocal$ThreadLocalMap.getEntry(ThreadLocal.java:376) at java.lang.ThreadLocal$ThreadLocalMap.access$000(ThreadLocal.java:261) at java.lang.ThreadLocal.get(ThreadLocal.java:146) at java.lang.StringCoding.deref(StringCoding.java:63) at java.lang.StringCoding.encode(StringCoding.java:330) at java.lang.String.getBytes(String.java:916) at java.io.UnixFileSystem.getBooleanAttributes0(Native Method) at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:242) at java.io.File.exists(File.java:813) at sun.misc.URLClassPath$FileLoader.getResource(URLClassPath.java:1080) at sun.misc.URLClassPath$FileLoader.findResource(URLClassPath.java:1047) at sun.misc.URLClassPath.findResource(URLClassPath.java:176) at java.net.URLClassLoader$2.run(URLClassLoader.java:551) at java.net.URLClassLoader$2.run(URLClassLoader.java:549) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findResource(URLClassLoader.java:548) at java.lang.ClassLoader.getResource(ClassLoader.java:1147) at org.apache.spark.Logging$class.initializeLogging(Logging.scala:109) at org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:97) at org.apache.spark.Logging$class.log(Logging.scala:36) at org.apache.spark.util.Utils$.log(Utils.scala:47)", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "created": "2014-03-05T11:09:16.000+0000", "updated": "2014-10-06T04:29:40.000+0000", "resolved": "2014-03-08T16:03:52.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1190\nSummary: Do not initialize log4j if slf4j log4j backend is not being used\nDescription: I already have a patch here just need to test it and commit. IIRC there were some issues with the maven build. https://github.com/apache/incubator-spark/pull/573 https://github.com/pwendell/incubator-spark/commit/66594e88e5be50fca073a7ef38fa62db4082b3c8 initialization with: java.lang.StackOverflowError at java.lang.ThreadLocal.access$400(ThreadLocal.java:72) at java.lang.ThreadLocal$ThreadLocalMap.getEntry(ThreadLocal.java:376) at java.lang.ThreadLocal$ThreadLocalMap.access$000(ThreadLocal.java:261) at java.lang.ThreadLocal.get(ThreadLocal.java:146) at java.lang.StringCoding.deref(StringCoding.java:63) at java.lang.StringCoding.encode(StringCoding.java:330) at java.lang.String.getBytes(String.java:916) at java.io.UnixFileSystem.getBooleanAttributes0(Native Method) at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:242) at java.io.File.exists(File.java:813) at sun.misc.URLClassPath$FileLoader.getResource(URLClassPath.java:1080) at sun.misc.URLClassPath$FileLoader.findResource(URLClassPath.java:1047) at sun.misc.URLClassPath.findResource(URLClassPath.java:176) at java.net.URLClassLoader$2.run(URLClassLoader.java:551) at java.net.URLClassLoader$2.run(URLClassLoader.java:549) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findResource(URLClassLoader.java:548) at java.lang.ClassLoader.getResource(ClassLoader.java:1147) at org.apache.spark.Logging$class.initializeLogging(Logging.scala:109) at org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:97) at org.apache.spark.Logging$class.log(Logging.scala:36) at org.apache.spark.util.Utils$.log(Utils.scala:47)", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "091814d0e90cdc2b76bb4c962dff8655", "issue_key": "SPARK-1191", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Convert configs to use SparkConf", "description": "There are many places in the yarn code that still use System.setProperty. We should convert those to use the SparkConf. One specific example is SPARK_YARN_MODE. There are others in like ApplicationMaster and Client. Note that currently some configs can't be set in sparkConf and properly picked up with SparkContext as sparkConf isn't really shared with the SparkContext. The only time we can get the sparkContext is after its been instantiated which is to late.", "reporter": "Thomas Graves", "assignee": null, "created": "2014-03-05T11:20:12.000+0000", "updated": "2015-11-08T11:12:17.000+0000", "resolved": "2015-11-08T11:12:17.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Thomas Graves", "body": "Note specifically on the SPARK_YARN_MODE config we may want to find a better way to indicate yarn mode. Its possible that if the user does certain operations before creating SparkContext in yarn-client mode that it actually create the SparkHadoopUtil.hadoop with the wrong class (it doesn't use the YarnSparkHadoopUtil when it should).", "created": "2014-03-05T12:45:14.885+0000"}, {"author": "Apache Spark", "body": "User 'CodingCat' has created a pull request for this issue: https://github.com/apache/spark/pull/2312", "created": "2014-09-07T13:44:43.232+0000"}, {"author": "Neelesh Srinivas Salian", "body": "Is this JIRA still relevant? The PR seems to be closed.", "created": "2015-11-08T02:22:48.630+0000"}, {"author": "Sean R. Owen", "body": "At this stage I don't think this will proceed", "created": "2015-11-08T11:12:17.689+0000"}], "num_comments": 4, "text": "Issue: SPARK-1191\nSummary: Convert configs to use SparkConf\nDescription: There are many places in the yarn code that still use System.setProperty. We should convert those to use the SparkConf. One specific example is SPARK_YARN_MODE. There are others in like ApplicationMaster and Client. Note that currently some configs can't be set in sparkConf and properly picked up with SparkContext as sparkConf isn't really shared with the SparkContext. The only time we can get the sparkContext is after its been instantiated which is to late.\n\nComments (4):\n1. Thomas Graves: Note specifically on the SPARK_YARN_MODE config we may want to find a better way to indicate yarn mode. Its possible that if the user does certain operations before creating SparkContext in yarn-client mode that it actually create the SparkHadoopUtil.hadoop with the wrong class (it doesn't use the YarnSparkHadoopUtil when it should).\n2. Apache Spark: User 'CodingCat' has created a pull request for this issue: https://github.com/apache/spark/pull/2312\n3. Neelesh Srinivas Salian: Is this JIRA still relevant? The PR seems to be closed.\n4. Sean R. Owen: At this stage I don't think this will proceed", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "7fd872d87158b5cf5016b1a020e2093a", "issue_key": "SPARK-1192", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Around 30 parameters in Spark are used but undocumented and some are having confusing name", "description": "I grep the code in core component, I found that around 30 parameters in the implementation is actually used but undocumented. By reading the source code, I found that some of them are actually very useful for the user. I suggest to make a complete document on the parameters. Also some parameters are having confusing names spark.shuffle.copier.threads - this parameters is to control how many threads you will use when you start a Netty-based shuffle service....but from the name, we cannot get this information spark.shuffle.sender.port - the similar problem with the above one, when you use Netty-based shuffle receiver, you will have to setup a Netty-based sender...this parameter is to setup the port used by the Netty sender, but the name cannot convey this information", "reporter": "Nan Zhu", "assignee": "Nan Zhu", "created": "2014-03-05T16:22:48.000+0000", "updated": "2015-02-12T20:37:28.000+0000", "resolved": "2015-02-12T20:37:28.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Sean R. Owen", "body": "PR is actually at https://github.com/apache/spark/pull/2312 and is misnamed. Is this still live though?", "created": "2014-10-13T18:00:44.629+0000"}, {"author": "Apache Spark", "body": "User 'CodingCat' has created a pull request for this issue: https://github.com/apache/spark/pull/2312", "created": "2014-10-13T18:03:33.228+0000"}, {"author": "Nan Zhu", "body": "yes, I resubmitted https://github.com/apache/spark/pull/2312 for Matei's request (removed some, add some....) it's still valid", "created": "2014-10-13T18:06:44.527+0000"}, {"author": "Sean R. Owen", "body": "PR was withdrawn; this probably deserves a rethink if it were reconsidered anyway so let's resolve.", "created": "2015-02-12T20:37:28.914+0000"}], "num_comments": 4, "text": "Issue: SPARK-1192\nSummary: Around 30 parameters in Spark are used but undocumented and some are having confusing name\nDescription: I grep the code in core component, I found that around 30 parameters in the implementation is actually used but undocumented. By reading the source code, I found that some of them are actually very useful for the user. I suggest to make a complete document on the parameters. Also some parameters are having confusing names spark.shuffle.copier.threads - this parameters is to control how many threads you will use when you start a Netty-based shuffle service....but from the name, we cannot get this information spark.shuffle.sender.port - the similar problem with the above one, when you use Netty-based shuffle receiver, you will have to setup a Netty-based sender...this parameter is to setup the port used by the Netty sender, but the name cannot convey this information\n\nComments (4):\n1. Sean R. Owen: PR is actually at https://github.com/apache/spark/pull/2312 and is misnamed. Is this still live though?\n2. Apache Spark: User 'CodingCat' has created a pull request for this issue: https://github.com/apache/spark/pull/2312\n3. Nan Zhu: yes, I resubmitted https://github.com/apache/spark/pull/2312 for Matei's request (removed some, add some....) it's still valid\n4. Sean R. Owen: PR was withdrawn; this probably deserves a rethink if it were reconsidered anyway so let's resolve.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "bd3729c9a975155e02541e05a8ca5c99", "issue_key": "SPARK-1193", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Inconsistent indendation between pom.xmls", "description": "e.g. core/pom.xml uses 4 spaces and graphx/pom.xml uses 2", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-03-05T18:01:30.000+0000", "updated": "2014-04-04T20:50:55.000+0000", "resolved": "2014-03-08T03:22:18.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Sanford Ryza", "body": "https://github.com/apache/spark/pull/91", "created": "2014-03-05T20:59:40.467+0000"}], "num_comments": 1, "text": "Issue: SPARK-1193\nSummary: Inconsistent indendation between pom.xmls\nDescription: e.g. core/pom.xml uses 4 spaces and graphx/pom.xml uses 2\n\nComments (1):\n1. Sanford Ryza: https://github.com/apache/spark/pull/91", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "9f178cb9284700e0f98605fed9c2e27d", "issue_key": "SPARK-1194", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "The same-RDD rule for cache replacement is not properly implemented", "description": "The same-RDD rule for cache replacement described in the original RDD paper prevents cycling partitions from the same RDD in and out. [Commit 6098f7e|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R182] meant to implement this, but I believe it introduced some problems. In the current implementation, when selecting candidate blocks to be swapped out, once we find a block from the same RDD that the block to be stored belongs to, [cache eviction fails and aborts|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R185]. -Also, LRU eviction (as described in the paper) is not employed.- A possible cache eviction strategy can be: keep selecting blocks _not_ from the RDD that the block to be stored belongs to until either enough free space can be ensured (cache eviction succeeds) or all such blocks are checked (cache eviction fails). -LRU should also be employed, but not necessarily in this issue.- Any thoughts? Especially, did I miss any apparent facts behind current implementation? *Update*: LRU is implemented with {{LinkedHashMap}} by setting constructor argument {{accessOrder}} to {{true}}.", "reporter": "liancheng", "assignee": "liancheng", "created": "2014-03-06T07:45:12.000+0000", "updated": "2014-03-07T23:28:59.000+0000", "resolved": "2014-03-07T23:28:59.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Nan Zhu", "body": "Is it possible to make the replacement policy as pluggable?", "created": "2014-03-06T11:27:53.324+0000"}], "num_comments": 1, "text": "Issue: SPARK-1194\nSummary: The same-RDD rule for cache replacement is not properly implemented\nDescription: The same-RDD rule for cache replacement described in the original RDD paper prevents cycling partitions from the same RDD in and out. [Commit 6098f7e|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R182] meant to implement this, but I believe it introduced some problems. In the current implementation, when selecting candidate blocks to be swapped out, once we find a block from the same RDD that the block to be stored belongs to, [cache eviction fails and aborts|https://github.com/apache/spark/commit/6098f7e87a88d0b847c402b95510cb07352db643#diff-f82f9761c5f006f4b2a7efb57ccb7699R185]. -Also, LRU eviction (as described in the paper) is not employed.- A possible cache eviction strategy can be: keep selecting blocks _not_ from the RDD that the block to be stored belongs to until either enough free space can be ensured (cache eviction succeeds) or all such blocks are checked (cache eviction fails). -LRU should also be employed, but not necessarily in this issue.- Any thoughts? Especially, did I miss any apparent facts behind current implementation? *Update*: LRU is implemented with {{LinkedHashMap}} by setting constructor argument {{accessOrder}} to {{true}}.\n\nComments (1):\n1. Nan Zhu: Is it possible to make the replacement policy as pluggable?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "71bd327ee8569865c7c141b50e9d2e3b", "issue_key": "SPARK-1195", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "set map_input_file environment variable in PipedRDD", "description": "Hadoop uses the config mapreduce.map.input.file to indicate the input filename to the map when the input split is of type FileSplit. Some of the hadoop input and output formats set or use this config. This config can also be used by user code. PipedRDD runs an external process and the configs aren't available to that process. Hadoop Streaming does something very similar and the way they make configs available is exporting them into the environment replacing '.' with '_'. Spark should also export this variable when launching the pipe command so the user code has access to that config. Note that the config mapreduce.map.input.file is the new one, the old one which is deprecated but not yet removed is map.input.file. So we should handle both.", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-03-06T09:07:47.000+0000", "updated": "2014-03-07T10:38:34.000+0000", "resolved": "2014-03-07T10:38:34.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Thomas Graves", "body": "https://github.com/apache/spark/pull/94", "created": "2014-03-06T12:15:14.690+0000"}], "num_comments": 1, "text": "Issue: SPARK-1195\nSummary: set map_input_file environment variable in PipedRDD\nDescription: Hadoop uses the config mapreduce.map.input.file to indicate the input filename to the map when the input split is of type FileSplit. Some of the hadoop input and output formats set or use this config. This config can also be used by user code. PipedRDD runs an external process and the configs aren't available to that process. Hadoop Streaming does something very similar and the way they make configs available is exporting them into the environment replacing '.' with '_'. Spark should also export this variable when launching the pipe command so the user code has access to that config. Note that the config mapreduce.map.input.file is the new one, the old one which is deprecated but not yet removed is map.input.file. So we should handle both.\n\nComments (1):\n1. Thomas Graves: https://github.com/apache/spark/pull/94", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "f4e4bf38cb4bbb07f38281ea9a791ccf", "issue_key": "SPARK-1196", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "val variables not available within RDD map on cluster app; are on shell or local", "description": "When this code  def foo = \"foo\" val bar = \"bar\" val data = sc.parallelize(Seq(\"a\")) data.map{a => print(1,foo,bar);a}.map{a => print(2,foo,bar);a}.map{a => print(3,foo,bar);a}.collect()  is run on a cluster on the spark shell a slave's stdout is  (1,foo,bar)(2,foo,bar)(3,foo,bar)  as expected. However when the code  import org.apache.spark.{SparkConf, SparkContext} import org.apache.spark.SparkContext._ object twitterAggregation extends App { val conf = new SparkConf() .setMaster(\"spark://xx.compute-1.amazonaws.com:7077\") .setAppName(\"testCase\") .setJars(List(\"target/scala-2.10/spark-test-case_2.10-1.0.jar\")) .setSparkHome(\"/root/spark/\") val sc = new SparkContext(conf) def foo = \"foo\" val bar = \"bar\" val data = sc.parallelize(Seq(\"a\")) data.map{a => print(1,foo,bar);a}.map{a => print(2,foo,bar);a}.map{a => print(3,foo,bar);a}.collect() }  is run against a cluster as an application via sbt the stdout on a slave is  (1,foo,null)(2,foo,null)(3,foo,null)  The variable declared with val is now null when the anon functions in the map are executed. When the application is run in local mode the output is  (1,foo,bar)(2,foo,bar)(3,foo,bar)  as wanted. build.sbt is  name := \"spark-test-case\" version := \"1.0\" scalaVersion:=\"2.10.3\" resolvers ++= Seq(\"Akka Repository\" at \"http://repo.akka.io/releases/\") libraryDependencies ++= Seq(\"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\")  To avoid firewall and NAT issues the project directory is rsynced onto the master where is is build with SBT 0.13.1  wget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm && rpm --install sbt.rpm sbt package && sbt run  Cluster created with scripts in the hadoop2 0.9.0 download.", "reporter": "Andrew Kerr", "assignee": null, "created": "2014-03-06T12:05:59.000+0000", "updated": "2014-11-08T10:14:39.000+0000", "resolved": "2014-11-08T10:14:38.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Kapil Malik", "body": "Hi Andrew ! Can you kindly re-confirm if the issue persists ? It's not reproducible for a standalone cluster with 3 slaves. I tried against the same scala / spark versions.", "created": "2014-03-27T13:08:00.446+0000"}], "num_comments": 1, "text": "Issue: SPARK-1196\nSummary: val variables not available within RDD map on cluster app; are on shell or local\nDescription: When this code  def foo = \"foo\" val bar = \"bar\" val data = sc.parallelize(Seq(\"a\")) data.map{a => print(1,foo,bar);a}.map{a => print(2,foo,bar);a}.map{a => print(3,foo,bar);a}.collect()  is run on a cluster on the spark shell a slave's stdout is  (1,foo,bar)(2,foo,bar)(3,foo,bar)  as expected. However when the code  import org.apache.spark.{SparkConf, SparkContext} import org.apache.spark.SparkContext._ object twitterAggregation extends App { val conf = new SparkConf() .setMaster(\"spark://xx.compute-1.amazonaws.com:7077\") .setAppName(\"testCase\") .setJars(List(\"target/scala-2.10/spark-test-case_2.10-1.0.jar\")) .setSparkHome(\"/root/spark/\") val sc = new SparkContext(conf) def foo = \"foo\" val bar = \"bar\" val data = sc.parallelize(Seq(\"a\")) data.map{a => print(1,foo,bar);a}.map{a => print(2,foo,bar);a}.map{a => print(3,foo,bar);a}.collect() }  is run against a cluster as an application via sbt the stdout on a slave is  (1,foo,null)(2,foo,null)(3,foo,null)  The variable declared with val is now null when the anon functions in the map are executed. When the application is run in local mode the output is  (1,foo,bar)(2,foo,bar)(3,foo,bar)  as wanted. build.sbt is  name := \"spark-test-case\" version := \"1.0\" scalaVersion:=\"2.10.3\" resolvers ++= Seq(\"Akka Repository\" at \"http://repo.akka.io/releases/\") libraryDependencies ++= Seq(\"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\")  To avoid firewall and NAT issues the project directory is rsynced onto the master where is is build with SBT 0.13.1  wget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm && rpm --install sbt.rpm sbt package && sbt run  Cluster created with scripts in the hadoop2 0.9.0 download.\n\nComments (1):\n1. Kapil Malik: Hi Andrew ! Can you kindly re-confirm if the issue persists ? It's not reproducible for a standalone cluster with 3 slaves. I tried against the same scala / spark versions.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "32116605ec5325b11f6ec55e952071c7", "issue_key": "SPARK-1197", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Rename yarn-standalone and fix up docs for running on YARN", "description": "yarn-standalone is a confusing name because the use of \"standalone\" is different than the use in the sense of Spark standalone cluster manager. It would also be nice to fix up some typos in the YARN docs and add a section on how to view container logs.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-03-06T12:31:52.000+0000", "updated": "2014-04-04T20:50:47.000+0000", "resolved": "2014-03-06T17:26:57.000+0000", "labels": [], "components": [], "comments": [{"author": "Sanford Ryza", "body": "https://github.com/apache/spark/pull/95", "created": "2014-03-06T12:36:54.327+0000"}], "num_comments": 1, "text": "Issue: SPARK-1197\nSummary: Rename yarn-standalone and fix up docs for running on YARN\nDescription: yarn-standalone is a confusing name because the use of \"standalone\" is different than the use in the sense of Spark standalone cluster manager. It would also be nice to fix up some typos in the YARN docs and add a section on how to view container logs.\n\nComments (1):\n1. Sanford Ryza: https://github.com/apache/spark/pull/95", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "592b5cf03d0395f1e55622afe46434ab", "issue_key": "SPARK-1198", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Allow pipes tasks to run in different sub-directories", "description": "Currently when a task runs, its working directory is the same as all the other tasks running on that Worker. If the tasks happen to output files to that working directory with the same name, collisions happen. We should add an option to allow the tasks to run in separate sub-directories to avoid those conflicts. I should clarify that the specific concern is when running the pipes command.", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-03-06T12:45:14.000+0000", "updated": "2014-04-05T00:17:11.000+0000", "resolved": "2014-04-05T00:17:11.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1198\nSummary: Allow pipes tasks to run in different sub-directories\nDescription: Currently when a task runs, its working directory is the same as all the other tasks running on that Worker. If the tasks happen to output files to that working directory with the same name, collisions happen. We should add an option to allow the tasks to run in separate sub-directories to avoid those conflicts. I should clarify that the specific concern is when running the pipes command.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "9e34d7f7bab19759f6bb8fdcec040c68", "issue_key": "SPARK-1199", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Type mismatch in Spark shell when using case class defined in shell", "description": "*NOTE: This issue was fixed in 1.0.1, but the fix was reverted in Spark 1.0.2 pending further testing. The final fix will be in Spark 1.1.0.* Define a class in the shell:  case class TestClass(a:String)  and an RDD  val data = sc.parallelize(Seq(\"a\")).map(TestClass(_))  define a function on it and map over the RDD  def itemFunc(a:TestClass):TestClass = a data.map(itemFunc)  Error:  <console>:19: error: type mismatch; found : TestClass => TestClass required: TestClass => ? data.map(itemFunc)  Similarly with a mapPartitions:  def partitionFunc(a:Iterator[TestClass]):Iterator[TestClass] = a data.mapPartitions(partitionFunc)   <console>:19: error: type mismatch; found : Iterator[TestClass] => Iterator[TestClass] required: Iterator[TestClass] => Iterator[?] Error occurred in an application involving default arguments. data.mapPartitions(partitionFunc)  The behavior is the same whether in local mode or on a cluster. This isn't specific to RDDs. A Scala collection in the Spark shell has the same problem.  scala> Seq(TestClass(\"foo\")).map(itemFunc) <console>:15: error: type mismatch; found : TestClass => TestClass required: TestClass => ? Seq(TestClass(\"foo\")).map(itemFunc) ^  When run in the Scala console (not the Spark shell) there are no type mismatch errors.", "reporter": "Andrew Kerr", "assignee": "Prashant Sharma", "created": "2014-03-06T12:51:03.000+0000", "updated": "2016-06-15T18:08:57.000+0000", "resolved": "2014-07-04T07:06:26.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "This looks similar to the problem reported in https://groups.google.com/d/msg/spark-users/bwAmbUgxWrA/HwP4Nv4adfEJ", "created": "2014-03-06T13:13:23.814+0000"}, {"author": "Andrew Kerr", "body": "The workaround from the above link doesn't help.  object TestClasses { case class TestClass(a:String) } import TestClasses._ def itemFunc(a:TestClass):TestClass = a Seq(TestClass(\"foo\")).map(itemFunc)   <console>:22: error: type mismatch; found : TestClasses.TestClass => TestClasses.TestClass required: TestClasses.TestClass => ? data.map(itemFunc) ^  From the Spark shell both locally and on a cluster.", "created": "2014-03-13T04:26:57.712+0000"}, {"author": "Michael Armbrust", "body": "I think I may be hitting a related issue when trying to created nested classes for use in SparkSQL.  scala> case class A(a: String) defined class A scala> case class B(b: A) defined class B scala> B(A(\"a\")) <console>:22: error: type mismatch; found : A required: A B(A(\"a\")) ^  In this case, the described workaround does work, but is kind of annoying. Mostly I want to bump this thread since Matei had talked about maybe fixing this using macros? Is that something we would like to consider for a future release.", "created": "2014-03-21T14:42:54.234+0000"}, {"author": "Piotr Kolaczkowski", "body": "+1 to fixing this. We're affected as well. Classes defined in Shell are inner classes, and therefore cannot be easily instantiated by reflection. They need additional reference to the outer object, which is non-trivial to obtain (is it obtainable at all without modifying Spark?).  scala> class Test defined class Test scala> new Test res5: Test = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test@4f755864 // good, so there is a default constructor and we can call it through reflection? // not so fast... scala> classOf[Test].getConstructor() java.lang.NoSuchMethodException: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test.<init>() ... scala> classOf[Test].getConstructors()(0) res7: java.lang.reflect.Constructor[_] = public $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test($iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)  The workaround does not work for us.", "created": "2014-04-25T07:26:31.190+0000"}, {"author": "Andrew Kerr", "body": "I have something of a workaround:  object MyTypes { case class TestClass(a:Int) } object MyLogic { import MyClasses._ def fn(b:TestClass) = TestClass(b.a * 2) val result = Seq(TestClass(1)).map(fn) } MyLogic.result // Seq[MyTypes.TestClass] = List(TestClass(2))  Still can't access TestClass outside an object.", "created": "2014-04-25T16:01:58.900+0000"}, {"author": "Michael Malak", "body": "See also additional test cases in https://issues.apache.org/jira/browse/SPARK-1836 which has now been marked as a duplicate.", "created": "2014-05-28T19:39:14.331+0000"}, {"author": "Patrick Wendell", "body": "Prashant said he could look into this - so I'm assigning it to him.", "created": "2014-06-17T17:34:38.882+0000"}, {"author": "Prashant Sharma", "body": "One work around is to use `:paste` command of repl to work with these kind of scenarios. So if you use :paste and put the whole thing at once it will work nicely. I am just mentioning it because I found it, we also have a slightly better fix on github PR.", "created": "2014-06-24T06:59:45.827+0000"}, {"author": "Andrea Ferretti", "body": "More examples on https://issues.apache.org/jira/browse/SPARK-2330 which should also be a duplicate", "created": "2014-07-03T09:59:42.886+0000"}, {"author": "Patrick Wendell", "body": "Resolved via: https://github.com/apache/spark/pull/1179", "created": "2014-07-04T07:06:26.956+0000"}, {"author": "Patrick Wendell", "body": "Just a note, I've reverted this fix in branch-1.0 the fix here caused other issues that were worse than the original bug (SPARK-2452). This will be fixed in 1.1.", "created": "2014-07-21T18:53:48.234+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/1179", "created": "2015-12-10T15:06:05.597+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/1176", "created": "2015-12-10T15:06:07.259+0000"}, {"author": "Oleksiy Dyagilev", "body": "I have problems with declaring case classes in shell, Spark 1.6 This doesn't work for me:  scala> case class ABCD() defined class ABCD scala> new ABCD() res33: ABCD = ABCD() scala> classOf[ABCD].getConstructor() java.lang.NoSuchMethodException: $iwC$$iwC$ABCD.<init>() at java.lang.Class.getConstructor0(Class.java:3074) at java.lang.Class.getConstructor(Class.java:1817) scala> classOf[ABCD].getConstructors() res31: Array[java.lang.reflect.Constructor[_]] = Array(public $iwC$$iwC$ABCD($iwC$$iwC))", "created": "2016-02-23T18:07:19.786+0000"}, {"author": "Michael Armbrust", "body": "All classes defined in the REPL are inner classes due to the way compilation works. Therefore there is not going to be a no-arg constructor. This is expected behavior.", "created": "2016-02-23T18:19:20.663+0000"}, {"author": "Oleksiy Dyagilev", "body": "Michael Armbrust, in my use case I have a library that relies on having a default constructor and I want to use this library in the REPL. Any workaround for that?", "created": "2016-02-23T19:01:17.659+0000"}, {"author": "Michael Armbrust", "body": "You will have to define your case classes in a jar instead of the REPL.", "created": "2016-02-23T19:02:15.151+0000"}, {"author": "Oleksiy Dyagilev", "body": "Thanks, any other options? I want to be able to define classes in the REPL.", "created": "2016-02-23T19:09:23.943+0000"}, {"author": "Michael Armbrust", "body": "Not that I know of. Also, please use the spark-user list instead of JIRA for tech support questions :)", "created": "2016-02-23T19:14:26.685+0000"}, {"author": "Prashant Sharma", "body": "Did you try the :paste option ?", "created": "2016-02-24T03:59:31.069+0000"}, {"author": "Oleksiy Dyagilev", "body": "Yes, I did. It doesn't help, the inner class still doesn't have a no-arg constructor visible with a reflection.", "created": "2016-02-24T13:41:02.257+0000"}], "num_comments": 21, "text": "Issue: SPARK-1199\nSummary: Type mismatch in Spark shell when using case class defined in shell\nDescription: *NOTE: This issue was fixed in 1.0.1, but the fix was reverted in Spark 1.0.2 pending further testing. The final fix will be in Spark 1.1.0.* Define a class in the shell:  case class TestClass(a:String)  and an RDD  val data = sc.parallelize(Seq(\"a\")).map(TestClass(_))  define a function on it and map over the RDD  def itemFunc(a:TestClass):TestClass = a data.map(itemFunc)  Error:  <console>:19: error: type mismatch; found : TestClass => TestClass required: TestClass => ? data.map(itemFunc)  Similarly with a mapPartitions:  def partitionFunc(a:Iterator[TestClass]):Iterator[TestClass] = a data.mapPartitions(partitionFunc)   <console>:19: error: type mismatch; found : Iterator[TestClass] => Iterator[TestClass] required: Iterator[TestClass] => Iterator[?] Error occurred in an application involving default arguments. data.mapPartitions(partitionFunc)  The behavior is the same whether in local mode or on a cluster. This isn't specific to RDDs. A Scala collection in the Spark shell has the same problem.  scala> Seq(TestClass(\"foo\")).map(itemFunc) <console>:15: error: type mismatch; found : TestClass => TestClass required: TestClass => ? Seq(TestClass(\"foo\")).map(itemFunc) ^  When run in the Scala console (not the Spark shell) there are no type mismatch errors.\n\nComments (21):\n1. Josh Rosen: This looks similar to the problem reported in https://groups.google.com/d/msg/spark-users/bwAmbUgxWrA/HwP4Nv4adfEJ\n2. Andrew Kerr: The workaround from the above link doesn't help.  object TestClasses { case class TestClass(a:String) } import TestClasses._ def itemFunc(a:TestClass):TestClass = a Seq(TestClass(\"foo\")).map(itemFunc)   <console>:22: error: type mismatch; found : TestClasses.TestClass => TestClasses.TestClass required: TestClasses.TestClass => ? data.map(itemFunc) ^  From the Spark shell both locally and on a cluster.\n3. Michael Armbrust: I think I may be hitting a related issue when trying to created nested classes for use in SparkSQL.  scala> case class A(a: String) defined class A scala> case class B(b: A) defined class B scala> B(A(\"a\")) <console>:22: error: type mismatch; found : A required: A B(A(\"a\")) ^  In this case, the described workaround does work, but is kind of annoying. Mostly I want to bump this thread since Matei had talked about maybe fixing this using macros? Is that something we would like to consider for a future release.\n4. Piotr Kolaczkowski: +1 to fixing this. We're affected as well. Classes defined in Shell are inner classes, and therefore cannot be easily instantiated by reflection. They need additional reference to the outer object, which is non-trivial to obtain (is it obtainable at all without modifying Spark?).  scala> class Test defined class Test scala> new Test res5: Test = $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test@4f755864 // good, so there is a default constructor and we can call it through reflection? // not so fast... scala> classOf[Test].getConstructor() java.lang.NoSuchMethodException: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test.<init>() ... scala> classOf[Test].getConstructors()(0) res7: java.lang.reflect.Constructor[_] = public $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$Test($iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)  The workaround does not work for us.\n5. Andrew Kerr: I have something of a workaround:  object MyTypes { case class TestClass(a:Int) } object MyLogic { import MyClasses._ def fn(b:TestClass) = TestClass(b.a * 2) val result = Seq(TestClass(1)).map(fn) } MyLogic.result // Seq[MyTypes.TestClass] = List(TestClass(2))  Still can't access TestClass outside an object.\n6. Michael Malak: See also additional test cases in https://issues.apache.org/jira/browse/SPARK-1836 which has now been marked as a duplicate.\n7. Patrick Wendell: Prashant said he could look into this - so I'm assigning it to him.\n8. Prashant Sharma: One work around is to use `:paste` command of repl to work with these kind of scenarios. So if you use :paste and put the whole thing at once it will work nicely. I am just mentioning it because I found it, we also have a slightly better fix on github PR.\n9. Andrea Ferretti: More examples on https://issues.apache.org/jira/browse/SPARK-2330 which should also be a duplicate\n10. Patrick Wendell: Resolved via: https://github.com/apache/spark/pull/1179", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "f6a99f2d9617c08c27ccbe80ef5517e1", "issue_key": "SPARK-1200", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Make it possible to use unmanaged AM in yarn-client mode", "description": "Using an unmanaged AM in yarn-client mode would allow apps to start up faster, but not requiring the container launcher AM to be launched on the cluster.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-03-06T14:08:54.000+0000", "updated": "2016-01-16T13:31:14.000+0000", "resolved": "2016-01-16T13:31:14.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Steve Loughran", "body": "You know, we could benefit all YARN apps if the AM launch request could also include a list of container requests to satisfy. This would remove the pipeline of client->AM->containers, and start requesting containers earlier. The allocated container list could just come in as callbacks once the AM is live, as container losses do on AM restart. It means the client would have to come up with an initial assessment of the priority containers to escalate. But publishing that information at launch time would help with the (proposed) Gang scheduling.", "created": "2015-03-19T14:01:34.277+0000"}, {"author": "Sean R. Owen", "body": "I think Sandy (Ryza) reported this too but I doubt this is still on the radar", "created": "2016-01-16T13:31:14.796+0000"}], "num_comments": 2, "text": "Issue: SPARK-1200\nSummary: Make it possible to use unmanaged AM in yarn-client mode\nDescription: Using an unmanaged AM in yarn-client mode would allow apps to start up faster, but not requiring the container launcher AM to be launched on the cluster.\n\nComments (2):\n1. Steve Loughran: You know, we could benefit all YARN apps if the AM launch request could also include a list of container requests to satisfy. This would remove the pipeline of client->AM->containers, and start requesting containers earlier. The allocated container list could just come in as callbacks once the AM is live, as container losses do on AM restart. It means the client would have to come up with an initial assessment of the priority containers to escalate. But publishing that information at launch time would help with the (proposed) Gang scheduling.\n2. Sean R. Owen: I think Sandy (Ryza) reported this too but I doubt this is still on the radar", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "966a0d0bcbd79301f861e5ebe148d0c6", "issue_key": "SPARK-1201", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Do not materialize partitions whenever possible in BlockManager", "description": "This is a slightly more complex version of SPARK-942 where we try to avoid unrolling iterators in other situations where it is possible. SPARK-942 focused on the case where the DISK_ONLY storage level was used. There are other cases though, such as if data is stored serialized and in memory and but there is not enough memory left to store the RDD.", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "created": "2014-03-06T14:49:28.000+0000", "updated": "2014-09-16T17:43:14.000+0000", "resolved": "2014-09-16T17:43:14.000+0000", "labels": [], "components": ["Block Manager", "Spark Core"], "comments": [{"author": "Mark Hamstra", "body": "What causes this to not be fixable within the scope of 1.0.1?", "created": "2014-06-13T22:18:37.863+0000"}, {"author": "Andrew Or", "body": "Depends on when we release 1.0.1. I am actually working on it at this moment, and I am close to having a PR.", "created": "2014-06-14T02:50:01.557+0000"}, {"author": "Mark Hamstra", "body": "Okay, but my question is really whether resolution of this issue will require new API that will exclude it from consideration from 1.0.x or whether this will be just implementation details that can be considered a bug fix and included in the maintenance branch.", "created": "2014-06-14T02:56:14.004+0000"}, {"author": "Andrew Or", "body": "It will mainly be a bug fix that doesn't change the API.", "created": "2014-06-14T03:20:43.179+0000"}, {"author": "Patrick Wendell", "body": "This was solved by SPARK-1777.", "created": "2014-09-16T17:43:14.413+0000"}], "num_comments": 5, "text": "Issue: SPARK-1201\nSummary: Do not materialize partitions whenever possible in BlockManager\nDescription: This is a slightly more complex version of SPARK-942 where we try to avoid unrolling iterators in other situations where it is possible. SPARK-942 focused on the case where the DISK_ONLY storage level was used. There are other cases though, such as if data is stored serialized and in memory and but there is not enough memory left to store the RDD.\n\nComments (5):\n1. Mark Hamstra: What causes this to not be fixable within the scope of 1.0.1?\n2. Andrew Or: Depends on when we release 1.0.1. I am actually working on it at this moment, and I am close to having a PR.\n3. Mark Hamstra: Okay, but my question is really whether resolution of this issue will require new API that will exclude it from consideration from 1.0.x or whether this will be just implementation details that can be considered a bug fix and included in the maintenance branch.\n4. Andrew Or: It will mainly be a bug fix that doesn't change the API.\n5. Patrick Wendell: This was solved by SPARK-1777.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "beb1b239fe5a2bbd767bca48135f4db4", "issue_key": "SPARK-1202", "issue_type": "New Feature", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Add a \"cancel\" button in the UI for stages", "description": "Seems like this would be really useful for people. It's not that hard, we just need to lookup the jobs associated with the stage and kill them. Might involve exposing some additional API's in SparkContext.", "reporter": "Patrick Wendell", "assignee": "Sundeep Narravula", "created": "2014-03-06T21:24:39.000+0000", "updated": "2014-04-21T17:55:38.000+0000", "resolved": "2014-04-21T17:55:38.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "ASF GitHub Bot", "body": "Github user sundeepn commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011156 @andrewor14 / @kayousterhout. Thanks for the comments. I have posted an additional commit with the changes. I have checked them in my branch but some reason they are not showing up here. Do you want me to issue another new pull request? Let me know. https://github.com/sundeepn/spark/commit/32c2ea58e5053623e5bf5955e52f7b7cd640ff64", "created": "2014-03-29T22:25:41.648+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011323 Build triggered. Build is starting -or- tests failed to complete.", "created": "2014-03-29T22:32:26.276+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011326 Build started. Build is starting -or- tests failed to complete.", "created": "2014-03-29T22:32:31.198+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011375 Build finished. Build is starting -or- tests failed to complete.", "created": "2014-03-29T22:34:25.190+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011376 Build is starting -or- tests failed to complete. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13581/", "created": "2014-03-29T22:34:25.354+0000"}, {"author": "ASF GitHub Bot", "body": "Github user kayousterhout commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011464 It looks like github is just moving slowly today...the commit just got pulled in. I took another look at this and have a question: what happens for stages that are used for multiple jobs? Right now, stageIdToJobId in the UI code you added just maps a stage to a single job id. So, if stage0 is used by JobA and jobB, the ui code only stores one of these jobs, and then cancelJob() will only be called for one of the jobs. cancelJob() ultimately calls DAGScheduler.handleJobCancellation(), which only cancels the stages that are independent to the job. So, because stage0 is not independent to either of the jobs, it won't get cancelled. Did I misunderstand this?", "created": "2014-03-29T22:39:19.742+0000"}, {"author": "ASF GitHub Bot", "body": "Github user kayousterhout commented on a diff in the pull request: https://github.com/apache/spark/pull/246#discussion_r11095610 --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala --- @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener { val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]()) stages(stage.stageId) = stage + + // Extract Job ID and double check if we have the details + val jobId = Option(stageSubmitted.properties).flatMap { + p => Option(p.getProperty(\"spark.job.id\")) + }.getOrElse(\"-1\").toInt --- End diff -- When we chatted about this I remember you saying that this code is to handle the case where the stage runs locally at the driver...but from glancing at the DAGScheduler code, it looks like onStageSubmitted() never gets called for the locally-run tasks, and they never show up at the UI. When do you need this code / when will the jobIdToStageIds mapping not already be set up correctly by OnJobStart?", "created": "2014-03-29T22:50:00.127+0000"}, {"author": "ASF GitHub Bot", "body": "Github user sundeepn commented on a diff in the pull request: https://github.com/apache/spark/pull/246#discussion_r11095628 --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala --- @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener { val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]()) stages(stage.stageId) = stage + + // Extract Job ID and double check if we have the details + val jobId = Option(stageSubmitted.properties).flatMap { + p => Option(p.getProperty(\"spark.job.id\")) + }.getOrElse(\"-1\").toInt --- End diff -- Well, this is only to ensure we can handle things if we get any scenarios where the onJobStart does not arrive before stageSubmitted. I am not familiar with the scheduling code sufficiently to rule that out. If you are sure, I can take this out.", "created": "2014-03-29T22:56:32.915+0000"}, {"author": "ASF GitHub Bot", "body": "Github user kayousterhout commented on a diff in the pull request: https://github.com/apache/spark/pull/246#discussion_r11095648 --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala --- @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener { val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]()) stages(stage.stageId) = stage + + // Extract Job ID and double check if we have the details + val jobId = Option(stageSubmitted.properties).flatMap { + p => Option(p.getProperty(\"spark.job.id\")) + }.getOrElse(\"-1\").toInt --- End diff -- Ah cool -- I looked at the ordering of the JobStart and StageSubmitted events more closely and I think you can safely remove this.", "created": "2014-03-29T23:01:39.864+0000"}, {"author": "ASF GitHub Bot", "body": "Github user sundeepn commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39012183 > what happens for stages that are used for multiple jobs? I misunderstood our conversation on job to stage mapping the other day. As you see, the code will currently not handle multiple job mappings. Is there a simple example I can use to generate such a scenario?", "created": "2014-03-29T23:10:44.357+0000"}], "num_comments": 10, "text": "Issue: SPARK-1202\nSummary: Add a \"cancel\" button in the UI for stages\nDescription: Seems like this would be really useful for people. It's not that hard, we just need to lookup the jobs associated with the stage and kill them. Might involve exposing some additional API's in SparkContext.\n\nComments (10):\n1. ASF GitHub Bot: Github user sundeepn commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011156 @andrewor14 / @kayousterhout. Thanks for the comments. I have posted an additional commit with the changes. I have checked them in my branch but some reason they are not showing up here. Do you want me to issue another new pull request? Let me know. https://github.com/sundeepn/spark/commit/32c2ea58e5053623e5bf5955e52f7b7cd640ff64\n2. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011323 Build triggered. Build is starting -or- tests failed to complete.\n3. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011326 Build started. Build is starting -or- tests failed to complete.\n4. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011375 Build finished. Build is starting -or- tests failed to complete.\n5. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011376 Build is starting -or- tests failed to complete. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13581/\n6. ASF GitHub Bot: Github user kayousterhout commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39011464 It looks like github is just moving slowly today...the commit just got pulled in. I took another look at this and have a question: what happens for stages that are used for multiple jobs? Right now, stageIdToJobId in the UI code you added just maps a stage to a single job id. So, if stage0 is used by JobA and jobB, the ui code only stores one of these jobs, and then cancelJob() will only be called for one of the jobs. cancelJob() ultimately calls DAGScheduler.handleJobCancellation(), which only cancels the stages that are independent to the job. So, because stage0 is not independent to either of the jobs, it won't get cancelled. Did I misunderstand this?\n7. ASF GitHub Bot: Github user kayousterhout commented on a diff in the pull request: https://github.com/apache/spark/pull/246#discussion_r11095610 --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala --- @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener { val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]()) stages(stage.stageId) = stage + + // Extract Job ID and double check if we have the details + val jobId = Option(stageSubmitted.properties).flatMap { + p => Option(p.getProperty(\"spark.job.id\")) + }.getOrElse(\"-1\").toInt --- End diff -- When we chatted about this I remember you saying that this code is to handle the case where the stage runs locally at the driver...but from glancing at the DAGScheduler code, it looks like onStageSubmitted() never gets called for the locally-run tasks, and they never show up at the UI. When do you need this code / when will the jobIdToStageIds mapping not already be set up correctly by OnJobStart?\n8. ASF GitHub Bot: Github user sundeepn commented on a diff in the pull request: https://github.com/apache/spark/pull/246#discussion_r11095628 --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala --- @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener { val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]()) stages(stage.stageId) = stage + + // Extract Job ID and double check if we have the details + val jobId = Option(stageSubmitted.properties).flatMap { + p => Option(p.getProperty(\"spark.job.id\")) + }.getOrElse(\"-1\").toInt --- End diff -- Well, this is only to ensure we can handle things if we get any scenarios where the onJobStart does not arrive before stageSubmitted. I am not familiar with the scheduling code sufficiently to rule that out. If you are sure, I can take this out.\n9. ASF GitHub Bot: Github user kayousterhout commented on a diff in the pull request: https://github.com/apache/spark/pull/246#discussion_r11095648 --- Diff: core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala --- @@ -116,6 +118,16 @@ private[ui] class JobProgressListener(conf: SparkConf) extends SparkListener { val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashMap[Int, StageInfo]()) stages(stage.stageId) = stage + + // Extract Job ID and double check if we have the details + val jobId = Option(stageSubmitted.properties).flatMap { + p => Option(p.getProperty(\"spark.job.id\")) + }.getOrElse(\"-1\").toInt --- End diff -- Ah cool -- I looked at the ordering of the JobStart and StageSubmitted events more closely and I think you can safely remove this.\n10. ASF GitHub Bot: Github user sundeepn commented on the pull request: https://github.com/apache/spark/pull/246#issuecomment-39012183 > what happens for stages that are used for multiple jobs? I misunderstood our conversation on job to stage mapping the other day. As you see, the code will currently not handle multiple job mappings. Is there a simple example I can use to generate such a scenario?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "9f7e9548ed3869530c5f60d9046af5e4", "issue_key": "SPARK-1203", "issue_type": "Bug", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "spark-shell on yarn-client race in properly getting hdfs delegation tokens", "description": "There seems to be a race when using the spark-shell on yarn (yarn-client mode) as to when it gets the hdfs delegation tokens. It appears to be that if you do an action that causes it to get a delegation token before the workers are launched things work fine, if you want until after the workers launch you get an error about not having a delegation token.", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-03-07T11:31:02.000+0000", "updated": "2015-04-09T12:05:50.000+0000", "resolved": "2014-03-19T08:07:28.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Thomas Graves", "body": "So this appears to have been something environment related. Or perhaps the KDC was having issues. I can't reproduce it now.", "created": "2014-03-07T11:57:33.347+0000"}, {"author": "bc Wong", "body": "I just hit something like this, an intermittent failure with HdfsTest in yarn-client mode. Tom, does your error look like this:  14/03/14 17:57:15 INFO cluster.YarnClientSchedulerBackend: Application report from ASM: appMasterRpcPort: 0 appStartTime: 1394845025101 yarnAppState: RUNNING 14/03/14 17:57:17 INFO cluster.YarnClientClusterScheduler: YarnClientClusterScheduler.postStartHook done 14/03/14 17:57:17 INFO storage.MemoryStore: ensureFreeSpace(215427) called with curMem=0, maxMem=1383491174 14/03/14 17:57:17 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 210.4 KB, free 1319.2 MB) Exception in thread \"main\" org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6211) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:461) ... at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:920) at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1336) at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:527) at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:505) at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121) at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100) at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80) at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202)", "created": "2014-03-14T18:20:12.590+0000"}, {"author": "Thomas Graves", "body": "I'll give it a try again today to see if I can reproduce also.", "created": "2014-03-17T06:12:21.501+0000"}, {"author": "Thomas Graves", "body": "Ok I'm able to reproduce it again. Debbugging it.", "created": "2014-03-18T08:17:29.218+0000"}, {"author": "Thomas Graves", "body": "So I was able to reproduce this issue when doing a saveAsTextFile on the client. If you wait for a while before doing anything I was hitting this. If you do operations immediately it doesn't happen. It looks like we aren't propogating the credentials properly. I have not been able to reproduce it running HdfsTest. That is a different case since your reading the file. That should be propogating the credentials in SparkContext.hadoopRDD. if you have more details please let me know. You are sure you were kinit'd?", "created": "2014-03-18T12:03:36.000+0000"}, {"author": "Thomas Graves", "body": "Note the reason this works if you do it quickly is because the original connection to the namenode is up and it reuses it.", "created": "2014-03-18T12:41:21.598+0000"}, {"author": "Thomas Graves", "body": "https://github.com/apache/spark/pull/173", "created": "2014-03-18T12:44:45.473+0000"}, {"author": "bc Wong", "body": "I found out why I was having problem with HdfsTest. I was integrating on 0.9 and missed the changes that set YARN_CLIENT_MODE in a couple of places. I got that working. And I also verified the fix for this bug. Thanks a lot, Tom!", "created": "2014-03-19T13:38:07.665+0000"}, {"author": "gu-chi", "body": "Seems I meet with the similar issue using saveAsNewAPIHadoopDataset I tried to reproduce with create SC and then sleep for 10mins, then do saveAsNewAPIHadoopDataset, still not able to reproduce, can u give some advice on how to reproduce, thx", "created": "2015-04-09T12:05:50.928+0000"}], "num_comments": 9, "text": "Issue: SPARK-1203\nSummary: spark-shell on yarn-client race in properly getting hdfs delegation tokens\nDescription: There seems to be a race when using the spark-shell on yarn (yarn-client mode) as to when it gets the hdfs delegation tokens. It appears to be that if you do an action that causes it to get a delegation token before the workers are launched things work fine, if you want until after the workers launch you get an error about not having a delegation token.\n\nComments (9):\n1. Thomas Graves: So this appears to have been something environment related. Or perhaps the KDC was having issues. I can't reproduce it now.\n2. bc Wong: I just hit something like this, an intermittent failure with HdfsTest in yarn-client mode. Tom, does your error look like this:  14/03/14 17:57:15 INFO cluster.YarnClientSchedulerBackend: Application report from ASM: appMasterRpcPort: 0 appStartTime: 1394845025101 yarnAppState: RUNNING 14/03/14 17:57:17 INFO cluster.YarnClientClusterScheduler: YarnClientClusterScheduler.postStartHook done 14/03/14 17:57:17 INFO storage.MemoryStore: ensureFreeSpace(215427) called with curMem=0, maxMem=1383491174 14/03/14 17:57:17 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 210.4 KB, free 1319.2 MB) Exception in thread \"main\" org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6211) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:461) ... at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:920) at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1336) at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:527) at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:505) at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121) at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100) at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80) at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202)\n3. Thomas Graves: I'll give it a try again today to see if I can reproduce also.\n4. Thomas Graves: Ok I'm able to reproduce it again. Debbugging it.\n5. Thomas Graves: So I was able to reproduce this issue when doing a saveAsTextFile on the client. If you wait for a while before doing anything I was hitting this. If you do operations immediately it doesn't happen. It looks like we aren't propogating the credentials properly. I have not been able to reproduce it running HdfsTest. That is a different case since your reading the file. That should be propogating the credentials in SparkContext.hadoopRDD. if you have more details please let me know. You are sure you were kinit'd?\n6. Thomas Graves: Note the reason this works if you do it quickly is because the original connection to the namenode is up and it reuses it.\n7. Thomas Graves: https://github.com/apache/spark/pull/173\n8. bc Wong: I found out why I was having problem with HdfsTest. I was integrating on 0.9 and missed the changes that set YARN_CLIENT_MODE in a couple of places. I got that working. And I also verified the fix for this bug. Thanks a lot, Tom!\n9. gu-chi: Seems I meet with the similar issue using saveAsNewAPIHadoopDataset I tried to reproduce with create SC and then sleep for 10mins, then do saveAsNewAPIHadoopDataset, still not able to reproduce, can u give some advice on how to reproduce, thx", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "a4627c6361dbca0b1c55d16d5d657d5c", "issue_key": "SPARK-1204", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "EC2 scripts upload private key", "description": "The EC2 scripts upload the private key that you supply. At least, this needs to be clearly documented in --help. Ideally, only the public key would be uploaded to all the instances. For the instances to connect to one another, a fresh key should be generated (locally or on the master).", "reporter": "Jaka Jancar", "assignee": null, "created": "2014-03-07T12:03:05.000+0000", "updated": "2015-01-11T07:29:03.000+0000", "resolved": "2015-01-11T07:29:03.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Nicholas Chammas", "body": "cc [~shivaram] Do you know off the top of your head if this is still the case?", "created": "2015-01-10T17:14:15.363+0000"}, {"author": "Shivaram Venkataraman", "body": "No - This was fixed in https://github.com/apache/spark/commit/b98572c70ad3932381a55f23f82600d7e435d2eb We generate a new ssh key on the master for each cluster launch and then copy that to all the slave machines", "created": "2015-01-11T06:00:19.916+0000"}, {"author": "Nicholas Chammas", "body": "Resolving this issue per Shivaram's comment.", "created": "2015-01-11T07:29:03.282+0000"}], "num_comments": 3, "text": "Issue: SPARK-1204\nSummary: EC2 scripts upload private key\nDescription: The EC2 scripts upload the private key that you supply. At least, this needs to be clearly documented in --help. Ideally, only the public key would be uploaded to all the instances. For the instances to connect to one another, a fresh key should be generated (locally or on the master).\n\nComments (3):\n1. Nicholas Chammas: cc [~shivaram] Do you know off the top of your head if this is still the case?\n2. Shivaram Venkataraman: No - This was fixed in https://github.com/apache/spark/commit/b98572c70ad3932381a55f23f82600d7e435d2eb We generate a new ssh key on the master for each cluster launch and then copy that to all the slave machines\n3. Nicholas Chammas: Resolving this issue per Shivaram's comment.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "0813fafbc7e1419b05e6dd9ec11b5819", "issue_key": "SPARK-1205", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Clean up callSite/origin/generator", "description": "There is some overlap and very little documentation in these classes. It would be good to clarify their use and clean them up.", "reporter": "Patrick Wendell", "assignee": "Patrick Wendell", "created": "2014-03-08T12:01:56.000+0000", "updated": "2014-03-30T04:15:33.000+0000", "resolved": "2014-03-10T16:29:36.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1205\nSummary: Clean up callSite/origin/generator\nDescription: There is some overlap and very little documentation in these classes. It would be good to clarify their use and clean them up.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "04c6b0a03099ca47be25a0ebfde6bfd5", "issue_key": "SPARK-1206", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Add python support for average and other summary satistics", "description": "We have a number of summary statistics in the DoubleRDDFunctions.scala. We should implement these in python too.", "reporter": "Holden Karau", "assignee": null, "created": "2014-03-08T14:09:25.000+0000", "updated": "2014-11-14T10:27:32.000+0000", "resolved": "2014-11-14T10:27:32.000+0000", "labels": [], "components": [], "comments": [{"author": "Holden Karau", "body": "whoops this is allready done I just didn't look in the correct place.", "created": "2014-03-08T14:16:47.237+0000"}, {"author": "Andrew Ash", "body": "Closing per [~holdenk_amp] as already done.", "created": "2014-11-14T10:27:32.669+0000"}], "num_comments": 2, "text": "Issue: SPARK-1206\nSummary: Add python support for average and other summary satistics\nDescription: We have a number of summary statistics in the DoubleRDDFunctions.scala. We should implement these in python too.\n\nComments (2):\n1. Holden Karau: whoops this is allready done I just didn't look in the correct place.\n2. Andrew Ash: Closing per [~holdenk_amp] as already done.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "a2b4053574c4b0a95d4ecebbb840356e", "issue_key": "SPARK-1207", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Make python support for histograms", "description": "", "reporter": "Holden Karau", "assignee": null, "created": "2014-03-08T14:10:14.000+0000", "updated": "2014-07-27T01:31:02.000+0000", "resolved": "2014-07-27T01:31:02.000+0000", "labels": [], "components": [], "comments": [{"author": "Holden Karau", "body": "I'll give this a shot this weekend.", "created": "2014-03-08T14:10:38.311+0000"}], "num_comments": 1, "text": "Issue: SPARK-1207\nSummary: Make python support for histograms\n\nComments (1):\n1. Holden Karau: I'll give this a shot this weekend.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "a7da88433efe6e04b1d9e2483316aff8", "issue_key": "SPARK-1208", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "after some hours of working the :4040 monitoring UI stops working.", "description": "This issue is inconsistent, but it did not exist in prior versions. The Driver app otherwise works normally. The log file below is from the driver. 2014-03-09 07:24:55,837 WARN [qtp1187052686-17453] AbstractHttpConnection - /stages/ java.util.NoSuchElementException: None.get at scala.None$.get(Option.scala:313) at scala.None$.get(Option.scala:311) at org.apache.spark.ui.jobs.StageTable.org$apache$spark$ui$jobs$StageTable$$stageRow(StageTable.scala:114) at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39) at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39) at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57) at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.AbstractTraversable.map(Traversable.scala:105) at org.apache.spark.ui.jobs.StageTable.stageTable(StageTable.scala:57) at org.apache.spark.ui.jobs.StageTable.toNodeSeq(StageTable.scala:39) at org.apache.spark.ui.jobs.IndexPage.render(IndexPage.scala:81) at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59) at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59) at org.apache.spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) at org.eclipse.jetty.server.Server.handle(Server.java:363) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483) at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628) at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) at java.lang.Thread.run(Thread.java:662)", "reporter": "Tal Sliwowicz", "assignee": null, "created": "2014-03-08T23:28:53.000+0000", "updated": "2014-09-29T07:52:45.000+0000", "resolved": "2014-09-29T07:52:45.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "Hey just have a few questions to help narrow this down: - Are you using the fair scheduler here? - Does this happen when a large number of stages is present (e.g. 1000+)? - Do you get an exception one time, every time, or intermittantly when you access the jobs page? Looking at the code it looks like it can't correctly lookup the pool name. In theory the deletion of the stage itself and the entry in the lookup table corresponding to the stage should be atomic, but maybe one is somehow happening without the other.", "created": "2014-03-09T17:33:51.145+0000"}, {"author": "Tal Sliwowicz", "body": "Hi, 1. Yes - the FAIR scheduler, although I am almost positive this happened in FIFO (the default) too. If important, we can switch to FIFO and see. 2. Yes (10s of thousands, our job runs repeatedly on the same context), although this is issue is inconsistent. In most cases, even with a very large number of stages, this does not happen. 3. The issue is not easily reproducible. It does not occur always (I do not have a scenario to make it start happening), but once it starts happening, it is totally consistent in the sense that it happens always.", "created": "2014-03-10T00:57:52.689+0000"}, {"author": "Tal Sliwowicz", "body": "This issue does not happen with the FIFO scheduler.", "created": "2014-04-16T09:01:42.268+0000"}, {"author": "Sean R. Owen", "body": "This appears to be a similar, if not the same issue, as in SPARK-2643. The discussion in the PR indicates this was resolved by a subsequent change: https://github.com/apache/spark/pull/1854#issuecomment-55061571", "created": "2014-09-29T07:52:45.506+0000"}], "num_comments": 4, "text": "Issue: SPARK-1208\nSummary: after some hours of working the :4040 monitoring UI stops working.\nDescription: This issue is inconsistent, but it did not exist in prior versions. The Driver app otherwise works normally. The log file below is from the driver. 2014-03-09 07:24:55,837 WARN [qtp1187052686-17453] AbstractHttpConnection - /stages/ java.util.NoSuchElementException: None.get at scala.None$.get(Option.scala:313) at scala.None$.get(Option.scala:311) at org.apache.spark.ui.jobs.StageTable.org$apache$spark$ui$jobs$StageTable$$stageRow(StageTable.scala:114) at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39) at org.apache.spark.ui.jobs.StageTable$$anonfun$toNodeSeq$1.apply(StageTable.scala:39) at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57) at org.apache.spark.ui.jobs.StageTable$$anonfun$stageTable$1.apply(StageTable.scala:57) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.AbstractTraversable.map(Traversable.scala:105) at org.apache.spark.ui.jobs.StageTable.stageTable(StageTable.scala:57) at org.apache.spark.ui.jobs.StageTable.toNodeSeq(StageTable.scala:39) at org.apache.spark.ui.jobs.IndexPage.render(IndexPage.scala:81) at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59) at org.apache.spark.ui.jobs.JobProgressUI$$anonfun$getHandlers$3.apply(JobProgressUI.scala:59) at org.apache.spark.ui.JettyUtils$$anon$1.handle(JettyUtils.scala:61) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1040) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:976) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) at org.eclipse.jetty.server.Server.handle(Server.java:363) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483) at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:628) at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) at java.lang.Thread.run(Thread.java:662)\n\nComments (4):\n1. Patrick McFadin: Hey just have a few questions to help narrow this down: - Are you using the fair scheduler here? - Does this happen when a large number of stages is present (e.g. 1000+)? - Do you get an exception one time, every time, or intermittantly when you access the jobs page? Looking at the code it looks like it can't correctly lookup the pool name. In theory the deletion of the stage itself and the entry in the lookup table corresponding to the stage should be atomic, but maybe one is somehow happening without the other.\n2. Tal Sliwowicz: Hi, 1. Yes - the FAIR scheduler, although I am almost positive this happened in FIFO (the default) too. If important, we can switch to FIFO and see. 2. Yes (10s of thousands, our job runs repeatedly on the same context), although this is issue is inconsistent. In most cases, even with a very large number of stages, this does not happen. 3. The issue is not easily reproducible. It does not occur always (I do not have a scenario to make it start happening), but once it starts happening, it is totally consistent in the sense that it happens always.\n3. Tal Sliwowicz: This issue does not happen with the FIFO scheduler.\n4. Sean R. Owen: This appears to be a similar, if not the same issue, as in SPARK-2643. The discussion in the PR indicates this was resolved by a subsequent change: https://github.com/apache/spark/pull/1854#issuecomment-55061571", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "9e56b426dea1257bcda1cbab0e0f84c0", "issue_key": "SPARK-1209", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "SparkHadoop{MapRed,MapReduce}Util should not use package org.apache.hadoop", "description": "It's private, so the change won't break compatibility", "reporter": "Sandy Ryza", "assignee": "Sean R. Owen", "created": "2014-03-09T22:19:12.000+0000", "updated": "2015-01-15T09:08:41.000+0000", "resolved": "2014-10-30T22:57:43.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Mark Grover", "body": "I did notice the same thing and did start some work on it: https://github.com/markgrover/spark/compare/master...package_fix I haven't tried compiling it yet but I will do so soonish. If you don't mind, I'd love to get this JIRA assigned to me. Thanks for filing it!", "created": "2014-03-09T22:24:54.641+0000"}, {"author": "Mark Grover", "body": "Thanks Sandy!", "created": "2014-03-09T22:29:50.751+0000"}, {"author": "Sandy Ryza", "body": "It doesn't look like this was actually fixed.", "created": "2014-06-19T01:33:16.227+0000"}, {"author": "Mark Grover", "body": "ok, I will take over. Thanks Sandy.", "created": "2014-06-20T03:41:13.193+0000"}, {"author": "Sean R. Owen", "body": "Yes, I wonder too, does SparkHadoopMapRedUtil and SparkHadoopMapReduceUtil need to live in {{org.apache.hadoop}} anymore? I assume they may have in the past to access some package-private Hadoop code. But I've tried moving them under {{org.apache.spark}} and compiling versus a few Hadoop versions and it all seems fine. Am I missing something or is this worth changing? it's private to Spark (well, org.apache right now by necessity) so think it's fair game to move. See https://github.com/srowen/spark/tree/SPARK-1209", "created": "2014-10-13T19:50:31.485+0000"}, {"author": "Sandy Ryza", "body": "Definitely worth changing, in my opinion. This has been bothering me for a while", "created": "2014-10-13T19:57:14.954+0000"}, {"author": "Patrick Wendell", "body": "It's possible this previously used package-private code that it doesn't need any more. I agree it's better not to pollute the Hadoop namespace, but I also think many people use this and I don't think it would be justified to break this API for the purpose of cleanliness of our own code. IIRC we tried to make this private[spark] earlier but people asked to open it up. I think it would be okay to deprecate the old one and add forwarder methods to a new thing inside of the spark package. But wholesale moving it IMO isn't justified for the purpose of cleanliness alone.", "created": "2014-10-14T00:36:53.253+0000"}, {"author": "Patrick Wendell", "body": "Actually a full set of forwarders might be overkill anyways since there's not much code.", "created": "2014-10-14T00:38:56.023+0000"}, {"author": "Sean R. Owen", "body": "Hm, it's {{private[apache]}} though. Couldn't this only be used by people writing in the {{org.apache}} namespace? naturally a project might do just this to access this code, but I hadn't though this was promised as an stable API. People that pull this trick can I suppose declare their hack in {{org.apache.spark}}, although that's a source change. I can set up a forwarder and deprecate to see how that looks but wanted to check if it's really these classes in question that are being used outside Spark.", "created": "2014-10-14T07:17:39.825+0000"}, {"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/2814", "created": "2014-10-15T15:37:12.128+0000"}, {"author": "Patrick Wendell", "body": "Hey Sean the class I thought this pertained to was SparkHadoopUtil: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "created": "2014-10-15T15:50:16.162+0000"}, {"author": "Sean R. Owen", "body": "... and why wouldn't you, that's the title of the JIRA, oops. It's not that class that moves or even changes actually, and yes it should not move. Let me fix the title and fix my PR too. Maybe that's a more palatable change.", "created": "2014-10-15T16:44:22.218+0000"}, {"author": "Patrick Wendell", "body": "Got it! Yeah so anything that is internal, of course would be great to move it to the Spark namespace if it's not necessary to be in Hadoop.", "created": "2014-10-16T03:17:03.776+0000"}, {"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3048", "created": "2014-11-01T11:44:55.499+0000"}], "num_comments": 14, "text": "Issue: SPARK-1209\nSummary: SparkHadoop{MapRed,MapReduce}Util should not use package org.apache.hadoop\nDescription: It's private, so the change won't break compatibility\n\nComments (14):\n1. Mark Grover: I did notice the same thing and did start some work on it: https://github.com/markgrover/spark/compare/master...package_fix I haven't tried compiling it yet but I will do so soonish. If you don't mind, I'd love to get this JIRA assigned to me. Thanks for filing it!\n2. Mark Grover: Thanks Sandy!\n3. Sandy Ryza: It doesn't look like this was actually fixed.\n4. Mark Grover: ok, I will take over. Thanks Sandy.\n5. Sean R. Owen: Yes, I wonder too, does SparkHadoopMapRedUtil and SparkHadoopMapReduceUtil need to live in {{org.apache.hadoop}} anymore? I assume they may have in the past to access some package-private Hadoop code. But I've tried moving them under {{org.apache.spark}} and compiling versus a few Hadoop versions and it all seems fine. Am I missing something or is this worth changing? it's private to Spark (well, org.apache right now by necessity) so think it's fair game to move. See https://github.com/srowen/spark/tree/SPARK-1209\n6. Sandy Ryza: Definitely worth changing, in my opinion. This has been bothering me for a while\n7. Patrick Wendell: It's possible this previously used package-private code that it doesn't need any more. I agree it's better not to pollute the Hadoop namespace, but I also think many people use this and I don't think it would be justified to break this API for the purpose of cleanliness of our own code. IIRC we tried to make this private[spark] earlier but people asked to open it up. I think it would be okay to deprecate the old one and add forwarder methods to a new thing inside of the spark package. But wholesale moving it IMO isn't justified for the purpose of cleanliness alone.\n8. Patrick Wendell: Actually a full set of forwarders might be overkill anyways since there's not much code.\n9. Sean R. Owen: Hm, it's {{private[apache]}} though. Couldn't this only be used by people writing in the {{org.apache}} namespace? naturally a project might do just this to access this code, but I hadn't though this was promised as an stable API. People that pull this trick can I suppose declare their hack in {{org.apache.spark}}, although that's a source change. I can set up a forwarder and deprecate to see how that looks but wanted to check if it's really these classes in question that are being used outside Spark.\n10. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/2814", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "eb5a1280256934dc48dc355e79544ac9", "issue_key": "SPARK-1216", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add a OneHotEncoder for handling categorical features", "description": "It would be nice to add something to MLLib to make it easy to do one-of-K encoding of categorical features. Something like: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-03-09T23:38:11.000+0000", "updated": "2015-02-26T11:23:24.000+0000", "resolved": "2015-02-26T11:23:24.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Martin Jaggi", "body": "could merge these two issues: https://issues.apache.org/jira/browse/SPARK-1303", "created": "2014-04-12T20:19:59.484+0000"}, {"author": "Sean R. Owen", "body": "[~sandyr] This is basically https://issues.apache.org/jira/browse/SPARK-4081 and Joseph has a PR for it now?", "created": "2014-11-11T15:08:47.771+0000"}, {"author": "Joseph K. Bradley", "body": "(Addressing old comments I just saw now...) [~jaggi] I'd recommend keeping this separate from [https://issues.apache.org/jira/browse/SPARK-1303] since it is a different kind of transformation. [~srowen] This is a bit different from [SPARK-4081]; see the comment in my PR: [https://github.com/apache/spark/pull/3000#issuecomment-62630207]. I'd recommend keeping them separate. I don't immediately see a good way to organize these, but it could be worth discussing.", "created": "2014-12-15T23:18:31.887+0000"}, {"author": "Sean R. Owen", "body": "I think this is duplicated by a similar JIRA for the new API.", "created": "2015-02-26T11:23:24.050+0000"}], "num_comments": 4, "text": "Issue: SPARK-1216\nSummary: Add a OneHotEncoder for handling categorical features\nDescription: It would be nice to add something to MLLib to make it easy to do one-of-K encoding of categorical features. Something like: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n\nComments (4):\n1. Martin Jaggi: could merge these two issues: https://issues.apache.org/jira/browse/SPARK-1303\n2. Sean R. Owen: [~sandyr] This is basically https://issues.apache.org/jira/browse/SPARK-4081 and Joseph has a PR for it now?\n3. Joseph K. Bradley: (Addressing old comments I just saw now...) [~jaggi] I'd recommend keeping this separate from [https://issues.apache.org/jira/browse/SPARK-1303] since it is a different kind of transformation. [~srowen] This is a bit different from [SPARK-4081]; see the comment in my PR: [https://github.com/apache/spark/pull/3000#issuecomment-62630207]. I'd recommend keeping them separate. I don't immediately see a good way to organize these, but it could be worth discussing.\n4. Sean R. Owen: I think this is duplicated by a similar JIRA for the new API.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "df864a2d891ec4d949ae98b848e9b75a", "issue_key": "SPARK-1210", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Prevent ContextClassLoader of Actor from becoming ClassLoader of Executor", "description": "Constructor of {{org.apache.spark.executor.Executor}} should not set context class loader of current thread, which is backend Actor's thread. Run the following code in local-mode REPL.  scala> case class Foo(i: Int) scala> val ret = sc.parallelize((1 to 100).map(Foo), 10).collect  This causes errors as follows:  ERROR actor.OneForOneStrategy: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo; java.lang.ArrayStoreException: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo; at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870) at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56) at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859) at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616) at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  This is because the class loaders to deserialize result {{Foo}} instances might be different from backend Actor's, and the Actor's class loader should be the same as Driver's. See PR: https://github.com/apache/spark/pull/15", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "created": "2014-03-10T01:34:36.000+0000", "updated": "2014-03-28T10:32:05.000+0000", "resolved": "2014-03-27T22:18:09.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "\\[edited from an earlier comment\\] Just to clarify there are two different issues going on here. One is that the classloader used in the executors is not set-up to correctly delegate to the active classloader in the thread that created it. Instead it delegates to the class loader of the class that defines Executor.scala. This is incorrect delegation and results in a problem where, when running in local mode, the Executor things there isn't an existing definition of a case class defined in the repl (e.g. Foo) and it goes and gets one over the HTTP served and ends up with a technically different definition of Foo. When that ends up back in the driver code it thinks the types are different and freaks out. A second issue is that there is code that, when the executor is created, sets the ClassLoader of the thread in which the Executor is created. This is not actually needed because user classes can only be created/referenced inside of the run() method of a TaskRunner. Also in some cases, such as when an Executor is created inside of an Actor with multiple threads, this leads to an inconstent state amongst threads in which executor code is executed. This can have bad consequences when the Executor and the DAGScheduler are sharing an ActorSystem because the DAGScheduler's classloader can get redefined. The code was added here: https://github.com/apache/spark/commit/b864c36a#diff-40f975518e47d34cc8ecd7a49440228dR50", "created": "2014-03-26T10:28:54.159+0000"}, {"author": "Patrick McFadin", "body": "I merged this into master. This might be worth back porting into 0.9 at some point.", "created": "2014-03-27T22:19:21.268+0000"}, {"author": "Patrick McFadin", "body": "I've created SPARK-1346 to track back-porting this into 0.9.2", "created": "2014-03-28T10:32:05.353+0000"}], "num_comments": 3, "text": "Issue: SPARK-1210\nSummary: Prevent ContextClassLoader of Actor from becoming ClassLoader of Executor\nDescription: Constructor of {{org.apache.spark.executor.Executor}} should not set context class loader of current thread, which is backend Actor's thread. Run the following code in local-mode REPL.  scala> case class Foo(i: Int) scala> val ret = sc.parallelize((1 to 100).map(Foo), 10).collect  This causes errors as follows:  ERROR actor.OneForOneStrategy: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo; java.lang.ArrayStoreException: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo; at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870) at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870) at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56) at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859) at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616) at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  This is because the class loaders to deserialize result {{Foo}} instances might be different from backend Actor's, and the Actor's class loader should be the same as Driver's. See PR: https://github.com/apache/spark/pull/15\n\nComments (3):\n1. Patrick McFadin: \\[edited from an earlier comment\\] Just to clarify there are two different issues going on here. One is that the classloader used in the executors is not set-up to correctly delegate to the active classloader in the thread that created it. Instead it delegates to the class loader of the class that defines Executor.scala. This is incorrect delegation and results in a problem where, when running in local mode, the Executor things there isn't an existing definition of a case class defined in the repl (e.g. Foo) and it goes and gets one over the HTTP served and ends up with a technically different definition of Foo. When that ends up back in the driver code it thinks the types are different and freaks out. A second issue is that there is code that, when the executor is created, sets the ClassLoader of the thread in which the Executor is created. This is not actually needed because user classes can only be created/referenced inside of the run() method of a TaskRunner. Also in some cases, such as when an Executor is created inside of an Actor with multiple threads, this leads to an inconstent state amongst threads in which executor code is executed. This can have bad consequences when the Executor and the DAGScheduler are sharing an ActorSystem because the DAGScheduler's classloader can get redefined. The code was added here: https://github.com/apache/spark/commit/b864c36a#diff-40f975518e47d34cc8ecd7a49440228dR50\n2. Patrick McFadin: I merged this into master. This might be worth back porting into 0.9 at some point.\n3. Patrick McFadin: I've created SPARK-1346 to track back-porting this into 0.9.2", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "e8f7948293f0203a45c252839a85203d", "issue_key": "SPARK-1211", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "In ApplicationMaster, set spark.master system property to \"yarn-cluster\"", "description": "This would make it so that users don't need to pass it in to their SparkConf. It won't break anything for apps that already pass it in.", "reporter": "Sanford Ryza", "assignee": "Sandy Ryza", "created": "2014-03-10T16:18:35.000+0000", "updated": "2014-04-04T20:50:50.000+0000", "resolved": "2014-03-10T18:41:14.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Sanford Ryza", "body": "https://github.com/apache/spark/pull/118", "created": "2014-03-10T16:30:01.696+0000"}], "num_comments": 1, "text": "Issue: SPARK-1211\nSummary: In ApplicationMaster, set spark.master system property to \"yarn-cluster\"\nDescription: This would make it so that users don't need to pass it in to their SparkConf. It won't break anything for apps that already pass it in.\n\nComments (1):\n1. Sanford Ryza: https://github.com/apache/spark/pull/118", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "a4754bc8108dce6bfe6cf170428df550", "issue_key": "SPARK-1230", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Enable SparkContext.addJars() to load classes not in CLASSPATH", "description": "", "reporter": "Patrick Wendell", "assignee": null, "created": "2014-03-10T22:46:26.000+0000", "updated": "2014-05-15T18:31:17.000+0000", "resolved": "2014-05-15T18:31:17.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Patrick Wendell", "body": "I forget what this actually means (hah) so I'm gonna close it for now.", "created": "2014-05-15T18:31:17.170+0000"}], "num_comments": 1, "text": "Issue: SPARK-1230\nSummary: Enable SparkContext.addJars() to load classes not in CLASSPATH\n\nComments (1):\n1. Patrick Wendell: I forget what this actually means (hah) so I'm gonna close it for now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "fa51d946cf4c212abc3ac2011a34618f", "issue_key": "SPARK-1231", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "DEAD worker should recover automaticly", "description": "master should send a response when DEAD worker sending a heartbeat. so worker could clean all applications and drivers and re-register itself to master.", "reporter": "Tianyi Wang", "assignee": null, "created": "2014-03-12T06:07:34.000+0000", "updated": "2014-11-14T10:46:45.000+0000", "resolved": "2014-11-14T10:46:45.000+0000", "labels": ["dead", "recover", "worker"], "components": ["Deploy"], "comments": [{"author": "Andrew Ash", "body": "Sorry [~tianyi], when I did my search for prior tickets in SPARK-3736 I didn't find this one. The issue of workers not recovering when disconnected has since been resolved though and will be released as part of Spark 1.2.0, so I'm closing this ticket as a duplicate. Please let us know if you have any troubles with that implementation once you start testing it. Thanks for reporting the bug! Andrew", "created": "2014-11-14T10:46:38.747+0000"}], "num_comments": 1, "text": "Issue: SPARK-1231\nSummary: DEAD worker should recover automaticly\nDescription: master should send a response when DEAD worker sending a heartbeat. so worker could clean all applications and drivers and re-register itself to master.\n\nComments (1):\n1. Andrew Ash: Sorry [~tianyi], when I did my search for prior tickets in SPARK-3736 I didn't find this one. The issue of workers not recovering when disconnected has since been resolved though and will be released as part of Spark 1.2.0, so I'm closing this ticket as a duplicate. Please let us know if you have any troubles with that implementation once you start testing it. Thanks for reporting the bug! Andrew", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "1ace5061a88aaf6b6399ea92218f04a9", "issue_key": "SPARK-1232", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "maven hadoop 0.23 yarn-alpha build broken", "description": "The maven hadoop 0.23 yarn build was broken. It looks like the avro dependency got removed by: https://github.com/apache/spark/pull/91/", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-03-12T08:17:49.000+0000", "updated": "2014-03-31T14:29:57.000+0000", "resolved": "2014-03-31T14:29:57.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Thomas Graves", "body": "broken by SPARK-1193", "created": "2014-03-20T12:18:32.397+0000"}], "num_comments": 1, "text": "Issue: SPARK-1232\nSummary: maven hadoop 0.23 yarn-alpha build broken\nDescription: The maven hadoop 0.23 yarn build was broken. It looks like the avro dependency got removed by: https://github.com/apache/spark/pull/91/\n\nComments (1):\n1. Thomas Graves: broken by SPARK-1193", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "6ac9cb19c1c2b205b97ac6ad5ac1f0a4", "issue_key": "SPARK-1233", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "spark on hadoop 0.23 yarn fails to run: java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH", "description": "Trying to run on Yarn (hadoop 0.23) I get the following exception: Exception in thread \"main\" java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH at java.lang.Class.getField(Class.java:1569) at org.apache.spark.deploy.yarn.ClientBase$.getDefaultMRApplicationClasspath(ClientBase.scala:417) at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393) at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.deploy.yarn.ClientBase$.populateHadoopClasspath(ClientBase.scala:392) at org.apache.spark.deploy.yarn.ClientBase$.populateClasspath(ClientBase.scala:444) at org.apache.spark.deploy.yarn.ClientBase$class.setupLaunchEnv(ClientBase.scala:274) at org.apache.spark.deploy.yarn.Client.setupLaunchEnv(Client.scala:37) at org.apache.spark.deploy.yarn.Client.runApp(Client.scala:67) at org.apache.spark.deploy.yarn.Client.run(Client.scala:84) at org.apache.spark.deploy.yarn.Client$.main(Client.scala:177) at org.apache.spark.deploy.yarn.Client.main(Client.scala)", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-03-12T09:05:08.000+0000", "updated": "2014-03-20T12:17:19.000+0000", "resolved": "2014-03-12T14:57:33.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Thomas Graves", "body": "https://github.com/apache/spark/pull/129 broken by https://github.com/apache/spark/pull/102 -> SPARK-1064", "created": "2014-03-20T12:17:04.301+0000"}], "num_comments": 1, "text": "Issue: SPARK-1233\nSummary: spark on hadoop 0.23 yarn fails to run: java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH\nDescription: Trying to run on Yarn (hadoop 0.23) I get the following exception: Exception in thread \"main\" java.lang.NoSuchFieldException: DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH at java.lang.Class.getField(Class.java:1569) at org.apache.spark.deploy.yarn.ClientBase$.getDefaultMRApplicationClasspath(ClientBase.scala:417) at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393) at org.apache.spark.deploy.yarn.ClientBase$$anonfun$6.apply(ClientBase.scala:393) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.deploy.yarn.ClientBase$.populateHadoopClasspath(ClientBase.scala:392) at org.apache.spark.deploy.yarn.ClientBase$.populateClasspath(ClientBase.scala:444) at org.apache.spark.deploy.yarn.ClientBase$class.setupLaunchEnv(ClientBase.scala:274) at org.apache.spark.deploy.yarn.Client.setupLaunchEnv(Client.scala:37) at org.apache.spark.deploy.yarn.Client.runApp(Client.scala:67) at org.apache.spark.deploy.yarn.Client.run(Client.scala:84) at org.apache.spark.deploy.yarn.Client$.main(Client.scala:177) at org.apache.spark.deploy.yarn.Client.main(Client.scala)\n\nComments (1):\n1. Thomas Graves: https://github.com/apache/spark/pull/129 broken by https://github.com/apache/spark/pull/102 -> SPARK-1064", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "73b5b32f5c82aa7ffc25ccfd8607aeca", "issue_key": "SPARK-1234", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "clean up typos and grammar issues in Spark on YARN page", "description": "The \"Launch spark application with yarn-client mode\" section of this of this page has several incomplete sentences, typos, etc.etc. http://spark.incubator.apache.org/docs/latest/running-on-yarn.html", "reporter": "Diana Carroll", "assignee": null, "created": "2014-03-12T10:03:51.000+0000", "updated": "2014-10-13T17:58:57.000+0000", "resolved": "2014-10-13T17:58:57.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Diana Carroll", "body": "pull request 130", "created": "2014-03-12T10:50:21.206+0000"}, {"author": "Sean R. Owen", "body": "Given the discussion in https://github.com/apache/spark/pull/130 , this was abandoned, but I also don't see the bad text on that page anymore anyhow. It probably got improved in another subsequent update.", "created": "2014-10-13T17:58:57.072+0000"}], "num_comments": 2, "text": "Issue: SPARK-1234\nSummary: clean up typos and grammar issues in Spark on YARN page\nDescription: The \"Launch spark application with yarn-client mode\" section of this of this page has several incomplete sentences, typos, etc.etc. http://spark.incubator.apache.org/docs/latest/running-on-yarn.html\n\nComments (2):\n1. Diana Carroll: pull request 130\n2. Sean R. Owen: Given the discussion in https://github.com/apache/spark/pull/130 , this was abandoned, but I also don't see the bad text on that page anymore anyhow. It probably got improved in another subsequent update.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "a1c6c603cfc14e00d12bf25b40b427e8", "issue_key": "SPARK-1235", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "DAGScheduler ignores exceptions thrown in handleTaskCompletion", "description": "If an exception gets thrown in the handleTaskCompletion method, the method exits, but the exception is caught somewhere (not clear where) and the DAGScheduler keeps running. Jobs hang as a result -- because not all of the task completion code gets run. This was first reported by Brad Miller on the mailing list: http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-pyspark-crash-on-mesos-td2256.html and this behavior seems to have changed since 0.8 (when, based on Brad's description, it sounds like an exception in handleTaskCompletion would cause the DAGScheduler to crash), suggesting that this may be related to the Scala 2.10.3. To reproduce this problem, add \"throw new Exception(\"foo\")\" anywhere in handleTaskCompletion and run any job locally. The job will hang and you can see the exception get printed in the logs.", "reporter": "Kay Ousterhout", "assignee": "Nan Zhu", "created": "2014-03-12T13:05:56.000+0000", "updated": "2014-04-25T23:06:13.000+0000", "resolved": "2014-04-25T23:06:08.000+0000", "labels": [], "components": [], "comments": [{"author": "Mark Hamstra", "body": "A big, relevant difference post-0.8 is that the DAGScheduler event loop was replaced with the eventProcessActor. That means that an uncaught exception thrown during that actor's processing of a message will kill that actor and restart a new one (while preserving the existing message queue without the message that resulted in the prior actor's death.) You can see some of the logging of that happening in Brad's 0.9 stack trace that shows \"ERROR OneForOneStrategy\" -- see http://doc.akka.io/docs/akka/2.0.5/general/supervision.html. At least in current versions of Akka, the handling of the actor's death and any necessary cleanup before restarting a fresh eventProcessActor would need to be done in the preRestart and postRestart hooks -- but I'm not 100% certain that the techniques are the same when using the Akka versions that Spark does.", "created": "2014-03-12T14:52:18.944+0000"}, {"author": "Nan Zhu", "body": "Hi, I will work on this issue Actually the problem is more about handleTaskCompletion, if any exception happens during the event processing of DAGScheduler, the system will hang, we need the fault-tolerance strategy when implement the akka actor", "created": "2014-03-19T18:51:52.630+0000"}, {"author": "Nan Zhu", "body": "PR: https://github.com/apache/spark/pull/186", "created": "2014-03-20T05:42:44.783+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38996202 Merged build triggered. Build is starting -or- tests failed to complete.", "created": "2014-03-29T13:57:30.140+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38996211 Merged build started. Build is starting -or- tests failed to complete.", "created": "2014-03-29T13:57:39.852+0000"}, {"author": "ASF GitHub Bot", "body": "Github user CodingCat commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38997523 It seems that when Jenkins is very busy, some weird thing can happen, in the last test, DAGScheduler even failed to create eventProcessingActor... I'm retesting it", "created": "2014-03-29T14:46:49.495+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38997636 All automated tests passed. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13572/", "created": "2014-03-29T14:50:17.430+0000"}, {"author": "ASF GitHub Bot", "body": "Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38997635 Merged build finished. All automated tests passed.", "created": "2014-03-29T14:50:17.434+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Resolved in https://github.com/apache/spark/pull/186.", "created": "2014-04-25T23:06:08.720+0000"}], "num_comments": 9, "text": "Issue: SPARK-1235\nSummary: DAGScheduler ignores exceptions thrown in handleTaskCompletion\nDescription: If an exception gets thrown in the handleTaskCompletion method, the method exits, but the exception is caught somewhere (not clear where) and the DAGScheduler keeps running. Jobs hang as a result -- because not all of the task completion code gets run. This was first reported by Brad Miller on the mailing list: http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-pyspark-crash-on-mesos-td2256.html and this behavior seems to have changed since 0.8 (when, based on Brad's description, it sounds like an exception in handleTaskCompletion would cause the DAGScheduler to crash), suggesting that this may be related to the Scala 2.10.3. To reproduce this problem, add \"throw new Exception(\"foo\")\" anywhere in handleTaskCompletion and run any job locally. The job will hang and you can see the exception get printed in the logs.\n\nComments (9):\n1. Mark Hamstra: A big, relevant difference post-0.8 is that the DAGScheduler event loop was replaced with the eventProcessActor. That means that an uncaught exception thrown during that actor's processing of a message will kill that actor and restart a new one (while preserving the existing message queue without the message that resulted in the prior actor's death.) You can see some of the logging of that happening in Brad's 0.9 stack trace that shows \"ERROR OneForOneStrategy\" -- see http://doc.akka.io/docs/akka/2.0.5/general/supervision.html. At least in current versions of Akka, the handling of the actor's death and any necessary cleanup before restarting a fresh eventProcessActor would need to be done in the preRestart and postRestart hooks -- but I'm not 100% certain that the techniques are the same when using the Akka versions that Spark does.\n2. Nan Zhu: Hi, I will work on this issue Actually the problem is more about handleTaskCompletion, if any exception happens during the event processing of DAGScheduler, the system will hang, we need the fault-tolerance strategy when implement the akka actor\n3. Nan Zhu: PR: https://github.com/apache/spark/pull/186\n4. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38996202 Merged build triggered. Build is starting -or- tests failed to complete.\n5. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38996211 Merged build started. Build is starting -or- tests failed to complete.\n6. ASF GitHub Bot: Github user CodingCat commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38997523 It seems that when Jenkins is very busy, some weird thing can happen, in the last test, DAGScheduler even failed to create eventProcessingActor... I'm retesting it\n7. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38997636 All automated tests passed. Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13572/\n8. ASF GitHub Bot: Github user AmplabJenkins commented on the pull request: https://github.com/apache/spark/pull/186#issuecomment-38997635 Merged build finished. All automated tests passed.\n9. Matei Alexandru Zaharia: Resolved in https://github.com/apache/spark/pull/186.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "3ed1b0b5bb3571e521313f5bb74203b0", "issue_key": "SPARK-1236", "issue_type": "Dependency upgrade", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Update Jetty to 9", "description": "See https://github.com/apache/spark/pull/113", "reporter": "Reynold Xin", "assignee": "Andrew Or", "created": "2014-03-12T16:30:49.000+0000", "updated": "2014-11-05T10:45:22.000+0000", "resolved": "2014-04-01T17:13:46.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "We merged this then decided to revert it due to a JVM 7 dependency. But let's try to at least upgrade to Jetty 8 before Spark 1.0", "created": "2014-03-25T16:30:41.496+0000"}], "num_comments": 1, "text": "Issue: SPARK-1236\nSummary: Update Jetty to 9\nDescription: See https://github.com/apache/spark/pull/113\n\nComments (1):\n1. Patrick McFadin: We merged this then decided to revert it due to a JVM 7 dependency. But let's try to at least upgrade to Jetty 8 before Spark 1.0", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "a7a7d883b7b7d287578133d8df55b7c7", "issue_key": "SPARK-1237", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Implicit ALS is not efficient in computing YtY", "description": "Computing YtY can be implemented using BLAS's DSPR operations instead of generating y_i y_i^T and then combining them.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-03-12T18:01:11.000+0000", "updated": "2014-03-18T19:21:18.000+0000", "resolved": "2014-03-13T00:45:04.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/spark/pull/131", "created": "2014-03-12T18:47:35.315+0000"}, {"author": "Xiangrui Meng", "body": "Merged.", "created": "2014-03-13T00:45:04.905+0000"}], "num_comments": 2, "text": "Issue: SPARK-1237\nSummary: Implicit ALS is not efficient in computing YtY\nDescription: Computing YtY can be implemented using BLAS's DSPR operations instead of generating y_i y_i^T and then combining them.\n\nComments (2):\n1. Xiangrui Meng: PR: https://github.com/apache/spark/pull/131\n2. Xiangrui Meng: Merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "7834762f2a669a3b315899112f5e63ca", "issue_key": "SPARK-1238", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "User should be able to set a random seed in ALS", "description": "In order to reproduce results, we should allow users to set a random seed in ALS.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-03-12T18:02:35.000+0000", "updated": "2014-03-18T19:21:29.000+0000", "resolved": "2014-03-13T00:45:33.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/spark/pull/131", "created": "2014-03-12T18:47:41.771+0000"}], "num_comments": 1, "text": "Issue: SPARK-1238\nSummary: User should be able to set a random seed in ALS\nDescription: In order to reproduce results, we should allow users to set a random seed in ALS.\n\nComments (1):\n1. Xiangrui Meng: PR: https://github.com/apache/spark/pull/131", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "5d688ca8ee6e4f540d4049edfa75824a", "issue_key": "SPARK-1239", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Improve fetching of map output statuses", "description": "Instead we should modify the way we fetch map output statuses to take both a mapper and a reducer - or we should just piggyback the statuses on each task.", "reporter": "Patrick Wendell", "assignee": "Thomas Graves", "created": "2014-03-12T18:55:23.000+0000", "updated": "2018-09-07T15:27:33.000+0000", "resolved": "2016-05-07T02:31:46.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Patrick McFadin", "body": "Hey [~CodingCat] I actually asked [~andrewor] to look at this already", "created": "2014-03-20T09:54:13.830+0000"}, {"author": "Nan Zhu", "body": "Oh, that's fine", "created": "2014-03-20T10:14:33.307+0000"}, {"author": "bc Wong", "body": "Is anyone working on this? If so, could someone please update the jira? Thanks!", "created": "2014-08-31T05:47:54.257+0000"}, {"author": "Patrick Wendell", "body": "I think [~andrewor] has this in his backlog but it's not actively being worked on. [~bcwalrus] do you or [~sandyr] want to take a crack?", "created": "2014-09-02T05:56:46.369+0000"}, {"author": "Kostas Sakellis", "body": "[~pwendell] I'd like to take a crack at this since it is affecting one of our customers.", "created": "2014-09-06T00:28:32.451+0000"}, {"author": "Andrew Or", "body": "I have reassigned it to you Kostas.", "created": "2014-09-08T22:30:51.764+0000"}, {"author": "Andrew Ash", "body": "For large statuses, would we expect that to exceed {{spark.akka.frameSize}} and cause the below exception?  2014-09-14T01:34:21.305 ERROR [spark-akka.actor.default-dispatcher-4] org.apache.spark.MapOutputTrackerMasterActor - Map output statuses were 13920119 bytes which exceeds spark.akka.frameSize (10485760 bytes).", "created": "2014-09-15T17:27:26.808+0000"}, {"author": "Patrick Wendell", "body": "Yes, the current state of the art is to just increase the frame size.", "created": "2014-09-15T17:54:20.926+0000"}, {"author": "DB Tsai", "body": "+1, we run into this issue as well.", "created": "2014-10-08T12:31:57.031+0000"}, {"author": "Josh Rosen", "body": "I'm re-assigning this to me since I've been working in this code recently and plan to completely re-write MapOutputStatusTracker to address this and a few other issues. I like the idea of piggybacking the status on the task launching RPC itself; if we took the approach of having reducers specify their reduce ids when fetching the statuses, then this would add an extra RPC for every task launch, which could result in a big latency increase.", "created": "2014-10-23T01:18:55.484+0000"}, {"author": "Kostas Sakellis", "body": "Apologies for not commenting on this JIRA sooner but since I'm new to Spark, it has taken a bit of time to wrap my head around how the different schedulers (DAG and Task) interoperate. I'm currently investigating completely removing the MapOutputTracker class. Instead, the DAGScheduler can push down the required map status' to the next stage. I'm thinking of modifying the DAGScheduler.submitMissingTasks(..) to take in (or query) the mapStatus' of the previous completed stage. When a new ShuffleMapTask gets created, we pass the filtered mapStatus' necessary for that task. The map status data can be stored in the TaskContext and used in the BlockStoreShuffleFetcher when block data is being read (currently uses the MapOutputTracker). What I'm investigating currently is how to filter the data when I create the ShuffleMapTask - the BlockStoreShuffleFetcher uses a shuffleId which I don't seem to have access to in the DAGScheduler. I haven't yet written any code to test this out so if anyone has any concerns please let me know. Also, [~joshrosen] pointed out that the size of the map output structure even after it has been filtered could still be very large. This is worth investigating.", "created": "2014-10-23T22:49:56.268+0000"}, {"author": "Patrick Wendell", "body": "Hey Kostas - there are a few other bugs that required a refactoring to solve them that will subsume this issue, so I think that's why [~joshrosen] is grabbing it. This change will involve fairly significant surgery to a very complex part of Spark, so it might not be the best beginner issue. If Josh's other changes don't end up fixing this, we can leave this open as an independent issue and see if it can be done surgically.", "created": "2014-10-23T22:57:35.180+0000"}, {"author": "Mridul Muralidharan", "body": "[~pwendell] Is there any update on this ? This is fairly commonly hitting us, and we are at 1Gig for framesize already now ...", "created": "2015-03-08T21:47:13.293+0000"}, {"author": "Kostas Sakellis", "body": "How many reduce side tasks do you have? Can you please attach your your logs that show the OOM errors/", "created": "2015-03-08T22:15:47.323+0000"}, {"author": "Patrick Wendell", "body": "It would be helpful if any users who have observed this could comment on the JIRA and give workload information. This has been more on the back burner since we've heard few reports of it on the mailing list, etc...", "created": "2015-03-08T23:54:02.935+0000"}, {"author": "Mridul Muralidharan", "body": "Hitting akka framesize for map outputtracker is very easy since we fetch whole output (m * r) - while I cant get into specifics of our jobs or share logs; but it is easy to see this hitting 1G for 100k mappers and 50k reducers. If this is not being looked into currently, I can add it to my list of things to fix - but if there is already work being done, I dont want to duplicate it. Even something trivial like what was done in task result would suffice (if we dont want the additional overhead of per per reduce map output generation at master).", "created": "2015-03-09T00:44:21.674+0000"}, {"author": "Alex Slusarenko", "body": "Hi, all. We have faced this issue many times. And I've seen about a dozen unanswered mailing lists where guys saw this problem. Currently, we have 250 000 map tasks and the same amount of reduce tasks. We have 200 slave nodes. The driver has 80 GB RAM. First we observed akka frame size limit exception and after increasing the limit we see OOM. Here is the corresponding part of the log:  ... 15/07/27 17:22:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 262144 tasks 15/07/27 17:22:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 233766, 10.47.190.240, PROCESS_LOCAL, 1215 bytes) 15/07/27 17:22:57 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 233767, 10.145.26.133, PROCESS_LOCAL, 1215 bytes) 15/07/27 17:22:57 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 233768, 10.51.191.206, PROCESS_LOCAL, 1215 bytes) ... 15/07/27 17:22:57 INFO TaskSetManager: Starting task 3197.0 in stage 1.0 (TID 236963, 10.99.197.178, PROCESS_LOCAL, 1215 bytes) 15/07/27 17:22:57 INFO TaskSetManager: Starting task 3198.0 in stage 1.0 (TID 236964, 10.65.148.16, PROCESS_LOCAL, 1215 bytes) 15/07/27 17:22:57 INFO TaskSetManager: Starting task 3199.0 in stage 1.0 (TID 236965, 10.123.204.224, PROCESS_LOCAL, 1215 bytes) 15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.145.30.250:38441 (size: 3.8 KB, free: 4.1 GB) 15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.140.170.222:35810 (size: 3.8 KB, free: 4.1 GB) 15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.7.205.149:43761 (size: 3.8 KB, free: 4.1 GB) ... 5/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.165.146.7:37388 (size: 3.8 KB, free: 4.1 GB) 15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.153.254.79:49517 (size: 3.8 KB, free: 4.1 GB) 15/07/27 17:22:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.95.198.154:53675 (size: 3.8 KB, free: 4.1 GB) 15/07/27 17:24:41 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 166509346 bytes 15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.109.157.235:39740 15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.166.156.78:59382 15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.152.41.131:47968 ... 15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.140.253.251:44621 15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.153.254.79:42648 15/07/27 17:24:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.169.230.246:45473 15/07/27 17:25:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.146.43.5:49989 in memory (size: 3.2 KB, free: 3.4 GB) 15/07/27 17:27:25 ERROR ActorSystemImpl: Uncaught fatal error from thread [sparkDriver-akka.remote.default-remote-dispatcher-47] shutting down ActorSystem [sparkDriver] java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3236) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877) at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply$mcV$sp(Serializer.scala:129) at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply(Serializer.scala:129) at akka.serialization.JavaSerializer$$anonfun$toBinary$1.apply(Serializer.scala:129) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57) at akka.serialization.JavaSerializer.toBinary(Serializer.scala:129) at akka.remote.MessageSerializer$.serialize(MessageSerializer.scala:36) at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845) at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57) at akka.remote.EndpointWriter.serializeMessage(Endpoint.scala:844) at akka.remote.EndpointWriter.writeSend(Endpoint.scala:747) at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:722) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) at akka.dispatch.Mailbox.run(Mailbox.scala:220) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ...  Do you need any additional info?", "created": "2015-07-29T11:49:43.746+0000"}, {"author": "Daniel Darabos", "body": "I can also add some data. I have a ShuffleMapStage with 82,714 tasks and then a ResultStage with 222,609 tasks. The driver cannot serialize this:  java.lang.OutOfMemoryError: Requested array size exceeds VM limit at java.util.Arrays.copyOf(Arrays.java:2271) ~[na:1.7.0_79] at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113) ~[na:1.7.0_79] at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) ~[na:1.7.0_79] at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140) ~[na:1.7.0_79] at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) ~[na:1.7.0_79] at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) ~[na:1.7.0_79] at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:146) ~[na:1.7.0_79] at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1893) ~[na:1.7.0_79] at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1874) ~[na:1.7.0_79] at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1821) ~[na:1.7.0_79] at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:718) ~[na:1.7.0_79] at java.io.ObjectOutputStream.close(ObjectOutputStream.java:739) ~[na:1.7.0_79] at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:362) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0] at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1294) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0] at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:361) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0] at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:312) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0] at org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(MapOutputTracker.scala:49) ~[spark-assembly-1.4.0-hadoop2.4.0.jar:1.4.0]  I see {{getSerializedMapOutputStatuses}} has changed a lot since 1.4.0 but it still returns an array sized proportional to _M * R_. How can this be part of a scalable system? How is this not a major issue for everyone? Am I doing something wrong? I'm now thinking that maybe if you have an overwhelming majority of empty or non-empty blocks, the bitmap will compress very well. But it's possible that I am ending up with a relatively even mix of empty and non-empty blocks, killing the compression. I have about 40 billion lines, _M * R_ is about 20 billion, so this seems plausible. It's also possible that I should have larger partitions. Due to the processing I do it's not possible -- it leads to the executors OOMing. But larger partitions would not be a scalable solution anyway. If _M_ and _R_ are reasonable now with some number of lines per partition, then when your data size doubles they will also double and _M * R_ will quadruple. At some point the number of lines per map output will be low enough that compression becomes ineffective. I see https://issues.apache.org/jira/browse/SPARK-11271 has recently decreased the map status size by 20%. That means in Spark 1.6 I will be able to process 1/sqrt(0.8) or 12% more data than now. The way I understand the situation the improvement required is orders of magnitude larger than that. I'm currently hitting this issue with 5 TB of input. If I tried processing 5 PB, the map status would be a million times larger. I like the premise of this JIRA ticket of not building the map status table in the first place. But a colleague of mine asks if perhaps we could even avoid tracking this data in the driver. If the driver just provided the reducers with the list of mappers they could each just ask the mappers directly for the list of blocks they should fetch.", "created": "2015-11-03T10:38:13.715+0000"}, {"author": "Alex Slusarenko", "body": "> How is this not a major issue for everyone? Daniel, I had exactly the same thoughts, but I found out that most of Spark users have small data. Most users need about 10 servers. I was even more surprised when I had to fix SPARK-6246 myself. Spark was not able to launch more than 100 instances on Amazon.", "created": "2015-11-04T14:49:01.156+0000"}, {"author": "Thomas Graves", "body": "I have another user hitting this also. The above mentions other issues that need to be addressed in MapOutputStatusTracker do you have links to those other issues?", "created": "2015-12-02T18:38:16.442+0000"}, {"author": "Daniel Darabos", "body": "I've read an interesting article about the \"Kylix\" butterfly allreduce (http://www.cs.berkeley.edu/~jfc/papers/14/Kylix.pdf). I think this is a direct solution to this problem and the authors say integration with Spark should be \"easy\". Perhaps the same approach could be simulated within the current Spark shuffle implementation. I think the idea is to break up the M*R shuffle into an M*K and a K*R shuffle, where K is much less then M or R. So those K partitions will be large, but that should be fine.", "created": "2016-02-03T13:05:25.550+0000"}, {"author": "Thomas Graves", "body": "So I have been looking at this and testing a few changes out. There are a few issues here but if we are looking at solving the driver memory bloat issue then I think this comes down to flow control issue. The Driver is trying to respond to all the map status requests and is shoving them out to Netty quicker then netty can send them and we end up using a lot of memory very quickly. Yes you can try to reduce the size of the MapStatuses but you can only do that to a point and you could still have the this issue. There are multiple possible ways to solve this. The approach I have been looking at is having the MapOutputTracker have its own queue and thread pool for handling requests. This gives us the flexibility to do multiple things: - We can make the reply synchronous (ie it waits for response from netty to start next reply) without blocking the normal dispatcher threads which do things like handling heartbeats, thus giving us flow control. We can decide to do this only if the map output status are above a certain size or do it all the time. You can adjust the thread pool size to handle more in parallel. you could make this more sophisticated in the future if we want to have some sort of send queue rather then blocking each thread. - We can easily synchronize incoming requests without blocking dispatcher threads so we don't serialize the same MapStatus multiple times. Background - one other problem I've been seeing is that you get a bunch of requests for map status in at once, we have a lot of dispatchers threads running in parallel, all of those do the check to see if the map status is cached, all of them report its not, and you have multiple threads all serializing the exact same map output statuses. - doesn't limit us with sending map status with Task data. ie if we want to change Spark in the future to start Reducer tasks before all map tasks finish (MapReduce does this now) this more easily works with that. I still need to do some more testing on this but I wanted to see what people thought of this approach? What I have implemented right now is a queue and threadpool in the MapOutputTracker to handle the requests. if its over 5MB (still deciding on this size) then when it replies it waits for it to actually send before grabbing the next request. For the second bullet above I did a somewhat simpler approach for now and when registerMapOutputs is called I have it cache the map status output then instead of waiting for a request to come in. This helps as it will make sure the last one is cached but if you have multiple then the others still won't be in the cache. We could either have it cache more or take an approach like I mention above to have it just synchronize and cache one upon the first request. One of the large jobs I'm using to test this is shuffling 15TB of data using 202000 map tasks going down to 500 reducers. The driver originally was using 20GB of memory, with my changes I was able to successfully run it with 5GB. The job has 50mb of serialized map output statuses and before my changes it took executors 40-70 seconds to fetch the map output status, with my change using 8 threads it took roughly the same. I need to get some more exact statistics here.", "created": "2016-03-21T18:58:13.756+0000"}, {"author": "Mridul Muralidharan", "body": "[~tgraves] For the last part (waiting bit) - why not make the threshold where you use Broadcast instead of direct serialization such that the problem 'goes away' ? For my case, I was using a fairly high number, but nothing stopping us from using say 1mb - which means number of outstanding requests which will cause memory issue becomes extremely high to the point of being not possible practically. In general, I dont like the point about waiting for IO to complete - different nodes might have different loads, which can cause driver not to respond to fast nodes because slow nodes cause the response not to be sent (over time).", "created": "2016-03-21T19:29:32.501+0000"}, {"author": "Thomas Graves", "body": "I do like the idea of broadcast and originally when I had tried it I had the issue mentioned in the second bullet point, but as long as we are synchronizing on the requests so we only broadcast it once we should be ok. It does seem to have some further constraints though too. With a sufficient large job I don't think it matters but what if we only have a small number of reducers, we broadcast it to all executors when only a couple need it. I guess that doesn't hurt much unless the other executors start going to the executors your reducers are on and add more load to them. Should be pretty minimal though. Broadcast also seems to make less sense when using the dynamic allocation. At least I've seen issues when executors go away, it fails fetch from that one, has to retry, etc, adding additional time. We recently specifically fixed one issue with this to make it go get locations again after certain number of failures. That time should be less now that we fixed that but I'll have to run the numbers. I'll do some more analysis/testing of this and see if that really matters. with a sufficient number of threads I don't think a few slow nodes would make much of a difference here, if you have that many slow nodes the shuffle itself is going to be impacted which I would see as a larger affect. The slow nodes could just as well affect the broadcast as well. Hopefully you skip those as it takes longer for those to get a chunk, buts its possible that once that slow one has a chunk or two, more and more executors start going to that one for the broadcast data instead of the driver thus slowing down more transfers. But its a good point and my current method would truly block (for a certain time) rather then being slow. Note that there is a timeout on waiting for the send to happen and when it does it closes the connection and executor would retry. You don't have to worry about that with broadcast. I'll do some more analysis with that approach. I wish Netty had some other built in mechanisms for flow control.", "created": "2016-03-21T22:18:35.653+0000"}, {"author": "Apache Spark", "body": "User 'tgravescs' has created a pull request for this issue: https://github.com/apache/spark/pull/12113", "created": "2016-04-01T18:52:04.761+0000"}, {"author": "Davies Liu", "body": "Issue resolved by pull request 12113 [https://github.com/apache/spark/pull/12113]", "created": "2016-05-07T02:31:46.231+0000"}, {"author": "harel gliksman", "body": "I am hitting a similar error on Spark 2.3.1 running on EMR cluster of 500 r3.2xlarge to process ~40 TB. There are ~ 200,000 map tasks that succeed. We use aggregateByKey with numPartition=300,000 but when the reduce sides tasks start the driver (which has 50GB memory) is failing with: 18/09/07 15:10:49 WARN Utils: Suppressing exception in finally: null java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822) at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719) at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:790) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1389) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:789) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Exception in thread \"map-output-dispatcher-0\" java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189) at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:787) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:786) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:786) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1380) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:789) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:174) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:397) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Suppressed: java.lang.OutOfMemoryError at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) at java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253) at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) at java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145) at java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1894) at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1875) at java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822) at java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719) at java.io.ObjectOutputStream.close(ObjectOutputStream.java:740) at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$2.apply$mcV$sp(MapOutputTracker.scala:790) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1389) ... 6 more", "created": "2018-09-07T15:27:33.057+0000"}], "num_comments": 27, "text": "Issue: SPARK-1239\nSummary: Improve fetching of map output statuses\nDescription: Instead we should modify the way we fetch map output statuses to take both a mapper and a reducer - or we should just piggyback the statuses on each task.\n\nComments (27):\n1. Patrick McFadin: Hey [~CodingCat] I actually asked [~andrewor] to look at this already\n2. Nan Zhu: Oh, that's fine\n3. bc Wong: Is anyone working on this? If so, could someone please update the jira? Thanks!\n4. Patrick Wendell: I think [~andrewor] has this in his backlog but it's not actively being worked on. [~bcwalrus] do you or [~sandyr] want to take a crack?\n5. Kostas Sakellis: [~pwendell] I'd like to take a crack at this since it is affecting one of our customers.\n6. Andrew Or: I have reassigned it to you Kostas.\n7. Andrew Ash: For large statuses, would we expect that to exceed {{spark.akka.frameSize}} and cause the below exception?  2014-09-14T01:34:21.305 ERROR [spark-akka.actor.default-dispatcher-4] org.apache.spark.MapOutputTrackerMasterActor - Map output statuses were 13920119 bytes which exceeds spark.akka.frameSize (10485760 bytes).\n8. Patrick Wendell: Yes, the current state of the art is to just increase the frame size.\n9. DB Tsai: +1, we run into this issue as well.\n10. Josh Rosen: I'm re-assigning this to me since I've been working in this code recently and plan to completely re-write MapOutputStatusTracker to address this and a few other issues. I like the idea of piggybacking the status on the task launching RPC itself; if we took the approach of having reducers specify their reduce ids when fetching the statuses, then this would add an extra RPC for every task launch, which could result in a big latency increase.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "1e76185b86c9e4d38839e5ee784d00be", "issue_key": "SPARK-1240", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "takeSample called on empty RDD never ends", "description": "It seems, that method takeSample ends in infinite loop if called on empty RDD, trying to collect enough samples. val list = List\\[String\\](\"aaa\") val rdd = sc.parallelize(list) rdd.takeSample(true, 1, System.nanoTime.toInt) val empty = rdd.filter(_ => false) empty.takeSample(true, 1, System.nanoTime.toInt)", "reporter": "Jaroslav Kamenik", "assignee": "Nan Zhu", "created": "2014-03-13T03:04:18.000+0000", "updated": "2014-03-16T22:38:01.000+0000", "resolved": "2014-03-16T22:18:10.000+0000", "labels": [], "components": [], "comments": [{"author": "Nan Zhu", "body": "Made a PR: https://github.com/apache/spark/pull/135", "created": "2014-03-13T11:23:11.574+0000"}], "num_comments": 1, "text": "Issue: SPARK-1240\nSummary: takeSample called on empty RDD never ends\nDescription: It seems, that method takeSample ends in infinite loop if called on empty RDD, trying to collect enough samples. val list = List\\[String\\](\"aaa\") val rdd = sc.parallelize(list) rdd.takeSample(true, 1, System.nanoTime.toInt) val empty = rdd.filter(_ => false) empty.takeSample(true, 1, System.nanoTime.toInt)\n\nComments (1):\n1. Nan Zhu: Made a PR: https://github.com/apache/spark/pull/135", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "2c8c8055cbafeab4438ccd2b7dc0b831", "issue_key": "SPARK-1241", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Support sliding in RDD", "description": "Sliding is useful for operations like creating n-grams, calculating total variation, numerical integration, etc.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-03-13T12:05:59.000+0000", "updated": "2015-05-01T23:25:30.000+0000", "resolved": "2014-04-11T19:07:38.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/spark/pull/136", "created": "2014-03-17T23:59:10.844+0000"}, {"author": "Frens Jan Rumph", "body": "Hi, I'm investigating use of Spark for matching patterns in symbolically represented time series where the sliding functionality as available from scala iterators would make life a lot easier. From the ticket I'd say this functionality is implemented (status resolved, fix version 1.0.0, ...), but I can't find it in the docs and the PR indicates that this functionality hasn't made it into Spark (just yet ...). Is this functionality available? Cheers, Frens Background: I want to compare strings of segments to other (larger) strings of segments. As segment strings may be split up over partitions, the more straight forward aproaches I could come up with don't work.", "created": "2014-09-25T20:45:50.659+0000"}, {"author": "Xiangrui Meng", "body": "This is implemented MLlib: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/rdd/SlidingRDD.scala . You can check the discussion about where to put it here: https://github.com/apache/spark/pull/136 .", "created": "2014-09-26T00:43:51.809+0000"}], "num_comments": 3, "text": "Issue: SPARK-1241\nSummary: Support sliding in RDD\nDescription: Sliding is useful for operations like creating n-grams, calculating total variation, numerical integration, etc.\n\nComments (3):\n1. Xiangrui Meng: PR: https://github.com/apache/spark/pull/136\n2. Frens Jan Rumph: Hi, I'm investigating use of Spark for matching patterns in symbolically represented time series where the sliding functionality as available from scala iterators would make life a lot easier. From the ticket I'd say this functionality is implemented (status resolved, fix version 1.0.0, ...), but I can't find it in the docs and the PR indicates that this functionality hasn't made it into Spark (just yet ...). Is this functionality available? Cheers, Frens Background: I want to compare strings of segments to other (larger) strings of segments. As segment strings may be split up over partitions, the more straight forward aproaches I could come up with don't work.\n3. Xiangrui Meng: This is implemented MLlib: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/rdd/SlidingRDD.scala . You can check the discussion about where to put it here: https://github.com/apache/spark/pull/136 .", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "b4e4a1c00cd05dcfaa6f82df8a8a38db", "issue_key": "SPARK-1242", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Add aggregate to python API", "description": "", "reporter": "Holden Karau", "assignee": "Holden Karau", "created": "2014-03-13T19:24:38.000+0000", "updated": "2014-04-25T06:08:36.000+0000", "resolved": "2014-04-25T06:08:36.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Holden Karau", "body": "Submitted a pull request for this https://github.com/apache/spark/pull/139", "created": "2014-03-13T19:36:56.120+0000"}], "num_comments": 1, "text": "Issue: SPARK-1242\nSummary: Add aggregate to python API\n\nComments (1):\n1. Holden Karau: Submitted a pull request for this https://github.com/apache/spark/pull/139", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "ca4f2330462776663e091933c6c7d55c", "issue_key": "SPARK-1243", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "spark compilation error", "description": "After issuing git pull from git master, spark could not compile any longer Here is the error message, it seems that it is related to jetty upgrade.@rxin > > > compile [info] Compiling 301 Scala sources and 19 Java sources to E:\\projects\\amplab\\spark\\core\\target\\scala-2.10\\classes... [warn] Class java.nio.channels.ReadPendingException not found - continuing with a stub. [error] [error] while compiling: E:\\projects\\amplab\\spark\\core\\src\\main\\scala\\org\\apache\\spark\\HttpServer.scala [error] during phase: erasure [error] library version: version 2.10.3 [error] compiler version: version 2.10.3 [error] reconstructed args: -Xmax-classfile-name 120 -deprecation -bootclasspath C:\\Java\\jdk1.6.0_27\\jre\\lib\\resources.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\rt.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\sunrsasign.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\jsse.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\jce.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\charsets.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\modules\\jdk.boot.jar;C:\\Java\\jdk1.6.0_27\\jre\\classes;C:\\Users\\Kand\\.sbt\\boot\\scala-2.10.3\\lib\\scala-library.jar -unchecked -classpath E:\\projects\\amplab\\spark\\core\\target\\scala-2.10\\classes;E:\\projects\\amplab\\spark\\lib_managed\\jars\\netty-all-4.0.17.Final.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-server-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\javax.servlet-api-3.1.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-http-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-util-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-io-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-plus-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-webapp-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-xml-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-servlet-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-security-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-jndi-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\javax.servlet-2.5.0.v201103041518.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\guava-14.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jsr305-1.3.9.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\log4j-1.2.17.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\slf4j-api-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\slf4j-log4j12-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jul-to-slf4j-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jcl-over-slf4j-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-daemon-1.0.10.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\compress-lzf-1.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\snappy-java-1.0.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\akka-remote_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\akka-actor_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\config-1.0.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\netty-3.6.6.Final.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\protobuf-java-2.4.1-shaded.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\uncommons-maths-1.2.2a.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\akka-slf4j_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-jackson_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-core_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-ast_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\paranamer-2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scalap-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scala-compiler-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scala-reflect-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-databind-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-annotations-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-core-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\colt-1.2.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\concurrent-1.3.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\mesos-0.13.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\protobuf-java-2.4.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-net-2.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jets3t-0.7.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-httpclient-3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hadoop-client-1.0.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hadoop-core-1.0.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\xmlenc-0.52.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-codec-1.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-math-2.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-configuration-1.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-collections-3.2.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-lang-2.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-digester-1.8.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-beanutils-1.7.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-beanutils-core-1.8.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-el-1.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hsqldb-1.8.0.10.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\oro-2.0.8.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jackson-mapper-asl-1.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jackson-core-asl-1.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-recipes-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-framework-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-client-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\zookeeper-3.4.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jline-0.9.94.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-core-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-jvm-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-json-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-graphite-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\chill_2.10-0.3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\chill-java-0.3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\kryo-2.21.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\reflectasm-1.07-shaded.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\minlog-1.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\objenesis-1.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\stream-2.5.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\fastutil-6.5.7.jar [error] [error] last tree to typer: TypeTree(class Logging$class) [error] symbol: class Logging$class in package spark (flags: abstract <trait> <implclass>) [error] symbol definition: abstract class Logging$class extends Logging [error] tpe: org.apache.spark.Logging$class [error] symbol owners: class Logging$class -> package spark [error] context owners: method start -> class HttpServer -> package spark [error] [error] == Enclosing template or block == [error] [error] Apply( // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector [error] \"connector\".\"setIdleTimeout\" // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector [error] 60000L [error] ) [error] [error] == Expanded type of tree == [error] [error] TypeRef(TypeSymbol(abstract class Logging$class extends Logging)) [error] [error] uncaught exception during compilation: java.lang.AssertionError [trace] Stack trace suppressed: run 'last core/compile:compile' for the full output. [error] (core/compile:compile) java.lang.AssertionError: assertion failed: java.nio.channels.ReadPendingException [error] Total time: 73 s, completed Mar 14, 2014 11:24:14 AM", "reporter": "Qiuzhuang Lian", "assignee": null, "created": "2014-03-13T20:33:54.000+0000", "updated": "2014-10-13T17:55:17.000+0000", "resolved": "2014-10-13T17:55:16.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "koert kuipers", "body": "i see same thing with java 6, but not with java 7", "created": "2014-03-17T14:41:30.177+0000"}, {"author": "Qiuzhuang Lian", "body": "Yes, it compiles successfully with JDK 7. Currently, Spark build scripts marks to use JDK 6. Should we update them to reflect to use JDK 7 instead?", "created": "2014-03-17T20:16:41.594+0000"}, {"author": "Sean R. Owen", "body": "This appears to be long since resolved by something else, perhaps a subsequent change to Jetty deps. I have never seen this personally, and Jenkins builds are fine.", "created": "2014-10-13T17:55:17.028+0000"}], "num_comments": 3, "text": "Issue: SPARK-1243\nSummary: spark compilation error\nDescription: After issuing git pull from git master, spark could not compile any longer Here is the error message, it seems that it is related to jetty upgrade.@rxin > > > compile [info] Compiling 301 Scala sources and 19 Java sources to E:\\projects\\amplab\\spark\\core\\target\\scala-2.10\\classes... [warn] Class java.nio.channels.ReadPendingException not found - continuing with a stub. [error] [error] while compiling: E:\\projects\\amplab\\spark\\core\\src\\main\\scala\\org\\apache\\spark\\HttpServer.scala [error] during phase: erasure [error] library version: version 2.10.3 [error] compiler version: version 2.10.3 [error] reconstructed args: -Xmax-classfile-name 120 -deprecation -bootclasspath C:\\Java\\jdk1.6.0_27\\jre\\lib\\resources.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\rt.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\sunrsasign.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\jsse.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\jce.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\charsets.jar;C:\\Java\\jdk1.6.0_27\\jre\\lib\\modules\\jdk.boot.jar;C:\\Java\\jdk1.6.0_27\\jre\\classes;C:\\Users\\Kand\\.sbt\\boot\\scala-2.10.3\\lib\\scala-library.jar -unchecked -classpath E:\\projects\\amplab\\spark\\core\\target\\scala-2.10\\classes;E:\\projects\\amplab\\spark\\lib_managed\\jars\\netty-all-4.0.17.Final.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-server-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\javax.servlet-api-3.1.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-http-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-util-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-io-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-plus-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-webapp-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-xml-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-servlet-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-security-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jetty-jndi-9.1.3.v20140225.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\javax.servlet-2.5.0.v201103041518.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\guava-14.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jsr305-1.3.9.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\log4j-1.2.17.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\slf4j-api-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\slf4j-log4j12-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jul-to-slf4j-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jcl-over-slf4j-1.7.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-daemon-1.0.10.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\compress-lzf-1.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\snappy-java-1.0.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\akka-remote_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\akka-actor_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\config-1.0.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\netty-3.6.6.Final.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\protobuf-java-2.4.1-shaded.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\uncommons-maths-1.2.2a.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\akka-slf4j_2.10-2.2.3-shaded-protobuf.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-jackson_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-core_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\json4s-ast_2.10-3.2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\paranamer-2.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scalap-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scala-compiler-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\scala-reflect-2.10.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-databind-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-annotations-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\jackson-core-2.3.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\colt-1.2.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\concurrent-1.3.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\mesos-0.13.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\protobuf-java-2.4.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-net-2.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jets3t-0.7.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-httpclient-3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hadoop-client-1.0.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hadoop-core-1.0.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\xmlenc-0.52.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-codec-1.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-math-2.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-configuration-1.6.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-collections-3.2.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-lang-2.4.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-digester-1.8.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-beanutils-1.7.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-beanutils-core-1.8.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\commons-el-1.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\hsqldb-1.8.0.10.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\oro-2.0.8.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jackson-mapper-asl-1.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jackson-core-asl-1.0.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-recipes-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-framework-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\curator-client-2.4.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\zookeeper-3.4.5.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\jline-0.9.94.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-core-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-jvm-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-json-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\metrics-graphite-3.0.0.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\chill_2.10-0.3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\chill-java-0.3.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\bundles\\kryo-2.21.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\reflectasm-1.07-shaded.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\minlog-1.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\objenesis-1.2.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\stream-2.5.1.jar;E:\\projects\\amplab\\spark\\lib_managed\\jars\\fastutil-6.5.7.jar [error] [error] last tree to typer: TypeTree(class Logging$class) [error] symbol: class Logging$class in package spark (flags: abstract <trait> <implclass>) [error] symbol definition: abstract class Logging$class extends Logging [error] tpe: org.apache.spark.Logging$class [error] symbol owners: class Logging$class -> package spark [error] context owners: method start -> class HttpServer -> package spark [error] [error] == Enclosing template or block == [error] [error] Apply( // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector [error] \"connector\".\"setIdleTimeout\" // def setIdleTimeout(x$1: Long): Unit in class AbstractConnector [error] 60000L [error] ) [error] [error] == Expanded type of tree == [error] [error] TypeRef(TypeSymbol(abstract class Logging$class extends Logging)) [error] [error] uncaught exception during compilation: java.lang.AssertionError [trace] Stack trace suppressed: run 'last core/compile:compile' for the full output. [error] (core/compile:compile) java.lang.AssertionError: assertion failed: java.nio.channels.ReadPendingException [error] Total time: 73 s, completed Mar 14, 2014 11:24:14 AM\n\nComments (3):\n1. koert kuipers: i see same thing with java 6, but not with java 7\n2. Qiuzhuang Lian: Yes, it compiles successfully with JDK 7. Currently, Spark build scripts marks to use JDK 6. Should we update them to reflect to use JDK 7 instead?\n3. Sean R. Owen: This appears to be long since resolved by something else, perhaps a subsequent change to Jetty deps. I have never seen this personally, and Jenkins builds are fine.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "79abd676a7fc979395630432f604dcb6", "issue_key": "SPARK-1244", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Log an exception if map output status message exceeds frame size", "description": "This causes a silent failure if not set correctly. The associated PR - https://github.com/apache/spark/pull/147", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "created": "2014-03-13T21:56:03.000+0000", "updated": "2014-03-30T04:15:14.000+0000", "resolved": "2014-03-17T14:30:23.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1244\nSummary: Log an exception if map output status message exceeds frame size\nDescription: This causes a silent failure if not set correctly. The associated PR - https://github.com/apache/spark/pull/147", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "9e71bfb21fa87138f33a5204fd33e18e", "issue_key": "SPARK-1245", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Can't read EMR HBase cluster from properly built Cloudera Spark Cluster.", "description": "Can't read EMR HBase cluster from properly built Cloudera Spark Cluster. If I scp hadoop-yarn-client-2.2.0.jar from our EMR hbase cluster lib dir and manually add it as a lib to my jar it does NOT give me a noSuchMethod error, but does give me a weird EOF exception (see below). Usually I use SBT to build Jars, but the EMR distros are very strange I can't find a proper repository for them. I'm thinking only thing we can do is get our sysadm to rebuild the hbase cluster to use a proper cloudera hbase / hadoop. SBT Dependencies include: \"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\", \"org.apache.hbase\" % \"hbase\" % \"0.94.7\", 14/03/11 19:08:06 WARN scheduler.TaskSetManager: Lost TID 95 (task 0.0:3) 14/03/11 19:08:06 WARN scheduler.TaskSetManager: Loss was due to java.io.EOFException java.io.EOFException at java.io.ObjectInputStream$BlockDataInputStream.readFully(ObjectInputStream.java:2744) at java.io.ObjectInputStream.readFully(ObjectInputStream.java:1015) at org.apache.hadoop.io.WritableUtils.readCompressedByteArray(WritableUtils.java:39) at org.apache.hadoop.io.WritableUtils.readCompressedString(WritableUtils.java:87) at org.apache.hadoop.io.WritableUtils.readCompressedStringArray(WritableUtils.java:185) at org.apache.hadoop.conf.Configuration.readFields(Configuration.java:2433) at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:280) at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:75) at org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:39) at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:165) at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56) at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:63) at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:139) at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)", "reporter": "Sam Abeyratne", "assignee": null, "created": "2014-03-14T03:04:13.000+0000", "updated": "2014-11-12T16:42:29.000+0000", "resolved": "2014-11-12T16:42:29.000+0000", "labels": [], "components": [], "comments": [{"author": "Sam Abeyratne", "body": "How can I create topics on: https://groups.google.com/forum/#!forum/spark-users ??", "created": "2014-03-14T06:10:39.559+0000"}, {"author": "Sean R. Owen", "body": "I'm guessing this is now either obsolete, or, a case of matching HBase / Hadoop versions exactly. Spark should be \"provided\", and not marking as such may mean the Spark Hadoop / cluster Hadoop / HBase Hadoop deps are colliding.", "created": "2014-11-12T16:42:29.605+0000"}], "num_comments": 2, "text": "Issue: SPARK-1245\nSummary: Can't read EMR HBase cluster from properly built Cloudera Spark Cluster.\nDescription: Can't read EMR HBase cluster from properly built Cloudera Spark Cluster. If I scp hadoop-yarn-client-2.2.0.jar from our EMR hbase cluster lib dir and manually add it as a lib to my jar it does NOT give me a noSuchMethod error, but does give me a weird EOF exception (see below). Usually I use SBT to build Jars, but the EMR distros are very strange I can't find a proper repository for them. I'm thinking only thing we can do is get our sysadm to rebuild the hbase cluster to use a proper cloudera hbase / hadoop. SBT Dependencies include: \"org.apache.spark\" % \"spark-core_2.10\" % \"0.9.0-incubating\", \"org.apache.hbase\" % \"hbase\" % \"0.94.7\", 14/03/11 19:08:06 WARN scheduler.TaskSetManager: Lost TID 95 (task 0.0:3) 14/03/11 19:08:06 WARN scheduler.TaskSetManager: Loss was due to java.io.EOFException java.io.EOFException at java.io.ObjectInputStream$BlockDataInputStream.readFully(ObjectInputStream.java:2744) at java.io.ObjectInputStream.readFully(ObjectInputStream.java:1015) at org.apache.hadoop.io.WritableUtils.readCompressedByteArray(WritableUtils.java:39) at org.apache.hadoop.io.WritableUtils.readCompressedString(WritableUtils.java:87) at org.apache.hadoop.io.WritableUtils.readCompressedStringArray(WritableUtils.java:185) at org.apache.hadoop.conf.Configuration.readFields(Configuration.java:2433) at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:280) at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:75) at org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:39) at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.broadcast.HttpBroadcast$.read(HttpBroadcast.scala:165) at org.apache.spark.broadcast.HttpBroadcast.readObject(HttpBroadcast.scala:56) at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at scala.collection.immutable.$colon$colon.readObject(List.scala:362) at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:63) at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:139) at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:40) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:62) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:195) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)\n\nComments (2):\n1. Sam Abeyratne: How can I create topics on: https://groups.google.com/forum/#!forum/spark-users ??\n2. Sean R. Owen: I'm guessing this is now either obsolete, or, a case of matching HBase / Hadoop versions exactly. Spark should be \"provided\", and not marking as such may mean the Spark Hadoop / cluster Hadoop / HBase Hadoop deps are colliding.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "abd15ec5ffd5a6467511d45d0d0d5416", "issue_key": "SPARK-1246", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add min max to the stat counter.", "description": "Augment the stat counter class with min max functions. It might be convenient to calculate all that in one go.", "reporter": "Prashant Sharma", "assignee": null, "created": "2014-03-14T04:43:25.000+0000", "updated": "2020-02-07T17:28:01.000+0000", "resolved": "2014-03-18T00:48:15.000+0000", "labels": [], "components": ["Documentation", "Java API", "PySpark", "Spark Core"], "comments": [{"author": "Prashant Sharma", "body": "Currently working on it. (wish I had assign rights.)", "created": "2014-03-14T04:55:11.024+0000"}, {"author": "Matei Alexandru Zaharia", "body": "This was fixed by Daniel McClary here: https://github.com/apache/spark/pull/144", "created": "2014-03-18T00:48:09.145+0000"}, {"author": "Apache Spark", "body": "User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/140", "created": "2015-12-10T15:06:04.124+0000"}], "num_comments": 3, "text": "Issue: SPARK-1246\nSummary: Add min max to the stat counter.\nDescription: Augment the stat counter class with min max functions. It might be convenient to calculate all that in one go.\n\nComments (3):\n1. Prashant Sharma: Currently working on it. (wish I had assign rights.)\n2. Matei Alexandru Zaharia: This was fixed by Daniel McClary here: https://github.com/apache/spark/pull/144\n3. Apache Spark: User 'ScrapCodes' has created a pull request for this issue: https://github.com/apache/spark/pull/140", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "96d7cd94bdd4786fbee7f385a10c6a98", "issue_key": "SPARK-1247", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Support some of the RDD double functions on float and int as well.", "description": "Pyspark is already agnostic, but currently Java and scala provides the implicit functions like stats/ histogram etc only if the RDD is on Double. This can be extended easily to support more numeric types.", "reporter": "Prashant Sharma", "assignee": "Prashant Sharma", "created": "2014-03-14T04:45:20.000+0000", "updated": "2020-02-07T17:28:00.000+0000", "resolved": "2015-03-02T15:59:44.000+0000", "labels": [], "components": ["Documentation", "Java API", "PySpark", "Spark Core"], "comments": [{"author": "Prashant Sharma", "body": "Trying to work on it.", "created": "2014-03-14T05:35:55.244+0000"}, {"author": "Ximo Guanter", "body": "This seems to be fixed by the following function:  implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) = new DoubleRDDFunctions(rdd.map(x => num.toDouble(x)))", "created": "2014-06-25T08:14:58.780+0000"}, {"author": "Sean R. Owen", "body": "Yes, this already works on {{RDD[Int]}} for example via implicits, which existed since 0.9 at least:  scala> val ints = sc.parallelize(Array(1,2,3)) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:21 scala> ints.mean ... 2.0  I think it may be prohibitive to specialize all this in Java, but, an RDD can easily be manually mapped to {{Double}}s in Java.", "created": "2015-03-02T15:59:44.451+0000"}], "num_comments": 3, "text": "Issue: SPARK-1247\nSummary: Support some of the RDD double functions on float and int as well.\nDescription: Pyspark is already agnostic, but currently Java and scala provides the implicit functions like stats/ histogram etc only if the RDD is on Double. This can be extended easily to support more numeric types.\n\nComments (3):\n1. Prashant Sharma: Trying to work on it.\n2. Ximo Guanter: This seems to be fixed by the following function:  implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) = new DoubleRDDFunctions(rdd.map(x => num.toDouble(x)))\n3. Sean R. Owen: Yes, this already works on {{RDD[Int]}} for example via implicits, which existed since 0.9 at least:  scala> val ints = sc.parallelize(Array(1,2,3)) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:21 scala> ints.mean ... 2.0  I think it may be prohibitive to specialize all this in Java, but, an RDD can easily be manually mapped to {{Double}}s in Java.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "0720f0b97931520a962b9a2f5a3ccecb", "issue_key": "SPARK-1248", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark build error with Apache Hadoop(Cloudera CDH4)", "description": "SPARK_HADOOP_VERSION=2.0.0-cdh4.5.0 SPARK_YARN=true sbt/sbt assembly -d > error.log", "reporter": "Guoqiang Li", "assignee": "Sean R. Owen", "created": "2014-03-14T05:03:46.000+0000", "updated": "2015-01-15T09:08:42.000+0000", "resolved": "2014-03-15T16:46:47.000+0000", "labels": [], "components": ["Build"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1248\nSummary: Spark build error with Apache Hadoop(Cloudera CDH4)\nDescription: SPARK_HADOOP_VERSION=2.0.0-cdh4.5.0 SPARK_YARN=true sbt/sbt assembly -d > error.log", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "773ca02e2836308136a4c785ddcc46a7", "issue_key": "SPARK-1249", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Cannot create graphx.Graph with no edges", "description": "Let's say I want a graph with a single node, no edges. (I actually want this for a unit test.) scala> val vs:spark.rdd.RDD[(spark.graphx.VertexId, String)] = sc.makeRDD(Seq((1L, \"x\"))) scala> val es:spark.rdd.RDD[spark.graphx.Edge[String]] = sc.makeRDD(Seq()) scala> val g:spark.graphx.Graph[String, String] = spark.graphx.Graph(vs, es) java.lang.IllegalArgumentException: Positive number of slices required at org.apache.spark.rdd.ParallelCollectionRDD$.slice(ParallelCollectionRDD.scala:116) at org.apache.spark.rdd.ParallelCollectionRDD.getPartitions(ParallelCollectionRDD.scala:95) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:205) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:31) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:205) at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:58) at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45) at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45) at scala.Option.orElse(Option.scala:257) at org.apache.spark.graphx.EdgeRDD.<init>(EdgeRDD.scala:45) at org.apache.spark.graphx.impl.GraphImpl$.createEdgeRDD(GraphImpl.scala:373) at org.apache.spark.graphx.impl.GraphImpl$.apply(GraphImpl.scala:319) at org.apache.spark.graphx.Graph$.apply(Graph.scala:411) My impression is that raising an IllegalArgumentException in ParallelCollectionRDD.slice() is a mistake. It should just return a Seq(Seq()). This would match the behavior of how \"\".split(\"x\") returns Seq(\"\"). Let me know if I'm missing something. I'm new to Spark/GraphX. But it looks like there is no test for corner cases with no nodes/edges in graphx/GraphSuite.scala. Probably an oversight? I'm happy to send a patch for this if you think I'm right.", "reporter": "Daniel Darabos", "assignee": null, "created": "2014-03-14T07:16:48.000+0000", "updated": "2014-03-14T08:16:39.000+0000", "resolved": "2014-03-14T08:16:39.000+0000", "labels": [], "components": ["GraphX", "Spark Core"], "comments": [{"author": "Daniel Darabos", "body": "Oh, I've now discovered spark.rdd.EmptyRDD! So that's how you do it. It does not seem very convenient to have to treat empty data as a special case though. What if I'm reading the data from a file, and the graph may or may not have edges? Thanks for bearing with me!", "created": "2014-03-14T07:45:03.707+0000"}, {"author": "Daniel Darabos", "body": "Looks like generally if an RDD ends up empty, it does not result in this exception, just when created with sc.makeRDD(). Don't mind me then.", "created": "2014-03-14T08:16:20.056+0000"}], "num_comments": 2, "text": "Issue: SPARK-1249\nSummary: Cannot create graphx.Graph with no edges\nDescription: Let's say I want a graph with a single node, no edges. (I actually want this for a unit test.) scala> val vs:spark.rdd.RDD[(spark.graphx.VertexId, String)] = sc.makeRDD(Seq((1L, \"x\"))) scala> val es:spark.rdd.RDD[spark.graphx.Edge[String]] = sc.makeRDD(Seq()) scala> val g:spark.graphx.Graph[String, String] = spark.graphx.Graph(vs, es) java.lang.IllegalArgumentException: Positive number of slices required at org.apache.spark.rdd.ParallelCollectionRDD$.slice(ParallelCollectionRDD.scala:116) at org.apache.spark.rdd.ParallelCollectionRDD.getPartitions(ParallelCollectionRDD.scala:95) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:205) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:31) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.rdd.RDD.partitions(RDD.scala:205) at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:58) at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45) at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45) at scala.Option.orElse(Option.scala:257) at org.apache.spark.graphx.EdgeRDD.<init>(EdgeRDD.scala:45) at org.apache.spark.graphx.impl.GraphImpl$.createEdgeRDD(GraphImpl.scala:373) at org.apache.spark.graphx.impl.GraphImpl$.apply(GraphImpl.scala:319) at org.apache.spark.graphx.Graph$.apply(Graph.scala:411) My impression is that raising an IllegalArgumentException in ParallelCollectionRDD.slice() is a mistake. It should just return a Seq(Seq()). This would match the behavior of how \"\".split(\"x\") returns Seq(\"\"). Let me know if I'm missing something. I'm new to Spark/GraphX. But it looks like there is no test for corner cases with no nodes/edges in graphx/GraphSuite.scala. Probably an oversight? I'm happy to send a patch for this if you think I'm right.\n\nComments (2):\n1. Daniel Darabos: Oh, I've now discovered spark.rdd.EmptyRDD! So that's how you do it. It does not seem very convenient to have to treat empty data as a special case though. What if I'm reading the data from a file, and the graph may or may not have edges? Thanks for bearing with me!\n2. Daniel Darabos: Looks like generally if an RDD ends up empty, it does not result in this exception, just when created with sc.makeRDD(). Don't mind me then.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "7069d7736a8860fe6164b61067cda5c5", "issue_key": "SPARK-1250", "issue_type": "Bug", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "Misleading comments in Spark startup scripts", "description": "A couple of the scripts in bin/ (run-example, spark-class) have the following comment in the code:  # Figure out where the Scala framework is installed  This suggests that the Scala framework is required to be installed before running Spark, which is misleading for newcomers. Instead, the comment should say:  # Figure out where Spark is installed  Will submit a pull request for this.", "reporter": "Sumedh Mungee", "assignee": "Sumedh Mungee", "created": "2014-03-14T11:13:51.000+0000", "updated": "2014-05-25T20:57:24.000+0000", "resolved": "2014-05-25T20:57:24.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Sumedh Mungee", "body": "Submitted pull request: https://github.com/apache/spark/pull/843", "created": "2014-05-20T22:33:07.782+0000"}], "num_comments": 1, "text": "Issue: SPARK-1250\nSummary: Misleading comments in Spark startup scripts\nDescription: A couple of the scripts in bin/ (run-example, spark-class) have the following comment in the code:  # Figure out where the Scala framework is installed  This suggests that the Scala framework is required to be installed before running Spark, which is misleading for newcomers. Instead, the comment should say:  # Figure out where Spark is installed  Will submit a pull request for this.\n\nComments (1):\n1. Sumedh Mungee: Submitted pull request: https://github.com/apache/spark/pull/843", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "8afc0f001ea0db93f2bca9df1ff91db7", "issue_key": "SPARK-1251", "issue_type": "Improvement", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Support for optimizing and executing structured queries", "description": "This is a proposal to add support for optimizing and executing relational queries in a manner that integrates tightly with the Core Spark API. The goal is to allow Spark users to both run SQL queries over data that is currently stored in RDDs as well as run SQL queries over external data sources (such as Hive), returning the results as an RDD. h1. Components Spark SQL support will be broken into three major components. h2. Catalyst An implementation-agnostic framework for manipulating trees of relational operators and expressions. Catalyst was first discussed during [a talk at the most recent Spark Summit|http://spark-summit.org/talk/armbrust-catalyst-a-query-optimization-framework-for-spark-and-shark/], and a more detailed design document for Catalyst can be found [here|https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPGU5M1Y/edit?usp=sharing]. Catalyst provides three main features: a TreeNode library for transforming trees that are expressed as Scala case classes, a logical plan representation for relational operators, and an expression library. A key design point here was to build an extensible system that could be extended for Streaming SQL and other dataflows that need optimization. h3. Public Interfaces Catalyst has no interfaces that are directly exposed to Spark users. h3. Dependencies Catalyst’s only dependency is on the [scala-logging interface for SLF4J by Typesafe|https://github.com/typesafehub/scala-logging]. This logging library integrates with Spark’s existing logging infrastructure, with the added benefit of making logging statements that are below the current logging level even cheaper through the use of Scala macros. h2. Spark SQL This component includes a set of standard physical operators (e.g. Project, Filter, Hash Join, Nested Loop Join, etc.), as well as a set of planning strategies that are used to select the specific physical operators that will be used to execute a given logical query plan. h3. Public Interfaces SqlContext, which takes as an argument a standard SparkContext. This interface provides the ability to register RDDs as tables and run over them SQL statements expressed as strings, returning the results as an RDD. There is also support for registering data stored in Parquet as a table. Finally, there is an experimental DSL that allows queries to be expressed using a LINQ-like syntax. h3. Dependencies Spark SQL’s only dependency is on the Parquet libraries, which have an active community and a small transitive dependency footprint. h2. Hive Support The Hive module adds support to Spark SQL for interacting with data and queries in the Hive ecosystem. This includes: * A mapping from a HiveQL AST to catalyst logical plans / expression trees. * An interface that allows queries to reference tables that are extant in a Hive MetaStore. * A table scan operator that can read data from Hive SerDes. * Wrappers for existing Hive UDFs, UDAFs, and UDTFs. * Support for passing DDL commands back to hive for execution. h3. Public Interfaces The Hive module provides a HiveContext which extends SqlContext with the ability to interact with existing Hive deployments using HiveQL. h3. Dependencies The Hive module has dependencies on Hive 0.12.0, specifically hive-metastore, hive-exec, and hive-serde. While these dependencies are on an unmodified version of Hive that can be obtained from Maven Central, they do introduce significant transitive dependencies to the project. Due to this large dependency tree, and to avoid possible conflicts with dependencies of existing Spark applications, the Hive module is not included in the Spark assembly. Instead, there is a separate, optional Hive assembly that is used only when present. h1. Changes to other Spark Components Dependencies on Catalyst and Spark SQL are added to the Spark assembly. The compute classpath is also changed to use the optional hive assembly if present. h1. Testing infrastructure Each of the submodules of Spark SQL has unit tests. There is also a framework for executing a subset of the query tests that are included in the Hive distribution, which augments the test coverage with an additional 647 multi-part tests. When this framework runs, the query test files are broken up into individual statements and executed using Spark SQL. The result of each query is then compared with a “golden” answer that has been pre-generated using a stock Hive system. While these tests are primarily intended to ensure Hive compatibility, they also act as integration tests for the entire Spark SQL task. h1. Relationship to Shark Unlike Shark, Spark SQL does not act as a drop in replacement for Hive or the HiveServer. Instead this new feature is intended to make it easier for Spark developers to run queries over structured data, using either SQL or the query DSL. After this sub-project graduates from Alpha status it will likely become a new optimizer/backend for the Shark project.", "reporter": "Michael Armbrust", "assignee": "Michael Armbrust", "created": "2014-03-14T12:57:00.000+0000", "updated": "2014-03-26T13:33:21.000+0000", "resolved": "2014-03-20T18:12:09.000+0000", "labels": [], "components": ["SQL"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1251\nSummary: Support for optimizing and executing structured queries\nDescription: This is a proposal to add support for optimizing and executing relational queries in a manner that integrates tightly with the Core Spark API. The goal is to allow Spark users to both run SQL queries over data that is currently stored in RDDs as well as run SQL queries over external data sources (such as Hive), returning the results as an RDD. h1. Components Spark SQL support will be broken into three major components. h2. Catalyst An implementation-agnostic framework for manipulating trees of relational operators and expressions. Catalyst was first discussed during [a talk at the most recent Spark Summit|http://spark-summit.org/talk/armbrust-catalyst-a-query-optimization-framework-for-spark-and-shark/], and a more detailed design document for Catalyst can be found [here|https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPGU5M1Y/edit?usp=sharing]. Catalyst provides three main features: a TreeNode library for transforming trees that are expressed as Scala case classes, a logical plan representation for relational operators, and an expression library. A key design point here was to build an extensible system that could be extended for Streaming SQL and other dataflows that need optimization. h3. Public Interfaces Catalyst has no interfaces that are directly exposed to Spark users. h3. Dependencies Catalyst’s only dependency is on the [scala-logging interface for SLF4J by Typesafe|https://github.com/typesafehub/scala-logging]. This logging library integrates with Spark’s existing logging infrastructure, with the added benefit of making logging statements that are below the current logging level even cheaper through the use of Scala macros. h2. Spark SQL This component includes a set of standard physical operators (e.g. Project, Filter, Hash Join, Nested Loop Join, etc.), as well as a set of planning strategies that are used to select the specific physical operators that will be used to execute a given logical query plan. h3. Public Interfaces SqlContext, which takes as an argument a standard SparkContext. This interface provides the ability to register RDDs as tables and run over them SQL statements expressed as strings, returning the results as an RDD. There is also support for registering data stored in Parquet as a table. Finally, there is an experimental DSL that allows queries to be expressed using a LINQ-like syntax. h3. Dependencies Spark SQL’s only dependency is on the Parquet libraries, which have an active community and a small transitive dependency footprint. h2. Hive Support The Hive module adds support to Spark SQL for interacting with data and queries in the Hive ecosystem. This includes: * A mapping from a HiveQL AST to catalyst logical plans / expression trees. * An interface that allows queries to reference tables that are extant in a Hive MetaStore. * A table scan operator that can read data from Hive SerDes. * Wrappers for existing Hive UDFs, UDAFs, and UDTFs. * Support for passing DDL commands back to hive for execution. h3. Public Interfaces The Hive module provides a HiveContext which extends SqlContext with the ability to interact with existing Hive deployments using HiveQL. h3. Dependencies The Hive module has dependencies on Hive 0.12.0, specifically hive-metastore, hive-exec, and hive-serde. While these dependencies are on an unmodified version of Hive that can be obtained from Maven Central, they do introduce significant transitive dependencies to the project. Due to this large dependency tree, and to avoid possible conflicts with dependencies of existing Spark applications, the Hive module is not included in the Spark assembly. Instead, there is a separate, optional Hive assembly that is used only when present. h1. Changes to other Spark Components Dependencies on Catalyst and Spark SQL are added to the Spark assembly. The compute classpath is also changed to use the optional hive assembly if present. h1. Testing infrastructure Each of the submodules of Spark SQL has unit tests. There is also a framework for executing a subset of the query tests that are included in the Hive distribution, which augments the test coverage with an additional 647 multi-part tests. When this framework runs, the query test files are broken up into individual statements and executed using Spark SQL. The result of each query is then compared with a “golden” answer that has been pre-generated using a stock Hive system. While these tests are primarily intended to ensure Hive compatibility, they also act as integration tests for the entire Spark SQL task. h1. Relationship to Shark Unlike Shark, Spark SQL does not act as a drop in replacement for Hive or the HiveServer. Instead this new feature is intended to make it easier for Spark developers to run queries over structured data, using either SQL or the query DSL. After this sub-project graduates from Alpha status it will likely become a new optimizer/backend for the Shark project.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "60552b38d9183c0e9b7561dc5444fe55", "issue_key": "SPARK-1252", "issue_type": "Bug", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "On YARN, use container-log4j.properties for executors", "description": "YARN provides a log4j.properties file that's distinct from the NodeManager log4j.properties. Containers are supposed to use this so that they don't try to write to the NodeManager log file.", "reporter": "Sandy Ryza", "assignee": "Sandy Ryza", "created": "2014-03-14T14:09:52.000+0000", "updated": "2014-04-07T18:30:05.000+0000", "resolved": "2014-04-07T18:30:05.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Thomas Graves", "body": "https://github.com/apache/spark/pull/148", "created": "2014-04-07T18:29:58.453+0000"}], "num_comments": 1, "text": "Issue: SPARK-1252\nSummary: On YARN, use container-log4j.properties for executors\nDescription: YARN provides a log4j.properties file that's distinct from the NodeManager log4j.properties. Containers are supposed to use this so that they don't try to write to the NodeManager log file.\n\nComments (1):\n1. Thomas Graves: https://github.com/apache/spark/pull/148", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "fd0d8848dcca6e0444975254f668e5bb", "issue_key": "SPARK-1253", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Need to load mapred-site.xml for reading mapreduce.application.classpath", "description": "In Spark on YARN, we use mapreduce.application.classpath to discover the location of the MR jars so that we can add them executor classpaths. This config comes from mapred-site.xml, which we aren't loading.", "reporter": "Sanford Ryza", "assignee": null, "created": "2014-03-14T14:21:22.000+0000", "updated": "2015-12-23T18:03:45.000+0000", "resolved": "2015-12-23T18:03:45.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Devaraj Kavali", "body": "[~sandyr], Do you want to work on this issue? I would like to provide a PR for this if you don't mind, Thanks.", "created": "2015-12-22T11:04:42.251+0000"}, {"author": "Sean R. Owen", "body": "I'm 99% sure Sandy was never working on it, so have unassigned it. I am also not sure this is still relevant.", "created": "2015-12-22T13:29:30.511+0000"}, {"author": "Devaraj Kavali", "body": "Thanks [~srowen] for letting me know. I tried to reproduce it but seems it is loading the mapred-site.xml file and giving the configurations updated in that file. I think it is not a problem anymore and it can be closed unless there are no other expectations here.", "created": "2015-12-23T17:15:01.272+0000"}], "num_comments": 3, "text": "Issue: SPARK-1253\nSummary: Need to load mapred-site.xml for reading mapreduce.application.classpath\nDescription: In Spark on YARN, we use mapreduce.application.classpath to discover the location of the MR jars so that we can add them executor classpaths. This config comes from mapred-site.xml, which we aren't loading.\n\nComments (3):\n1. Devaraj Kavali: [~sandyr], Do you want to work on this issue? I would like to provide a PR for this if you don't mind, Thanks.\n2. Sean R. Owen: I'm 99% sure Sandy was never working on it, so have unassigned it. I am also not sure this is still relevant.\n3. Devaraj Kavali: Thanks [~srowen] for letting me know. I tried to reproduce it but seems it is loading the mapred-site.xml file and giving the configurations updated in that file. I think it is not a problem anymore and it can be closed unless there are no other expectations here.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "184d7f7564ab3da7c2847004fb595db7", "issue_key": "SPARK-1254", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Consolidate, order, and harmonize repository declarations in Maven/SBT builds", "description": "This suggestion addresses a few minor suboptimalities with how repositories are handled. 1) Use HTTPS consistently to access repos, instead of HTTP 2) Consolidate repository declarations in the parent POM file, in the case of the Maven build, so that their ordering can be controlled to put the fully optional Cloudera repo at the end, after required repos. (This was prompted by the untimely failure of the Cloudera repo this week, which made the Spark build fail. #2 would have prevented that.) 3) Update SBT build to match Maven build in this regard 4) Update SBT build to *not* refer to Sonatype snapshot repos. This wasn't in Maven, and a build generally would not refer to external snapshots, but I'm not 100% sure on this one.", "reporter": "Sean R. Owen", "assignee": "Sean R. Owen", "created": "2014-03-14T14:33:30.000+0000", "updated": "2015-01-15T09:08:42.000+0000", "resolved": "2014-03-15T16:46:14.000+0000", "labels": [], "components": ["Build"], "comments": [{"author": "Patrick McFadin", "body": "This was partially reverted to move Maven's repo back to HTTP: https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commitdiff;h=abf6714e27cf07a13819b35a4ca50ff9bb28b65c;hp=646e55405b433fdedc9601dab91f99832b641f87", "created": "2014-03-23T10:57:58.993+0000"}], "num_comments": 1, "text": "Issue: SPARK-1254\nSummary: Consolidate, order, and harmonize repository declarations in Maven/SBT builds\nDescription: This suggestion addresses a few minor suboptimalities with how repositories are handled. 1) Use HTTPS consistently to access repos, instead of HTTP 2) Consolidate repository declarations in the parent POM file, in the case of the Maven build, so that their ordering can be controlled to put the fully optional Cloudera repo at the end, after required repos. (This was prompted by the untimely failure of the Cloudera repo this week, which made the Spark build fail. #2 would have prevented that.) 3) Update SBT build to match Maven build in this regard 4) Update SBT build to *not* refer to Sonatype snapshot repos. This wasn't in Maven, and a build generally would not refer to external snapshots, but I'm not 100% sure on this one.\n\nComments (1):\n1. Patrick McFadin: This was partially reverted to move Maven's repo back to HTTP: https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commitdiff;h=abf6714e27cf07a13819b35a4ca50ff9bb28b65c;hp=646e55405b433fdedc9601dab91f99832b641f87", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "20a177942e9df9272a6c936d6d20ca2d", "issue_key": "SPARK-1255", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Allow user to pass Serializer object instead of class name for shuffle.", "description": "This is more general than simply passing a string name and leaves more room for performance optimizations. Note that this is technically an API breaking change - but I suspect nobody else in this world has used this API other than me in GraphX and Shark.", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2014-03-15T01:10:45.000+0000", "updated": "2014-03-16T09:58:13.000+0000", "resolved": "2014-03-16T09:58:13.000+0000", "labels": [], "components": [], "comments": [{"author": "Reynold Xin", "body": "Pull request submitted: https://github.com/apache/spark/pull/149", "created": "2014-03-15T01:15:04.718+0000"}], "num_comments": 1, "text": "Issue: SPARK-1255\nSummary: Allow user to pass Serializer object instead of class name for shuffle.\nDescription: This is more general than simply passing a string name and leaves more room for performance optimizations. Note that this is technically an API breaking change - but I suspect nobody else in this world has used this API other than me in GraphX and Shark.\n\nComments (1):\n1. Reynold Xin: Pull request submitted: https://github.com/apache/spark/pull/149", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "99c8d01e2506aeb51e5288bf80bcf4f1", "issue_key": "SPARK-1256", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Master web UI and Worker web UI returns a 404 error", "description": "", "reporter": "Guoqiang Li", "assignee": "Guoqiang Li", "created": "2014-03-15T03:58:22.000+0000", "updated": "2014-03-19T10:57:45.000+0000", "resolved": "2014-03-18T21:58:23.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1256\nSummary: Master web UI and Worker web UI returns a 404 error", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "f7307f0ddde0a0ba70132f4b836b6d40", "issue_key": "SPARK-1257", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Endless running task when using pyspark with input file containing a long line", "description": "When launching any pyspark applications with an input file containing a very long line(about 70000 characters), the job will be hanging and never stops. The application UI shows that there is a task running endlessly. There will be no problem using the scala version with the same input.", "reporter": "Hanchen Su", "assignee": "Josh Rosen", "created": "2014-03-15T04:36:33.000+0000", "updated": "2014-07-25T21:53:33.000+0000", "resolved": "2014-07-25T21:53:33.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Josh Rosen", "body": "I suspect that this was caused by SPARK-1043, which has been fixed for Spark 0.9.1.", "created": "2014-03-16T11:21:29.175+0000"}, {"author": "Matthew Farrellee", "body": "recommend close as resolved w/ option for filer to reopen if the issue reproduces in 1.0 /cc: [~pwendell] [~joshrosen]", "created": "2014-07-02T14:00:49.171+0000"}], "num_comments": 2, "text": "Issue: SPARK-1257\nSummary: Endless running task when using pyspark with input file containing a long line\nDescription: When launching any pyspark applications with an input file containing a very long line(about 70000 characters), the job will be hanging and never stops. The application UI shows that there is a task running endlessly. There will be no problem using the scala version with the same input.\n\nComments (2):\n1. Josh Rosen: I suspect that this was caused by SPARK-1043, which has been fixed for Spark 0.9.1.\n2. Matthew Farrellee: recommend close as resolved w/ option for filer to reopen if the issue reproduces in 1.0 /cc: [~pwendell] [~joshrosen]", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "f5357644b4b0dd25f924089b03781c58", "issue_key": "SPARK-1258", "issue_type": "Improvement", "status": "Resolved", "priority": "Trivial", "resolution": null, "summary": "RDD.countByValue optimization", "description": "Class Object2LongOpenHashMap has method add(key, incr) (addTo in new version) for incrementation value assigned to the key. It should be faster than currently used map.put(v, map.getLong(v) + 1L) .", "reporter": "Jaroslav Kamenik", "assignee": null, "created": "2014-03-15T05:52:30.000+0000", "updated": "2014-09-15T14:47:10.000+0000", "resolved": "2014-09-15T14:47:10.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Jaroslav Kamenik", "body": "same is used in RDD.countByValueApprox and similar in RDD.countByValue.mergeMaps - m1.put(entry.getKey, m1.getLong(entry.getKey) + entry.getLongValue) and GroupedCountEvaluator.merge - sums.put(entry.getKey, sums.getLong(entry.getKey) + entry.getLongValue)", "created": "2014-03-15T06:52:25.931+0000"}, {"author": "Sean R. Owen", "body": "I'm taking the liberty of closing this, since this refers to an optimization using fastutil classes, which were removed from Spark. An equivalent optimization is employed now, using Spark's OpenHashMap.", "created": "2014-09-15T14:47:10.657+0000"}], "num_comments": 2, "text": "Issue: SPARK-1258\nSummary: RDD.countByValue optimization\nDescription: Class Object2LongOpenHashMap has method add(key, incr) (addTo in new version) for incrementation value assigned to the key. It should be faster than currently used map.put(v, map.getLong(v) + 1L) .\n\nComments (2):\n1. Jaroslav Kamenik: same is used in RDD.countByValueApprox and similar in RDD.countByValue.mergeMaps - m1.put(entry.getKey, m1.getLong(entry.getKey) + entry.getLongValue) and GroupedCountEvaluator.merge - sums.put(entry.getKey, sums.getLong(entry.getKey) + entry.getLongValue)\n2. Sean R. Owen: I'm taking the liberty of closing this, since this refers to an optimization using fastutil classes, which were removed from Spark. An equivalent optimization is employed now, using Spark's OpenHashMap.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "00e9f4c6fda08225d48ea637cb5009ca", "issue_key": "SPARK-1259", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Make RDD locally iterable", "description": "I've got big RDD(1gb) in yarn cluster. On local machine, which use this cluster I have only 512 mb. I'd like to iterate over values in RDD on my local machine. I can't use collect(), because it would create too big array locally which more then my heap. I need some iterative way.", "reporter": "Egor Pakhomov", "assignee": "Egor Pakhomov", "created": "2014-03-16T05:10:25.000+0000", "updated": "2015-01-28T20:40:49.000+0000", "resolved": "2014-04-06T23:44:04.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Egor Pakhomov", "body": "https://github.com/apache/spark/pull/156", "created": "2014-03-16T05:17:51.033+0000"}, {"author": "Mark Hamstra", "body": "Fix version should probably be 1.0.x, since this is new functionality, not bug fix.", "created": "2014-03-17T10:28:04.913+0000"}, {"author": "Patrick McFadin", "body": "Ah yes good catch.", "created": "2014-03-17T12:22:48.537+0000"}], "num_comments": 3, "text": "Issue: SPARK-1259\nSummary: Make RDD locally iterable\nDescription: I've got big RDD(1gb) in yarn cluster. On local machine, which use this cluster I have only 512 mb. I'd like to iterate over values in RDD on my local machine. I can't use collect(), because it would create too big array locally which more then my heap. I need some iterative way.\n\nComments (3):\n1. Egor Pakhomov: https://github.com/apache/spark/pull/156\n2. Mark Hamstra: Fix version should probably be 1.0.x, since this is new functionality, not bug fix.\n3. Patrick McFadin: Ah yes good catch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "7ed90ea4d783d7a8d52488f44a1ca7af", "issue_key": "SPARK-1260", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "faster construction of features with intercept", "description": "The current implementation uses `Array(1.0, features: _*)` to construct a new array with intercept. This is not efficient for big arrays because `Array.apply` uses a for loop that iterates over the arguments. `Array.+:` is a better choice here.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-03-17T11:47:41.000+0000", "updated": "2014-03-18T19:29:16.000+0000", "resolved": "2014-03-18T19:29:16.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/spark/pull/161", "created": "2014-03-17T11:50:48.158+0000"}, {"author": "Xiangrui Meng", "body": "To mark fix versions.", "created": "2014-03-18T19:22:38.204+0000"}, {"author": "Xiangrui Meng", "body": "To mark resolved.", "created": "2014-03-18T19:29:09.789+0000"}, {"author": "Xiangrui Meng", "body": "merged.", "created": "2014-03-18T19:29:16.189+0000"}], "num_comments": 4, "text": "Issue: SPARK-1260\nSummary: faster construction of features with intercept\nDescription: The current implementation uses `Array(1.0, features: _*)` to construct a new array with intercept. This is not efficient for big arrays because `Array.apply` uses a for loop that iterates over the arguments. `Array.+:` is a better choice here.\n\nComments (4):\n1. Xiangrui Meng: PR: https://github.com/apache/spark/pull/161\n2. Xiangrui Meng: To mark fix versions.\n3. Xiangrui Meng: To mark resolved.\n4. Xiangrui Meng: merged.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "3e978ab81894ff1cc911f49e6fbdae99", "issue_key": "SPARK-1261", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Docs don't explain how to run Python examples", "description": "The main Spark docs overview page has a section called Running the Examples and Shell, which explains how to start both the Scala and Python spark shells, but only how to run the Scala examples, not the python examples.", "reporter": "Diana Carroll", "assignee": "Diana Carroll", "created": "2014-03-17T12:02:47.000+0000", "updated": "2014-03-17T17:37:51.000+0000", "resolved": "2014-03-17T17:37:51.000+0000", "labels": [], "components": ["Documentation"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1261\nSummary: Docs don't explain how to run Python examples\nDescription: The main Spark docs overview page has a section called Running the Examples and Shell, which explains how to start both the Scala and Python spark shells, but only how to run the Scala examples, not the python examples.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "feb4e6b128bbb776df1963e4394053fd", "issue_key": "SPARK-1262", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Numerical drift in computation of matrix inverse leads to invalid results in ALS", "description": "In what follows I cannot offer an expert analysis and remedy, however I will describe the problem I'm seeing and the strategy I've used to mitigate it. The ALS {{updateBlock()}} method includes a call to {{Solve.solvePositive()}} from JBlas. Generally speaking, when we call {{solvePositive(A, B)}} for symmetric, positive definite {{A}}, this method computes {{x}} from the matrix equation {{Ax = B}}. Or, in other words, it computes {{A}}^-1^{{B}}. As mentioned, one of the preconditions on A is that it be symmetric. In ALS, we call this method on {{fullXtX}} or {{fullXtX.add(YtY.value.get)}}, both of which should be symmetric. However, for implicit ALS and rank > 1, some kind of imprecision in the computation of this inverse tends to make successive values of {{fullXtX.add(YtY.value.get)}} less and less symmetric. From my experience this can and does alter the model produced by ALS significantly, leading to very different and counterintuitive user-item recommendations compared to a solution in which this asymmetry does not exist. An approach I've seen taken against this problem from the Oryx codebase is to cast each value in the vector returned from {{Solve.solvePositive()}} to a float. That has worked for me. I've also tried alternative implementations of a linear system solver. The solver that Oryx uses, {{RRQRDecomposition}} from commons math v3, seems to lead to better solutions, but still drifts unless the results are cast to floats. The Colt numerical computing libraries include a solver called {{QRDecomposition}}, which may be superior to the one used in JBlas. However, I haven't tested it. I'm working on a unit test to exercise this bug. I discovered it by \"tracing\" the behavior of {{ALS.scala}} with various logging statements inserted into the code. If nothing else, add a line before the calls to {{Solve.solvePositive()}} in {{updateBlock()}} to validate that {{fullXtX}} or {{fullXtX.add(YtY.value.get)}} are symmetric.", "reporter": "Michael Allman", "assignee": null, "created": "2014-03-17T16:34:01.000+0000", "updated": "2015-01-21T06:58:28.000+0000", "resolved": "2014-03-18T16:41:36.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "Solve.solvePositive calls NativeBLAS.dposv, which only looks at the upper triangular part of the matrix to compute Cholesky factorization while ignoring the lower triangular part. It doesn't seem to be the root cause of the problem. It would be super helpful if you can create a unit test to re-produce the bug. -Xiangrui", "created": "2014-03-17T17:18:41.534+0000"}, {"author": "Sean Owen", "body": "Agree, though this might still be a symptom of something of interest. Michael is it YtY that is not symmetric? I can't see how fullXtX would not be symmetric. If it's not, somehow I'd want to figure out just why. Maybe I can look with a debugger too. Are you saying you see this in Oryx too? which matrix \"still drifts\"? The use of floats is not driven by this issue but simply storage economy. It might mask some round-off error somewhere as a side-effect. FWIW Oryx does not use jblas or the Cholesky decomposition. It uses the QR decomposition to solve Ax=B. QR is somewhat slower but might give better accuracy. This alone could be the difference. I had never run empirical tests to figure out how much it might impact the results. I suppose you could hack up the code to use CholeskyDecomposition to see if it gives more similar results (I can help offline with that.)", "created": "2014-03-17T18:02:30.146+0000"}, {"author": "Michael Allman", "body": "Producing a unit test for this may be infeasible at this time. For one thing, the \"unit\" here is hidden behind private methods. For another, there's no way to initialize the algorithm to a known value. What if I provided a patched version of ALS.scala with log tracing and a driver program instead? I can make the problem visually obvious that way.", "created": "2014-03-18T11:38:42.029+0000"}, {"author": "Xiangrui Meng", "body": "A patched ALS with a sample data would be sufficient. Do worry about private methods.", "created": "2014-03-18T12:56:46.866+0000"}, {"author": "Michael Allman", "body": "The underlying assertion of this bug report is that the implementation is producing some asymmetric matrices that should be symmetric. However, I misread my debugging session output. The output matrices I was interpreting as asymmetric are in fact symmetric.", "created": "2014-03-18T16:41:36.111+0000"}], "num_comments": 5, "text": "Issue: SPARK-1262\nSummary: Numerical drift in computation of matrix inverse leads to invalid results in ALS\nDescription: In what follows I cannot offer an expert analysis and remedy, however I will describe the problem I'm seeing and the strategy I've used to mitigate it. The ALS {{updateBlock()}} method includes a call to {{Solve.solvePositive()}} from JBlas. Generally speaking, when we call {{solvePositive(A, B)}} for symmetric, positive definite {{A}}, this method computes {{x}} from the matrix equation {{Ax = B}}. Or, in other words, it computes {{A}}^-1^{{B}}. As mentioned, one of the preconditions on A is that it be symmetric. In ALS, we call this method on {{fullXtX}} or {{fullXtX.add(YtY.value.get)}}, both of which should be symmetric. However, for implicit ALS and rank > 1, some kind of imprecision in the computation of this inverse tends to make successive values of {{fullXtX.add(YtY.value.get)}} less and less symmetric. From my experience this can and does alter the model produced by ALS significantly, leading to very different and counterintuitive user-item recommendations compared to a solution in which this asymmetry does not exist. An approach I've seen taken against this problem from the Oryx codebase is to cast each value in the vector returned from {{Solve.solvePositive()}} to a float. That has worked for me. I've also tried alternative implementations of a linear system solver. The solver that Oryx uses, {{RRQRDecomposition}} from commons math v3, seems to lead to better solutions, but still drifts unless the results are cast to floats. The Colt numerical computing libraries include a solver called {{QRDecomposition}}, which may be superior to the one used in JBlas. However, I haven't tested it. I'm working on a unit test to exercise this bug. I discovered it by \"tracing\" the behavior of {{ALS.scala}} with various logging statements inserted into the code. If nothing else, add a line before the calls to {{Solve.solvePositive()}} in {{updateBlock()}} to validate that {{fullXtX}} or {{fullXtX.add(YtY.value.get)}} are symmetric.\n\nComments (5):\n1. Xiangrui Meng: Solve.solvePositive calls NativeBLAS.dposv, which only looks at the upper triangular part of the matrix to compute Cholesky factorization while ignoring the lower triangular part. It doesn't seem to be the root cause of the problem. It would be super helpful if you can create a unit test to re-produce the bug. -Xiangrui\n2. Sean Owen: Agree, though this might still be a symptom of something of interest. Michael is it YtY that is not symmetric? I can't see how fullXtX would not be symmetric. If it's not, somehow I'd want to figure out just why. Maybe I can look with a debugger too. Are you saying you see this in Oryx too? which matrix \"still drifts\"? The use of floats is not driven by this issue but simply storage economy. It might mask some round-off error somewhere as a side-effect. FWIW Oryx does not use jblas or the Cholesky decomposition. It uses the QR decomposition to solve Ax=B. QR is somewhat slower but might give better accuracy. This alone could be the difference. I had never run empirical tests to figure out how much it might impact the results. I suppose you could hack up the code to use CholeskyDecomposition to see if it gives more similar results (I can help offline with that.)\n3. Michael Allman: Producing a unit test for this may be infeasible at this time. For one thing, the \"unit\" here is hidden behind private methods. For another, there's no way to initialize the algorithm to a known value. What if I provided a patched version of ALS.scala with log tracing and a driver program instead? I can make the problem visually obvious that way.\n4. Xiangrui Meng: A patched ALS with a sample data would be sufficient. Do worry about private methods.\n5. Michael Allman: The underlying assertion of this bug report is that the implementation is producing some asymmetric matrices that should be symmetric. However, I misread my debugging session output. The output matrices I was interpreting as asymmetric are in fact symmetric.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "fa8c0129cf8efe4b29a6137f21bbba23", "issue_key": "SPARK-1263", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Implicit ALS unnecessarily recomputes factor matrices", "description": "Implicit ALS unnecessarily recomputes the user and product factor matrices. As suggested by Xiangrui Meng on the mailing list, this is probably due to their reuse in computing YtY and YtCuY. I suggest persisting these matrices to avoid this duplicate computation. I'm working on a simple pull request to address this issue.", "reporter": "Michael Allman", "assignee": "Xiangrui Meng", "created": "2014-03-17T17:05:29.000+0000", "updated": "2014-03-17T23:57:31.000+0000", "resolved": "2014-03-17T23:56:36.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "https://spark-project.atlassian.net/browse/SPARK-1266", "created": "2014-03-17T23:56:36.667+0000"}, {"author": "Xiangrui Meng", "body": "The PR mentioned in SPARK-1266 should fix this.", "created": "2014-03-17T23:57:31.670+0000"}], "num_comments": 2, "text": "Issue: SPARK-1263\nSummary: Implicit ALS unnecessarily recomputes factor matrices\nDescription: Implicit ALS unnecessarily recomputes the user and product factor matrices. As suggested by Xiangrui Meng on the mailing list, this is probably due to their reuse in computing YtY and YtCuY. I suggest persisting these matrices to avoid this duplicate computation. I'm working on a simple pull request to address this issue.\n\nComments (2):\n1. Xiangrui Meng: https://spark-project.atlassian.net/browse/SPARK-1266\n2. Xiangrui Meng: The PR mentioned in SPARK-1266 should fix this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "ce99f8d47cc8424b10c39d4d86cb9e69", "issue_key": "SPARK-1264", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Documentation for setting heap sizes across all configurations", "description": "As a user, there are lots of places to configure heap sizes, and it takes a bit of trial and error to figure out how to configure what you want. We need some more clear documentation on how set these for the cross product of Spark components (master, worker, executor, driver, shell) and deployment modes (Standalone, YARN, Mesos, EC2?). I'm happy to do the authoring if someone can help pull together the relevant details. Here's the best I've got so far:  # Standalone cluster Master - SPARK_DAEMON_MEMORY - default: 512mb Worker - SPARK_DAEMON_MEMORY vs SPARK_WORKER_MEMORY? - default: ? See WorkerArguments.inferDefaultMemory() Executor - spark.executor.memory Driver - SPARK_DRIVER_MEMORY - default: 512mb Shell - A pre-built driver so SPARK_DRIVER_MEMORY - default: 512mb # EC2 cluster Master - ? Worker - ? Executor - ? Driver - ? Shell - ? # Mesos cluster Master - SPARK_DAEMON_MEMORY Worker - SPARK_DAEMON_MEMORY Executor - SPARK_EXECUTOR_MEMORY Driver - SPARK_DRIVER_MEMORY Shell - A pre-built driver so SPARK_DRIVER_MEMORY # YARN cluster Master - SPARK_MASTER_MEMORY ? Worker - SPARK_WORKER_MEMORY ? Executor - SPARK_EXECUTOR_MEMORY Driver - SPARK_DRIVER_MEMORY Shell - A pre-built driver so SPARK_DRIVER_MEMORY", "reporter": "Andrew Ash", "assignee": null, "created": "2014-03-17T18:33:32.000+0000", "updated": "2016-01-04T14:45:17.000+0000", "resolved": "2016-01-04T14:45:17.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Andrew Ash", "body": "Here's what I've seen can control what in various cluster configurations, but I'm not sure I caught them all. Master: SPARK_DAEMON_MEMORY SPARK_MASTER_MEMORY SPARK_MASTER_OPTS (Xmx) Worker: SPARK_DAEMON_MEMORY SPARK_WORKER_MEMORY SPARK_WORKER_OPTS (Xmx) Executor: SPARK_EXECUTOR_MEMORY SPARK_EXECUTOR_OPTS (Xmx) spark.executor.memory Driver: SPARK_DRIVER_MEMORY SPARK_REPL_OPTS (Xmx)", "created": "2014-03-17T18:39:18.939+0000"}, {"author": "Aaron Davidson", "body": "In YARN mode, SPARK_MASTER_MEMORY and SPARK_WORKER_MEMORY were actually misnomers -- they actually configured the driver and executor memory, respectively. We have deprecated it so it is no longer documented. In standalone mode, SPARK_WORKER_MEMORY is actually something completely different: it is the amount of memory that a worker advertises as available for drivers to launch executors. The sum of the memory used by executors spawned from a worker cannot exceed SPARK_WORKER_MEMORY. It is documented in our standalone cluster configuration. The *_OPTS series cannot be generally used to configure memory, because ./spark-class ALWAYS sets Xms and Xmx, even when we just use the default memory of 512MB. Also, not related to memory, but SPARK_JAVA_OPTS is used by all four components.", "created": "2014-03-19T10:08:48.184+0000"}, {"author": "Sean R. Owen", "body": "I think this is mostly obsolete by now", "created": "2016-01-04T14:45:17.685+0000"}], "num_comments": 3, "text": "Issue: SPARK-1264\nSummary: Documentation for setting heap sizes across all configurations\nDescription: As a user, there are lots of places to configure heap sizes, and it takes a bit of trial and error to figure out how to configure what you want. We need some more clear documentation on how set these for the cross product of Spark components (master, worker, executor, driver, shell) and deployment modes (Standalone, YARN, Mesos, EC2?). I'm happy to do the authoring if someone can help pull together the relevant details. Here's the best I've got so far:  # Standalone cluster Master - SPARK_DAEMON_MEMORY - default: 512mb Worker - SPARK_DAEMON_MEMORY vs SPARK_WORKER_MEMORY? - default: ? See WorkerArguments.inferDefaultMemory() Executor - spark.executor.memory Driver - SPARK_DRIVER_MEMORY - default: 512mb Shell - A pre-built driver so SPARK_DRIVER_MEMORY - default: 512mb # EC2 cluster Master - ? Worker - ? Executor - ? Driver - ? Shell - ? # Mesos cluster Master - SPARK_DAEMON_MEMORY Worker - SPARK_DAEMON_MEMORY Executor - SPARK_EXECUTOR_MEMORY Driver - SPARK_DRIVER_MEMORY Shell - A pre-built driver so SPARK_DRIVER_MEMORY # YARN cluster Master - SPARK_MASTER_MEMORY ? Worker - SPARK_WORKER_MEMORY ? Executor - SPARK_EXECUTOR_MEMORY Driver - SPARK_DRIVER_MEMORY Shell - A pre-built driver so SPARK_DRIVER_MEMORY\n\nComments (3):\n1. Andrew Ash: Here's what I've seen can control what in various cluster configurations, but I'm not sure I caught them all. Master: SPARK_DAEMON_MEMORY SPARK_MASTER_MEMORY SPARK_MASTER_OPTS (Xmx) Worker: SPARK_DAEMON_MEMORY SPARK_WORKER_MEMORY SPARK_WORKER_OPTS (Xmx) Executor: SPARK_EXECUTOR_MEMORY SPARK_EXECUTOR_OPTS (Xmx) spark.executor.memory Driver: SPARK_DRIVER_MEMORY SPARK_REPL_OPTS (Xmx)\n2. Aaron Davidson: In YARN mode, SPARK_MASTER_MEMORY and SPARK_WORKER_MEMORY were actually misnomers -- they actually configured the driver and executor memory, respectively. We have deprecated it so it is no longer documented. In standalone mode, SPARK_WORKER_MEMORY is actually something completely different: it is the amount of memory that a worker advertises as available for drivers to launch executors. The sum of the memory used by executors spawned from a worker cannot exceed SPARK_WORKER_MEMORY. It is documented in our standalone cluster configuration. The *_OPTS series cannot be generally used to configure memory, because ./spark-class ALWAYS sets Xms and Xmx, even when we just use the default memory of 512MB. Also, not related to memory, but SPARK_JAVA_OPTS is used by all four components.\n3. Sean R. Owen: I think this is mostly obsolete by now", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "6cc567e704833b557019949154151e2d", "issue_key": "SPARK-1265", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Fix 404 not found error in UI introduced in Jetty 9.0 upgrade", "description": "We recently upgraded Jetty from v7.6.8 to v9.1.3. This introduced a 404 not found HTTP error when accessing the root of any UI. The problem is that the existing code attaches two handlers to the root, one of which is a handler for static resources. In Jetty 9.1.3, we are no longer allowed to do this. Instead, we are supposed to use ResourceHandler rather than ServletContextHandler for serving static resources: http://stackoverflow.com/questions/10284584/serving-static-files-w-embedded-jetty.", "reporter": "Andrew Or", "assignee": "Andrew Or", "created": "2014-03-17T19:50:10.000+0000", "updated": "2014-03-19T10:57:45.000+0000", "resolved": "2014-03-19T10:54:44.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Andrew Or", "body": "Fixed in 1256, or PR #150.", "created": "2014-03-19T10:54:44.928+0000"}], "num_comments": 1, "text": "Issue: SPARK-1265\nSummary: Fix 404 not found error in UI introduced in Jetty 9.0 upgrade\nDescription: We recently upgraded Jetty from v7.6.8 to v9.1.3. This introduced a 404 not found HTTP error when accessing the root of any UI. The problem is that the existing code attaches two handlers to the root, one of which is a handler for static resources. In Jetty 9.1.3, we are no longer allowed to do this. Instead, we are supposed to use ResourceHandler rather than ServletContextHandler for serving static resources: http://stackoverflow.com/questions/10284584/serving-static-files-w-embedded-jetty.\n\nComments (1):\n1. Andrew Or: Fixed in 1256, or PR #150.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "bd2769e2c397ca95895f53e6c89d8064", "issue_key": "SPARK-1266", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Persist factors in implicit ALS", "description": "In implicit ALS computation, the user or product factor is used twice in each iteration. Caching can certainly help accelerate the computation.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-03-17T21:40:44.000+0000", "updated": "2014-03-18T17:21:55.000+0000", "resolved": "2014-03-18T17:21:55.000+0000", "labels": [], "components": ["MLlib"], "comments": [{"author": "Xiangrui Meng", "body": "PR: https://github.com/apache/spark/pull/165", "created": "2014-03-17T21:53:17.900+0000"}], "num_comments": 1, "text": "Issue: SPARK-1266\nSummary: Persist factors in implicit ALS\nDescription: In implicit ALS computation, the user or product factor is used twice in each iteration. Caching can certainly help accelerate the computation.\n\nComments (1):\n1. Xiangrui Meng: PR: https://github.com/apache/spark/pull/165", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "8655410ef80b1d796f0091159d736f89", "issue_key": "SPARK-1270", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "An optimized gradient descent implementation", "description": "Current implementation of GradientDescent is inefficient in some aspects, especially in high-latency network. I propose a new implementation of GradientDescent, which follows a parallelism model called GradientDescentWithLocalUpdate, inspired by Jeff Dean's DistBelief and Eric Xing's SSP. With a few modifications of runMiniBatchSGD, the GradientDescentWithLocalUpdate can outperform the original sequential version by about 4x without sacrificing accuracy, and can be easily adopted by most classification and regression algorithms in MLlib.", "reporter": "Xusen Yin", "assignee": null, "created": "2014-03-17T23:24:27.000+0000", "updated": "2016-01-16T13:31:28.000+0000", "resolved": "2016-01-16T13:31:28.000+0000", "labels": ["GradientDescent", "MLLib,"], "components": ["MLlib"], "comments": [{"author": "Peng Cheng", "body": "Yo, any follow up story on this one? I'm curious to know the local update part, as DistBelief has non-local model server shards.", "created": "2014-10-02T18:41:38.078+0000"}, {"author": "Apache Spark", "body": "User 'yoshidakuy' has created a pull request for this issue: https://github.com/apache/spark/pull/10663", "created": "2016-01-08T08:11:04.820+0000"}], "num_comments": 2, "text": "Issue: SPARK-1270\nSummary: An optimized gradient descent implementation\nDescription: Current implementation of GradientDescent is inefficient in some aspects, especially in high-latency network. I propose a new implementation of GradientDescent, which follows a parallelism model called GradientDescentWithLocalUpdate, inspired by Jeff Dean's DistBelief and Eric Xing's SSP. With a few modifications of runMiniBatchSGD, the GradientDescentWithLocalUpdate can outperform the original sequential version by about 4x without sacrificing accuracy, and can be easily adopted by most classification and regression algorithms in MLlib.\n\nComments (2):\n1. Peng Cheng: Yo, any follow up story on this one? I'm curious to know the local update part, as DistBelief has non-local model server shards.\n2. Apache Spark: User 'yoshidakuy' has created a pull request for this issue: https://github.com/apache/spark/pull/10663", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "990421ffb54665384b0a20a9094b8fdd", "issue_key": "SPARK-1267", "issue_type": "Sub-task", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Add a pip installer for PySpark", "description": "Please refer to this mail archive, http://mail-archives.apache.org/mod_mbox/spark-user/201311.mbox/%3CCAOEPXP7jKiw-3M8eh2giBcs8gEkZ1upHpGb=FqOUcVSCYwjhNg@mail.gmail.com%3E", "reporter": "Prabin Banka", "assignee": "Holden Karau", "created": "2014-03-18T02:46:06.000+0000", "updated": "2016-11-17T04:16:31.000+0000", "resolved": "2016-11-16T22:23:15.000+0000", "labels": ["pyspark"], "components": ["PySpark"], "comments": [{"author": "Prabin Banka", "body": "We can write a simple setup.py file, for pyspark source distribution. Any end user, who intend to use pyspark modules need to do a pip install of pyspark and set the SPARK_HOME env variable, before importing the pyspark into this code. Also, we can introduce one more environment variable, say SPARK_VERSION, this needs to be validated against the pyspark installed version, during the import time. A dictionary could be maintained in a text file under spark/python, to validate the compatibility of pyspark and spark. Will this be sufficient ?", "created": "2014-03-19T02:18:51.111+0000"}, {"author": "Prabin Banka", "body": "@Josh.. please comment.", "created": "2014-03-19T02:20:00.826+0000"}, {"author": "Alex Gaudio", "body": "I'm all for pip installable pyspark, but I'm confused about the ideal way to install the pyspark code. I'd also prefer to avoid introducing an extra variable, SPARK_VERSION. It seems to me that if we had the typical setup.py file that downloaded code from PyPi, then users would have to deal with differences in dependencies between the python version in PyPi and in their code pointed to by SPARK_HOME. Additionally, users would still need to download the spark jars or set SPARK_HOME, which means two (possibly different) versions of the python code are flying around. The fact that users have to manage the version, download spark into SPARK_HOME, and pip install pyspark doesn't seem quite right. What do you think about this: We create a setup.py file that requires SPARK_HOME be set in the environment (requiring that the user have downloaded Spark) BEFORE the pyspark code gets installed. An additional idea we could consider: Then, when pip or a user calls pyspark, we have \"python setup.py install\" redirect to \"python setup.py develop.\" This installs pyspark in \"development mode\" and means that the pyspark code pointed to by $SPARK_HOME/python is the source of truth. (more about development mode here: https://pythonhosted.org/setuptools/setuptools.html#development-mode). My thinking for this is that since users need to specify SPARK_HOME, we might as well keep the python library with the spark code (as it currently is) to avoid potential compatibility conflicts. As a maintainer, we also don't need to update PyPi with the latest version of pyspark. Using \"develop mode\" as default may be a bad idea. I also don't know how to automatically prefer \"setup.py develop\" over \"setup.py install\". Last, and perhaps most obvious, if we create a setup.py file, we could also probably no longer include the py4j egg in the spark downloads as we'd rely on setuptools to provide the external libraries.", "created": "2014-08-02T04:24:48.309+0000"}, {"author": "Chandan Kumar", "body": "[~adgaudio] I had similar reservations about the approach. I will try to investigate the possibility of using 'develop' mode.", "created": "2014-08-20T05:11:42.338+0000"}, {"author": "Davies Liu", "body": "Because PySpark depends on Spark packages, Python user can not use it after 'pip install pyspark', so there is not too much benefits from this. Once we release PySpark separated from Spark, then we should keep the compatability across versions of PySpark and Spark, it will be a nightmare for us (we can not move fast to improve the implementation of PySpark). So, I think we can not do this in near future. [~prabinb], do you mind to close the PR?", "created": "2014-10-31T19:11:31.475+0000"}, {"author": "Prabin Banka", "body": "Closing this PR for now.", "created": "2014-11-08T11:37:56.323+0000"}, {"author": "Apache Spark", "body": "User 'alope107' has created a pull request for this issue: https://github.com/apache/spark/pull/8318", "created": "2015-08-19T19:18:10.276+0000"}, {"author": "Holden Karau", "body": "re-opening after discussion on mailing list and PR thread.", "created": "2016-10-27T00:49:33.976+0000"}, {"author": "Apache Spark", "body": "User 'holdenk' has created a pull request for this issue: https://github.com/apache/spark/pull/15659", "created": "2016-10-27T08:43:05.261+0000"}, {"author": "Josh Rosen", "body": "Merged into master (2.2) and will consider for 2.1.", "created": "2016-11-16T22:23:16.057+0000"}], "num_comments": 10, "text": "Issue: SPARK-1267\nSummary: Add a pip installer for PySpark\nDescription: Please refer to this mail archive, http://mail-archives.apache.org/mod_mbox/spark-user/201311.mbox/%3CCAOEPXP7jKiw-3M8eh2giBcs8gEkZ1upHpGb=FqOUcVSCYwjhNg@mail.gmail.com%3E\n\nComments (10):\n1. Prabin Banka: We can write a simple setup.py file, for pyspark source distribution. Any end user, who intend to use pyspark modules need to do a pip install of pyspark and set the SPARK_HOME env variable, before importing the pyspark into this code. Also, we can introduce one more environment variable, say SPARK_VERSION, this needs to be validated against the pyspark installed version, during the import time. A dictionary could be maintained in a text file under spark/python, to validate the compatibility of pyspark and spark. Will this be sufficient ?\n2. Prabin Banka: @Josh.. please comment.\n3. Alex Gaudio: I'm all for pip installable pyspark, but I'm confused about the ideal way to install the pyspark code. I'd also prefer to avoid introducing an extra variable, SPARK_VERSION. It seems to me that if we had the typical setup.py file that downloaded code from PyPi, then users would have to deal with differences in dependencies between the python version in PyPi and in their code pointed to by SPARK_HOME. Additionally, users would still need to download the spark jars or set SPARK_HOME, which means two (possibly different) versions of the python code are flying around. The fact that users have to manage the version, download spark into SPARK_HOME, and pip install pyspark doesn't seem quite right. What do you think about this: We create a setup.py file that requires SPARK_HOME be set in the environment (requiring that the user have downloaded Spark) BEFORE the pyspark code gets installed. An additional idea we could consider: Then, when pip or a user calls pyspark, we have \"python setup.py install\" redirect to \"python setup.py develop.\" This installs pyspark in \"development mode\" and means that the pyspark code pointed to by $SPARK_HOME/python is the source of truth. (more about development mode here: https://pythonhosted.org/setuptools/setuptools.html#development-mode). My thinking for this is that since users need to specify SPARK_HOME, we might as well keep the python library with the spark code (as it currently is) to avoid potential compatibility conflicts. As a maintainer, we also don't need to update PyPi with the latest version of pyspark. Using \"develop mode\" as default may be a bad idea. I also don't know how to automatically prefer \"setup.py develop\" over \"setup.py install\". Last, and perhaps most obvious, if we create a setup.py file, we could also probably no longer include the py4j egg in the spark downloads as we'd rely on setuptools to provide the external libraries.\n4. Chandan Kumar: [~adgaudio] I had similar reservations about the approach. I will try to investigate the possibility of using 'develop' mode.\n5. Davies Liu: Because PySpark depends on Spark packages, Python user can not use it after 'pip install pyspark', so there is not too much benefits from this. Once we release PySpark separated from Spark, then we should keep the compatability across versions of PySpark and Spark, it will be a nightmare for us (we can not move fast to improve the implementation of PySpark). So, I think we can not do this in near future. [~prabinb], do you mind to close the PR?\n6. Prabin Banka: Closing this PR for now.\n7. Apache Spark: User 'alope107' has created a pull request for this issue: https://github.com/apache/spark/pull/8318\n8. Holden Karau: re-opening after discussion on mailing list and PR thread.\n9. Apache Spark: User 'holdenk' has created a pull request for this issue: https://github.com/apache/spark/pull/15659\n10. Josh Rosen: Merged into master (2.2) and will consider for 2.1.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "0641848d1a72dca70811d50af69563a1", "issue_key": "SPARK-1268", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Adding XOR and AND-NOT operations to spark.util.collection.BitSet", "description": "BitSet collection is missing some important bit-wise operations. Symmetric difference (xor) in particular is useful for computing some distance metrics (e.g. Hamming). Difference (and-not) as well.", "reporter": "Petko Nikolov", "assignee": null, "created": "2014-03-18T08:18:53.000+0000", "updated": "2014-04-30T00:41:51.000+0000", "resolved": "2014-04-30T00:41:51.000+0000", "labels": ["starter"], "components": ["Spark Core"], "comments": [{"author": "Petko Nikolov", "body": "PR: https://github.com/apache/spark/pull/172", "created": "2014-03-18T08:24:25.023+0000"}], "num_comments": 1, "text": "Issue: SPARK-1268\nSummary: Adding XOR and AND-NOT operations to spark.util.collection.BitSet\nDescription: BitSet collection is missing some important bit-wise operations. Symmetric difference (xor) in particular is useful for computing some distance metrics (e.g. Hamming). Difference (and-not) as well.\n\nComments (1):\n1. Petko Nikolov: PR: https://github.com/apache/spark/pull/172", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "35e69a0a7ce0b1d991e965a66b7e7ae8", "issue_key": "SPARK-1269", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add Ability to Make Distribution with Tachyon", "description": "It would be nice to have a way where people can manually bundle Tachyon with Spark if they would like.", "reporter": "Patrick Wendell", "assignee": null, "created": "2014-03-18T14:49:41.000+0000", "updated": "2014-03-30T04:15:23.000+0000", "resolved": "2014-03-18T22:05:52.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1269\nSummary: Add Ability to Make Distribution with Tachyon\nDescription: It would be nice to have a way where people can manually bundle Tachyon with Spark if they would like.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "3b10330a485b15804da3d9652cbc53fa", "issue_key": "SPARK-1271", "issue_type": "Improvement", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Use Iterable[X] in co-group and group-by signatures", "description": "This API change will allow us to externalize these things down the road. Note that this will be a backwards incompatible change for users, but the solution is very simple, in cases where previously `Seq`'s were expected, the user can just call `toSeq` on the returned iterable.", "reporter": "Patrick Wendell", "assignee": "Holden Karau", "created": "2014-03-18T17:37:19.000+0000", "updated": "2014-04-09T01:18:25.000+0000", "resolved": "2014-04-09T01:18:25.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Holden Karau", "body": "Working on it :)", "created": "2014-03-25T14:27:46.326+0000"}, {"author": "Holden Karau", "body": "https://github.com/apache/spark/pull/242", "created": "2014-04-02T03:16:52.225+0000"}], "num_comments": 2, "text": "Issue: SPARK-1271\nSummary: Use Iterable[X] in co-group and group-by signatures\nDescription: This API change will allow us to externalize these things down the road. Note that this will be a backwards incompatible change for users, but the solution is very simple, in cases where previously `Seq`'s were expected, the user can just call `toSeq` on the returned iterable.\n\nComments (2):\n1. Holden Karau: Working on it :)\n2. Holden Karau: https://github.com/apache/spark/pull/242", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "56148c21229b5ce1adf187e86f785811", "issue_key": "SPARK-1272", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Don't fail job if some local directories are buggy", "description": "If Spark cannot create shuffle directories inside of a local directory it might make sense to just log an error and continue, provided that at least one valid shuffle directory exists. Otherwise if a single disk is wonky the entire job can fail. The down side is that this might mask failures if the person actually misconfigures the local directories to point to the wrong disk(s).", "reporter": "Patrick Wendell", "assignee": null, "created": "2014-03-18T18:02:16.000+0000", "updated": "2020-05-17T18:30:39.000+0000", "resolved": "2019-05-21T05:36:28.000+0000", "labels": ["bulk-closed"], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "OuyangJin", "body": "@Patrick How about assign this to me", "created": "2014-03-22T21:10:15.511+0000"}, {"author": "Sean R. Owen", "body": "[~qqsun8819] I'd also like to see this implemented, as it should be easy. I don't think you need it assigned (I can't assign it myself); just open a PR with your proposed change?", "created": "2014-12-29T17:20:18.328+0000"}, {"author": "He Tianyi", "body": "BTW, is there a good approach to skip failed disk if {{DiskBlockManager}} is using hashing strategy? AFAIK, we can skip corrupted disk by using next disk until reaching a success or exhausted all disks. And do the same when trying to locate existing file. Is this viable?", "created": "2016-05-23T04:06:52.877+0000"}], "num_comments": 3, "text": "Issue: SPARK-1272\nSummary: Don't fail job if some local directories are buggy\nDescription: If Spark cannot create shuffle directories inside of a local directory it might make sense to just log an error and continue, provided that at least one valid shuffle directory exists. Otherwise if a single disk is wonky the entire job can fail. The down side is that this might mask failures if the person actually misconfigures the local directories to point to the wrong disk(s).\n\nComments (3):\n1. OuyangJin: @Patrick How about assign this to me\n2. Sean R. Owen: [~qqsun8819] I'd also like to see this implemented, as it should be easy. I don't think you need it assigned (I can't assign it myself); just open a PR with your proposed change?\n3. He Tianyi: BTW, is there a good approach to skip failed disk if {{DiskBlockManager}} is using hashing strategy? AFAIK, we can skip corrupted disk by using next disk until reaching a success or exhausted all disks. And do the same when trying to locate existing file. Is this viable?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.155019"}}
{"id": "e7edfa7b9fe6d0d1d1e7a30974057438", "issue_key": "SPARK-1273", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "MLlib v0.9.1 release", "description": "Pick MLlib bug fixes, improvements, and doc updates since v0.9.0 for v0.9.1.", "reporter": "Xiangrui Meng", "assignee": "Xiangrui Meng", "created": "2014-03-18T19:17:47.000+0000", "updated": "2014-03-21T15:00:15.000+0000", "resolved": "2014-03-21T15:00:15.000+0000", "labels": [], "components": ["MLlib"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1273\nSummary: MLlib v0.9.1 release\nDescription: Pick MLlib bug fixes, improvements, and doc updates since v0.9.0 for v0.9.1.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "e54dde04eba13cb25295302f6cfa4197", "issue_key": "SPARK-1274", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Add dev scripts to merge PRs and create releases from master to branch-0.9", "description": "Master branch of Spark has scripts in SPARK_HOME/dev to merge PRs and create release candidates. These should also be present in branch-0.9 for making future releases.", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "created": "2014-03-18T20:41:31.000+0000", "updated": "2014-03-18T22:19:34.000+0000", "resolved": "2014-03-18T22:19:34.000+0000", "labels": [], "components": ["Project Infra"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1274\nSummary: Add dev scripts to merge PRs and create releases from master to branch-0.9\nDescription: Master branch of Spark has scripts in SPARK_HOME/dev to merge PRs and create release candidates. These should also be present in branch-0.9 for making future releases.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "ca27e43c97800b79603d71a4f8f656e3", "issue_key": "SPARK-1275", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Made SPARK_HOME/dev/tests executable", "description": "dev/run-tests was causing Jenkins tests to fail.", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "created": "2014-03-18T22:45:46.000+0000", "updated": "2014-03-19T16:42:05.000+0000", "resolved": "2014-03-19T16:42:05.000+0000", "labels": [], "components": ["Project Infra"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1275\nSummary: Made SPARK_HOME/dev/tests executable\nDescription: dev/run-tests was causing Jenkins tests to fail.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "43c2cc0b45add1bb6f4c1b43a7bb8687", "issue_key": "SPARK-1276", "issue_type": "New Feature", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Add a Simple History Server for the UI for Yarn / Mesos", "description": "For the 1.0 release we should have a basic history server for non-standalone modes (e.g. Yarn, Mesos). It can just be initialized with a storage directory and watch for new applications. Ideally this should be able to respect UI security filters as well. This might require us to add some sort of \"COMPLETED\" file or some synchronization to know when the UI has finished running so we don't try to load the file a bunch of times.", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "created": "2014-03-18T23:33:35.000+0000", "updated": "2014-04-10T17:40:43.000+0000", "resolved": "2014-04-10T17:40:43.000+0000", "labels": [], "components": ["Web UI"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1276\nSummary: Add a Simple History Server for the UI for Yarn / Mesos\nDescription: For the 1.0 release we should have a basic history server for non-standalone modes (e.g. Yarn, Mesos). It can just be initialized with a storage directory and watch for new applications. Ideally this should be able to respect UI security filters as well. This might require us to add some sort of \"COMPLETED\" file or some synchronization to know when the UI has finished running so we don't try to load the file a bunch of times.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "4d38614f27d9cb9b5b6306b6f15b5ba0", "issue_key": "SPARK-1277", "issue_type": "New Feature", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Automatically set the UI persistence directory based on cluster settings", "description": "More details forthcoming", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "created": "2014-03-18T23:36:37.000+0000", "updated": "2014-04-16T16:22:20.000+0000", "resolved": "2014-04-16T16:22:20.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick Wendell", "body": "Subsumed by other configuration patches.", "created": "2014-04-16T16:22:20.446+0000"}], "num_comments": 1, "text": "Issue: SPARK-1277\nSummary: Automatically set the UI persistence directory based on cluster settings\nDescription: More details forthcoming\n\nComments (1):\n1. Patrick Wendell: Subsumed by other configuration patches.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "298f4706b4edd997aa0c6d99e46265ef", "issue_key": "SPARK-1278", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Improper use of SimpleDateFormat", "description": "SimpleDateFormat is not thread-safe. Some places use the same SimpleDateFormat object without safeguard in the multiple threads. It will cause that the Web UI displays improper date.", "reporter": "Shixiong Zhu", "assignee": "Shixiong Zhu", "created": "2014-03-19T06:44:48.000+0000", "updated": "2014-03-21T16:41:08.000+0000", "resolved": "2014-03-21T16:40:50.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Shixiong Zhu", "body": "PR: https://github.com/apache/spark/pull/179", "created": "2014-03-20T20:29:41.916+0000"}], "num_comments": 1, "text": "Issue: SPARK-1278\nSummary: Improper use of SimpleDateFormat\nDescription: SimpleDateFormat is not thread-safe. Some places use the same SimpleDateFormat object without safeguard in the multiple threads. It will cause that the Web UI displays improper date.\n\nComments (1):\n1. Shixiong Zhu: PR: https://github.com/apache/spark/pull/179", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "c00133bb9f7925cbf451a2be58e19fee", "issue_key": "SPARK-1279", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Stage.name return \"apply at Option.scala:120\"", "description": "", "reporter": "Guoqiang Li", "assignee": null, "created": "2014-03-19T07:34:33.000+0000", "updated": "2014-09-27T18:52:54.000+0000", "resolved": "2014-09-27T18:52:54.000+0000", "labels": [], "components": [], "comments": [{"author": "Sean R. Owen", "body": "Obvious accidental dupe of SPARK-1280", "created": "2014-09-27T18:52:54.262+0000"}], "num_comments": 1, "text": "Issue: SPARK-1279\nSummary: Stage.name return \"apply at Option.scala:120\"\n\nComments (1):\n1. Sean R. Owen: Obvious accidental dupe of SPARK-1280", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "7901fedb85baa3a3a4222f4ed51d1def", "issue_key": "SPARK-1280", "issue_type": "Bug", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Stage.name return \"apply at Option.scala:120\"", "description": "", "reporter": "Guoqiang Li", "assignee": "Aaron Davidson", "created": "2014-03-19T07:35:48.000+0000", "updated": "2014-09-27T18:53:40.000+0000", "resolved": "2014-03-25T13:29:33.000+0000", "labels": [], "components": [], "comments": [{"author": "Aaron Davidson", "body": "Here's what the stack trace looks like:  java.lang.RuntimeException at org.apache.spark.util.Utils$.getCallSiteInfo(Utils.scala:687) at org.apache.spark.util.Utils$.formatCallSiteInfo$default$1(Utils.scala:723) at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880) at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.SparkContext.getCallSite(SparkContext.scala:880) at org.apache.spark.SparkContext.runJob(SparkContext.scala:898) at org.apache.spark.SparkContext.runJob(SparkContext.scala:920) at org.apache.spark.SparkContext.runJob(SparkContext.scala:934) at org.apache.spark.SparkContext.runJob(SparkContext.scala:948) at org.apache.spark.rdd.RDD.collect(RDD.scala:657) ... at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:981) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala)  It appears this was accidentally introduced when SparkContext#getCallSite was changed to use the Option pattern.", "created": "2014-03-24T12:52:13.178+0000"}], "num_comments": 1, "text": "Issue: SPARK-1280\nSummary: Stage.name return \"apply at Option.scala:120\"\n\nComments (1):\n1. Aaron Davidson: Here's what the stack trace looks like:  java.lang.RuntimeException at org.apache.spark.util.Utils$.getCallSiteInfo(Utils.scala:687) at org.apache.spark.util.Utils$.formatCallSiteInfo$default$1(Utils.scala:723) at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880) at org.apache.spark.SparkContext$$anonfun$getCallSite$1.apply(SparkContext.scala:880) at scala.Option.getOrElse(Option.scala:120) at org.apache.spark.SparkContext.getCallSite(SparkContext.scala:880) at org.apache.spark.SparkContext.runJob(SparkContext.scala:898) at org.apache.spark.SparkContext.runJob(SparkContext.scala:920) at org.apache.spark.SparkContext.runJob(SparkContext.scala:934) at org.apache.spark.SparkContext.runJob(SparkContext.scala:948) at org.apache.spark.rdd.RDD.collect(RDD.scala:657) ... at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:981) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala)  It appears this was accidentally introduced when SparkContext#getCallSite was changed to use the Option pattern.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "6b6743cca46d830b991a308a5e7f5e78", "issue_key": "SPARK-1281", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Partitioning in ALS", "description": "There are some minor issues about partitioning with the current implementation of ALS: 1. Mod-based partitioner is used for mapping users/products to blocks. This might cause problems if the ids contains information. For example, the last digit may indicate the user/product type. This can be fixed by hashing. 2. HashPartitioner is used on the initial partition. This is the same as the mod-based partitioner when the key is a positive integer. But it is certainly error-prone.", "reporter": "Xiangrui Meng", "assignee": "Tor Myklebust", "created": "2014-03-19T10:15:31.000+0000", "updated": "2014-04-23T02:23:42.000+0000", "resolved": "2014-04-23T02:23:32.000+0000", "labels": [], "components": ["MLlib"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1281\nSummary: Partitioning in ALS\nDescription: There are some minor issues about partitioning with the current implementation of ALS: 1. Mod-based partitioner is used for mapping users/products to blocks. This might cause problems if the ids contains information. For example, the last digit may indicate the user/product type. This can be fixed by hashing. 2. HashPartitioner is used on the initial partition. This is the same as the mod-based partitioner when the key is a positive integer. But it is certainly error-prone.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "ec24fc2074a37517ad8e7767f95febd4", "issue_key": "SPARK-1282", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Create spark-contrib repo for 1.0", "description": "Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects. Some questions: - Who should host this repo, and where should it be hosted? - Github would be a strong preference from usability standpoint - There is talk that Apache might have some facility for this - Contents. Should it simply be links? Git submodules?", "reporter": "Evan Chan", "assignee": null, "created": "2014-03-19T16:08:31.000+0000", "updated": "2014-03-21T14:55:42.000+0000", "resolved": "2014-03-21T14:55:42.000+0000", "labels": [], "components": ["Project Infra"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1282\nSummary: Create spark-contrib repo for 1.0\nDescription: Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects. Some questions: - Who should host this repo, and where should it be hosted? - Github would be a strong preference from usability standpoint - There is talk that Apache might have some facility for this - Contents. Should it simply be links? Git submodules?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "698eaf85fbbda7d22d10d13ac07b4c8c", "issue_key": "SPARK-1283", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Create spark-contrib repo for 1.0", "description": "Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects. Some questions: - Who should host this repo, and where should it be hosted? - Github would be a strong preference from usability standpoint - There is talk that Apache might have some facility for this - Contents. Should it simply be links? Git submodules?", "reporter": "Evan Chan", "assignee": null, "created": "2014-03-19T16:10:43.000+0000", "updated": "2015-01-27T17:58:24.000+0000", "resolved": "2015-01-27T17:58:24.000+0000", "labels": [], "components": ["Project Infra"], "comments": [{"author": "Patrick McFadin", "body": "I think the main Apache support for this is via Apache Extras (which is on Google Code). To me that seems pretty dated and probably not the best option: http://community.apache.org/apache-extras/faq.html In my mind the coolest thing might be a github repo that uses submodules. In terms of hosting, it's possible the AMPLab could host it - I've reached out to them and it seems like there is some interest. We could also create a new github account called spark-contrib and just give the Spark committers access to it somehow.", "created": "2014-03-19T16:50:03.040+0000"}, {"author": "Evan Chan", "body": "+1 for Amplab-hosted spark-contrib or the new github account. As for access to a spark-contrib github account, how about Spark committers + maintainers for the individual projects for each repo?", "created": "2014-03-23T23:12:45.051+0000"}, {"author": "Evan Chan", "body": "I have another idea that may be simpler than a repo. Why not just create a landing page on the Spark docs for contrib projects? We can just edit it via standard PR process. It would be simpler, and not require any hosting, faster to get done.", "created": "2014-03-27T10:18:59.757+0000"}, {"author": "Evan Chan", "body": "<ping> Any more comments? Objections to creating a landing page for contrib projects in the Spark docs?", "created": "2014-05-08T18:04:16.684+0000"}, {"author": "Patrick Wendell", "body": "[~velvia] Yeah - do you want to submit a PR? This seems like a good idea to me.", "created": "2014-05-08T20:32:23.710+0000"}, {"author": "Evan Chan", "body": "Yep, will submit a PR> -- The fruit of silence is prayer; the fruit of prayer is faith; the fruit of faith is love; the fruit of love is service; the fruit of service is peace. -- Mother Teresa", "created": "2014-05-14T07:28:15.780+0000"}, {"author": "Sean R. Owen", "body": "Is this obsolete given the existence of spark-packages.org now?", "created": "2015-01-27T17:33:04.914+0000"}, {"author": "Evan Chan", "body": "Yeah this is not needed anymore, with spark-packages.org", "created": "2015-01-27T17:58:24.433+0000"}], "num_comments": 8, "text": "Issue: SPARK-1283\nSummary: Create spark-contrib repo for 1.0\nDescription: Let's create a \"spark-contrib\" repo to host community projects for the Spark ecosystem that don't quite belong in core, but are very important nevertheless. It would be linked to from official Spark documentation and web site, and help provide visibility for community projects. Some questions: - Who should host this repo, and where should it be hosted? - Github would be a strong preference from usability standpoint - There is talk that Apache might have some facility for this - Contents. Should it simply be links? Git submodules?\n\nComments (8):\n1. Patrick McFadin: I think the main Apache support for this is via Apache Extras (which is on Google Code). To me that seems pretty dated and probably not the best option: http://community.apache.org/apache-extras/faq.html In my mind the coolest thing might be a github repo that uses submodules. In terms of hosting, it's possible the AMPLab could host it - I've reached out to them and it seems like there is some interest. We could also create a new github account called spark-contrib and just give the Spark committers access to it somehow.\n2. Evan Chan: +1 for Amplab-hosted spark-contrib or the new github account. As for access to a spark-contrib github account, how about Spark committers + maintainers for the individual projects for each repo?\n3. Evan Chan: I have another idea that may be simpler than a repo. Why not just create a landing page on the Spark docs for contrib projects? We can just edit it via standard PR process. It would be simpler, and not require any hosting, faster to get done.\n4. Evan Chan: <ping> Any more comments? Objections to creating a landing page for contrib projects in the Spark docs?\n5. Patrick Wendell: [~velvia] Yeah - do you want to submit a PR? This seems like a good idea to me.\n6. Evan Chan: Yep, will submit a PR> -- The fruit of silence is prayer; the fruit of prayer is faith; the fruit of faith is love; the fruit of love is service; the fruit of service is peace. -- Mother Teresa\n7. Sean R. Owen: Is this obsolete given the existence of spark-packages.org now?\n8. Evan Chan: Yeah this is not needed anymore, with spark-packages.org", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "f4df4cb981405e091ad06d1e7f5ca4b1", "issue_key": "SPARK-1284", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "pyspark hangs after IOError on Executor", "description": "When running a reduceByKey over a cached RDD, Python fails with an exception, but the failure is not detected by the task runner. Spark and the pyspark shell hang waiting for the task to finish. The error is:  PySpark worker failed with exception: Traceback (most recent call last): File \"/home/hadoop/spark/python/pyspark/worker.py\", line 77, in main serializer.dump_stream(func(split_index, iterator), outfile) File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 182, in dump_stream self.serializer.dump_stream(self._batched(iterator), stream) File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 118, in dump_stream self._write_with_length(obj, stream) File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 130, in _write_with_length stream.write(serialized) IOError: [Errno 104] Connection reset by peer 14/03/19 22:48:15 INFO scheduler.TaskSetManager: Serialized task 4.0:0 as 4257 bytes in 47 ms Traceback (most recent call last): File \"/home/hadoop/spark/python/pyspark/daemon.py\", line 117, in launch_worker worker(listen_sock) File \"/home/hadoop/spark/python/pyspark/daemon.py\", line 107, in worker outfile.flush() IOError: [Errno 32] Broken pipe  I can reproduce the error by running take(10) on the cached RDD before running reduceByKey (which looks at the whole input file). Affects Version 1.0.0-SNAPSHOT (4d88030486)", "reporter": "Jim Blomo", "assignee": "Davies Liu", "created": "2014-03-19T16:16:01.000+0000", "updated": "2014-10-02T21:23:03.000+0000", "resolved": "2014-10-02T21:23:03.000+0000", "labels": [], "components": ["PySpark"], "comments": [{"author": "Matthew Farrellee", "body": "[~jblomo] - will you add a reproducer script to this issue? i did a simple test based on what you suggested w/ the tip of master and could not reproduce -  $ ./dist/bin/pyspark Python 2.7.5 (default, Feb 19 2014, 13:47:28) [GCC 4.8.2 20131212 (Red Hat 4.8.2-7)] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. ... Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 1.0.0-SNAPSHOT /_/ Using Python version 2.7.5 (default, Feb 19 2014 13:47:28) SparkContext available as sc. >>> data = sc.textFile('/etc/passwd') 14/07/02 07:03:59 INFO MemoryStore: ensureFreeSpace(32816) called with curMem=0, maxMem=308910489 14/07/02 07:03:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 294.6 MB) >>> data.cache() /etc/passwd MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 >>> data.take(10) ...[expected output]... >>> data.flatMap(lambda line: line.split(':')).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).collect() ...[expected output, no hang]...", "created": "2014-07-02T14:11:37.370+0000"}, {"author": "Davies Liu", "body": "[~jblomo], could you reproduce this on master or 1.1 branch? Maybe the pyspark did not hange after this error message, the take() had finished successfully before the error message pop up. The noisy error messages had been fixed in PR https://github.com/apache/spark/pull/1625", "created": "2014-08-11T18:43:53.630+0000"}, {"author": "Jim Blomo", "body": "I will try to reproduce on the 1.1 branch later this week, thanks for the update!", "created": "2014-08-11T20:00:43.744+0000"}, {"author": "Jim Blomo", "body": "Hi, having trouble compiling either master or branch-1.1, I sent a request to the mailing list for help. Are there any compiled snapshots?", "created": "2014-08-14T22:26:12.261+0000"}, {"author": "Matthew Farrellee", "body": "[~jblomo] master should be buildable again, please give it another shot", "created": "2014-08-26T18:35:41.485+0000"}, {"author": "Davies Liu", "body": "I think this is an logging issue ,should be fixed by https://github.com/apache/spark/pull/1625, so close it. If anyone meet this again, we can reopen it.", "created": "2014-10-02T21:23:03.206+0000"}], "num_comments": 6, "text": "Issue: SPARK-1284\nSummary: pyspark hangs after IOError on Executor\nDescription: When running a reduceByKey over a cached RDD, Python fails with an exception, but the failure is not detected by the task runner. Spark and the pyspark shell hang waiting for the task to finish. The error is:  PySpark worker failed with exception: Traceback (most recent call last): File \"/home/hadoop/spark/python/pyspark/worker.py\", line 77, in main serializer.dump_stream(func(split_index, iterator), outfile) File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 182, in dump_stream self.serializer.dump_stream(self._batched(iterator), stream) File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 118, in dump_stream self._write_with_length(obj, stream) File \"/home/hadoop/spark/python/pyspark/serializers.py\", line 130, in _write_with_length stream.write(serialized) IOError: [Errno 104] Connection reset by peer 14/03/19 22:48:15 INFO scheduler.TaskSetManager: Serialized task 4.0:0 as 4257 bytes in 47 ms Traceback (most recent call last): File \"/home/hadoop/spark/python/pyspark/daemon.py\", line 117, in launch_worker worker(listen_sock) File \"/home/hadoop/spark/python/pyspark/daemon.py\", line 107, in worker outfile.flush() IOError: [Errno 32] Broken pipe  I can reproduce the error by running take(10) on the cached RDD before running reduceByKey (which looks at the whole input file). Affects Version 1.0.0-SNAPSHOT (4d88030486)\n\nComments (6):\n1. Matthew Farrellee: [~jblomo] - will you add a reproducer script to this issue? i did a simple test based on what you suggested w/ the tip of master and could not reproduce -  $ ./dist/bin/pyspark Python 2.7.5 (default, Feb 19 2014, 13:47:28) [GCC 4.8.2 20131212 (Red Hat 4.8.2-7)] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. ... Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 1.0.0-SNAPSHOT /_/ Using Python version 2.7.5 (default, Feb 19 2014 13:47:28) SparkContext available as sc. >>> data = sc.textFile('/etc/passwd') 14/07/02 07:03:59 INFO MemoryStore: ensureFreeSpace(32816) called with curMem=0, maxMem=308910489 14/07/02 07:03:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 294.6 MB) >>> data.cache() /etc/passwd MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 >>> data.take(10) ...[expected output]... >>> data.flatMap(lambda line: line.split(':')).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).collect() ...[expected output, no hang]...\n2. Davies Liu: [~jblomo], could you reproduce this on master or 1.1 branch? Maybe the pyspark did not hange after this error message, the take() had finished successfully before the error message pop up. The noisy error messages had been fixed in PR https://github.com/apache/spark/pull/1625\n3. Jim Blomo: I will try to reproduce on the 1.1 branch later this week, thanks for the update!\n4. Jim Blomo: Hi, having trouble compiling either master or branch-1.1, I sent a request to the mailing list for help. Are there any compiled snapshots?\n5. Matthew Farrellee: [~jblomo] master should be buildable again, please give it another shot\n6. Davies Liu: I think this is an logging issue ,should be fixed by https://github.com/apache/spark/pull/1625, so close it. If anyone meet this again, we can reopen it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "0343c0d5ec8e35316ef4bb8c7c758822", "issue_key": "SPARK-1285", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Back porting streaming doc updates to 0.9", "description": "", "reporter": "Tathagata Das", "assignee": "Tathagata Das", "created": "2014-03-19T17:59:20.000+0000", "updated": "2014-03-20T12:40:23.000+0000", "resolved": "2014-03-20T12:40:23.000+0000", "labels": [], "components": ["DStreams"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1285\nSummary: Back porting streaming doc updates to 0.9", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "f0bc48b32800b300535d9fa0dda6e0fb", "issue_key": "SPARK-1286", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Make usage of spark-env.sh idempotent", "description": "Various spark scripts load spark-env.sh. This can cause growth of any variables that may be appended to (SPARK_CLASSPATH, SPARK_REPL_OPTS) and it makes the precedence order for options specified in spark-env.sh less clear. One use-case for the latter is that we want to set options from the command-line of spark-shell, but these options will be overridden by subsequent loading of spark-env.sh. If we were to load the spark-env.sh first and then set our command-line options, we could guarantee correct precedence order.", "reporter": "Aaron Davidson", "assignee": "Aaron Davidson", "created": "2014-03-19T18:28:03.000+0000", "updated": "2014-03-28T14:38:20.000+0000", "resolved": "2014-03-28T14:38:20.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Patrick McFadin", "body": "Relevant: https://github.com/apache/incubator-spark/pull/326", "created": "2014-03-19T22:07:15.922+0000"}], "num_comments": 1, "text": "Issue: SPARK-1286\nSummary: Make usage of spark-env.sh idempotent\nDescription: Various spark scripts load spark-env.sh. This can cause growth of any variables that may be appended to (SPARK_CLASSPATH, SPARK_REPL_OPTS) and it makes the precedence order for options specified in spark-env.sh less clear. One use-case for the latter is that we want to set options from the command-line of spark-shell, but these options will be overridden by subsequent loading of spark-env.sh. If we were to load the spark-env.sh first and then set our command-line options, we could guarantee correct precedence order.\n\nComments (1):\n1. Patrick McFadin: Relevant: https://github.com/apache/incubator-spark/pull/326", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "2055ffd0f235da3a4f737c38b8b874e9", "issue_key": "SPARK-1287", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "yarn alpha and stable Client calculateAMMemory routines are different", "description": "The yarn alpha version of calculateAMMemory takes into account the minimum resource capability and also subtracts out YarnAllocationHandler.MEMORY_OVERHEAD. The yarn stable version just sets the -Xmx to whatever is passed in by the user. The 2 of these should be the same. Personally I also think its weird how spark currently takes whatever user passes in for memory and adds YarnAllocationHandler.MEMORY_OVERHEAD to request to RM. This can be confusing to users. We should revisit all of this and commonize the stable/alpha code where possible.", "reporter": "Thomas Graves", "assignee": null, "created": "2014-03-20T08:51:52.000+0000", "updated": "2014-08-22T21:50:10.000+0000", "resolved": "2014-08-22T21:50:01.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Thomas Graves", "body": "duplicate of SPARK-2140", "created": "2014-08-22T21:50:01.330+0000"}], "num_comments": 1, "text": "Issue: SPARK-1287\nSummary: yarn alpha and stable Client calculateAMMemory routines are different\nDescription: The yarn alpha version of calculateAMMemory takes into account the minimum resource capability and also subtracts out YarnAllocationHandler.MEMORY_OVERHEAD. The yarn stable version just sets the -Xmx to whatever is passed in by the user. The 2 of these should be the same. Personally I also think its weird how spark currently takes whatever user passes in for memory and adds YarnAllocationHandler.MEMORY_OVERHEAD to request to RM. This can be confusing to users. We should revisit all of this and commonize the stable/alpha code where possible.\n\nComments (1):\n1. Thomas Graves: duplicate of SPARK-2140", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "a5e1db35c3f9bd7659ab1e72474555d3", "issue_key": "SPARK-1288", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "yarn stable finishApplicationMaster incomplete", "description": "The yarn stable version of ApplicationMaster.finishApplicationMaster is incomplete. It doesn't set the diagnostic message or app trcking url.", "reporter": "Thomas Graves", "assignee": "Thomas Graves", "created": "2014-03-20T09:44:46.000+0000", "updated": "2014-04-17T21:39:22.000+0000", "resolved": "2014-04-17T21:39:22.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Thomas Graves", "body": "https://github.com/apache/spark/pull/362", "created": "2014-04-09T00:29:52.298+0000"}], "num_comments": 1, "text": "Issue: SPARK-1288\nSummary: yarn stable finishApplicationMaster incomplete\nDescription: The yarn stable version of ApplicationMaster.finishApplicationMaster is incomplete. It doesn't set the diagnostic message or app trcking url.\n\nComments (1):\n1. Thomas Graves: https://github.com/apache/spark/pull/362", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "564403e17facf321636553097fcd30e0", "issue_key": "SPARK-1289", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "improve yarn stable error logging when passing bad arguments", "description": "There is a difference in the errors shown to the user between yarn alpha and yarn stable (I was running in yarn-cluster mode). For instance if I call SparkHdfsLR with a non-existent hdfs file, yarn alpha shows me the following error in the logs: Caused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://nn1.com:8020/user/testuser/lr_data_foo.txt at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:231) at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:251) I do the same things on yarn stable and all I see is: 14/03/20 17:26:04 INFO yarn.ApplicationMaster: finishApplicationMaster with FAILED it doesn't give the user why it failed.", "reporter": "Thomas Graves", "assignee": null, "created": "2014-03-20T10:31:36.000+0000", "updated": "2015-03-07T16:35:31.000+0000", "resolved": "2015-03-07T16:35:31.000+0000", "labels": [], "components": ["YARN"], "comments": [{"author": "Sean R. Owen", "body": "Obsolete because it concerns yarn-alpha", "created": "2015-03-07T16:35:31.610+0000"}], "num_comments": 1, "text": "Issue: SPARK-1289\nSummary: improve yarn stable error logging when passing bad arguments\nDescription: There is a difference in the errors shown to the user between yarn alpha and yarn stable (I was running in yarn-cluster mode). For instance if I call SparkHdfsLR with a non-existent hdfs file, yarn alpha shows me the following error in the logs: Caused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://nn1.com:8020/user/testuser/lr_data_foo.txt at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:231) at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:251) I do the same things on yarn stable and all I see is: 14/03/20 17:26:04 INFO yarn.ApplicationMaster: finishApplicationMaster with FAILED it doesn't give the user why it failed.\n\nComments (1):\n1. Sean R. Owen: Obsolete because it concerns yarn-alpha", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}
{"id": "e414bbcc0de3f84315a1936a8a290846", "issue_key": "SPARK-1290", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "PythonAccumulatorParam throws uncaught exception", "description": "As reported here: http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-pyspark-crash-on-mesos-td2256.html the PythonAccumulatorParam can throw a SparkException that doesn't get caught, leading to early exit of the DAGScheduler's handleTaskCompletion function. While the DAGScheduler gets restarted, the offending task is never cleaned up properly and the corresponding job will hang. We should catch this exception and fail the task as a result. This is related to SPARK-1235 but should be fixed separately, since we should be doing our best to never throw exceptions inside of the DAGScheduler.", "reporter": "Kay Ousterhout", "assignee": "Kay Ousterhout", "created": "2014-03-20T13:59:55.000+0000", "updated": "2014-03-20T14:24:24.000+0000", "resolved": "2014-03-20T14:24:24.000+0000", "labels": [], "components": [], "comments": [{"author": "Kay Ousterhout", "body": "After further thought I've decided to close this -- I don't think we can gracefully handle this problem, because if we've lost connection with the Python master (which is when the exception gets thrown), we're not going to make any progress in the future.", "created": "2014-03-20T14:24:12.713+0000"}], "num_comments": 1, "text": "Issue: SPARK-1290\nSummary: PythonAccumulatorParam throws uncaught exception\nDescription: As reported here: http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-pyspark-crash-on-mesos-td2256.html the PythonAccumulatorParam can throw a SparkException that doesn't get caught, leading to early exit of the DAGScheduler's handleTaskCompletion function. While the DAGScheduler gets restarted, the offending task is never cleaned up properly and the corresponding job will hang. We should catch this exception and fail the task as a result. This is related to SPARK-1235 but should be fixed separately, since we should be doing our best to never throw exceptions inside of the DAGScheduler.\n\nComments (1):\n1. Kay Ousterhout: After further thought I've decided to close this -- I don't think we can gracefully handle this problem, because if we've lost connection with the Python master (which is when the exception gets thrown), we're not going to make any progress in the future.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.170723"}}

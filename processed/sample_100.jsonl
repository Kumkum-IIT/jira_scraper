{"id": "a7df12054e3b906db89e9c8a2b239eb2", "issue_key": "HADOOP-676", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "JobClient should print user friendly messages for standard errors", "description": "The exceptions for non-existent input dirs or already existing output dirs should generate user-friendly error messages.", "reporter": "Owen O'Malley", "assignee": "Sanjay Dahiya", "created": "2006-11-02T23:34:41.000+0000", "updated": "2009-07-08T16:51:59.000+0000", "resolved": "2006-12-07T20:17:07.000+0000", "labels": [], "components": [], "comments": [{"author": "Sanjay Dahiya", "body": "This patch adds 3 new exceptions - InvalidJobConfException (mendatory attributes missing or incorrect) - InvalidFileTypeException (expecting file / got dir or vice versa ) - FileAlreadyExistsException (as it says) It throws these exceptions from job client and prints better error messages. All exceptions extend IOException to conform with existing API but are added in throws declaration to appear in javadocs. Is there any other exception you would like to be thrown/printed ?", "created": "2006-11-07T11:26:44.000+0000"}, {"author": "Owen O'Malley", "body": "+1", "created": "2006-12-05T23:38:51.000+0000"}, {"author": "Hadoop QA", "body": "+1, http://issues.apache.org/jira/secure/attachment/12344465/Hadoop-676.patch applied and successfully tested against trunk revision r482999", "created": "2006-12-06T18:30:28.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Sanjay!", "created": "2006-12-07T20:17:07.000+0000"}], "num_comments": 4, "text": "Issue: HADOOP-676\nSummary: JobClient should print user friendly messages for standard errors\nDescription: The exceptions for non-existent input dirs or already existing output dirs should generate user-friendly error messages.\n\nComments (4):\n1. Sanjay Dahiya: This patch adds 3 new exceptions - InvalidJobConfException (mendatory attributes missing or incorrect) - InvalidFileTypeException (expecting file / got dir or vice versa ) - FileAlreadyExistsException (as it says) It throws these exceptions from job client and prints better error messages. All exceptions extend IOException to conform with existing API but are added in throws declaration to appear in javadocs. Is there any other exception you would like to be thrown/printed ?\n2. Owen O'Malley: +1\n3. Hadoop QA: +1, http://issues.apache.org/jira/secure/attachment/12344465/Hadoop-676.patch applied and successfully tested against trunk revision r482999\n4. Doug Cutting: I just committed this. Thanks, Sanjay!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.434402"}}
{"id": "60bdbac4d9c633aff58e824c3168c01e", "issue_key": "KAFKA-457", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Leader Re-election is broken in rev. 1368092", "description": "Steps to reproduce: 1. Check out rev. 1368092 and execute the command: $ ./sbt update package 2. Replace <kafka_home>/system_test/single_host_multi_brokers/bin/run-test.sh with the attached script because the logging message related to \"shut down completed\" and \"leader state transition\" has been modified in this revision. 3. Execute the command bin/run-test.sh 4. The test seems to be hung. 5. The reason is that there is no leader re-election happening after the first leader is terminated. By checking the logs, there may be only 1 \"completed the leader state transition\" log message found in the server logs: $ grep \"completed the leader state transition\" * kafka_server_3.log:[2012-08-13 10:07:58,085] INFO Replica Manager on Broker 3, completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) 6. The correct behavior as in rev 1367821, re-election log messages are found after leader termination as follows: $ grep \"completed the leader state transition\" * kafka_server_1.log:[2012-08-13 09:40:23,542] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) kafka_server_1.log:[2012-08-13 09:42:17,881] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) kafka_server_2.log:[2012-08-13 09:40:29,082] INFO Broker 2 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) kafka_server_3.log:[2012-08-13 09:44:06,695] INFO Broker 3 completed the leader state transition for topic mytest part", "reporter": "John Fung", "assignee": "Yang Ye", "created": "2012-08-13T17:26:47.000+0000", "updated": "2012-08-15T23:26:22.000+0000", "resolved": "2012-08-15T23:26:22.000+0000", "labels": [], "components": [], "comments": [{"author": "John Fung", "body": "This script \"run-test.sh\" contains the following changes 1. KAFKA-380-v5.patch 2. Updated log message patterns for looking up leader election in revision 1368092", "created": "2012-08-13T17:39:02.762+0000"}, {"author": "Yang Ye", "body": "The behaviour of the code is changed after kafka-369 is checked in, and the system tests all work well. So this jira should be marked as resolved.", "created": "2012-08-15T01:02:12.873+0000"}, {"author": "John Fung", "body": "Hi Victor, Thank you for your fix. I have tested rev. 1373633 for system_test/single_host_multi_brokers/run-test.sh and it is working correctly. Regards, John", "created": "2012-08-15T23:26:22.816+0000"}], "num_comments": 3, "text": "Issue: KAFKA-457\nSummary: Leader Re-election is broken in rev. 1368092\nDescription: Steps to reproduce: 1. Check out rev. 1368092 and execute the command: $ ./sbt update package 2. Replace <kafka_home>/system_test/single_host_multi_brokers/bin/run-test.sh with the attached script because the logging message related to \"shut down completed\" and \"leader state transition\" has been modified in this revision. 3. Execute the command bin/run-test.sh 4. The test seems to be hung. 5. The reason is that there is no leader re-election happening after the first leader is terminated. By checking the logs, there may be only 1 \"completed the leader state transition\" log message found in the server logs: $ grep \"completed the leader state transition\" * kafka_server_3.log:[2012-08-13 10:07:58,085] INFO Replica Manager on Broker 3, completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) 6. The correct behavior as in rev 1367821, re-election log messages are found after leader termination as follows: $ grep \"completed the leader state transition\" * kafka_server_1.log:[2012-08-13 09:40:23,542] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) kafka_server_1.log:[2012-08-13 09:42:17,881] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) kafka_server_2.log:[2012-08-13 09:40:29,082] INFO Broker 2 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager) kafka_server_3.log:[2012-08-13 09:44:06,695] INFO Broker 3 completed the leader state transition for topic mytest part\n\nComments (3):\n1. John Fung: This script \"run-test.sh\" contains the following changes 1. KAFKA-380-v5.patch 2. Updated log message patterns for looking up leader election in revision 1368092\n2. Yang Ye: The behaviour of the code is changed after kafka-369 is checked in, and the system tests all work well. So this jira should be marked as resolved.\n3. John Fung: Hi Victor, Thank you for your fix. I have tested rev. 1373633 for system_test/single_host_multi_brokers/run-test.sh and it is working correctly. Regards, John", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.678223"}}
{"id": "38a49a0d1840c59f785b47d45253496f", "issue_key": "KAFKA-103", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Make whitelist/blacklist mirror configs more consistent", "description": "The blacklist config for kafka mirrors is a comma separated list of topics. However, the whitelist config is a comma-separated list of \"topics:numthreads\" pairs, which allows for a multi-threaded consumer in the mirror. It will be good to keep the two configs consistent in format. So, we can make the whitelist config a comma-separated list of topics and provide a config (say, kafka.mirror.consumer.threads) that will specify the number of threads to use for all topics in the whitelist (if present).", "reporter": "Joel Jacob Koshy", "assignee": "Jun Rao", "created": "2011-08-15T19:12:56.000+0000", "updated": "2011-11-14T00:40:04.000+0000", "resolved": "2011-11-14T00:40:04.000+0000", "labels": [], "components": [], "comments": [{"author": "Joel Jacob Koshy", "body": "Hmm.. looks like the mirror topics patch actually ignored the numthreads in the whitelist. In any event, this patch allows a global numthreads for the embedded consumer.", "created": "2011-08-16T00:09:48.703+0000"}, {"author": "Joel Jacob Koshy", "body": "Sorry about deleting that.. I forgot to update the property in system test.", "created": "2011-08-16T00:40:31.215+0000"}, {"author": "Jun Rao", "body": "Thanks, Joel. Just committed this.", "created": "2011-08-17T23:34:26.418+0000"}, {"author": "Neha Narkhede", "body": "The behavior of the mirror.topics.whitelist is backwards incompatible. If some users currently have a whitelist setup with the old format (topic1:numPartitions1, topic2:numPartitions2), it is ideal to just rename \"embedded.consumer.topics\" to \"mirror.topics.whitelist\". Instead, this change forces those users to change their white-listed topics list to strip off the number of partitions. It would be ideal if the number of partitions is just ignored, with a little warning message. That way the change is backwards compatible. If a user just renames \"embedded.consumer.topics\" to \"mirror.topics.whitelist\" and leaves the number of partitions, currently the corp replica behavior is to not mirror anything at all. That is unintuitive.", "created": "2011-08-18T21:19:28.554+0000"}, {"author": "Joel Jacob Koshy", "body": "Note that this is an incremental patch. That is a good point - i.e., we have told clients that we are changing the old whitelist config to mirror.topics.whitelist. If they simply do a rename then nothing is going to get mirrored, so I agree that backward-compatibility is important. btw, I think a better fix (that would also help KAFKA-104) is to disallow invalid topics, but it would be better to think about that more carefully.", "created": "2011-08-18T23:39:22.040+0000"}, {"author": "Jun Rao", "body": "I actually don't think that we need to make this backward compatible because the # of threads specified in the old format will be ignored in the new config. If we support the old format, users will assume that the number of consumer threads is in effect. It's better to make this incompatible so the users can realize there is a problem.", "created": "2011-08-19T22:12:51.833+0000"}, {"author": "Joel Jacob Koshy", "body": "At least per kafka-103-patch.v1 it is not totally ignored. So if the whitelist config contains: SomeTopic:1 then SomeTopic will not get mirrored because it will fail the isTopicAllowed filter in KafkaServerStartable. i.e., no topics will get mirrored.", "created": "2011-08-19T22:34:03.518+0000"}, {"author": "Neha Narkhede", "body": "Jun, even if we don't want to support the older format, the behavior of silently not mirroring any data is also not acceptable. What would be helpful is to either log it as a warning or throw an InvalidConfigException.", "created": "2011-08-20T06:00:28.125+0000"}, {"author": "Jun Rao", "body": "However, we haven't officially released anything that exposes whitelist in the config. We are just iterating in trunk.", "created": "2011-08-20T06:24:26.991+0000"}, {"author": "Jun Rao", "body": "In summary, I think it's simpler if we enforce a single format for any property value. In this case, to help users identify problems, we can probably log the whitelisted and blacklisted topics, if any.", "created": "2011-08-20T16:09:56.873+0000"}, {"author": "Jay Kreps", "body": "Is this complete?", "created": "2011-11-12T21:14:38.960+0000"}, {"author": "Jun Rao", "body": "I think the concerns raised here can be addressed with clear documentation on how mirroring is done.", "created": "2011-11-14T00:40:04.830+0000"}], "num_comments": 12, "text": "Issue: KAFKA-103\nSummary: Make whitelist/blacklist mirror configs more consistent\nDescription: The blacklist config for kafka mirrors is a comma separated list of topics. However, the whitelist config is a comma-separated list of \"topics:numthreads\" pairs, which allows for a multi-threaded consumer in the mirror. It will be good to keep the two configs consistent in format. So, we can make the whitelist config a comma-separated list of topics and provide a config (say, kafka.mirror.consumer.threads) that will specify the number of threads to use for all topics in the whitelist (if present).\n\nComments (12):\n1. Joel Jacob Koshy: Hmm.. looks like the mirror topics patch actually ignored the numthreads in the whitelist. In any event, this patch allows a global numthreads for the embedded consumer.\n2. Joel Jacob Koshy: Sorry about deleting that.. I forgot to update the property in system test.\n3. Jun Rao: Thanks, Joel. Just committed this.\n4. Neha Narkhede: The behavior of the mirror.topics.whitelist is backwards incompatible. If some users currently have a whitelist setup with the old format (topic1:numPartitions1, topic2:numPartitions2), it is ideal to just rename \"embedded.consumer.topics\" to \"mirror.topics.whitelist\". Instead, this change forces those users to change their white-listed topics list to strip off the number of partitions. It would be ideal if the number of partitions is just ignored, with a little warning message. That way the change is backwards compatible. If a user just renames \"embedded.consumer.topics\" to \"mirror.topics.whitelist\" and leaves the number of partitions, currently the corp replica behavior is to not mirror anything at all. That is unintuitive.\n5. Joel Jacob Koshy: Note that this is an incremental patch. That is a good point - i.e., we have told clients that we are changing the old whitelist config to mirror.topics.whitelist. If they simply do a rename then nothing is going to get mirrored, so I agree that backward-compatibility is important. btw, I think a better fix (that would also help KAFKA-104) is to disallow invalid topics, but it would be better to think about that more carefully.\n6. Jun Rao: I actually don't think that we need to make this backward compatible because the # of threads specified in the old format will be ignored in the new config. If we support the old format, users will assume that the number of consumer threads is in effect. It's better to make this incompatible so the users can realize there is a problem.\n7. Joel Jacob Koshy: At least per kafka-103-patch.v1 it is not totally ignored. So if the whitelist config contains: SomeTopic:1 then SomeTopic will not get mirrored because it will fail the isTopicAllowed filter in KafkaServerStartable. i.e., no topics will get mirrored.\n8. Neha Narkhede: Jun, even if we don't want to support the older format, the behavior of silently not mirroring any data is also not acceptable. What would be helpful is to either log it as a warning or throw an InvalidConfigException.\n9. Jun Rao: However, we haven't officially released anything that exposes whitelist in the config. We are just iterating in trunk.\n10. Jun Rao: In summary, I think it's simpler if we enforce a single format for any property value. In this case, to help users identify problems, we can probably log the whitelisted and blacklisted topics, if any.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.518119"}}
{"id": "cb943914e63d8018f3a4a92ff95eae19", "issue_key": "SPARK-422", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "End task instead of just exiting in LocalScheduler for tasks that throw exceptions", "description": "(our Shark testing suite driver was hanging on tests for which an exception was thrown on a worker)", "reporter": "Antonio Lupher", "assignee": null, "created": "0012-04-22T23:39:00.000+0000", "updated": "2012-10-19T22:50:20.000+0000", "resolved": "2012-10-19T22:50:20.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Two comments: - Don't leave commented-out code in the repo (the //System.exit(1)); we can always see the old code using version control. - Do Spark's own unit tests (sbt/sbt test) still pass? Some of them do test exceptions thrown in tasks (but use the LocalScheduler setting that allows some failures).", "created": "2012-04-23T09:51:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from alupher: Yep, the tests pass. I've removed the commented-out line.", "created": "2012-04-23T10:43:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: Looks good, thanks!", "created": "2012-04-24T15:18:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-127, originally reported by alupher", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-422\nSummary: End task instead of just exiting in LocalScheduler for tasks that throw exceptions\nDescription: (our Shark testing suite driver was hanging on tests for which an exception was thrown on a worker)\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: Two comments: - Don't leave commented-out code in the repo (the //System.exit(1)); we can always see the old code using version control. - Do Spark's own unit tests (sbt/sbt test) still pass? Some of them do test exceptions thrown in tasks (but use the LocalScheduler setting that allows some failures).\n2. Patrick McFadin: Github comment from alupher: Yep, the tests pass. I've removed the commented-out line.\n3. Patrick McFadin: Github comment from mateiz: Looks good, thanks!\n4. Patrick McFadin: Imported from Github issue spark-127, originally reported by alupher", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.954178"}}
{"id": "965ee1f312f3dfd5689e9e2432decae3", "issue_key": "SPARK-512", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Shuffle operation", "description": "This is a big issue... the shuffle operation needs to be designed first!", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-04-03T23:07:00.000+0000", "updated": "2012-10-22T14:55:36.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed in master now.", "created": "2011-05-26T11:37:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-4, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-512\nSummary: Shuffle operation\nDescription: This is a big issue... the shuffle operation needs to be designed first!\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: This is fixed in master now.\n2. Patrick McFadin: Imported from Github issue spark-4, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.929776"}}
{"id": "163e8ad90a6ce981ec42e2c4c12d2e04", "issue_key": "KAFKA-915", "issue_type": "Bug", "status": "Closed", "priority": "Critical", "resolution": null, "summary": "System Test - Mirror Maker testcase_5001 failed", "description": "This case passes if brokers are set to partition = 1, replicas = 1 It fails if brokers are set to partition = 5, replicas = 3 (consistently reproducible) This test case is set up as shown below. 1. Start 2 ZK as a cluster in Source 2. Start 2 ZK as a cluster in Target 3. Start 3 brokers as a cluster in Source (partition = 1, replicas = 1) 4. Start 3 brokers as a cluster in Target (partition = 1, replicas = 1) 5. Start 1 MM 6. Start ProducerPerformance to send some data 7. After Producer is done, start ConsoleConsumer to consume data 8. Stop all processes and validate if there is any data loss. 9. No failure is introduced to any process in this test Attached a tar file which contains the logs and system test output for both cases.", "reporter": "John Fung", "assignee": "Joel Jacob Koshy", "created": "2013-05-22T21:14:38.000+0000", "updated": "2013-08-02T23:18:55.000+0000", "resolved": "2013-08-02T23:18:48.000+0000", "labels": ["kafka-0.8", "replication-testing"], "components": [], "comments": [{"author": "John Fung", "body": "This issue is not reproducible in the following commit (or before it) : commit 988d4d8e65a14390abd748318a64e281e4a37c19 Author: Neha Narkhede <neha.narkhede@gmail.com> Date: Tue Apr 30 17:20:54 2013 -0700", "created": "2013-05-22T23:07:28.530+0000"}, {"author": "Neha Narkhede", "body": "Joel, could you take a quick look ?", "created": "2013-05-29T16:45:22.466+0000"}, {"author": "Joel Jacob Koshy", "body": "This failure is due to the fact that the leaderAndIsr request has not yet made it to the brokers until after the mirror maker's rebalance completes. This is related to the issue reported in KAFKA-956. Previously (before we started caching metadata at the brokers) the partition information was retrieved directly from zk. The fix for now would be to use the create topic admin before starting the mirror maker (or move the producer performance start up to well before the mirror maker startup).", "created": "2013-07-04T00:45:03.707+0000"}, {"author": "John Fung", "body": "Thanks Joel for the suggestion. It's working by calling the create topic admin before starting mirror maker. kafka-915-v1.patch is uploaded for the change.", "created": "2013-07-08T17:10:35.247+0000"}, {"author": "John Fung", "body": "Hi Joel, After apply kafka-915-v1.patch (which is to create topic manually before starting mirror maker), testcase_5001 passes. However, testcase_5003 & testcase_5005 are failing due to data loss. Thanks, John", "created": "2013-07-09T21:31:34.608+0000"}, {"author": "Joel Jacob Koshy", "body": "+1 on the patch. I actually could not reproduce the other failures, so I'll check this in. ======================================================== _test_case_name : testcase_5001 _test_class_name : MirrorMakerTest arg : bounce_leader : false arg : bounce_mirror_maker : false arg : message_producing_free_time_sec : 15 arg : num_iteration : 1 arg : num_messages_to_produce_per_producer_call : 50 arg : num_partition : 1 arg : replica_factor : 3 arg : sleep_seconds_between_producer_calls : 1 validation_status : Unique messages from consumer on [test_1] : 500 Unique messages from producer on [test_1] : 500 Validate for data matched on topic [test_1] : PASSED Validate for merged log segment checksum in cluster [source] : PASSED Validate for merged log segment checksum in cluster [target] : PASSED ======================================================== _test_case_name : testcase_5002 _test_class_name : MirrorMakerTest validation_status : ======================================================== _test_case_name : testcase_5003 _test_class_name : MirrorMakerTest arg : bounce_leader : false arg : bounce_mirror_maker : true arg : bounced_entity_downtime_sec : 30 arg : message_producing_free_time_sec : 15 arg : num_iteration : 1 arg : num_messages_to_produce_per_producer_call : 50 arg : num_partition : 1 arg : replica_factor : 3 arg : sleep_seconds_between_producer_calls : 1 validation_status : Unique messages from consumer on [test_1] : 2200 Unique messages from producer on [test_1] : 2200 Validate for data matched on topic [test_1] : PASSED Validate for merged log segment checksum in cluster [source] : PASSED Validate for merged log segment checksum in cluster [target] : PASSED ======================================================== _test_case_name : testcase_5004 _test_class_name : MirrorMakerTest validation_status : ======================================================== _test_case_name : testcase_5005 _test_class_name : MirrorMakerTest arg : bounce_leader : false arg : bounce_mirror_maker : true arg : bounced_entity_downtime_sec : 30 arg : message_producing_free_time_sec : 15 arg : num_iteration : 1 arg : num_messages_to_produce_per_producer_call : 50 arg : num_partition : 2 arg : replica_factor : 3 arg : sleep_seconds_between_producer_calls : 1 validation_status : Unique messages from consumer on [test_1] : 1400 Unique messages from consumer on [test_2] : 1400 Unique messages from producer on [test_1] : 1400 Unique messages from producer on [test_2] : 1400 Validate for data matched on topic [test_1] : PASSED Validate for data matched on topic [test_2] : PASSED Validate for merged log segment checksum in cluster [source] : PASSED Validate for merged log segment checksum in cluster [target] : PASSED ========================================================", "created": "2013-08-02T23:13:16.944+0000"}], "num_comments": 6, "text": "Issue: KAFKA-915\nSummary: System Test - Mirror Maker testcase_5001 failed\nDescription: This case passes if brokers are set to partition = 1, replicas = 1 It fails if brokers are set to partition = 5, replicas = 3 (consistently reproducible) This test case is set up as shown below. 1. Start 2 ZK as a cluster in Source 2. Start 2 ZK as a cluster in Target 3. Start 3 brokers as a cluster in Source (partition = 1, replicas = 1) 4. Start 3 brokers as a cluster in Target (partition = 1, replicas = 1) 5. Start 1 MM 6. Start ProducerPerformance to send some data 7. After Producer is done, start ConsoleConsumer to consume data 8. Stop all processes and validate if there is any data loss. 9. No failure is introduced to any process in this test Attached a tar file which contains the logs and system test output for both cases.\n\nComments (6):\n1. John Fung: This issue is not reproducible in the following commit (or before it) : commit 988d4d8e65a14390abd748318a64e281e4a37c19 Author: Neha Narkhede <neha.narkhede@gmail.com> Date: Tue Apr 30 17:20:54 2013 -0700\n2. Neha Narkhede: Joel, could you take a quick look ?\n3. Joel Jacob Koshy: This failure is due to the fact that the leaderAndIsr request has not yet made it to the brokers until after the mirror maker's rebalance completes. This is related to the issue reported in KAFKA-956. Previously (before we started caching metadata at the brokers) the partition information was retrieved directly from zk. The fix for now would be to use the create topic admin before starting the mirror maker (or move the producer performance start up to well before the mirror maker startup).\n4. John Fung: Thanks Joel for the suggestion. It's working by calling the create topic admin before starting mirror maker. kafka-915-v1.patch is uploaded for the change.\n5. John Fung: Hi Joel, After apply kafka-915-v1.patch (which is to create topic manually before starting mirror maker), testcase_5001 passes. However, testcase_5003 & testcase_5005 are failing due to data loss. Thanks, John\n6. Joel Jacob Koshy: +1 on the patch. I actually could not reproduce the other failures, so I'll check this in. ======================================================== _test_case_name : testcase_5001 _test_class_name : MirrorMakerTest arg : bounce_leader : false arg : bounce_mirror_maker : false arg : message_producing_free_time_sec : 15 arg : num_iteration : 1 arg : num_messages_to_produce_per_producer_call : 50 arg : num_partition : 1 arg : replica_factor : 3 arg : sleep_seconds_between_producer_calls : 1 validation_status : Unique messages from consumer on [test_1] : 500 Unique messages from producer on [test_1] : 500 Validate for data matched on topic [test_1] : PASSED Validate for merged log segment checksum in cluster [source] : PASSED Validate for merged log segment checksum in cluster [target] : PASSED ======================================================== _test_case_name : testcase_5002 _test_class_name : MirrorMakerTest validation_status : ======================================================== _test_case_name : testcase_5003 _test_class_name : MirrorMakerTest arg : bounce_leader : false arg : bounce_mirror_maker : true arg : bounced_entity_downtime_sec : 30 arg : message_producing_free_time_sec : 15 arg : num_iteration : 1 arg : num_messages_to_produce_per_producer_call : 50 arg : num_partition : 1 arg : replica_factor : 3 arg : sleep_seconds_between_producer_calls : 1 validation_status : Unique messages from consumer on [test_1] : 2200 Unique messages from producer on [test_1] : 2200 Validate for data matched on topic [test_1] : PASSED Validate for merged log segment checksum in cluster [source] : PASSED Validate for merged log segment checksum in cluster [target] : PASSED ======================================================== _test_case_name : testcase_5004 _test_class_name : MirrorMakerTest validation_status : ======================================================== _test_case_name : testcase_5005 _test_class_name : MirrorMakerTest arg : bounce_leader : false arg : bounce_mirror_maker : true arg : bounced_entity_downtime_sec : 30 arg : message_producing_free_time_sec : 15 arg : num_iteration : 1 arg : num_messages_to_produce_per_producer_call : 50 arg : num_partition : 2 arg : replica_factor : 3 arg : sleep_seconds_between_producer_calls : 1 validation_status : Unique messages from consumer on [test_1] : 1400 Unique messages from consumer on [test_2] : 1400 Unique messages from producer on [test_1] : 1400 Unique messages from producer on [test_2] : 1400 Validate for data matched on topic [test_1] : PASSED Validate for data matched on topic [test_2] : PASSED Validate for merged log segment checksum in cluster [source] : PASSED Validate for merged log segment checksum in cluster [target] : PASSED ========================================================", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.894267"}}
{"id": "958962c92f4809b0ed8717e44b9898cf", "issue_key": "KAFKA-572", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Merged log segment checksums mismatched in Leader failure System Test case", "description": "", "reporter": "John Fung", "assignee": null, "created": "2012-10-13T05:12:37.000+0000", "updated": "2012-10-13T22:22:38.000+0000", "resolved": "2012-10-13T22:22:38.000+0000", "labels": [], "components": [], "comments": [{"author": "John Fung", "body": "* Test Description: 1. Start a 3-broker cluster as source 2. Send messages to source cluster 3. Find leader and terminate it (kill -15) 4. Start the broker shortly 5. Start a consumer to consume data 6. Compare the MessageID in the data between producer log and consumer log. * To reproduce this issue, please do the followings: 1. Download the latest 0.8 branch 2. Apply the patch attached to this JIRA 3. Build kafka by running \"./sbt update package\" 4. Execute the test in directory \"system_test\" : \"python -B system_test_runner.py\" * Output from the test - No data loss but merged log segment checksums mismatched 2012-10-12 22:27:35,760 - INFO - ====================================================== 2012-10-12 22:27:35,760 - INFO - validating data matched 2012-10-12 22:27:35,760 - INFO - ====================================================== 2012-10-12 22:27:35,771 - INFO - no. of unique messages on topic [test_1] sent from publisher : 500 (kafka_system_test_utils) 2012-10-12 22:27:35,771 - INFO - no. of unique messages on topic [test_1] received by consumer : 500 (kafka_system_test_utils) 2012-10-12 22:27:35,771 - INFO - ================================================ 2012-10-12 22:27:35,771 - INFO - validating merged broker log segment checksums 2012-10-12 22:27:35,771 - INFO - ================================================ {u'kafka_server_1_logs:test_1-0': 'd70c8d37634b0b08cd407eb042e77ef8', u'kafka_server_2_logs:test_1-0': 'd70c8d37634b0b08cd407eb042e77ef8', u'kafka_server_3_logs:test_1-0': '924d30d5f0d2a8ba9ef45f7cba88e192'} 2012-10-12 22:27:35,774 - ERROR - merged log segment checksum in test_1-0 mismatched (kafka_system_test_utils)", "created": "2012-10-13T05:32:35.065+0000"}, {"author": "John Fung", "body": "* Data Log Segment files sizes: system_test/replication_testsuite/testcase_0102/logs $ find broker-* -name '00*.log' -ls 9702225 12 -rw-r--r-- 1 jfung eng 10279 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000301.log 9702226 12 -rw-r--r-- 1 jfung eng 10271 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000000.log 9702227 12 -rw-r--r-- 1 jfung eng 10293 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000201.log 9702228 12 -rw-r--r-- 1 jfung eng 10292 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000101.log 9702230 12 -rw-r--r-- 1 jfung eng 10178 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000401.log 9702239 12 -rw-r--r-- 1 jfung eng 10279 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000301.log 9702240 12 -rw-r--r-- 1 jfung eng 10271 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000000.log 9702241 12 -rw-r--r-- 1 jfung eng 10293 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000201.log 9702242 12 -rw-r--r-- 1 jfung eng 10292 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000101.log 9702244 12 -rw-r--r-- 1 jfung eng 10178 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000401.log 9702252 12 -rw-r--r-- 1 jfung eng 10279 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000361.log 9702255 4 -rw-r--r-- 1 jfung eng 4010 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000461.log 9702256 12 -rw-r--r-- 1 jfung eng 10271 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000000.log 9702257 8 -rw-r--r-- 1 jfung eng 5657 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000101.log 9702259 12 -rw-r--r-- 1 jfung eng 10280 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000261.log 9702262 12 -rw-r--r-- 1 jfung eng 10816 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000156.log", "created": "2012-10-13T05:37:28.269+0000"}, {"author": "John Fung", "body": "This issue is due to a bug in the System Test script (log segment files were not sorted before merging). So mark this Fixed.", "created": "2012-10-13T22:22:38.414+0000"}], "num_comments": 3, "text": "Issue: KAFKA-572\nSummary: Merged log segment checksums mismatched in Leader failure System Test case\n\nComments (3):\n1. John Fung: * Test Description: 1. Start a 3-broker cluster as source 2. Send messages to source cluster 3. Find leader and terminate it (kill -15) 4. Start the broker shortly 5. Start a consumer to consume data 6. Compare the MessageID in the data between producer log and consumer log. * To reproduce this issue, please do the followings: 1. Download the latest 0.8 branch 2. Apply the patch attached to this JIRA 3. Build kafka by running \"./sbt update package\" 4. Execute the test in directory \"system_test\" : \"python -B system_test_runner.py\" * Output from the test - No data loss but merged log segment checksums mismatched 2012-10-12 22:27:35,760 - INFO - ====================================================== 2012-10-12 22:27:35,760 - INFO - validating data matched 2012-10-12 22:27:35,760 - INFO - ====================================================== 2012-10-12 22:27:35,771 - INFO - no. of unique messages on topic [test_1] sent from publisher : 500 (kafka_system_test_utils) 2012-10-12 22:27:35,771 - INFO - no. of unique messages on topic [test_1] received by consumer : 500 (kafka_system_test_utils) 2012-10-12 22:27:35,771 - INFO - ================================================ 2012-10-12 22:27:35,771 - INFO - validating merged broker log segment checksums 2012-10-12 22:27:35,771 - INFO - ================================================ {u'kafka_server_1_logs:test_1-0': 'd70c8d37634b0b08cd407eb042e77ef8', u'kafka_server_2_logs:test_1-0': 'd70c8d37634b0b08cd407eb042e77ef8', u'kafka_server_3_logs:test_1-0': '924d30d5f0d2a8ba9ef45f7cba88e192'} 2012-10-12 22:27:35,774 - ERROR - merged log segment checksum in test_1-0 mismatched (kafka_system_test_utils)\n2. John Fung: * Data Log Segment files sizes: system_test/replication_testsuite/testcase_0102/logs $ find broker-* -name '00*.log' -ls 9702225 12 -rw-r--r-- 1 jfung eng 10279 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000301.log 9702226 12 -rw-r--r-- 1 jfung eng 10271 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000000.log 9702227 12 -rw-r--r-- 1 jfung eng 10293 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000201.log 9702228 12 -rw-r--r-- 1 jfung eng 10292 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000101.log 9702230 12 -rw-r--r-- 1 jfung eng 10178 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000401.log 9702239 12 -rw-r--r-- 1 jfung eng 10279 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000301.log 9702240 12 -rw-r--r-- 1 jfung eng 10271 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000000.log 9702241 12 -rw-r--r-- 1 jfung eng 10293 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000201.log 9702242 12 -rw-r--r-- 1 jfung eng 10292 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000101.log 9702244 12 -rw-r--r-- 1 jfung eng 10178 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000401.log 9702252 12 -rw-r--r-- 1 jfung eng 10279 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000361.log 9702255 4 -rw-r--r-- 1 jfung eng 4010 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000461.log 9702256 12 -rw-r--r-- 1 jfung eng 10271 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000000.log 9702257 8 -rw-r--r-- 1 jfung eng 5657 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000101.log 9702259 12 -rw-r--r-- 1 jfung eng 10280 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000261.log 9702262 12 -rw-r--r-- 1 jfung eng 10816 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000156.log\n3. John Fung: This issue is due to a bug in the System Test script (log segment files were not sorted before merging). So mark this Fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.729074"}}
{"id": "85d199ac03cf4903d15067501290cb31", "issue_key": "KAFKA-420", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "maintain HW correctly with only 1 replica in ISR", "description": "Currently, the HW maintenance logic is only triggered when handling fetch requests from the follower. As a result, if the ISR has only 1 replica, the HW won't be incremented since there is no request from the follower to trigger the maintenance logic.", "reporter": "Jun Rao", "assignee": "Jun Rao", "created": "2012-07-26T17:12:17.000+0000", "updated": "2012-08-25T00:47:20.000+0000", "resolved": "2012-08-25T00:47:16.000+0000", "labels": ["bugs"], "components": ["core"], "comments": [{"author": "Jun Rao", "body": "First of all, we need to think through what message commits mean when there is only 1 replica in ISR. Does it mean message is in memory in the only replica or do we have to wait for the message to be flushed to disk. It seems to me that the latter is probably more meaningful and is compatible with what we have in 0.7. If the latter is what we want to implement, we will need to add a call to ReplicaManager.maybeIncrementLeaderHW from the log flushing logic.", "created": "2012-07-26T17:18:15.452+0000"}, {"author": "Jun Rao", "body": "Attach patch v1. This is a simpler fix. Instead of waiting for data flushed to disk, this patch just advances the HW after the data is in memory. If a user cares about durability, he can always increase the replication factor. Patched an existing unit test to expose the problem. Patched 3 places where we may need to increment the HW: (1) new produce requests coming to the leader; (2) ISR shrinks; (3) a replica becomes the leader.", "created": "2012-08-22T03:41:13.605+0000"}, {"author": "Neha Narkhede", "body": "Looks good. +1", "created": "2012-08-24T02:07:42.976+0000"}, {"author": "Jun Rao", "body": "Thanks for the review. Rebased and committed to 0.8.", "created": "2012-08-25T00:47:16.617+0000"}], "num_comments": 4, "text": "Issue: KAFKA-420\nSummary: maintain HW correctly with only 1 replica in ISR\nDescription: Currently, the HW maintenance logic is only triggered when handling fetch requests from the follower. As a result, if the ISR has only 1 replica, the HW won't be incremented since there is no request from the follower to trigger the maintenance logic.\n\nComments (4):\n1. Jun Rao: First of all, we need to think through what message commits mean when there is only 1 replica in ISR. Does it mean message is in memory in the only replica or do we have to wait for the message to be flushed to disk. It seems to me that the latter is probably more meaningful and is compatible with what we have in 0.7. If the latter is what we want to implement, we will need to add a call to ReplicaManager.maybeIncrementLeaderHW from the log flushing logic.\n2. Jun Rao: Attach patch v1. This is a simpler fix. Instead of waiting for data flushed to disk, this patch just advances the HW after the data is in memory. If a user cares about durability, he can always increase the replication factor. Patched an existing unit test to expose the problem. Patched 3 places where we may need to increment the HW: (1) new produce requests coming to the leader; (2) ISR shrinks; (3) a replica becomes the leader.\n3. Neha Narkhede: Looks good. +1\n4. Jun Rao: Thanks for the review. Rebased and committed to 0.8.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.653140"}}
{"id": "7e146c25882e0e092291813e0d131daa", "issue_key": "HADOOP-850", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Add Writable implementations for variable-length integer types.", "description": "Currently Hadoop supports only three basic integer-like types: ByteWritable, IntWritable and LongWritable. They provide a fixed tradeoff between their value range and on-disk space consumption. But it is sometimes useful to be able to store integer values with broader allowed range, but less space consumption when possible. This is especially useful when storing very long series of values, combined with delta encoding. Lucene already implements variable-length encoding for positive int and long. I propose to add similar Writable implementations, which use the same encoding methods.", "reporter": "Andrzej Bialecki", "assignee": "Andrzej Bialecki", "created": "2006-12-27T19:11:46.000+0000", "updated": "2007-01-05T23:22:53.000+0000", "resolved": "2007-01-03T18:57:50.000+0000", "labels": [], "components": ["io"], "comments": [{"author": "Andrzej Bialecki", "body": "Implementation of VIntWritable and VLongWritable. If there are no objections I'd like to commit them soon.", "created": "2006-12-27T19:13:45.000+0000"}, {"author": "Hadoop QA", "body": "-1, because the javadoc command appears to have generated warning messages when testing the latest attachment (http://issues.apache.org/jira/secure/attachment/12347952/variable-ints.patch) against trunk revision r489707. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch.", "created": "2006-12-27T19:33:29.000+0000"}, {"author": "Andrzej Bialecki", "body": "Fix javadoc warnings (references to external classes in Lucene).", "created": "2006-12-27T20:04:10.000+0000"}, {"author": "Hadoop QA", "body": "+1, because http://issues.apache.org/jira/secure/attachment/12347954/variable-ints.patch applied and successfully tested against trunk revision r489707.", "created": "2006-12-27T20:41:29.000+0000"}, {"author": "Owen O'Malley", "body": "Please use the VInt implementation that is already in org.apache.hadoop.io.WritableUtils. It makes sense to have a WritableComparable class for this, but I think it is confusing to have two incompatible vint implementations.", "created": "2006-12-28T03:17:32.000+0000"}, {"author": "Andrzej Bialecki", "body": "Indeed, I missed the implementation in WritableUtils, which is even better than Lucene's as it supports also negative numbers. Here's an updated patch.", "created": "2006-12-28T19:29:32.000+0000"}, {"author": "Owen O'Malley", "body": "Do we really need both VInt and VLong? They will serialize in exactly the same space, so all you are saving is in memory space. Other than that question/comment +1.", "created": "2006-12-29T08:05:31.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Andrzej!", "created": "2007-01-03T18:57:50.836+0000"}], "num_comments": 8, "text": "Issue: HADOOP-850\nSummary: Add Writable implementations for variable-length integer types.\nDescription: Currently Hadoop supports only three basic integer-like types: ByteWritable, IntWritable and LongWritable. They provide a fixed tradeoff between their value range and on-disk space consumption. But it is sometimes useful to be able to store integer values with broader allowed range, but less space consumption when possible. This is especially useful when storing very long series of values, combined with delta encoding. Lucene already implements variable-length encoding for positive int and long. I propose to add similar Writable implementations, which use the same encoding methods.\n\nComments (8):\n1. Andrzej Bialecki: Implementation of VIntWritable and VLongWritable. If there are no objections I'd like to commit them soon.\n2. Hadoop QA: -1, because the javadoc command appears to have generated warning messages when testing the latest attachment (http://issues.apache.org/jira/secure/attachment/12347952/variable-ints.patch) against trunk revision r489707. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch.\n3. Andrzej Bialecki: Fix javadoc warnings (references to external classes in Lucene).\n4. Hadoop QA: +1, because http://issues.apache.org/jira/secure/attachment/12347954/variable-ints.patch applied and successfully tested against trunk revision r489707.\n5. Owen O'Malley: Please use the VInt implementation that is already in org.apache.hadoop.io.WritableUtils. It makes sense to have a WritableComparable class for this, but I think it is confusing to have two incompatible vint implementations.\n6. Andrzej Bialecki: Indeed, I missed the implementation in WritableUtils, which is even better than Lucene's as it supports also negative numbers. Here's an updated patch.\n7. Owen O'Malley: Do we really need both VInt and VLong? They will serialize in exactly the same space, so all you are saving is in memory space. Other than that question/comment +1.\n8. Doug Cutting: I just committed this. Thanks, Andrzej!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.496862"}}
{"id": "ac4c740ab3bc51e95ba23a934b468b90", "issue_key": "HADOOP-248", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "locating map outputs via random probing is inefficient", "description": "Currently the ReduceTaskRunner polls the JobTracker for a random list of map tasks asking for their output locations. It would be better if the JobTracker kept an ordered log and the interface was changed to: class MapLocationResults { public int getTimestamp(); public MapOutputLocation[] getLocations(); } interface InterTrackerProtocol { ... MapLocationResults locateMapOutputs(int prevTimestamp); } with the intention that each time a ReduceTaskRunner calls locateMapOutputs, it passes back the \"timestamp\" that it got from the previous result. That way, reduces can easily find the new MapOutputs. This should help the \"ramp up\" when the maps first start finishing.", "reporter": "Owen O'Malley", "assignee": "Devaraj Das", "created": "2006-05-24T06:25:33.000+0000", "updated": "2013-05-02T02:29:03.000+0000", "resolved": "2007-02-22T20:22:42.000+0000", "labels": [], "components": [], "comments": [{"author": "Devaraj Das", "body": "Propose the following: 1) Modify the \"TaskCompletionEvent[] getTaskCompletionEvents(String jobid, int fromEventId)\" defined in IntertrackerProtocol and JobSubmissionProtocol to have a new argument that will signify how many events we want to fetch. We may get a smaller number depending on how many events got registered for the job. So, it becomes: TaskCompletionEvent[] getTaskCompletionEvents(String jobid, int fromEventId, int maxEvents) This will generally be more scalable. In the case of map-output-fetches, it helps in the way that we do the same thing as we do today (except that the randomness is not there and the TT exactly knows which maps finished). 2) Since the events IDs are numbered in a monotonically increasing sequence for a Job, we don't need to maintain timestamps (as the original comment on this bug suggests). 3) Add a \"boolean isMapTask()\" method to TaskCompletionEvent class that will return true if the event is from a map task, false otherwise. Comments?", "created": "2007-01-17T11:27:59.412+0000"}, {"author": "Sameer Paranjpye", "body": "Sounds good. How do we deal with map failures? Do we get multiple completion events when there are re-tries?", "created": "2007-01-17T18:29:45.975+0000"}, {"author": "Owen O'Malley", "body": "Sounds good, Devaraj. The events are per a taskid, not a tipid. So different attempts to run \"map 0\" would result in different events. That said, however, we probably should make another event \"lost\" or something for tasks that are lost because their output had problems or the task tracker was lost. We may also want to flag the \"complete\" events of lost tasks as obsolete so that reduces don't see them and try and fetch their outputs.", "created": "2007-01-17T19:19:38.989+0000"}, {"author": "Devaraj Das", "body": "The patch does the following: 1) Modifies the protocol version for InterTrackerProtocol since there is a major change there in the way map output is fetched. The method locateMapOutputs has been removed. 2) The getTaskCompletion method in both InterTrackerProtocol and JobSubmission has been changed to take an extra argument - the max no. of events we want to fetch from the JobTracker. 3) Two more fields are added in TaskCompletionEvents - the real-ID portion of the taskId string (for e.g., if the taskId is task_0001_m_000003_0, the real-ID is 3 within the job), and another boolean field to indicate whether the task is a map or not. By the way, these could have done on the Reduce side also by parsing the taskId string, but I think this is a more general way of doing it and it also is in line with the thought of having \"ID objects\" in the future. 4) Only 10 events are fetched at a time by the JobClient 5) A new value OBSELETE has been added to TaskCompletionEvent.Status to signify lost tasks (which were earlier reported as SUCCEEDED). For this, whenever a FAILED/KILLED TaskStatus is got, it is checked whether a SUCCEEDED was earlier reported for the same taskId, and if so that event is marked as OBSELETE. 6) The number of events probed by the ReduceTaskRunner at any time is equal to max(5*numCopiers, 50). Feedback appreciated..", "created": "2007-01-24T13:47:14.772+0000"}, {"author": "Devaraj Das", "body": "This patch does better failure handling. It saves the location of a failed map output fetch in a hashmap for later retrial. Also, it has logic which makes it prefer a newer location to the saved one.", "created": "2007-01-25T16:50:28.795+0000"}, {"author": "Hadoop QA", "body": "+1, because http://issues.apache.org/jira/secure/attachment/12349617/248-initial8.patch applied and successfully tested against trunk revision r499156.", "created": "2007-01-25T17:50:52.648+0000"}, {"author": "Owen O'Malley", "body": "This is a minor modification of Devaraj's patch that fixes a spelling mistake (OBSELETE) and use TaskInProgress.partition instead of parsing the task id.", "created": "2007-01-25T23:40:58.930+0000"}, {"author": "Hadoop QA", "body": "+1, because http://issues.apache.org/jira/secure/attachment/12349653/248-9.patch applied and successfully tested against trunk revision r500023.", "created": "2007-01-26T00:08:42.749+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Devaraj!", "created": "2007-01-26T23:38:10.404+0000"}, {"author": "Doug Cutting", "body": "I just reverted this, since it was causing things to hang.", "created": "2007-01-29T21:04:48.252+0000"}, {"author": "Devaraj Das", "body": "This had a problem introduced unintentionally in the last submission (by Owen, when he corrected the spelling of OBSOLETE, etc.). The problem was that there is a variable called fromEventId which is used to track from which eventId a tasktracker should fetch events from from the jobtracker. This was earlier a IntWritable object, so that set(<somenumber>) could be done on the object and the new value of the 'int' within the object could be seen even when the method invocation returned. This variable was changed to an int and instead \"fromEventId += <somenumber>\" was done. Unfortunately, this would not be visible when the method invocation returned and hence the TaskTracker would get stuck at a particular eventId and would make no forward progress... Attached is the new patch which has the IntWritable stuff put back in, and also the method JobClient.listEvents has been modified to take two extra args - fromEventId, numEvents (this method didn't exist when I was earlier working on this issue). The JobSubmissionProtocol version has been changed also to reflect the change in the getTaskCompletionEvents protocol method (missed this in the earlier patch).", "created": "2007-02-22T14:45:51.745+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Devaraj!", "created": "2007-02-22T20:22:42.541+0000"}, {"author": "Hadoop QA", "body": "-1, because the patch command could not apply the latest attachment (http://issues.apache.org/jira/secure/attachment/12351812/248-fixed1.patch) as a patch to trunk revision r510644. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch", "created": "2007-02-22T21:01:33.305+0000"}, {"author": "Nigel Daley", "body": "Sorry, ignore Hadoop QA's -1.", "created": "2007-02-22T21:02:51.500+0000"}], "num_comments": 14, "text": "Issue: HADOOP-248\nSummary: locating map outputs via random probing is inefficient\nDescription: Currently the ReduceTaskRunner polls the JobTracker for a random list of map tasks asking for their output locations. It would be better if the JobTracker kept an ordered log and the interface was changed to: class MapLocationResults { public int getTimestamp(); public MapOutputLocation[] getLocations(); } interface InterTrackerProtocol { ... MapLocationResults locateMapOutputs(int prevTimestamp); } with the intention that each time a ReduceTaskRunner calls locateMapOutputs, it passes back the \"timestamp\" that it got from the previous result. That way, reduces can easily find the new MapOutputs. This should help the \"ramp up\" when the maps first start finishing.\n\nComments (14):\n1. Devaraj Das: Propose the following: 1) Modify the \"TaskCompletionEvent[] getTaskCompletionEvents(String jobid, int fromEventId)\" defined in IntertrackerProtocol and JobSubmissionProtocol to have a new argument that will signify how many events we want to fetch. We may get a smaller number depending on how many events got registered for the job. So, it becomes: TaskCompletionEvent[] getTaskCompletionEvents(String jobid, int fromEventId, int maxEvents) This will generally be more scalable. In the case of map-output-fetches, it helps in the way that we do the same thing as we do today (except that the randomness is not there and the TT exactly knows which maps finished). 2) Since the events IDs are numbered in a monotonically increasing sequence for a Job, we don't need to maintain timestamps (as the original comment on this bug suggests). 3) Add a \"boolean isMapTask()\" method to TaskCompletionEvent class that will return true if the event is from a map task, false otherwise. Comments?\n2. Sameer Paranjpye: Sounds good. How do we deal with map failures? Do we get multiple completion events when there are re-tries?\n3. Owen O'Malley: Sounds good, Devaraj. The events are per a taskid, not a tipid. So different attempts to run \"map 0\" would result in different events. That said, however, we probably should make another event \"lost\" or something for tasks that are lost because their output had problems or the task tracker was lost. We may also want to flag the \"complete\" events of lost tasks as obsolete so that reduces don't see them and try and fetch their outputs.\n4. Devaraj Das: The patch does the following: 1) Modifies the protocol version for InterTrackerProtocol since there is a major change there in the way map output is fetched. The method locateMapOutputs has been removed. 2) The getTaskCompletion method in both InterTrackerProtocol and JobSubmission has been changed to take an extra argument - the max no. of events we want to fetch from the JobTracker. 3) Two more fields are added in TaskCompletionEvents - the real-ID portion of the taskId string (for e.g., if the taskId is task_0001_m_000003_0, the real-ID is 3 within the job), and another boolean field to indicate whether the task is a map or not. By the way, these could have done on the Reduce side also by parsing the taskId string, but I think this is a more general way of doing it and it also is in line with the thought of having \"ID objects\" in the future. 4) Only 10 events are fetched at a time by the JobClient 5) A new value OBSELETE has been added to TaskCompletionEvent.Status to signify lost tasks (which were earlier reported as SUCCEEDED). For this, whenever a FAILED/KILLED TaskStatus is got, it is checked whether a SUCCEEDED was earlier reported for the same taskId, and if so that event is marked as OBSELETE. 6) The number of events probed by the ReduceTaskRunner at any time is equal to max(5*numCopiers, 50). Feedback appreciated..\n5. Devaraj Das: This patch does better failure handling. It saves the location of a failed map output fetch in a hashmap for later retrial. Also, it has logic which makes it prefer a newer location to the saved one.\n6. Hadoop QA: +1, because http://issues.apache.org/jira/secure/attachment/12349617/248-initial8.patch applied and successfully tested against trunk revision r499156.\n7. Owen O'Malley: This is a minor modification of Devaraj's patch that fixes a spelling mistake (OBSELETE) and use TaskInProgress.partition instead of parsing the task id.\n8. Hadoop QA: +1, because http://issues.apache.org/jira/secure/attachment/12349653/248-9.patch applied and successfully tested against trunk revision r500023.\n9. Doug Cutting: I just committed this. Thanks, Devaraj!\n10. Doug Cutting: I just reverted this, since it was causing things to hang.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.257719"}}
{"id": "cdfba62221e1ae0298cc21976f54e9d8", "issue_key": "KAFKA-357", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Refactor zookeeper code in KafkaZookeeper into reusable components", "description": "Currently, we stuck a lot of zookeeper code in KafkaZookeeper. This includes leader election, ISR maintenance etc. However, it will be good to wrap up related code in separate components that make logical sense. A good example of this is the ZKQueue data structure.", "reporter": "Neha Narkhede", "assignee": null, "created": "2012-06-01T01:29:31.000+0000", "updated": "2017-08-17T12:18:13.000+0000", "resolved": "2017-08-17T12:18:13.000+0000", "labels": [], "components": [], "comments": [{"author": "Manikumar", "body": "Zookeeper related code is getting refactored in KAFKA-5027/KAFKA-5501", "created": "2017-08-17T12:18:13.567+0000"}], "num_comments": 1, "text": "Issue: KAFKA-357\nSummary: Refactor zookeeper code in KafkaZookeeper into reusable components\nDescription: Currently, we stuck a lot of zookeeper code in KafkaZookeeper. This includes leader election, ISR maintenance etc. However, it will be good to wrap up related code in separate components that make logical sense. A good example of this is the ZKQueue data structure.\n\nComments (1):\n1. Manikumar: Zookeeper related code is getting refactored in KAFKA-5027/KAFKA-5501", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.609433"}}
{"id": "634de6da4b27224c6fd9384c3a090a4b", "issue_key": "HADOOP-447", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "DistributedFileSystem.getBlockSize(Path) does not resolve absolute path", "description": "getBlockSize() does not check for an absolute path like the rest of the DistributeFileSystem API does. Consequently getBlockSize(Path) does not work with relative paths.", "reporter": "Benjamin Reed", "assignee": "Raghu Angadi", "created": "2006-08-11T17:40:11.000+0000", "updated": "2009-07-08T16:42:00.000+0000", "resolved": "2006-11-20T23:48:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Raghu Angadi", "body": "Is there a way reproduce this? Is the fix simply replacing public long getBlockSize(Path f) throws IOException { return dfs.getBlockSize(f); } with public long getBlockSize(Path f) throws IOException { return dfs.getBlockSize( makeAbsolute(f) ); } in DistributedFileSystem.java ? thanks.", "created": "2006-11-16T01:43:33.000+0000"}, {"author": "Raghu Angadi", "body": "patch for the fix mentioned above. Looks like using makeAbsolute(f) in place of 'f' fixes it.", "created": "2006-11-16T23:30:10.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Raghu.", "created": "2006-11-20T23:48:00.000+0000"}], "num_comments": 3, "text": "Issue: HADOOP-447\nSummary: DistributedFileSystem.getBlockSize(Path) does not resolve absolute path\nDescription: getBlockSize() does not check for an absolute path like the rest of the DistributeFileSystem API does. Consequently getBlockSize(Path) does not work with relative paths.\n\nComments (3):\n1. Raghu Angadi: Is there a way reproduce this? Is the fix simply replacing public long getBlockSize(Path f) throws IOException { return dfs.getBlockSize(f); } with public long getBlockSize(Path f) throws IOException { return dfs.getBlockSize( makeAbsolute(f) ); } in DistributedFileSystem.java ? thanks.\n2. Raghu Angadi: patch for the fix mentioned above. Looks like using makeAbsolute(f) in place of 'f' fixes it.\n3. Doug Cutting: I just committed this. Thanks, Raghu.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.352717"}}
{"id": "126429971d5cceb0e91ab092fc2c41c8", "issue_key": "SPARK-1009", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Updated MLlib docs to show how to use it in Python", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": "Hossein Falaki", "created": "2013-12-30T19:29:31.000+0000", "updated": "2014-01-07T23:24:37.000+0000", "resolved": "2014-01-07T23:24:37.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Hossein Falaki", "body": "PR #322", "created": "2014-01-07T23:24:37.689+0000"}], "num_comments": 1, "text": "Issue: SPARK-1009\nSummary: Updated MLlib docs to show how to use it in Python\n\nComments (1):\n1. Hossein Falaki: PR #322", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.106056"}}
{"id": "9cbbbbeaa62d4f1d69e1126c11b11f78", "issue_key": "KAFKA-131", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Hadoop Consumer goes into an infinite loop when kafka.request.limit is set to -1", "description": "There is a bug in KafkaETLContext.java where in a new Iterator instance is being created every time. This causes endless loops.", "reporter": "Sam William", "assignee": null, "created": "2011-09-02T19:20:19.000+0000", "updated": "2011-09-29T18:39:11.000+0000", "resolved": "2011-09-13T01:20:01.000+0000", "labels": ["patch"], "components": ["contrib"], "comments": [{"author": "Sam William", "body": "Adding an instance variable _respIterator , so that response.iterator() isn't called multiple times", "created": "2011-09-02T20:38:34.211+0000"}, {"author": "Sam William", "body": "Fix for the infinite loop bug in KafkaETLContext.java", "created": "2011-09-02T21:38:53.121+0000"}, {"author": "Richard Park", "body": "Just one thing's missing. On line 201 (of original file): _offset += msgAndOffset.offset(); That's incorrect. The msgAndOffset returns the offset, not the offset increment. So it should be: _offset = msgAndOffset.offset();", "created": "2011-09-02T23:36:16.920+0000"}, {"author": "Sam William", "body": "Adding the fix for offsets (Line 201)", "created": "2011-09-06T17:34:01.948+0000"}, {"author": "Richard Park", "body": "Great. I'm good with this patch.", "created": "2011-09-06T17:41:28.195+0000"}, {"author": "Jun Rao", "body": "Thanks Sam and Richard. I just committed this.", "created": "2011-09-07T21:48:07.038+0000"}, {"author": "Felix GV", "body": "I just wanted to point out that this bug seems to happen whether kafka.request.limit is set to -1 or not. The current 0.6 release that is available on the site is not very usable because of this bug... The current trunk does fix this problem though, which is great. Thanks :) !", "created": "2011-09-29T18:08:54.659+0000"}, {"author": "Blake Matheny", "body": "We have run into this as well Felix. I'd like to backport whatever change fixed this in trunk into our 0.6.1 branch. Any idea where I should look?", "created": "2011-09-29T18:23:46.862+0000"}, {"author": "Sam William", "body": "Blake, You could apply the attached patch (https://issues.apache.org/jira/secure/attachment/12493181/KAFKA-131.patch) to the file KafkaETLContext.java", "created": "2011-09-29T18:30:13.420+0000"}, {"author": "Blake Matheny", "body": "Sorry I should have been more clear. We actually run into this issue not using the KafkaETL. We occasionally see regular consumers go into a loop (continue to fetch the same offset), I thought the comment from Felix was referring to that specifically.", "created": "2011-09-29T18:39:11.006+0000"}], "num_comments": 10, "text": "Issue: KAFKA-131\nSummary: Hadoop Consumer goes into an infinite loop when kafka.request.limit is set to -1\nDescription: There is a bug in KafkaETLContext.java where in a new Iterator instance is being created every time. This causes endless loops.\n\nComments (10):\n1. Sam William: Adding an instance variable _respIterator , so that response.iterator() isn't called multiple times\n2. Sam William: Fix for the infinite loop bug in KafkaETLContext.java\n3. Richard Park: Just one thing's missing. On line 201 (of original file): _offset += msgAndOffset.offset(); That's incorrect. The msgAndOffset returns the offset, not the offset increment. So it should be: _offset = msgAndOffset.offset();\n4. Sam William: Adding the fix for offsets (Line 201)\n5. Richard Park: Great. I'm good with this patch.\n6. Jun Rao: Thanks Sam and Richard. I just committed this.\n7. Felix GV: I just wanted to point out that this bug seems to happen whether kafka.request.limit is set to -1 or not. The current 0.6 release that is available on the site is not very usable because of this bug... The current trunk does fix this problem though, which is great. Thanks :) !\n8. Blake Matheny: We have run into this as well Felix. I'd like to backport whatever change fixed this in trunk into our 0.6.1 branch. Any idea where I should look?\n9. Sam William: Blake, You could apply the attached patch (https://issues.apache.org/jira/secure/attachment/12493181/KAFKA-131.patch) to the file KafkaETLContext.java\n10. Blake Matheny: Sorry I should have been more clear. We actually run into this issue not using the KafkaETL. We occasionally see regular consumers go into a loop (continue to fetch the same offset), I thought the comment from Felix was referring to that specifically.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.518119"}}
{"id": "c84ba57fef7753f0c84c3a7cf9e7f5e1", "issue_key": "KAFKA-123", "issue_type": "Bug", "status": "Closed", "priority": "Trivial", "resolution": null, "summary": "Rubygem gemspec doesn't build correctly", "description": "The gemspec file doesn't build correctly because lib/kafka/error_codes.rb is not included in the gemspec files. I've created a pull request in github that fixes the problem. It's really simple. Simpler to fix than to write this ticket. https://github.com/kafka-dev/kafka/pull/41", "reporter": "John Le", "assignee": null, "created": "2011-08-26T21:31:11.000+0000", "updated": "2014-02-10T23:03:01.000+0000", "resolved": "2013-03-04T17:02:50.000+0000", "labels": ["ruby"], "components": ["clients"], "comments": [{"author": "Jun Rao", "body": "John, Thanks for the fix. Would you mind attaching the patch directly here? You need to grant Apache the permission to use your patch and I can't do that for you. Thanks again.", "created": "2011-09-09T15:40:57.485+0000"}, {"author": "Colin B.", "body": "This was a duplicate of Kafka-135 and is already fixed. Please close this bug.", "created": "2013-03-04T16:50:04.352+0000"}], "num_comments": 2, "text": "Issue: KAFKA-123\nSummary: Rubygem gemspec doesn't build correctly\nDescription: The gemspec file doesn't build correctly because lib/kafka/error_codes.rb is not included in the gemspec files. I've created a pull request in github that fixes the problem. It's really simple. Simpler to fix than to write this ticket. https://github.com/kafka-dev/kafka/pull/41\n\nComments (2):\n1. Jun Rao: John, Thanks for the fix. Would you mind attaching the patch directly here? You need to grant Apache the permission to use your patch and I can't do that for you. Thanks again.\n2. Colin B.: This was a duplicate of Kafka-135 and is already fixed. Please close this bug.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.518119"}}
{"id": "d5ebbae2150b3b7b9a9e7ce8e32f87ef", "issue_key": "KAFKA-384", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Fix intermittent test failures and remove unnecessary sleeps", "description": "Seeing intermittent failures in 0.8 unit tests. Also, many sleeps can be removed (with producer acks in place) and I think MockTime isn't used in some places where it should.", "reporter": "Joel Jacob Koshy", "assignee": "Neha Narkhede", "created": "2012-06-29T22:29:29.000+0000", "updated": "2012-07-31T23:28:30.000+0000", "resolved": "2012-07-31T23:28:30.000+0000", "labels": [], "components": [], "comments": [{"author": "Neha Narkhede", "body": "This patch removes sleep statements from the unit tests. Changes include - 1. All sleep statements except those related to a scheduler are removed. So 2-3 sleep statements that exercise the producer side queue expiration logic have not been removed 2. For the Log tests, passed in a Time parameter to the Log. Kafka server already takes in an optional Time parameter that defaults to SystemTime. However, it wasn't passed around in LogManager. Fixed it so KafkaServer passes its Time variable to LogManager which passes it to Log. This is useful in removing all sleep statements from unit tests for the log manager and logs", "created": "2012-07-29T19:24:17.174+0000"}, {"author": "Jun Rao", "body": "Thanks for the patch. It looks good. Just 1 comment: 1. AsyncProducerTest.testQueueTimeExpired(): This is an existing issue, but probably can be fixed in this patch too. It seems that we should do producerSendThread.shutdown after EasyMock.verify(mockHandler). Shutdown always sends all remaining messages in the buffer. If we call shutdown before verify, it's not clear if the send was triggered by timeout or shutdown.", "created": "2012-07-30T20:12:13.839+0000"}, {"author": "Neha Narkhede", "body": "Thanks for the review, Jun. That is a good point. I fixed that and also removed the reference to mockTime since that is not useful here.", "created": "2012-07-30T20:39:49.004+0000"}, {"author": "Joel Jacob Koshy", "body": "+1 for v2.", "created": "2012-07-30T20:44:23.849+0000"}, {"author": "Jay Kreps", "body": "You are my hero...", "created": "2012-07-30T21:42:12.295+0000"}, {"author": "Jun Rao", "body": "+1 on v2 too.", "created": "2012-07-30T21:57:37.334+0000"}, {"author": "Neha Narkhede", "body": "KAFKA-343 checkin broke some unit tests and cause others to hang. I think I might have to hold off on the checkin until that is either fixed or reverted.", "created": "2012-07-31T18:19:14.821+0000"}, {"author": "Neha Narkhede", "body": "Thanks all for the review! Committed the v2 patch.", "created": "2012-07-31T23:28:30.467+0000"}], "num_comments": 8, "text": "Issue: KAFKA-384\nSummary: Fix intermittent test failures and remove unnecessary sleeps\nDescription: Seeing intermittent failures in 0.8 unit tests. Also, many sleeps can be removed (with producer acks in place) and I think MockTime isn't used in some places where it should.\n\nComments (8):\n1. Neha Narkhede: This patch removes sleep statements from the unit tests. Changes include - 1. All sleep statements except those related to a scheduler are removed. So 2-3 sleep statements that exercise the producer side queue expiration logic have not been removed 2. For the Log tests, passed in a Time parameter to the Log. Kafka server already takes in an optional Time parameter that defaults to SystemTime. However, it wasn't passed around in LogManager. Fixed it so KafkaServer passes its Time variable to LogManager which passes it to Log. This is useful in removing all sleep statements from unit tests for the log manager and logs\n2. Jun Rao: Thanks for the patch. It looks good. Just 1 comment: 1. AsyncProducerTest.testQueueTimeExpired(): This is an existing issue, but probably can be fixed in this patch too. It seems that we should do producerSendThread.shutdown after EasyMock.verify(mockHandler). Shutdown always sends all remaining messages in the buffer. If we call shutdown before verify, it's not clear if the send was triggered by timeout or shutdown.\n3. Neha Narkhede: Thanks for the review, Jun. That is a good point. I fixed that and also removed the reference to mockTime since that is not useful here.\n4. Joel Jacob Koshy: +1 for v2.\n5. Jay Kreps: You are my hero...\n6. Jun Rao: +1 on v2 too.\n7. Neha Narkhede: KAFKA-343 checkin broke some unit tests and cause others to hang. I think I might have to hold off on the checkin until that is either fixed or reverted.\n8. Neha Narkhede: Thanks all for the review! Committed the v2 patch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.625252"}}
{"id": "33f64258ea711d4de8645dbce402a399", "issue_key": "KAFKA-896", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "merge 0.8 (988d4d8e65a14390abd748318a64e281e4a37c19) to trunk", "description": "Files that have conflicts: # Unmerged paths: # (use \"git add/rm <file>...\" as appropriate to mark resolution) # # both modified: core/src/main/scala/kafka/admin/PreferredReplicaLeaderElectionCommand.scala # both modified: core/src/main/scala/kafka/cluster/Partition.scala # both modified: core/src/main/scala/kafka/log/FileMessageSet.scala # both modified: core/src/main/scala/kafka/log/Log.scala # both modified: core/src/main/scala/kafka/log/LogManager.scala # both modified: core/src/main/scala/kafka/message/ByteBufferMessageSet.scala # deleted by us: core/src/main/scala/kafka/server/HighwaterMarkCheckpoint.scala # both modified: core/src/main/scala/kafka/server/KafkaApis.scala # both modified: core/src/main/scala/kafka/server/ReplicaFetcherThread.scala # both modified: core/src/main/scala/kafka/server/ReplicaManager.scala # both modified: core/src/main/scala/kafka/tools/DumpLogSegments.scala # both modified: core/src/test/scala/unit/kafka/admin/AdminTest.scala # both modified: core/src/test/scala/unit/kafka/utils/TestUtils.scala # Files have to be changed manually due to merge. # Changed but not updated: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: core/src/main/scala/kafka/log/LogSegment.scala # modified: core/src/test/scala/unit/kafka/producer/ProducerTest.scala #", "reporter": "Jun Rao", "assignee": "Jun Rao", "created": "2013-05-03T02:14:37.000+0000", "updated": "2013-07-08T23:20:12.000+0000", "resolved": "2013-07-08T23:20:12.000+0000", "labels": [], "components": [], "comments": [{"author": "Jun Rao", "body": "Attach the merge. Most of the conflicts are easy to resolve. Non-trivial ones are in KafkaApis and Partition. Verified that all unit tests pass after the merge.", "created": "2013-05-03T02:20:50.998+0000"}, {"author": "Jun Rao", "body": "Attach patch v2. Merged the changes in ListTopicCommand.scala to TopicCommand.scala and delete ListTopicCommand.scala", "created": "2013-05-03T23:29:21.031+0000"}, {"author": "Jay Kreps", "body": "1. PreferredReplicaLeaderElectionCommand -- this needs to be moved into AdminUtils. Let's please stop adding random commands like this... 2. Partition - OffsetMap--shouldn't this be deleted? 3. Why are we changing the property broker.list to metadata.broker.list? That is a pretty unintuitive name.", "created": "2013-06-25T18:37:11.949+0000"}, {"author": "Jun Rao", "body": "Thanks for the review. Committed to trunk after addressing the comments below. 1. The command is already there. We can file new jiras to make it an api. 2. This is added since it's useful to know when a high watermark doesn't exist. 3. This is because the usage of broker list is different btw 0.7 and 0.8. In 0.7, it's used to send the produce requests. In 0.8 it's only used to send metadata requests.", "created": "2013-07-08T23:20:12.113+0000"}], "num_comments": 4, "text": "Issue: KAFKA-896\nSummary: merge 0.8 (988d4d8e65a14390abd748318a64e281e4a37c19) to trunk\nDescription: Files that have conflicts: # Unmerged paths: # (use \"git add/rm <file>...\" as appropriate to mark resolution) # # both modified: core/src/main/scala/kafka/admin/PreferredReplicaLeaderElectionCommand.scala # both modified: core/src/main/scala/kafka/cluster/Partition.scala # both modified: core/src/main/scala/kafka/log/FileMessageSet.scala # both modified: core/src/main/scala/kafka/log/Log.scala # both modified: core/src/main/scala/kafka/log/LogManager.scala # both modified: core/src/main/scala/kafka/message/ByteBufferMessageSet.scala # deleted by us: core/src/main/scala/kafka/server/HighwaterMarkCheckpoint.scala # both modified: core/src/main/scala/kafka/server/KafkaApis.scala # both modified: core/src/main/scala/kafka/server/ReplicaFetcherThread.scala # both modified: core/src/main/scala/kafka/server/ReplicaManager.scala # both modified: core/src/main/scala/kafka/tools/DumpLogSegments.scala # both modified: core/src/test/scala/unit/kafka/admin/AdminTest.scala # both modified: core/src/test/scala/unit/kafka/utils/TestUtils.scala # Files have to be changed manually due to merge. # Changed but not updated: # (use \"git add <file>...\" to update what will be committed) # (use \"git checkout -- <file>...\" to discard changes in working directory) # # modified: core/src/main/scala/kafka/log/LogSegment.scala # modified: core/src/test/scala/unit/kafka/producer/ProducerTest.scala #\n\nComments (4):\n1. Jun Rao: Attach the merge. Most of the conflicts are easy to resolve. Non-trivial ones are in KafkaApis and Partition. Verified that all unit tests pass after the merge.\n2. Jun Rao: Attach patch v2. Merged the changes in ListTopicCommand.scala to TopicCommand.scala and delete ListTopicCommand.scala\n3. Jay Kreps: 1. PreferredReplicaLeaderElectionCommand -- this needs to be moved into AdminUtils. Let's please stop adding random commands like this... 2. Partition - OffsetMap--shouldn't this be deleted? 3. Why are we changing the property broker.list to metadata.broker.list? That is a pretty unintuitive name.\n4. Jun Rao: Thanks for the review. Committed to trunk after addressing the comments below. 1. The command is already there. We can file new jiras to make it an api. 2. This is added since it's useful to know when a high watermark doesn't exist. 3. This is because the usage of broker list is different btw 0.7 and 0.8. In 0.7, it's used to send the produce requests. In 0.8 it's only used to send metadata requests.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.878538"}}
{"id": "97b2b300b5db3a11197b04aa31552f94", "issue_key": "KAFKA-953", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Remove release-zip from README we are not releasing with it", "description": "", "reporter": "Joe Stein", "assignee": null, "created": "2013-06-24T11:27:40.000+0000", "updated": "2013-09-10T16:44:13.000+0000", "resolved": "2013-09-10T16:44:13.000+0000", "labels": ["0.8.0-beta1"], "components": [], "comments": [{"author": "Jun Rao", "body": "We actually do support the release-zip target, as well as release-tar.", "created": "2013-09-10T16:44:13.187+0000"}], "num_comments": 1, "text": "Issue: KAFKA-953\nSummary: Remove release-zip from README we are not releasing with it\n\nComments (1):\n1. Jun Rao: We actually do support the release-zip target, as well as release-tar.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.894267"}}
{"id": "f97aeb3f39ba4ebc66b645cf16bb7524", "issue_key": "HADOOP-74", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "hash blocks into dfs.data.dirs", "description": "When dfs.data.dir has multiple values, we currently start a DataNode for each (all in the same JVM). Instead we should run a single DataNode that stores block files into the different directories. This will reduce the number of connections to the namenode. We cannot hash because different devices might be different amounts full. So the datanode will need to keep a table mapping from block id to file location, and add new blocks to less full devices.", "reporter": "Doug Cutting", "assignee": "Konstantin Shvachko", "created": "2006-03-11T06:33:10.000+0000", "updated": "2009-07-08T16:41:49.000+0000", "resolved": "2006-03-25T06:18:20.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: HADOOP-74\nSummary: hash blocks into dfs.data.dirs\nDescription: When dfs.data.dir has multiple values, we currently start a DataNode for each (all in the same JVM). Instead we should run a single DataNode that stores block files into the different directories. This will reduce the number of connections to the namenode. We cannot hash because different devices might be different amounts full. So the datanode will need to keep a table mapping from block id to file location, and add new blocks to less full devices.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.207370"}}
{"id": "fc37dec3a08b247605e392e4fe4748d8", "issue_key": "HADOOP-501", "issue_type": "Bug", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "toString(resources, sb) in Configuration.java throws a ClassCastException since resources can be loaded from an URL", "description": "Object obj = i.next(); if (obj instanceof Path) { sb.append((Path)obj); } else { sb.append((String)obj); } If obj is an URL -> ClassCastException. Moreover, I think the test before appending the resource to the StringBuffer is not really necessary since the method take an Object as argument. Why not simply have : Object obj = i.next(); sb.append(obj); I have attached a patch to fix this.", "reporter": "Thomas Friol", "assignee": null, "created": "2006-09-01T15:02:12.000+0000", "updated": "2006-09-08T21:20:03.000+0000", "resolved": "2006-09-05T19:09:40.000+0000", "labels": [], "components": [], "comments": [{"author": "Doug Cutting", "body": "I just committed this. The patch file was not properly generated (with 'svn diff' from the project root) but the change was simple enough to recreate. Thanks, Thomas!", "created": "2006-09-05T19:09:24.000+0000"}], "num_comments": 1, "text": "Issue: HADOOP-501\nSummary: toString(resources, sb) in Configuration.java throws a ClassCastException since resources can be loaded from an URL\nDescription: Object obj = i.next(); if (obj instanceof Path) { sb.append((Path)obj); } else { sb.append((String)obj); } If obj is an URL -> ClassCastException. Moreover, I think the test before appending the resource to the StringBuffer is not really necessary since the method take an Object as argument. Why not simply have : Object obj = i.next(); sb.append(obj); I have attached a patch to fix this.\n\nComments (1):\n1. Doug Cutting: I just committed this. The patch file was not properly generated (with 'svn diff' from the project root) but the change was simple enough to recreate. Thanks, Thomas!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.377384"}}
{"id": "c0235227a56e2f86bd25dd1a527626e6", "issue_key": "KAFKA-109", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "CompressionUtils introduces a GZIP header while compressing empty message sets", "description": "The CompressionUtils helper class takes in a sequence of messages and compresses those, using the appropriate codec. But even if it receives an empty sequence, it still ends up adding a GZIP compression header to the data, efffectively \"adding\" data to the resulting ByteBuffer. This doesn't match with the behavior for uncompressed empty message sets. CompressionUtils should be fixed by removing this side-effect.", "reporter": "Neha Narkhede", "assignee": null, "created": "2011-08-18T09:10:53.000+0000", "updated": "2011-09-13T01:27:39.000+0000", "resolved": "2011-09-13T01:27:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Neha Narkhede", "body": "This patch handles the behavior of ByteBufferMessageSet for compression of empty list of messages. This modifies the ByteBufferMessageSet to create an empty byte buffer, in this case, instead of attaching a GZIP header to it. There are a couple of reasons to do this - 1. To maintain consistent behavior between an empty uncompressed message set and an empty compressed message set 2. To avoid attaching extraneous header information to non-existing data, effectively occupying space on disk", "created": "2011-08-18T10:34:48.140+0000"}, {"author": "Jun Rao", "body": "+1. The patch looks good.", "created": "2011-08-18T16:03:03.608+0000"}, {"author": "Jun Rao", "body": "Actually, this doesn't cover javaapi.ByteBufferMessageSet. It seems that javaapi.ByteBufferMessageSet duplicates some of the constructor code in ByteBufferMessageSet. We should avoid doing that.", "created": "2011-08-18T17:03:09.669+0000"}, {"author": "Neha Narkhede", "body": "This is a revised patch that refactors the constructors of both java and scala ByteBufferMessageSet into a common API in MessageSet. This ensures that the bug fix exists both in the Java API as well as the Scala API", "created": "2011-08-18T19:55:36.798+0000"}, {"author": "Jun Rao", "body": "+1", "created": "2011-08-18T21:43:47.652+0000"}], "num_comments": 5, "text": "Issue: KAFKA-109\nSummary: CompressionUtils introduces a GZIP header while compressing empty message sets\nDescription: The CompressionUtils helper class takes in a sequence of messages and compresses those, using the appropriate codec. But even if it receives an empty sequence, it still ends up adding a GZIP compression header to the data, efffectively \"adding\" data to the resulting ByteBuffer. This doesn't match with the behavior for uncompressed empty message sets. CompressionUtils should be fixed by removing this side-effect.\n\nComments (5):\n1. Neha Narkhede: This patch handles the behavior of ByteBufferMessageSet for compression of empty list of messages. This modifies the ByteBufferMessageSet to create an empty byte buffer, in this case, instead of attaching a GZIP header to it. There are a couple of reasons to do this - 1. To maintain consistent behavior between an empty uncompressed message set and an empty compressed message set 2. To avoid attaching extraneous header information to non-existing data, effectively occupying space on disk\n2. Jun Rao: +1. The patch looks good.\n3. Jun Rao: Actually, this doesn't cover javaapi.ByteBufferMessageSet. It seems that javaapi.ByteBufferMessageSet duplicates some of the constructor code in ByteBufferMessageSet. We should avoid doing that.\n4. Neha Narkhede: This is a revised patch that refactors the constructors of both java and scala ByteBufferMessageSet into a common API in MessageSet. This ensures that the bug fix exists both in the Java API as well as the Scala API\n5. Jun Rao: +1", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.518119"}}
{"id": "c3ca1a54bdce78708006d0e00ac56875", "issue_key": "HADOOP-317", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "\"connection was forcibly closed\" Exception in RPC on Windows", "description": "I see a lot of exceptions caused by RPC in the nightly build on Windows. The most often is thrown by RPC on the namenode, saying 06/06/21 19:28:36 INFO ipc.Server: Server listener on port 7017: readAndProcess threw exception java.io.IOException: An existing connection was forcibly closed by the remote host. Count of bytes read: 0 java.io.IOException: An existing connection was forcibly closed by the remote host at sun.nio.ch.SocketDispatcher.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233) at sun.nio.ch.IOUtil.read(IOUtil.java:200) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:207) at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:374) at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:289) at org.apache.hadoop.ipc.Server$Listener.run(Server.java:210) I am not sure how serious that is, since the tests do not fail, and the name node does not crash. Besides the RPC we should probably also check that the unit tests actually fail if they need to.", "reporter": "Konstantin Shvachko", "assignee": "Doug Cutting", "created": "2006-06-23T01:51:36.000+0000", "updated": "2006-08-03T17:46:48.000+0000", "resolved": "2006-06-23T04:06:42.000+0000", "labels": [], "components": ["ipc"], "comments": [{"author": "Doug Cutting", "body": "I just committed a fix to this.", "created": "2006-06-23T04:06:42.000+0000"}], "num_comments": 1, "text": "Issue: HADOOP-317\nSummary: \"connection was forcibly closed\" Exception in RPC on Windows\nDescription: I see a lot of exceptions caused by RPC in the nightly build on Windows. The most often is thrown by RPC on the namenode, saying 06/06/21 19:28:36 INFO ipc.Server: Server listener on port 7017: readAndProcess threw exception java.io.IOException: An existing connection was forcibly closed by the remote host. Count of bytes read: 0 java.io.IOException: An existing connection was forcibly closed by the remote host at sun.nio.ch.SocketDispatcher.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233) at sun.nio.ch.IOUtil.read(IOUtil.java:200) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:207) at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:374) at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:289) at org.apache.hadoop.ipc.Server$Listener.run(Server.java:210) I am not sure how serious that is, since the tests do not fail, and the name node does not crash. Besides the RPC we should probably also check that the unit tests actually fail if they need to.\n\nComments (1):\n1. Doug Cutting: I just committed a fix to this.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.297657"}}
{"id": "7c297acba2a7a291501fdc4ba97eb3ad", "issue_key": "KAFKA-815", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Improve SimpleConsumerShell to take in a max messages config option", "description": "It's useful to have a max-messages option on the SimpleConsumerShell similar to other tools.", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "created": "2013-03-19T21:43:36.000+0000", "updated": "2015-04-08T01:44:24.000+0000", "resolved": "2015-04-08T01:44:24.000+0000", "labels": [], "components": ["tools"], "comments": [{"author": "Neha Narkhede", "body": "- Added a max-messages option to SimpleConsumerShell - Improved some logging - Very often, I've wanted to use a no op message formatter instead of having the messages go to standard out on every invocation. So added that.", "created": "2013-03-19T22:05:35.557+0000"}, {"author": "Jun Rao", "body": "Thanks for the patch. +1.", "created": "2013-03-19T23:49:28.764+0000"}, {"author": "Gwen Shapira", "body": "was committed", "created": "2015-04-08T01:44:24.827+0000"}], "num_comments": 3, "text": "Issue: KAFKA-815\nSummary: Improve SimpleConsumerShell to take in a max messages config option\nDescription: It's useful to have a max-messages option on the SimpleConsumerShell similar to other tools.\n\nComments (3):\n1. Neha Narkhede: - Added a max-messages option to SimpleConsumerShell - Improved some logging - Very often, I've wanted to use a no op message formatter instead of having the messages go to standard out on every invocation. So added that.\n2. Jun Rao: Thanks for the patch. +1.\n3. Gwen Shapira: was committed", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.846733"}}
{"id": "7086221fc1868e18c11cae7ec915fcd6", "issue_key": "HADOOP-1039", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Reduce the time taken by TestCheckpoint", "description": "TestCheckpoint starts and kills MiniDFSCluster about 7 times. Each restart of the MiniDFScluster incurs about 15 seconds of wait time. This increases the total time needed to run TestCheckpoint.", "reporter": "Dhruba Borthakur", "assignee": "Dhruba Borthakur", "created": "2007-02-25T02:06:54.000+0000", "updated": "2009-07-08T16:42:18.000+0000", "resolved": "2007-02-26T19:56:36.000+0000", "labels": [], "components": [], "comments": [{"author": "Dhruba Borthakur", "body": "This patch reduces the time taken to restart MiniDFSCluster. A new method waitActive() blocks the calling thread till all the datanodes have checked in with the namenode.", "created": "2007-02-25T02:10:16.888+0000"}, {"author": "Dhruba Borthakur", "body": "Review comments from Milind: Dhruba, Looks good. There are a few whitespace-only changes. Plus, the growing number of constructors for MiniDFSCluster concerns me. Maybe the right thing to do is to have a single constructor, and change other tests to specify defaults. - Milind I am submitting the patch because I would like to attack the case of having multiple constructors for MiniDFSCluster as a seperate issue.", "created": "2007-02-25T02:18:46.549+0000"}, {"author": "Hadoop QA", "body": "+1, because http://issues.apache.org/jira/secure/attachment/12351980/FastTestCheckpoint2.patch applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/511100. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch", "created": "2007-02-25T02:56:49.537+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Dhruba!", "created": "2007-02-26T19:56:36.148+0000"}], "num_comments": 4, "text": "Issue: HADOOP-1039\nSummary: Reduce the time taken by TestCheckpoint\nDescription: TestCheckpoint starts and kills MiniDFSCluster about 7 times. Each restart of the MiniDFScluster incurs about 15 seconds of wait time. This increases the total time needed to run TestCheckpoint.\n\nComments (4):\n1. Dhruba Borthakur: This patch reduces the time taken to restart MiniDFSCluster. A new method waitActive() blocks the calling thread till all the datanodes have checked in with the namenode.\n2. Dhruba Borthakur: Review comments from Milind: Dhruba, Looks good. There are a few whitespace-only changes. Plus, the growing number of constructors for MiniDFSCluster concerns me. Maybe the right thing to do is to have a single constructor, and change other tests to specify defaults. - Milind I am submitting the patch because I would like to attack the case of having multiple constructors for MiniDFSCluster as a seperate issue.\n3. Hadoop QA: +1, because http://issues.apache.org/jira/secure/attachment/12351980/FastTestCheckpoint2.patch applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/511100. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch\n4. Doug Cutting: I just committed this. Thanks, Dhruba!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.549701"}}
{"id": "17b9229c1e36b6387e7d4f3e98596660", "issue_key": "HADOOP-724", "issue_type": "Bug", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "bin/hadoop:111 uses java directly, it should use JAVA_HOME", "description": "JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} java org.apache.hadoop.util.PlatformName` should use JAVA_HOME instead of java.", "reporter": "Sanjay Dahiya", "assignee": "Arun Murthy", "created": "2006-11-15T18:56:10.000+0000", "updated": "2011-09-30T20:10:36.000+0000", "resolved": "2006-11-20T23:58:05.000+0000", "labels": [], "components": ["scripts"], "comments": [{"author": "Arun Murthy", "body": "Attached patch.", "created": "2006-11-15T19:35:14.000+0000"}, {"author": "Doug Cutting", "body": "Should we avoid starting a JVM at all if neither lib/native nor build/native exist?", "created": "2006-11-15T20:22:53.000+0000"}, {"author": "Arun Murthy", "body": "Hmm... we could do that; however it wouldn't be useful if we are going to check-in prebuilt Linux libraries into lib/native/Linux-i386-32 ... What are your current thoughts on checking-in prebuilt linux libs Doug?", "created": "2006-11-16T07:01:50.000+0000"}, {"author": "Doug Cutting", "body": "> What are your current thoughts on checking-in prebuilt linux libs Doug? I'm not sure. Not comitting them means they'll never be out of date. On the other hand, it means that folks who wish to use them must install a native build environment (gcc, etc.). The proper way to do this is to package different downloads for different platforms, a 32-bit linux tar file, a Windows tar file, etc. Perhaps the primary release could be platform independent, then we could also release platform-specific native libraries as a separate download? What do others think? In summary, I'm reluctant to commit them until we have a bit more experience. Given that, we might as well avoid launching another JVM per command.", "created": "2006-11-16T18:25:49.000+0000"}, {"author": "Arun Murthy", "body": "Sounds good... I think it makes sense to distribute the Linux-i386-32 libraries as a separate download (atleast in the short/medium term). Here is an updated patch incorporating your suggestion...", "created": "2006-11-17T04:21:06.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Arun!", "created": "2006-11-20T23:58:05.000+0000"}, {"author": "Hudson", "body": "Integrated in Hadoop-Hdfs-trunk-Commit #1065 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1065/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java", "created": "2011-09-29T22:43:11.797+0000"}, {"author": "Hudson", "body": "Integrated in Hadoop-Common-trunk-Commit #987 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/987/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java", "created": "2011-09-29T22:45:18.787+0000"}, {"author": "Hudson", "body": "Integrated in Hadoop-Mapreduce-trunk-Commit #1009 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1009/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java", "created": "2011-09-29T23:14:39.946+0000"}, {"author": "Hudson", "body": "Integrated in Hadoop-Hdfs-trunk #816 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/816/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java", "created": "2011-09-30T12:46:00.485+0000"}, {"author": "Hudson", "body": "Integrated in Hadoop-Mapreduce-trunk #846 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/846/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java", "created": "2011-09-30T20:10:36.288+0000"}], "num_comments": 11, "text": "Issue: HADOOP-724\nSummary: bin/hadoop:111 uses java directly, it should use JAVA_HOME\nDescription: JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} java org.apache.hadoop.util.PlatformName` should use JAVA_HOME instead of java.\n\nComments (11):\n1. Arun Murthy: Attached patch.\n2. Doug Cutting: Should we avoid starting a JVM at all if neither lib/native nor build/native exist?\n3. Arun Murthy: Hmm... we could do that; however it wouldn't be useful if we are going to check-in prebuilt Linux libraries into lib/native/Linux-i386-32 ... What are your current thoughts on checking-in prebuilt linux libs Doug?\n4. Doug Cutting: > What are your current thoughts on checking-in prebuilt linux libs Doug? I'm not sure. Not comitting them means they'll never be out of date. On the other hand, it means that folks who wish to use them must install a native build environment (gcc, etc.). The proper way to do this is to package different downloads for different platforms, a 32-bit linux tar file, a Windows tar file, etc. Perhaps the primary release could be platform independent, then we could also release platform-specific native libraries as a separate download? What do others think? In summary, I'm reluctant to commit them until we have a bit more experience. Given that, we might as well avoid launching another JVM per command.\n5. Arun Murthy: Sounds good... I think it makes sense to distribute the Linux-i386-32 libraries as a separate download (atleast in the short/medium term). Here is an updated patch incorporating your suggestion...\n6. Doug Cutting: I just committed this. Thanks, Arun!\n7. Hudson: Integrated in Hadoop-Hdfs-trunk-Commit #1065 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/1065/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java\n8. Hudson: Integrated in Hadoop-Common-trunk-Commit #987 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/987/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java\n9. Hudson: Integrated in Hadoop-Mapreduce-trunk-Commit #1009 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1009/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java\n10. Hudson: Integrated in Hadoop-Hdfs-trunk #816 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/816/]) HADOOP-7693. Enhance AvroRpcEngine to support the new #addProtocol introduced in HADOOP-724. cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177399 Files : * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AvroRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestAvroRpc.java", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.450902"}}
{"id": "63b70b5df19d6ab2cd3bde6b3b6b5b83", "issue_key": "HADOOP-968", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Reduce shuffle and merge should be done a child JVM", "description": "The Reduce's shuffle and initial merge is done in the TaskTracker's JVM. It would be better to have it run in the Task's child JVM. The advantages are: 1. The class path and environment would be set up correctly. 2. User code doesn't need to be loaded into the TaskTracker. 3. Lower memory usage and contention in the TaskTracker.", "reporter": "Owen O'Malley", "assignee": "Devaraj Das", "created": "2007-02-01T18:09:45.000+0000", "updated": "2009-07-08T16:52:08.000+0000", "resolved": "2007-04-17T19:57:46.000+0000", "labels": [], "components": [], "comments": [{"author": "Devaraj Das", "body": "The salient points of the design: On the TaskTracker 1) The TaskTracker maintains the list of TaskCompletionEvents for a *job*. Whenever a ReduceTask is assigned to a TaskTracker it extracts the JobId out of that. 2) For that jobid it starts fetching MapTask completion events as long as any ReduceTask for that job is in the SHUFFLE phase (this ensures that the TaskTracker sees all MapTask lost events and keeps an updated cache of all events). When all the ReduceTasks for a given job have gone past the SHUFFLE phase, the TaskTracker does not fetch any more MapTask completion events until another ReduceTask gets assigned to it. If no other ReduceTask from the same job gets assigned to it, and the job completes, it clears the cache of TaskCompletionEvents. 3) The event-fetcher thread blocks on runningJobs object. Whenever the method addTaskToJob in TaskTracker adds a new Task to a job, it invokes runningJobs.notify(), so that the event-fetcher thread can unblock and continue. 4) The event-fetcher thread also goes through the runningJobs and immediately stops fetching events for those jobs that have been killed/failed. On the TaskUmbilicalProtocol, ReduceTaskRunner & ReduceTask: 1) A new method - TaskCompletionEvent[] getSuccessMapCompleteEvents(String taskId, int fromIndex, int maxLocs) throws IOException; - has been added for enabling the ReduceTask to fetch TaskCompletionEvents cached at the TaskTracker. The semantics of this method are mirrored to the one in InterTrackerProtocol - getTaskCompletionEvents, except that in the umbilical protocol, we are interested in just the successful map events. Fetch failures are handled in the same way as is done today. Thus, most of the fetcher code in ReduceTaskRunner remains the same (the code now is part of ReduceTask in a new class called ReduceCopier, and the ReduceTaskRunner very closely matches to MapTaskRunner in terms of functionality/code). Comments?", "created": "2007-04-03T18:14:08.779+0000"}, {"author": "Devaraj Das", "body": "Attached patch for review. The unit tests run fine with the patch and I am in the process of running the sort benchmark.", "created": "2007-04-05T16:37:25.277+0000"}, {"author": "Devaraj Das", "body": "A well-tested patch.", "created": "2007-04-06T17:55:49.248+0000"}, {"author": "Devaraj Das", "body": "This is w.r.t the current (HADOOP-1218 made it go out of sync with the trunk), plus, it has some changes in TaskTracker.java.", "created": "2007-04-10T06:05:08.452+0000"}, {"author": "Devaraj Das", "body": "This has the potential to quickly go stale since the patch touches many files in major ways. So would appreciate a quick review/commit on this one. Thanks.", "created": "2007-04-13T06:43:31.035+0000"}, {"author": "Hadoop QA", "body": "+1 http://issues.apache.org/jira/secure/attachment/12355215/968.apr10.patch applied and successfully tested against trunk revision r528230. Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/42/testReport/ Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/42/console", "created": "2007-04-13T07:10:20.349+0000"}, {"author": "Owen O'Malley", "body": "1. I notice that a lot of your iterators are not typed causing you to do casts of itr.next(). 2. In many cases, the loop \"for(Item item: itemSet){..}\" is easier to read and more concise. 3. Maps should not be iterated through using: for(Map.Entry<Key,Value> item: myMap) {...} rather than: Iterator itr = myMap.keySet().iterator(); while (itr.hasNext()) { Value value = myMap.get(itr.next()); ... } 4. It looks like each reduce from a job will cause its job's FetchState to be added to the list a multiple time, so it will fetch multiple times per a loop. 5. I'd remove the sleep from queryJobTracker and move it to the MapEventsFetcherThread's run loop. 6. The doFetch is badly named, since it doesn't actually do the fetch. It should be called findReduces or something. 7. The name of the parameter of the first parameter in TaskUmbilicalProtocol.getMapCompletionEvents is \"taskid\", but if fact it is a job id. 8. The MapEventsFetcherThread's name doesn't need to include the task in the normal case, but I guess for unit tests it might be useful. 9. I assume that the shuffle code in ReduceTask matches the old code in ReduceTaskRunner. *smile*", "created": "2007-04-13T21:16:07.349+0000"}, {"author": "Owen O'Malley", "body": "Point 3 should be \"Maps SHOULD BE iterated through using\". Sorry for any confusion.", "created": "2007-04-13T21:30:03.154+0000"}, {"author": "Devaraj Das", "body": "Thanks for the review, Owen. Some comments below. > 1. I notice that a lot of your iterators are not typed causing you to do casts of itr.next > 2. In many cases, the loop \"for(Item item: itemSet){..}\" is easier to read and more > concise. > 3. Maps should not be iterated through using: > for(Map.Entry<Key,Value> item: myMap) {...} Done (old habits die hard *smile*). > 4. It looks like each reduce from a job will cause its job's FetchState to be added to > the list a multiple time, so it will fetch multiple times per a loop. No change. There is already a \"break\" statement in the loop as soon as one FetchState gets added. > 5. I'd remove the sleep from queryJobTracker and move it to the > MapEventsFetcherThread's run loop. Done > 6. The doFetch is badly named, since it doesn't actually do the fetch. It should be > called findReduces or something. Changed that to reducesInShuffle > 7. The name of the parameter of the first parameter in > TaskUmbilicalProtocol.getMapCompletionEvents is \"taskid\", but if fact it is a job id. Made the name change in TaskUmbilicalProtocol.java > 8. The MapEventsFetcherThread's name doesn't need to include the task in the > normal case, but I guess for unit tests it might be useful. No change > 9. I assume that the shuffle code in ReduceTask matches the old code in > ReduceTaskRunner. *smile* *Smile* yes the only change that has been introduced to take care of variable initializations (for example, the variable reduceTask's initialization is different).", "created": "2007-04-14T13:04:37.893+0000"}, {"author": "Devaraj Das", "body": "This removes a redundant call to System.currentTimeMillis in the TaskTracker.", "created": "2007-04-14T18:11:42.342+0000"}, {"author": "Hadoop QA", "body": "-1, build or testing failed 2 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12355549/968.apr14.patch against trunk revision r528230. Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/50/testReport/ Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/50/console Please note that this message is automatically generated and may represent a problem with the automation system and not the patch.", "created": "2007-04-15T06:27:18.776+0000"}, {"author": "Doug Cutting", "body": "Here's a re-indented version of this patch.", "created": "2007-04-16T22:47:30.342+0000"}, {"author": "Devaraj Das", "body": "This patch addresses an issue to do with metrics reporting wherein stopMonitoring was not called for the ReduceTask. This potentially could lead to hung ReduceTasks after they are finished since the task JVM might not be able to exit until the (non-daemon) monitoring thread goes away.", "created": "2007-04-17T06:29:32.653+0000"}, {"author": "Nigel Daley", "body": "+1. The latest patch passes my nightly suit of benchmarks. I have resubmitted it to the patch process.", "created": "2007-04-17T17:45:27.252+0000"}, {"author": "David Bowen", "body": "Devaraj, please can you clarify what you fixed with regard to the lack of calls to stopMonitoring? I think you're right that it is a problem, but I would expect the fix to be to change the Timer constructor in o.a.h.metrics.spi.AbstractMetricsContext. Instead of the zero-arg constructor we should use the two arg constructor that takes a thread name and a boolean isDaemon. I didn't see this change in the patch.", "created": "2007-04-17T18:16:48.526+0000"}, {"author": "Devaraj Das", "body": "I am pasting the relevant block of code from the latest patch (methodname :TaskTracker.java::Child::main()). Basically, I invoke the close() method of metricsContext for the context \"mapred\" just before the point where the child task closes the log manager (LogManager.shutdown()) and it is about to die. I thought this was the best way to stop the monitoring and let the task exit nicely without having to touch other parts of the metrics code. } finally { + MetricsContext metricsContext = MetricsUtil.getContext(\"mapred\"); + metricsContext.close(); // Shutting down log4j of the child-vm... // This assumes that on return from Task.run() // there is no more logging done. Makes sense?", "created": "2007-04-17T18:31:22.306+0000"}, {"author": "Doug Cutting", "body": "I think David's point was that it would be best to fix the metrics code so that its thread *is* a daemon thread, rather than to expect client code to explicitly stop that thread.", "created": "2007-04-17T18:55:09.665+0000"}, {"author": "Devaraj Das", "body": "Ok, changed the Timer constructor to the two-argument one. Also, retained the metricsContext.close() call that I had in the last patch. In general, I think that it is a good idea to call close() on an object if the close() method is clearly documented. Does this seem all right?", "created": "2007-04-17T19:23:58.220+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Devaraj!", "created": "2007-04-17T19:57:46.961+0000"}, {"author": "Hadoop QA", "body": "-1, could not apply patch. The patch command could not apply the latest attachment http://issues.apache.org/jira/secure/attachment/12355713/968-with-metrics-fix.new.patch as a patch to trunk revision r529763. Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/58/console Please note that this message is automatically generated and may represent a problem with the automation system and not the patch.", "created": "2007-04-17T21:44:18.554+0000"}, {"author": "Hadoop QA", "body": "Integrated in Hadoop-Nightly #61 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/61/)", "created": "2007-04-18T11:04:30.159+0000"}], "num_comments": 21, "text": "Issue: HADOOP-968\nSummary: Reduce shuffle and merge should be done a child JVM\nDescription: The Reduce's shuffle and initial merge is done in the TaskTracker's JVM. It would be better to have it run in the Task's child JVM. The advantages are: 1. The class path and environment would be set up correctly. 2. User code doesn't need to be loaded into the TaskTracker. 3. Lower memory usage and contention in the TaskTracker.\n\nComments (21):\n1. Devaraj Das: The salient points of the design: On the TaskTracker 1) The TaskTracker maintains the list of TaskCompletionEvents for a *job*. Whenever a ReduceTask is assigned to a TaskTracker it extracts the JobId out of that. 2) For that jobid it starts fetching MapTask completion events as long as any ReduceTask for that job is in the SHUFFLE phase (this ensures that the TaskTracker sees all MapTask lost events and keeps an updated cache of all events). When all the ReduceTasks for a given job have gone past the SHUFFLE phase, the TaskTracker does not fetch any more MapTask completion events until another ReduceTask gets assigned to it. If no other ReduceTask from the same job gets assigned to it, and the job completes, it clears the cache of TaskCompletionEvents. 3) The event-fetcher thread blocks on runningJobs object. Whenever the method addTaskToJob in TaskTracker adds a new Task to a job, it invokes runningJobs.notify(), so that the event-fetcher thread can unblock and continue. 4) The event-fetcher thread also goes through the runningJobs and immediately stops fetching events for those jobs that have been killed/failed. On the TaskUmbilicalProtocol, ReduceTaskRunner & ReduceTask: 1) A new method - TaskCompletionEvent[] getSuccessMapCompleteEvents(String taskId, int fromIndex, int maxLocs) throws IOException; - has been added for enabling the ReduceTask to fetch TaskCompletionEvents cached at the TaskTracker. The semantics of this method are mirrored to the one in InterTrackerProtocol - getTaskCompletionEvents, except that in the umbilical protocol, we are interested in just the successful map events. Fetch failures are handled in the same way as is done today. Thus, most of the fetcher code in ReduceTaskRunner remains the same (the code now is part of ReduceTask in a new class called ReduceCopier, and the ReduceTaskRunner very closely matches to MapTaskRunner in terms of functionality/code). Comments?\n2. Devaraj Das: Attached patch for review. The unit tests run fine with the patch and I am in the process of running the sort benchmark.\n3. Devaraj Das: A well-tested patch.\n4. Devaraj Das: This is w.r.t the current (HADOOP-1218 made it go out of sync with the trunk), plus, it has some changes in TaskTracker.java.\n5. Devaraj Das: This has the potential to quickly go stale since the patch touches many files in major ways. So would appreciate a quick review/commit on this one. Thanks.\n6. Hadoop QA: +1 http://issues.apache.org/jira/secure/attachment/12355215/968.apr10.patch applied and successfully tested against trunk revision r528230. Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/42/testReport/ Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/42/console\n7. Owen O'Malley: 1. I notice that a lot of your iterators are not typed causing you to do casts of itr.next(). 2. In many cases, the loop \"for(Item item: itemSet){..}\" is easier to read and more concise. 3. Maps should not be iterated through using: for(Map.Entry<Key,Value> item: myMap) {...} rather than: Iterator itr = myMap.keySet().iterator(); while (itr.hasNext()) { Value value = myMap.get(itr.next()); ... } 4. It looks like each reduce from a job will cause its job's FetchState to be added to the list a multiple time, so it will fetch multiple times per a loop. 5. I'd remove the sleep from queryJobTracker and move it to the MapEventsFetcherThread's run loop. 6. The doFetch is badly named, since it doesn't actually do the fetch. It should be called findReduces or something. 7. The name of the parameter of the first parameter in TaskUmbilicalProtocol.getMapCompletionEvents is \"taskid\", but if fact it is a job id. 8. The MapEventsFetcherThread's name doesn't need to include the task in the normal case, but I guess for unit tests it might be useful. 9. I assume that the shuffle code in ReduceTask matches the old code in ReduceTaskRunner. *smile*\n8. Owen O'Malley: Point 3 should be \"Maps SHOULD BE iterated through using\". Sorry for any confusion.\n9. Devaraj Das: Thanks for the review, Owen. Some comments below. > 1. I notice that a lot of your iterators are not typed causing you to do casts of itr.next > 2. In many cases, the loop \"for(Item item: itemSet){..}\" is easier to read and more > concise. > 3. Maps should not be iterated through using: > for(Map.Entry<Key,Value> item: myMap) {...} Done (old habits die hard *smile*). > 4. It looks like each reduce from a job will cause its job's FetchState to be added to > the list a multiple time, so it will fetch multiple times per a loop. No change. There is already a \"break\" statement in the loop as soon as one FetchState gets added. > 5. I'd remove the sleep from queryJobTracker and move it to the > MapEventsFetcherThread's run loop. Done > 6. The doFetch is badly named, since it doesn't actually do the fetch. It should be > called findReduces or something. Changed that to reducesInShuffle > 7. The name of the parameter of the first parameter in > TaskUmbilicalProtocol.getMapCompletionEvents is \"taskid\", but if fact it is a job id. Made the name change in TaskUmbilicalProtocol.java > 8. The MapEventsFetcherThread's name doesn't need to include the task in the > normal case, but I guess for unit tests it might be useful. No change > 9. I assume that the shuffle code in ReduceTask matches the old code in > ReduceTaskRunner. *smile* *Smile* yes the only change that has been introduced to take care of variable initializations (for example, the variable reduceTask's initialization is different).\n10. Devaraj Das: This removes a redundant call to System.currentTimeMillis in the TaskTracker.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.529901"}}
{"id": "58dee34a403584ec1dea1c50b9bae125", "issue_key": "HADOOP-247", "issue_type": "Bug", "status": "Closed", "priority": "Critical", "resolution": null, "summary": "The Reduce Task thread for reporting progress during the sort exits in case of any IOException", "description": "The Reduce task thread for reporting progress during the sort, exits in case of any exception (except InterruptedException). The solution would be to continue the thread in case of an exception.", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "created": "2006-05-24T03:52:00.000+0000", "updated": "2009-07-08T16:51:48.000+0000", "resolved": "2006-05-24T06:26:04.000+0000", "labels": [], "components": [], "comments": [{"author": "Mahadev Konar", "body": "This patch makes the thread exit on an InterruptedException and continues on a Throwable().", "created": "2006-05-24T03:54:12.000+0000"}, {"author": "Mahadev Konar", "body": "Modified the patch so it follows 80 character lines.", "created": "2006-05-24T04:25:30.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Mahadev!", "created": "2006-05-24T06:26:04.000+0000"}], "num_comments": 3, "text": "Issue: HADOOP-247\nSummary: The Reduce Task thread for reporting progress during the sort exits in case of any IOException\nDescription: The Reduce task thread for reporting progress during the sort, exits in case of any exception (except InterruptedException). The solution would be to continue the thread in case of an exception.\n\nComments (3):\n1. Mahadev Konar: This patch makes the thread exit on an InterruptedException and continues on a Throwable().\n2. Mahadev Konar: Modified the patch so it follows 80 character lines.\n3. Doug Cutting: I just committed this. Thanks, Mahadev!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.255973"}}
{"id": "e88e53226d0d7af1a6b8c886fdc81278", "issue_key": "SPARK-999", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Report More Instrumentation for Task Execution Time in UI", "description": "We should report finer-grained information about task execution time inside of the Spark UI. Here is a proposal of exactly what we should report: Task execution goes through a few stages on the Executor. 1. Deserializing the task 2. Executing the task. This pipelines a few things: --> Reading shuffle input --> Running whatever function on the RDD --> Writing shuffle output 3. Serializing the result I'd propose we should report the following five timing metrics. Man of these are already tracked in TaskMetrics. - Time spent deserializing the task on the executor (executorDeserializeTime) - Total execution time for the task (executorRunTime) -- Time spent blocking on shuffle reads during the task (fetchWaitTime) -- Time spent blocking on shuffle writes during the task (shuffleWriteTime) - Time spent serializing the result (not currently tracked) Reporting all of these in the Stage UI table would be great. Bonus points if you can find some better way to visualize them. Note that the time spent serializing the result is currently not tracked. We should figure out if we can do this in a simple way - it seems like you could modify TaskResult to contain an already serialized buffer instead of the result itself. Then you could first serialize that result, update the TaskMetrics and then serialize them (we wouldn't track the time to serialize the metrics themselves). If this is too much performance overhead we could also write a custom serializer for the broader result struct (containing the accumulators, metrics, and result). One other missing thing here is the ability to track various metrics if the task is reading or writing from HDFS or doing some other expensive thing within it's own execution. It would be nice to add support for counters and such in there, but we can keep that outside of the scope of this JIRA.", "reporter": "Patrick Wendell", "assignee": null, "created": "2013-12-12T12:07:53.000+0000", "updated": "2014-03-30T04:15:06.000+0000", "resolved": "2014-01-05T23:17:02.000+0000", "labels": [], "components": [], "comments": [{"author": "xiajunluan", "body": "Hi Patrick I would like to finish this improvement, could you assign this one to me?", "created": "2013-12-13T21:28:28.265+0000"}, {"author": "Patrick McFadin", "body": "Hey - someone else actually wanted to do this and asked me to write up an outline so they could get started. Let me double check with them though, if they aren't still planning to do this I can assign it to you.", "created": "2013-12-13T21:43:42.362+0000"}], "num_comments": 2, "text": "Issue: SPARK-999\nSummary: Report More Instrumentation for Task Execution Time in UI\nDescription: We should report finer-grained information about task execution time inside of the Spark UI. Here is a proposal of exactly what we should report: Task execution goes through a few stages on the Executor. 1. Deserializing the task 2. Executing the task. This pipelines a few things: --> Reading shuffle input --> Running whatever function on the RDD --> Writing shuffle output 3. Serializing the result I'd propose we should report the following five timing metrics. Man of these are already tracked in TaskMetrics. - Time spent deserializing the task on the executor (executorDeserializeTime) - Total execution time for the task (executorRunTime) -- Time spent blocking on shuffle reads during the task (fetchWaitTime) -- Time spent blocking on shuffle writes during the task (shuffleWriteTime) - Time spent serializing the result (not currently tracked) Reporting all of these in the Stage UI table would be great. Bonus points if you can find some better way to visualize them. Note that the time spent serializing the result is currently not tracked. We should figure out if we can do this in a simple way - it seems like you could modify TaskResult to contain an already serialized buffer instead of the result itself. Then you could first serialize that result, update the TaskMetrics and then serialize them (we wouldn't track the time to serialize the metrics themselves). If this is too much performance overhead we could also write a custom serializer for the broader result struct (containing the accumulators, metrics, and result). One other missing thing here is the ability to track various metrics if the task is reading or writing from HDFS or doing some other expensive thing within it's own execution. It would be nice to add support for counters and such in there, but we can keep that outside of the scope of this JIRA.\n\nComments (2):\n1. xiajunluan: Hi Patrick I would like to finish this improvement, could you assign this one to me?\n2. Patrick McFadin: Hey - someone else actually wanted to do this and asked me to write up an outline so they could get started. Let me double check with them though, if they aren't still planning to do this I can assign it to you.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.104052"}}
{"id": "f099b270c1c8bb3587f7df55e00efdec", "issue_key": "KAFKA-903", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "[0.8.0 - windows] FATAL - [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed", "description": "This FATAL shuts down both brokers on windows, {2013-05-10 18:23:57,636} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0] {2013-05-10 18:23:57,637} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 178 for topics [robert_v_2x0,0] to broker 1 on 192.168.1.100:9093 {2013-05-10 18:23:57,689} FATAL [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed {2013-05-10 18:23:57,739} INFO [Thread-4] (Logging.scala:67) - [Kafka Server 0], shutting down Furthermore, attempts to restart them fail, with the following log: {2013-05-10 19:14:52,156} INFO [Thread-1] (Logging.scala:67) - [Kafka Server 0], started {2013-05-10 19:14:52,157} INFO [ZkClient-EventThread-32-localhost:2181] (Logging.scala:67) - New leader is 0 {2013-05-10 19:14:52,193} DEBUG [ZkClient-EventThread-32-localhost:2181] (ZkEventThread.java:79) - Delivering event #1 done {2013-05-10 19:14:52,193} DEBUG [ZkClient-EventThread-32-localhost:2181] (ZkEventThread.java:69) - Delivering event #4 ZkEvent[Data of /controller_epoch changed sent to kafka.controller.ControllerEpochListener@5cb88f42] {2013-05-10 19:14:52,210} DEBUG [SyncThread:0] (FinalRequestProcessor.java:78) - Processing request:: sessionid:0x13e9127882e0001 type:exists cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/controller_epoch {2013-05-10 19:14:52,210} DEBUG [SyncThread:0] (FinalRequestProcessor.java:160) - sessionid:0x13e9127882e0001 type:exists cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/controller_epoch {2013-05-10 19:14:52,213} DEBUG [Thread-1-SendThread(localhost:2181)] (ClientCnxn.java:838) - Reading reply sessionid:0x13e9127882e0001, packet:: clientPath:null serverPath:null finished:false header:: 29,3 replyHeader:: 29,37,0 request:: '/controller_epoch,T response:: s{16,36,1368231712816,1368234889961,1,0,0,0,1,0,16} {2013-05-10 19:14:52,219} INFO [Thread-5] (Logging.scala:67) - [Kafka Server 0], shutting down", "reporter": "Rob Withers", "assignee": "Jun Rao", "created": "2013-05-11T01:18:14.000+0000", "updated": "2017-09-28T01:27:04.000+0000", "resolved": "2013-06-03T23:52:27.000+0000", "labels": [], "components": ["core"], "comments": [{"author": "Jun Rao", "body": "Attach a patch. Rob, could you give it a try?", "created": "2013-05-11T05:04:21.549+0000"}, {"author": "Rob Withers", "body": "I'd love too. Could you build me a jar and email it too me, please? :)", "created": "2013-05-11T05:28:16.617+0000"}, {"author": "Jun Rao", "body": "Attach the kafka jar with the patch.", "created": "2013-05-11T05:53:18.149+0000"}, {"author": "Rob Withers", "body": "That jar is missing a lot - it only has a MANIFEST. Should I be able to do anything with it?", "created": "2013-05-11T06:32:43.956+0000"}, {"author": "Jun Rao", "body": "Attach the right jar this time.", "created": "2013-05-11T15:39:44.697+0000"}, {"author": "Rob Withers", "body": "It still fails. Where should I look in the jar to ensure the patch is in there? Which class method? Also, could you generate a jar for me with sources? thanks a lot and happy saturday! {2013-05-11 10:00:58,630} DEBUG [local-vat] (Logging.scala:51) - Producer sent messages with correlation id 232 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,637} DEBUG [local-vat] (Logging.scala:51) - Getting broker partition info for topic robert_v_2x0 {2013-05-11 10:00:58,638} DEBUG [local-vat] (Logging.scala:51) - Partition [robert_v_2x0,0] has leader 0 {2013-05-11 10:00:58,639} DEBUG [local-vat] (Logging.scala:51) - Broker partitions registered for topic: robert_v_2x0 are 0 {2013-05-11 10:00:58,639} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0] {2013-05-11 10:00:58,640} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 234 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,648} DEBUG [kafka-request-handler-1] (Logging.scala:51) - [Kafka Request Handler 1 on Broker 0], handles request Request(1,sun.nio.ch.SelectionKeyImpl@2b12cf49,null,1368288058646,/192.168.1.100:60622) {2013-05-11 10:00:58,649} DEBUG [kafka-request-handler-1] (Logging.scala:51) - Adding index entry 115 => 5220900 to 00000000000000000000.index. {2013-05-11 10:00:58,650} DEBUG [kafka-request-handler-1] (Logging.scala:51) - Partition [robert_v_2x0,0] on broker 0: Highwatermark for partition [robert_v_2x0,0] updated to 116 {2013-05-11 10:00:58,650} DEBUG [kafka-request-handler-1] (Logging.scala:51) - [KafkaApi-0] Produce to local log in 2 ms {2013-05-11 10:00:58,656} DEBUG [local-vat] (Logging.scala:51) - Producer sent messages with correlation id 234 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,666} DEBUG [local-vat] (Logging.scala:51) - Getting broker partition info for topic robert_v_2x0 {2013-05-11 10:00:58,667} DEBUG [local-vat] (Logging.scala:51) - Partition [robert_v_2x0,0] has leader 0 {2013-05-11 10:00:58,667} DEBUG [local-vat] (Logging.scala:51) - Broker partitions registered for topic: robert_v_2x0 are 0 {2013-05-11 10:00:58,669} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0] {2013-05-11 10:00:58,670} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 236 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,716} DEBUG [kafka-request-handler-0] (Logging.scala:51) - [Kafka Request Handler 0 on Broker 0], handles request Request(1,sun.nio.ch.SelectionKeyImpl@2b12cf49,null,1368288058674,/192.168.1.100:60622) {2013-05-11 10:00:58,717} DEBUG [kafka-request-handler-0] (Logging.scala:51) - Adding index entry 116 => 5232005 to 00000000000000000000.index. {2013-05-11 10:00:58,718} DEBUG [kafka-request-handler-0] (Logging.scala:51) - Partition [robert_v_2x0,0] on broker 0: Highwatermark for partition [robert_v_2x0,0] updated to 117 {2013-05-11 10:00:58,718} DEBUG [kafka-request-handler-0] (Logging.scala:51) - [KafkaApi-0] Produce to local log in 2 ms {2013-05-11 10:00:58,726} FATAL [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed", "created": "2013-05-11T16:09:42.358+0000"}, {"author": "Jun Rao", "body": "Hmm, this may have to do with Windows not supporting file.renameTo() if the target file already exists (http://stackoverflow.com/questions/1000183/reliable-file-renameto-alternative-on-windows). We periodically checkpoint high watermarks to disk. The way we do this is to first write all values to a tmp file replication-offset-checkpoint.tmp and then rename the tmp file to replication-offset-checkpoint. This way, if there is any I/O error during checkpointing, we still have the old checkpoint file for use. Not sure what the best way to do this in Windows. Could you try java 7 and see if it still has the same issue (the above link suggests that it's fixed in jdk 7)?", "created": "2013-05-11T19:56:04.633+0000"}, {"author": "Rob Withers", "body": "I am on jdk 7.0.17, That thread mentions apache.commons.io.FileUtils.moveFile(). Also, my suggestion to be done with the issue: I know there is a way to detect platform, just write a small strategy pattern for the general platform and for windows and plug her in.", "created": "2013-05-11T20:39:45.776+0000"}, {"author": "Jun Rao", "body": "Attach patch v2. If a file can't be renamed, it deletes the target file first. The implication is that during a hard crash, if the high watermark file is missing, one has to manually rename it from the temporary high watermark file.", "created": "2013-05-11T22:09:27.844+0000"}, {"author": "Jun Rao", "body": "Deleted the old jar and a attach a new jar built with patch v2.", "created": "2013-05-11T22:10:44.918+0000"}, {"author": "Rob Withers", "body": "Jun, it looks like this works. I have 23 MB in the log file, on broker0. No replication and I guess I am only sending to partition 0. So, please close this issue. I am still having trouble with consumption, but given it is a weekend, I am busy with other stuff. I will dig in to it later and report my progress. Thanks for all your help, rob", "created": "2013-05-12T21:50:02.200+0000"}, {"author": "Jay Kreps", "body": "I don't think this rename functionality is such a good idea. There are many ways a rename can fail: (1) permissions, (2) disk errors, (3) bad target name, (4) rename is to another volume, etc. Java doesn't differentiate these, so in any of these cases we would then try to delete the file. Not sure this is a good idea. Another approach would be to add a Utils.IsWindows = System.getProperty(\"os.name\").startsWith(\"Windows\") and using this to fall back to the funky non-atomic behavior.", "created": "2013-05-16T18:31:05.164+0000"}, {"author": "Sriram", "body": "There are two claims in this JIRA - 1. \"this may have to do with Windows not supporting file.renameTo() if the target file already exists\" 2. renameTo is not atomic in windows Claim 1 is wrong. MoveFileEx is the native api that helps you to rename an existing file. \"MOVEFILE_REPLACE_EXISTING\" is the flag you would use. This might be a bug in the java api or as the SO link indicates, does not work for non empty directories. (http://msdn.microsoft.com/en-us/library/windows/desktop/aa365240(v=vs.85).aspx) Claim 2 is possible depending on the OS settings. The caller of MoveFileEx is supposed to handle the failure.", "created": "2013-05-16T20:59:34.137+0000"}, {"author": "Timothy Chen", "body": "Do you know when this is going to be pushed to 0.8 branch?", "created": "2013-05-28T22:30:09.151+0000"}, {"author": "Jun Rao", "body": "Attach patch v3. To address Jay's concern, instead of using a generic renameTo util, only falls back to the non-atomic renameTo in checkpointing the high watermark file. Since both files are in the same dir and we control the naming, those other causes you listed that can fail renameTo won't happen. I didn't do the os level checking since I am not sure it that works well for environments like cygwin. We could guard this under a broker config parameter, but I am not sure if it's worth it. For Sriram's concern, this seems to be at least a problem for some versions of java on Windows since other projects like Hadoop (https://issues.apache.org/jira/browse/HADOOP-959) have also seen this before.", "created": "2013-05-29T04:47:20.834+0000"}, {"author": "Neha Narkhede", "body": "+1 on v3", "created": "2013-05-31T17:56:58.125+0000"}, {"author": "Jay Kreps", "body": "+1", "created": "2013-06-03T23:32:10.543+0000"}, {"author": "Jun Rao", "body": "Thanks for the review. Committed v3 to 0.8.", "created": "2013-06-03T23:52:27.301+0000"}], "num_comments": 18, "text": "Issue: KAFKA-903\nSummary: [0.8.0 - windows] FATAL - [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed\nDescription: This FATAL shuts down both brokers on windows, {2013-05-10 18:23:57,636} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0] {2013-05-10 18:23:57,637} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 178 for topics [robert_v_2x0,0] to broker 1 on 192.168.1.100:9093 {2013-05-10 18:23:57,689} FATAL [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed {2013-05-10 18:23:57,739} INFO [Thread-4] (Logging.scala:67) - [Kafka Server 0], shutting down Furthermore, attempts to restart them fail, with the following log: {2013-05-10 19:14:52,156} INFO [Thread-1] (Logging.scala:67) - [Kafka Server 0], started {2013-05-10 19:14:52,157} INFO [ZkClient-EventThread-32-localhost:2181] (Logging.scala:67) - New leader is 0 {2013-05-10 19:14:52,193} DEBUG [ZkClient-EventThread-32-localhost:2181] (ZkEventThread.java:79) - Delivering event #1 done {2013-05-10 19:14:52,193} DEBUG [ZkClient-EventThread-32-localhost:2181] (ZkEventThread.java:69) - Delivering event #4 ZkEvent[Data of /controller_epoch changed sent to kafka.controller.ControllerEpochListener@5cb88f42] {2013-05-10 19:14:52,210} DEBUG [SyncThread:0] (FinalRequestProcessor.java:78) - Processing request:: sessionid:0x13e9127882e0001 type:exists cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/controller_epoch {2013-05-10 19:14:52,210} DEBUG [SyncThread:0] (FinalRequestProcessor.java:160) - sessionid:0x13e9127882e0001 type:exists cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/controller_epoch {2013-05-10 19:14:52,213} DEBUG [Thread-1-SendThread(localhost:2181)] (ClientCnxn.java:838) - Reading reply sessionid:0x13e9127882e0001, packet:: clientPath:null serverPath:null finished:false header:: 29,3 replyHeader:: 29,37,0 request:: '/controller_epoch,T response:: s{16,36,1368231712816,1368234889961,1,0,0,0,1,0,16} {2013-05-10 19:14:52,219} INFO [Thread-5] (Logging.scala:67) - [Kafka Server 0], shutting down\n\nComments (18):\n1. Jun Rao: Attach a patch. Rob, could you give it a try?\n2. Rob Withers: I'd love too. Could you build me a jar and email it too me, please? :)\n3. Jun Rao: Attach the kafka jar with the patch.\n4. Rob Withers: That jar is missing a lot - it only has a MANIFEST. Should I be able to do anything with it?\n5. Jun Rao: Attach the right jar this time.\n6. Rob Withers: It still fails. Where should I look in the jar to ensure the patch is in there? Which class method? Also, could you generate a jar for me with sources? thanks a lot and happy saturday! {2013-05-11 10:00:58,630} DEBUG [local-vat] (Logging.scala:51) - Producer sent messages with correlation id 232 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,637} DEBUG [local-vat] (Logging.scala:51) - Getting broker partition info for topic robert_v_2x0 {2013-05-11 10:00:58,638} DEBUG [local-vat] (Logging.scala:51) - Partition [robert_v_2x0,0] has leader 0 {2013-05-11 10:00:58,639} DEBUG [local-vat] (Logging.scala:51) - Broker partitions registered for topic: robert_v_2x0 are 0 {2013-05-11 10:00:58,639} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0] {2013-05-11 10:00:58,640} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 234 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,648} DEBUG [kafka-request-handler-1] (Logging.scala:51) - [Kafka Request Handler 1 on Broker 0], handles request Request(1,sun.nio.ch.SelectionKeyImpl@2b12cf49,null,1368288058646,/192.168.1.100:60622) {2013-05-11 10:00:58,649} DEBUG [kafka-request-handler-1] (Logging.scala:51) - Adding index entry 115 => 5220900 to 00000000000000000000.index. {2013-05-11 10:00:58,650} DEBUG [kafka-request-handler-1] (Logging.scala:51) - Partition [robert_v_2x0,0] on broker 0: Highwatermark for partition [robert_v_2x0,0] updated to 116 {2013-05-11 10:00:58,650} DEBUG [kafka-request-handler-1] (Logging.scala:51) - [KafkaApi-0] Produce to local log in 2 ms {2013-05-11 10:00:58,656} DEBUG [local-vat] (Logging.scala:51) - Producer sent messages with correlation id 234 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,666} DEBUG [local-vat] (Logging.scala:51) - Getting broker partition info for topic robert_v_2x0 {2013-05-11 10:00:58,667} DEBUG [local-vat] (Logging.scala:51) - Partition [robert_v_2x0,0] has leader 0 {2013-05-11 10:00:58,667} DEBUG [local-vat] (Logging.scala:51) - Broker partitions registered for topic: robert_v_2x0 are 0 {2013-05-11 10:00:58,669} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0] {2013-05-11 10:00:58,670} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 236 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092 {2013-05-11 10:00:58,716} DEBUG [kafka-request-handler-0] (Logging.scala:51) - [Kafka Request Handler 0 on Broker 0], handles request Request(1,sun.nio.ch.SelectionKeyImpl@2b12cf49,null,1368288058674,/192.168.1.100:60622) {2013-05-11 10:00:58,717} DEBUG [kafka-request-handler-0] (Logging.scala:51) - Adding index entry 116 => 5232005 to 00000000000000000000.index. {2013-05-11 10:00:58,718} DEBUG [kafka-request-handler-0] (Logging.scala:51) - Partition [robert_v_2x0,0] on broker 0: Highwatermark for partition [robert_v_2x0,0] updated to 117 {2013-05-11 10:00:58,718} DEBUG [kafka-request-handler-0] (Logging.scala:51) - [KafkaApi-0] Produce to local log in 2 ms {2013-05-11 10:00:58,726} FATAL [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed\n7. Jun Rao: Hmm, this may have to do with Windows not supporting file.renameTo() if the target file already exists (http://stackoverflow.com/questions/1000183/reliable-file-renameto-alternative-on-windows). We periodically checkpoint high watermarks to disk. The way we do this is to first write all values to a tmp file replication-offset-checkpoint.tmp and then rename the tmp file to replication-offset-checkpoint. This way, if there is any I/O error during checkpointing, we still have the old checkpoint file for use. Not sure what the best way to do this in Windows. Could you try java 7 and see if it still has the same issue (the above link suggests that it's fixed in jdk 7)?\n8. Rob Withers: I am on jdk 7.0.17, That thread mentions apache.commons.io.FileUtils.moveFile(). Also, my suggestion to be done with the issue: I know there is a way to detect platform, just write a small strategy pattern for the general platform and for windows and plug her in.\n9. Jun Rao: Attach patch v2. If a file can't be renamed, it deletes the target file first. The implication is that during a hard crash, if the high watermark file is missing, one has to manually rename it from the temporary high watermark file.\n10. Jun Rao: Deleted the old jar and a attach a new jar built with patch v2.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.878538"}}
{"id": "13f36dd9cdc6d1cedb5717ffe255b166", "issue_key": "SPARK-1113", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "External Spilling Bug - hash collision causes NoSuchElementException", "description": "When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int.MAX_VALUE signifies that the corresponding StreamBuffer is empty. However, Int.MAX_VALUE is a perfectly legitimate hash value. If there exists a key with this value, then ExternalAppendOnlyMap does not differentiate between empty StreamBuffers and StreamBuffers that contain only this key. As a result, a NoSuchElementException is thrown - https://github.com/apache/incubator-spark/blob/95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L304. java.util.NoSuchElementException (java.util.NoSuchElementException) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:277) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:212) org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)", "reporter": "Andrew Or", "assignee": "Andrew Or", "created": "2014-02-20T15:53:19.000+0000", "updated": "2014-03-09T17:43:00.000+0000", "resolved": "2014-02-22T17:00:44.000+0000", "labels": [], "components": ["Shuffle", "Spark Core"], "comments": [{"author": "Andrew Or", "body": "PR opened at https://github.com/apache/incubator-spark/pull/624", "created": "2014-02-20T19:21:02.700+0000"}], "num_comments": 1, "text": "Issue: SPARK-1113\nSummary: External Spilling Bug - hash collision causes NoSuchElementException\nDescription: When reading KV pairs back from disk, ExternalAppendOnlyMap maintains a StreamBuffer for each spilled file. These StreamBuffers are ordered by key hash code, and a hash of Int.MAX_VALUE signifies that the corresponding StreamBuffer is empty. However, Int.MAX_VALUE is a perfectly legitimate hash value. If there exists a key with this value, then ExternalAppendOnlyMap does not differentiate between empty StreamBuffers and StreamBuffers that contain only this key. As a result, a NoSuchElementException is thrown - https://github.com/apache/incubator-spark/blob/95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala#L304. java.util.NoSuchElementException (java.util.NoSuchElementException) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:277) org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:212) org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)\n\nComments (1):\n1. Andrew Or: PR opened at https://github.com/apache/incubator-spark/pull/624", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.123570"}}
{"id": "de649840d341fe36029312817e12fe79", "issue_key": "HADOOP-440", "issue_type": "Wish", "status": "Closed", "priority": "Major", "resolution": null, "summary": "In streaming, error messages issued in stream mappers or reducers don't go anywhere", "description": "I would like such error messages to go to the machines' logs at least.", "reporter": "Dick King", "assignee": "Sanjay Dahiya", "created": "2006-08-10T01:51:06.000+0000", "updated": "2009-07-08T17:05:33.000+0000", "resolved": "2007-08-07T16:57:06.000+0000", "labels": [], "components": [], "comments": [{"author": "Owen O'Malley", "body": "This was fixed back in 0.11.", "created": "2007-08-07T16:57:06.853+0000"}], "num_comments": 1, "text": "Issue: HADOOP-440\nSummary: In streaming, error messages issued in stream mappers or reducers don't go anywhere\nDescription: I would like such error messages to go to the machines' logs at least.\n\nComments (1):\n1. Owen O'Malley: This was fixed back in 0.11.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.348698"}}
{"id": "3e6ccb150c2e0f7af41bf73c81c3c10b", "issue_key": "SPARK-544", "issue_type": "New Feature", "status": "Resolved", "priority": null, "resolution": null, "summary": "Provide a Configuration class in addition to system properties", "description": "This is a much better option for people who want to connect to multiple Spark clusters in the same program, and for unit tests.", "reporter": "Matei Alexandru Zaharia", "assignee": "Matei Alexandru Zaharia", "created": "0012-06-15T22:29:00.000+0000", "updated": "2014-04-30T00:43:06.000+0000", "resolved": "2014-04-30T00:43:05.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: In order to configure my applications, I have used the following system from twitter: https://github.com/twitter/util/blob/master/util- core/src/main/scala/com/twitter/util/Config.scala Here is a rationale: http://robey.lag.net/2012/03/26/why-config.html Tim On Friday, June 15, 2012 11:29:53 PM you wrote: > This is a much better option for people who want to connect to multiple > Spark clusters in the same program, and for unit tests. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/140 -- Timothy Hunter Ph.D Student Computer Science University of California - Berkeley www.eecs.berkeley.edu/~tjhunter/ T. 404 421 3075", "created": "2012-06-17T19:42:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-140, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Denny Britz", "body": "I like the approach in http://robey.lag.net/2012/03/26/why-config.html. The config classes in https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Config.scala is not much extra code either. Matei, are you okay with using this?", "created": "2012-10-25T15:06:21.028+0000"}, {"author": "Matei Alexandru Zaharia", "body": "I find this kind of confusing and too specific to Scala. I'd prefer something where you just say string key-value pairs. Then it will be easy to explain to existing users, and it will be easy to support it in Java, Python and Shark as well.", "created": "2012-10-25T16:14:44.640+0000"}, {"author": "Denny Britz", "body": "Yeah, it is pretty Scala specific, that's a fair point.", "created": "2012-10-25T16:25:43.586+0000"}, {"author": "Ankur Bansal", "body": "Why not just use regular commons-configuration or JSON based configuration. They are fairly universal and very readable. I would be more than happy to take this up if no one else is working on this already. I went ahead an did a simple grep of the `System.properies` and listed out all the properties as a json document and as a properties file. IMO JSON looks *nicer* but properties files are easier to handle. Any opinions? https://gist.github.com/ankurcha/5655646", "created": "2013-05-26T23:24:13.211+0000"}, {"author": "Shane Huang", "body": "We started a discussion about the design of this on dev mailing list and collected a few opinions, which I summarized below. The link to the original discussion is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html. 1) Define a Configuration class which contains all the options available for Spark application. A Configuration instance can be de-/serialized from/to a formatted file. Most of us tend to agree that Typesafe Config library is a good choice for the Configuration class. 2) Each application (SparkContext) has one Configuration instance and it is initialized by the application which creates it (either coded in app (apps could explicitly read from io stream or command line arguments), or system properties, or env vars). 3) For an application the overriding rule should be code > system properties > env vars. Over time we will deprecate the env vars and maybe even system properties. 4) When launching an Executor on a slave node, the Configuration is firstly initialized using the node-local configuration file as default (instead of the env vars at present), and then the Configuration passed from application driver context will override specific options specified in default. Certain options in app's Configuration will always override those in node-local, because these options need to be the consistent across all the slave nodes, e.g. spark.serializer. In this case if any such options is not set in app's Config, a value will be provided by the system. On the other hand, some options in app's Config will never override those in node-local. as they're not meat to be set in app, e.g. spark.local.dir.", "created": "2013-09-21T20:57:11.816+0000"}, {"author": "rektide de la fey", "body": "\"Inventing a class with its own interface to hold a piece of information is like inventing a new language to write every short story.\" http://www.codequarterly.com/2011/rich-hickey/ Is there tech that can be leveraged here? It's- alas- a library on top of a rather sizable framework, but Chronos, for example, uses Dropwizard Configurable AssetsBundle for configuration (https://github.com/bazaarvoice/dropwizard-configurable-assets-bundle). I get the idea that the use case here, the need, is to serialize some state around, but Configuration is a massive ops concern and it'd be good to party up, I feel.", "created": "2013-09-25T15:25:29.656+0000"}, {"author": "rektide de la fey", "body": "Aurora- a Mesos scheduler from Twitter, the other Mesos scheduler- has code commited as of two days ago. They saw fit to create IDL definitions for tasks and their associate configuration. Not recommending any particular solution here nor there, but perhaps and if perhaps not maybe still interesting & useful reference- https://github.com/twitter/aurora/blob/master/thrift/src/main/thrift/com/twitter/aurora/gen/api.thrift#L130-L160 or specifically now, https://github.com/twitter/aurora/commit/c248931344e46a0e99bfbad6fdf3e08d7473008b#L130-160", "created": "2013-09-25T17:30:15.014+0000"}, {"author": "Evan Chan", "body": "re: @rektide Typesafe Config is used in Akka, Spray, Play, and other Scala frameworks. We use it for all of our Scala apps. If you look at how Akka uses it, they don't need to build a Configuration class on top. Instead, you just pass in a com.typesafe.Config object into your class, say SparkContext. For non-Java/Scala apps, they can write JSON config files, which Typesafe Config can parse easily. The Config object can be initialized and created in multiple ways, but typically from a file, or from a Map, and you can merge it with a default file loaded from resources / jar, or even with system properties. For example, let's say you have class SparkContext(config: Config) { val port = config.getInt(\"spark.port\") That's pretty succinct for getting the value out. The advantage of a config class is that you have a type-safe access to the properties, but the disadvantage is that you have to maintain the API. I honestly feel like it's been OK to use the config without a formal class. -Evan", "created": "2013-09-25T23:58:02.139+0000"}, {"author": "Evan Chan", "body": "By the way, I've started work on this, using Typesafe Config-based configuration objects (which can parse from JSON as well). The first, primary goal is to move completely away from using System properties as global variables (there are multiple places that get, then set, for example, \"spark.driver.port\"). This will be a big step towards being able to safely have multiple contexts within the same process. This will also allow much richer config options than simple strings.", "created": "2013-09-28T08:13:28.396+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Cool, looking forward to it!", "created": "2013-09-28T09:44:50.653+0000"}, {"author": "Evan Chan", "body": "Pull Request for the first part has been submitted, in case anybody wants to have a look: https://github.com/apache/incubator-spark/pull/55", "created": "2013-10-14T13:34:39.475+0000"}, {"author": "Evan Chan", "body": "By the way, new progress is taking place in a new pull request: https://github.com/apache/incubator-spark/pull/230", "created": "2013-12-12T10:21:43.595+0000"}], "num_comments": 14, "text": "Issue: SPARK-544\nSummary: Provide a Configuration class in addition to system properties\nDescription: This is a much better option for people who want to connect to multiple Spark clusters in the same program, and for unit tests.\n\nComments (14):\n1. Patrick McFadin: Github comment from tjhunter: In order to configure my applications, I have used the following system from twitter: https://github.com/twitter/util/blob/master/util- core/src/main/scala/com/twitter/util/Config.scala Here is a rationale: http://robey.lag.net/2012/03/26/why-config.html Tim On Friday, June 15, 2012 11:29:53 PM you wrote: > This is a much better option for people who want to connect to multiple > Spark clusters in the same program, and for unit tests. > > --- > Reply to this email directly or view it on GitHub: > https://github.com/mesos/spark/issues/140 -- Timothy Hunter Ph.D Student Computer Science University of California - Berkeley www.eecs.berkeley.edu/~tjhunter/ T. 404 421 3075\n2. Patrick McFadin: Imported from Github issue spark-140, originally reported by mateiz\n3. Denny Britz: I like the approach in http://robey.lag.net/2012/03/26/why-config.html. The config classes in https://github.com/twitter/util/blob/master/util-core/src/main/scala/com/twitter/util/Config.scala is not much extra code either. Matei, are you okay with using this?\n4. Matei Alexandru Zaharia: I find this kind of confusing and too specific to Scala. I'd prefer something where you just say string key-value pairs. Then it will be easy to explain to existing users, and it will be easy to support it in Java, Python and Shark as well.\n5. Denny Britz: Yeah, it is pretty Scala specific, that's a fair point.\n6. Ankur Bansal: Why not just use regular commons-configuration or JSON based configuration. They are fairly universal and very readable. I would be more than happy to take this up if no one else is working on this already. I went ahead an did a simple grep of the `System.properies` and listed out all the properties as a json document and as a properties file. IMO JSON looks *nicer* but properties files are easier to handle. Any opinions? https://gist.github.com/ankurcha/5655646\n7. Shane Huang: We started a discussion about the design of this on dev mailing list and collected a few opinions, which I summarized below. The link to the original discussion is http://www.mail-archive.com/dev@spark.incubator.apache.org/msg00446.html. 1) Define a Configuration class which contains all the options available for Spark application. A Configuration instance can be de-/serialized from/to a formatted file. Most of us tend to agree that Typesafe Config library is a good choice for the Configuration class. 2) Each application (SparkContext) has one Configuration instance and it is initialized by the application which creates it (either coded in app (apps could explicitly read from io stream or command line arguments), or system properties, or env vars). 3) For an application the overriding rule should be code > system properties > env vars. Over time we will deprecate the env vars and maybe even system properties. 4) When launching an Executor on a slave node, the Configuration is firstly initialized using the node-local configuration file as default (instead of the env vars at present), and then the Configuration passed from application driver context will override specific options specified in default. Certain options in app's Configuration will always override those in node-local, because these options need to be the consistent across all the slave nodes, e.g. spark.serializer. In this case if any such options is not set in app's Config, a value will be provided by the system. On the other hand, some options in app's Config will never override those in node-local. as they're not meat to be set in app, e.g. spark.local.dir.\n8. rektide de la fey: \"Inventing a class with its own interface to hold a piece of information is like inventing a new language to write every short story.\" http://www.codequarterly.com/2011/rich-hickey/ Is there tech that can be leveraged here? It's- alas- a library on top of a rather sizable framework, but Chronos, for example, uses Dropwizard Configurable AssetsBundle for configuration (https://github.com/bazaarvoice/dropwizard-configurable-assets-bundle). I get the idea that the use case here, the need, is to serialize some state around, but Configuration is a massive ops concern and it'd be good to party up, I feel.\n9. rektide de la fey: Aurora- a Mesos scheduler from Twitter, the other Mesos scheduler- has code commited as of two days ago. They saw fit to create IDL definitions for tasks and their associate configuration. Not recommending any particular solution here nor there, but perhaps and if perhaps not maybe still interesting & useful reference- https://github.com/twitter/aurora/blob/master/thrift/src/main/thrift/com/twitter/aurora/gen/api.thrift#L130-L160 or specifically now, https://github.com/twitter/aurora/commit/c248931344e46a0e99bfbad6fdf3e08d7473008b#L130-160\n10. Evan Chan: re: @rektide Typesafe Config is used in Akka, Spray, Play, and other Scala frameworks. We use it for all of our Scala apps. If you look at how Akka uses it, they don't need to build a Configuration class on top. Instead, you just pass in a com.typesafe.Config object into your class, say SparkContext. For non-Java/Scala apps, they can write JSON config files, which Typesafe Config can parse easily. The Config object can be initialized and created in multiple ways, but typically from a file, or from a Map, and you can merge it with a default file loaded from resources / jar, or even with system properties. For example, let's say you have class SparkContext(config: Config) { val port = config.getInt(\"spark.port\") That's pretty succinct for getting the value out. The advantage of a config class is that you have a type-safe access to the properties, but the disadvantage is that you have to maintain the API. I honestly feel like it's been OK to use the config without a formal class. -Evan", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.961085"}}
{"id": "7cc96dab367ba94286fa0f2814333898", "issue_key": "KAFKA-27", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Provide a default encoder for kafka.producer.Producer", "description": "Today, the \"serializer.class\" parameter in Producer is required. It is reasonable, however, to provide a default no-op Encoder<Message>. The client would have to type the Producer accordingly though. For example, this would work - val producer = new Producer<String, Message>(config) This wouldn't - val producer = new Producer<String, String>(config", "reporter": null, "assignee": null, "created": "2011-07-19T21:32:15.000+0000", "updated": "2011-07-19T21:32:15.000+0000", "resolved": "2011-07-19T21:32:15.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: KAFKA-27\nSummary: Provide a default encoder for kafka.producer.Producer\nDescription: Today, the \"serializer.class\" parameter in Producer is required. It is reasonable, however, to provide a default no-op Encoder<Message>. The client would have to type the Producer accordingly though. For example, this would work - val producer = new Producer<String, Message>(config) This wouldn't - val producer = new Producer<String, String>(config", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.490446"}}
{"id": "a7229a855eebb6472b5069e70c908b58", "issue_key": "KAFKA-654", "issue_type": "Bug", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "Irrecoverable error while trying to roll a segment that already exists", "description": "I tried setting up a 5 broker 0.8 cluster and sending messages to 100s of topics on it. For a couple of topic partitions, the produce requests never succeed since they fail on the leader with the following error - [2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000 0.log already exists; deleting it first (kafka.log.Log) [2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000 0.index already exists; deleting it first (kafka.log.Log) [2012-12-05 22:54:05,715] ERROR [ReplicaFetcherThread-1-0-on-broker-2], Error due to (kafka.server.R eplicaFetcherThread) kafka.common.KafkaException: Trying to roll a new log segment for topic partition NusWriteEvent-4 with start offset 0 while it already exsits at kafka.log.Log.rollToOffset(Log.scala:456) at kafka.log.Log.roll(Log.scala:434) at kafka.log.Log.maybeRoll(Log.scala:423) at kafka.log.Log.append(Log.scala:257) at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:125) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:108) at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125) at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344) at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:108) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "created": "2012-12-06T06:19:04.000+0000", "updated": "2017-12-19T23:43:16.000+0000", "resolved": "2012-12-10T18:24:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Joel Jacob Koshy", "body": "FWIW, it should be easy to reproduce this - I just saw it on a two node cluster / one topic but am looking at another issue right now. [2012-12-06 10:10:18,380] ERROR [ReplicaFetcherThread-0-0-on-broker-1], Error due to (kafka.server.ReplicaFetcherThread) kafka.common.KafkaException: Trying to roll a new log segment for topic partition abc-0 with start offset 0 while it already exsits at kafka.log.Log.rollToOffset(Log.scala:456) at kafka.log.Log.roll(Log.scala:434) at kafka.log.Log.maybeRoll(Log.scala:423) at kafka.log.Log.append(Log.scala:257) at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:125) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:108) at scala.collection.immutable.Map$Map1.foreach(Map.scala:105) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:108) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50) [2012-12-06 10:10:18,382] INFO [ReplicaFetcherThread-0-0-on-broker-1], Stopped (kafka.server.ReplicaFetcherThread)", "created": "2012-12-06T18:17:22.020+0000"}, {"author": "Jay Kreps", "body": "I think this is almost certainly a bug in the log. I will come by and discuss.", "created": "2012-12-06T18:37:01.132+0000"}, {"author": "Neha Narkhede", "body": "The bug is inside Log.truncateAndStartWithNewOffset val deletedSegments = segments.trunc(segments.view.size) segments.append(new LogSegment(dir, newOffset, indexIntervalBytes = indexIntervalBytes, maxIndexSize = maxIndexSize)) deleteSegments(deletedSegments) The order of deleteSegments and segments.append is reversed. Due to this, we end up adding an already full index as the last entry in the segments array, but delete it later from disk. During maybeRoll, it finds the index to be full and errors on the rolling of the new log segment. However, in my testing, I saw this on the leader as well. I've not been able to fix that yet.", "created": "2012-12-06T20:31:54.635+0000"}, {"author": "Neha Narkhede", "body": "Patch to fix the problem on the follower as explained above. However, I couldn't reproduce this issue on the leader. I guess we can file another JIRA if we are able to find that issue again", "created": "2012-12-09T20:00:47.216+0000"}, {"author": "Jay Kreps", "body": "+1 I had fixed this on trunk: http://svn.apache.org/repos/asf/kafka/trunk/core/src/main/scala/kafka/log/Log.scala Double checked that it didn't get reintroduced in the async delete patch: https://issues.apache.org/jira/secure/attachment/12559981/KAFKA-636-v1.patch So it looks like it was only on 0.8.", "created": "2012-12-10T17:34:56.251+0000"}, {"author": "Neha Narkhede", "body": "Thanks for the review, committed patch v1 to 0.8 branch.", "created": "2012-12-10T18:24:00.819+0000"}], "num_comments": 6, "text": "Issue: KAFKA-654\nSummary: Irrecoverable error while trying to roll a segment that already exists\nDescription: I tried setting up a 5 broker 0.8 cluster and sending messages to 100s of topics on it. For a couple of topic partitions, the produce requests never succeed since they fail on the leader with the following error - [2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000 0.log already exists; deleting it first (kafka.log.Log) [2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000 0.index already exists; deleting it first (kafka.log.Log) [2012-12-05 22:54:05,715] ERROR [ReplicaFetcherThread-1-0-on-broker-2], Error due to (kafka.server.R eplicaFetcherThread) kafka.common.KafkaException: Trying to roll a new log segment for topic partition NusWriteEvent-4 with start offset 0 while it already exsits at kafka.log.Log.rollToOffset(Log.scala:456) at kafka.log.Log.roll(Log.scala:434) at kafka.log.Log.maybeRoll(Log.scala:423) at kafka.log.Log.append(Log.scala:257) at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:125) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:108) at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125) at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344) at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:108) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)\n\nComments (6):\n1. Joel Jacob Koshy: FWIW, it should be easy to reproduce this - I just saw it on a two node cluster / one topic but am looking at another issue right now. [2012-12-06 10:10:18,380] ERROR [ReplicaFetcherThread-0-0-on-broker-1], Error due to (kafka.server.ReplicaFetcherThread) kafka.common.KafkaException: Trying to roll a new log segment for topic partition abc-0 with start offset 0 while it already exsits at kafka.log.Log.rollToOffset(Log.scala:456) at kafka.log.Log.roll(Log.scala:434) at kafka.log.Log.maybeRoll(Log.scala:423) at kafka.log.Log.append(Log.scala:257) at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:125) at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:108) at scala.collection.immutable.Map$Map1.foreach(Map.scala:105) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:108) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50) [2012-12-06 10:10:18,382] INFO [ReplicaFetcherThread-0-0-on-broker-1], Stopped (kafka.server.ReplicaFetcherThread)\n2. Jay Kreps: I think this is almost certainly a bug in the log. I will come by and discuss.\n3. Neha Narkhede: The bug is inside Log.truncateAndStartWithNewOffset val deletedSegments = segments.trunc(segments.view.size) segments.append(new LogSegment(dir, newOffset, indexIntervalBytes = indexIntervalBytes, maxIndexSize = maxIndexSize)) deleteSegments(deletedSegments) The order of deleteSegments and segments.append is reversed. Due to this, we end up adding an already full index as the last entry in the segments array, but delete it later from disk. During maybeRoll, it finds the index to be full and errors on the rolling of the new log segment. However, in my testing, I saw this on the leader as well. I've not been able to fix that yet.\n4. Neha Narkhede: Patch to fix the problem on the follower as explained above. However, I couldn't reproduce this issue on the leader. I guess we can file another JIRA if we are able to find that issue again\n5. Jay Kreps: +1 I had fixed this on trunk: http://svn.apache.org/repos/asf/kafka/trunk/core/src/main/scala/kafka/log/Log.scala Double checked that it didn't get reintroduced in the async delete patch: https://issues.apache.org/jira/secure/attachment/12559981/KAFKA-636-v1.patch So it looks like it was only on 0.8.\n6. Neha Narkhede: Thanks for the review, committed patch v1 to 0.8 branch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.784903"}}
{"id": "f2d2c318e8db551babe88d44d09758f2", "issue_key": "HADOOP-953", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "huge log files", "description": "On our system, it's not uncommon to get 20 MB of logs with each MapReduce job. It would be very helpful if it were possible to configure Hadoop daemons to write logs only when major things happen, but the only conf options I could find are for increasing the amount of output. The disk is really a bottleneck for us, and I believe that short jobs would run much more quickly with less disk usage. We also believe that the high disk usage might be triggering a kernel bug on some of our machines, causing them to crash. If the 20 MB of logs went down to 20 KB, we would probably still have all of the information we needed. Thanks!", "reporter": "Andrew McNabb", "assignee": null, "created": "2007-01-29T21:56:36.000+0000", "updated": "2011-08-11T18:53:03.000+0000", "resolved": "2011-08-11T18:53:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Owen O'Malley", "body": "This is already possible. Just change the HADOOP_ROOT_LOGGER variable in bin/hadoop-daemon.sh to WARN,DRFA or even ERROR,DRFA. (We really should make it a default rather than hard coding it in the script.) It will dramatically cut down the messages.", "created": "2007-02-06T08:23:44.950+0000"}, {"author": "Andrew McNabb", "body": "It seems like half of configuration is done in environment variables, and half is done in the xml config files. I wouldn't mind if things are available in both, but it's really hard when some have to be done in one place and half have to be done in another. I'm especially confused in this case because there is a logging option in the xml file: dfs.namenode.logging.level. Anyway, thank you very much for alerting me to the HADOOP_ROOT_LOGGER enviroment variable. That will definitely help in the short term. Thanks.", "created": "2007-02-06T16:52:52.449+0000"}, {"author": "Doug Cutting", "body": "> We really should make it a default rather than hard coding it in the script. Rather I think we should perhaps convert many INFO level messages to DEBUG. The contract with developers is that INFO messages are archived in log files. FATAL propagates to the user promptly, WARN may be summarized to the user (e.g., a count of warnings, w/ option to view), and INFO is available in log files. DEBUG and TRACE are not normally viewed. So, e.g., important state transitions should be logged at INFO level, so that folks can see when they occurred. We don't want to silence all INFO-level messages. We may however wish to reduce their number dramatically.", "created": "2007-02-06T17:26:44.936+0000"}, {"author": "Edward J. Yoon", "body": "I would recommend the HADOOP-3210 idea for an huge log files.", "created": "2008-09-19T05:42:16.664+0000"}], "num_comments": 4, "text": "Issue: HADOOP-953\nSummary: huge log files\nDescription: On our system, it's not uncommon to get 20 MB of logs with each MapReduce job. It would be very helpful if it were possible to configure Hadoop daemons to write logs only when major things happen, but the only conf options I could find are for increasing the amount of output. The disk is really a bottleneck for us, and I believe that short jobs would run much more quickly with less disk usage. We also believe that the high disk usage might be triggering a kernel bug on some of our machines, causing them to crash. If the 20 MB of logs went down to 20 KB, we would probably still have all of the information we needed. Thanks!\n\nComments (4):\n1. Owen O'Malley: This is already possible. Just change the HADOOP_ROOT_LOGGER variable in bin/hadoop-daemon.sh to WARN,DRFA or even ERROR,DRFA. (We really should make it a default rather than hard coding it in the script.) It will dramatically cut down the messages.\n2. Andrew McNabb: It seems like half of configuration is done in environment variables, and half is done in the xml config files. I wouldn't mind if things are available in both, but it's really hard when some have to be done in one place and half have to be done in another. I'm especially confused in this case because there is a logging option in the xml file: dfs.namenode.logging.level. Anyway, thank you very much for alerting me to the HADOOP_ROOT_LOGGER enviroment variable. That will definitely help in the short term. Thanks.\n3. Doug Cutting: > We really should make it a default rather than hard coding it in the script. Rather I think we should perhaps convert many INFO level messages to DEBUG. The contract with developers is that INFO messages are archived in log files. FATAL propagates to the user promptly, WARN may be summarized to the user (e.g., a count of warnings, w/ option to view), and INFO is available in log files. DEBUG and TRACE are not normally viewed. So, e.g., important state transitions should be logged at INFO level, so that folks can see when they occurred. We don't want to silence all INFO-level messages. We may however wish to reduce their number dramatically.\n4. Edward J. Yoon: I would recommend the HADOOP-3210 idea for an huge log files.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.512387"}}
{"id": "1e084dd2f6b83fdbca78e437d4a1a552", "issue_key": "SPARK-1012", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "DAGScheduler Exception", "description": "I get When running the following code:  import org.apache.spark.mllib.recommendation.ALS import org.apache.spark.mllib.recommendation.Rating // Load and parse the data val data = sc.textFile(\"mllib/data/als/test.data\") val ratings = data.map(_.split(',') match { case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble) }) // Build the recommendation model using ALS val numIterations = 20 val model = ALS.train(ratings, 1, 20, 0.01) // Evaluate the model on rating data val ratesAndPreds = ratings.map{ case Rating(user, item, rate) => (rate, model.predict(user, item))} val MSE = ratesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/ratesAndPreds.count  I get:  org.apache.spark.SparkException: Job aborted: Task 2.0:0 failed 1 times (most recent failure: Exception failure: scala.MatchError: null) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1024) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1024) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:205) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  The problem is deterministic. In addition ratesAndPreds has exactly 16 elements:  scala> ratesAndPreds.take(16) res1: Array[(Double, Double)] = Array((5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928)) scala> ratesAndPreds.count()  Throws the exception again.", "reporter": "Hossein Falaki", "assignee": "Hossein Falaki", "created": "2014-01-02T16:19:27.000+0000", "updated": "2014-01-07T22:02:31.000+0000", "resolved": "2014-01-07T22:02:31.000+0000", "labels": ["DAGScheduler", "MLLib,"], "components": ["Spark Core"], "comments": [{"author": "Hossein Falaki", "body": "The problem is that MatrixFactorizationModel has a reference to two RDDs (userFeatures, and productFeatures). As a result, when passed inside a closure all the bad things happen. The solution is augmenting MatrixFactorizaitonModel with a bulk prediction method that takes an RDD of test data and returns a prediction RDD.", "created": "2014-01-02T16:52:34.082+0000"}, {"author": "Hossein Falaki", "body": "Submitted this pull request to offer a viable method for bulk prediction.", "created": "2014-01-03T16:02:52.144+0000"}, {"author": "Hossein Falaki", "body": "Fixed with PR #328", "created": "2014-01-07T22:02:31.418+0000"}], "num_comments": 3, "text": "Issue: SPARK-1012\nSummary: DAGScheduler Exception\nDescription: I get When running the following code:  import org.apache.spark.mllib.recommendation.ALS import org.apache.spark.mllib.recommendation.Rating // Load and parse the data val data = sc.textFile(\"mllib/data/als/test.data\") val ratings = data.map(_.split(',') match { case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble) }) // Build the recommendation model using ALS val numIterations = 20 val model = ALS.train(ratings, 1, 20, 0.01) // Evaluate the model on rating data val ratesAndPreds = ratings.map{ case Rating(user, item, rate) => (rate, model.predict(user, item))} val MSE = ratesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce(_ + _)/ratesAndPreds.count  I get:  org.apache.spark.SparkException: Job aborted: Task 2.0:0 failed 1 times (most recent failure: Exception failure: scala.MatchError: null) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026) at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1024) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1024) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:617) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:617) at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:205) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  The problem is deterministic. In addition ratesAndPreds has exactly 16 elements:  scala> ratesAndPreds.take(16) res1: Array[(Double, Double)] = Array((5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (5.0,2.9995813468435633), (1.0,2.999581386744982), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928), (1.0,2.999581373444509), (5.0,2.999581413345928)) scala> ratesAndPreds.count()  Throws the exception again.\n\nComments (3):\n1. Hossein Falaki: The problem is that MatrixFactorizationModel has a reference to two RDDs (userFeatures, and productFeatures). As a result, when passed inside a closure all the bad things happen. The solution is augmenting MatrixFactorizaitonModel with a bulk prediction method that takes an RDD of test data and returns a prediction RDD.\n2. Hossein Falaki: Submitted this pull request to offer a viable method for bulk prediction.\n3. Hossein Falaki: Fixed with PR #328", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.107799"}}
{"id": "96f4217a807434b6c0b6a87fc88e25d2", "issue_key": "SPARK-683", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation", "description": "A simple saveAsObjectFile() leads to the following error. org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, boolean, boolean, short, long) at java.lang.Class.getMethod(Class.java:1622) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:416) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)", "reporter": "Tathagata Das", "assignee": null, "created": "2013-02-04T17:13:37.000+0000", "updated": "2014-10-29T09:17:12.000+0000", "resolved": "2014-09-11T08:59:09.000+0000", "labels": [], "components": ["EC2"], "comments": [{"author": "Patrick McFadin", "body": "I think the default hadoop version might have changed in the build. Are you sure you compiled spark with the same hadoop version as is on the AMI?", "created": "2013-02-04T20:37:09.789+0000"}, {"author": "Shivaram Venkataraman", "body": "That was the problem. We changed the default hadoop version to 1.0 in 0.7.0 -- We should either change the AMI to run HDFS v1.0 or change Spark on the AMI to make sure users don't run into this.", "created": "2013-02-04T20:42:51.310+0000"}, {"author": "Patrick McFadin", "body": "Ya I ran into this testing streaming code. Probably same as TD.", "created": "2013-02-04T20:51:04.732+0000"}, {"author": "Shivaram Venkataraman", "body": "Hopefully this will help whoever makes the AMI for 0.7. I tried setting up Hadoop 1.0.3 on the existing AMI and the configuration we have right now works fine out of the box. All I had to do was: wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.3/hadoop-1.0.3.tar.gz tar -xf hadoop-1.0.3.tar.gz # Copy conf files from existing hdfs setup", "created": "2013-02-21T15:07:18.950+0000"}, {"author": "Sean R. Owen", "body": "I think this is likely long since obsolete or fixed, since Spark, Hadoop and AMI Hadoop versions have moved forward, and have not heard of this issue in recent memory.", "created": "2014-09-11T08:59:09.954+0000"}, {"author": "Sean R. Owen", "body": "PS I think this also turns out to be the same as SPARK-4078", "created": "2014-10-29T09:17:12.673+0000"}], "num_comments": 6, "text": "Issue: SPARK-683\nSummary: Spark 0.7 with Hadoop 1.0 does not work with current AMI's HDFS installation\nDescription: A simple saveAsObjectFile() leads to the following error. org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NoSuchMethodException: org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, boolean, boolean, short, long) at java.lang.Class.getMethod(Class.java:1622) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:416) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\nComments (6):\n1. Patrick McFadin: I think the default hadoop version might have changed in the build. Are you sure you compiled spark with the same hadoop version as is on the AMI?\n2. Shivaram Venkataraman: That was the problem. We changed the default hadoop version to 1.0 in 0.7.0 -- We should either change the AMI to run HDFS v1.0 or change Spark on the AMI to make sure users don't run into this.\n3. Patrick McFadin: Ya I ran into this testing streaming code. Probably same as TD.\n4. Shivaram Venkataraman: Hopefully this will help whoever makes the AMI for 0.7. I tried setting up Hadoop 1.0.3 on the existing AMI and the configuration we have right now works fine out of the box. All I had to do was: wget http://archive.apache.org/dist/hadoop/core/hadoop-1.0.3/hadoop-1.0.3.tar.gz tar -xf hadoop-1.0.3.tar.gz # Copy conf files from existing hdfs setup\n5. Sean R. Owen: I think this is likely long since obsolete or fixed, since Spark, Hadoop and AMI Hadoop versions have moved forward, and have not heard of this issue in recent memory.\n6. Sean R. Owen: PS I think this also turns out to be the same as SPARK-4078", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "dbc44f9088805026260bf03aed7784cc", "issue_key": "SPARK-411", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "The default broadcast implementation should not use HDFS", "description": "There seems to be a bug in some versions of HDFS that disallow it being initialized in two threads, and more generally, some users might not have HDFS. The default implementation should just launch an in-process HTTP server.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0012-06-06T16:35:00.000+0000", "updated": "2012-10-22T14:55:37.000+0000", "resolved": "2012-10-19T22:50:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-139, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-411\nSummary: The default broadcast implementation should not use HDFS\nDescription: There seems to be a bug in some versions of HDFS that disallow it being initialized in two threads, and more generally, some users might not have HDFS. The default implementation should just launch an in-process HTTP server.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-139, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.959085"}}
{"id": "ce12c0ea6852faaf3d5f86231efe33bd", "issue_key": "KAFKA-637", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Separate log4j environment variable from KAFKA_OPTS in kafka-run-class.sh", "description": "The kafka-run-class.sh script has an environment variable for 'KAFKA_OPTS' which defines the log4j configuration file with other system variables. By moving the log4j settings into their own variable we can set that configuration file without having to also include the system parameters. This way if the script changes in the next release our management scripts won't have (potentially) unexpected side effects. We change the logging configuration file in the test environment so we can run multiple brokers on a single machine for testing. Without this option all the logs for all the brokers get jumbled together.", "reporter": "Chris Curtin", "assignee": "Jay Kreps", "created": "2012-11-28T19:02:11.000+0000", "updated": "2013-07-11T23:34:04.000+0000", "resolved": "2013-07-11T23:34:04.000+0000", "labels": [], "components": [], "comments": [{"author": "David Arthur", "body": "Something like this? I can now invoke the bin script like: LOG4J_OPTS=\"-Dlog4j.debug=true -Dlog4j.configuration=file:config/log4j.properties\" ./bin/kafka-server-start.sh config/server.properties", "created": "2012-11-29T03:47:54.675+0000"}, {"author": "Jay Kreps", "body": "This is sensible. Merged this in with KAFKA-718 which has a few other command line cleanups. Would you mind taking a look at that and let's continue any discussion there.", "created": "2013-07-11T23:34:04.066+0000"}], "num_comments": 2, "text": "Issue: KAFKA-637\nSummary: Separate log4j environment variable from KAFKA_OPTS in kafka-run-class.sh\nDescription: The kafka-run-class.sh script has an environment variable for 'KAFKA_OPTS' which defines the log4j configuration file with other system variables. By moving the log4j settings into their own variable we can set that configuration file without having to also include the system parameters. This way if the script changes in the next release our management scripts won't have (potentially) unexpected side effects. We change the logging configuration file in the test environment so we can run multiple brokers on a single machine for testing. Without this option all the logs for all the brokers get jumbled together.\n\nComments (2):\n1. David Arthur: Something like this? I can now invoke the bin script like: LOG4J_OPTS=\"-Dlog4j.debug=true -Dlog4j.configuration=file:config/log4j.properties\" ./bin/kafka-server-start.sh config/server.properties\n2. Jay Kreps: This is sensible. Merged this in with KAFKA-718 which has a few other command line cleanups. Would you mind taking a look at that and let's continue any discussion there.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.773107"}}
{"id": "a2c4625d34d0ffee5d340e669a35cac4", "issue_key": "KAFKA-882", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Enhance 0.7 ProducerPerformance to send sequential MessageID as in 0.8", "description": "Also update related files in the following in Kafka 0.8: 1. system_test/migration_tool_testsuite/config 2. system_test/migration_tool_testsuite/0.7/lib/kafka-perf-0.7.0.jar (after this JIRA is completed) 3. system_test/migration_tool_testsuite/testcase_900x/testcase_900x_properties.json", "reporter": "John Fung", "assignee": "John Fung", "created": "2013-04-26T16:35:47.000+0000", "updated": "2013-07-23T17:30:19.000+0000", "resolved": "2013-07-23T17:30:19.000+0000", "labels": ["kafka"], "components": [], "comments": [{"author": "John Fung", "body": "uploaded kafka-882-v1.patch", "created": "2013-07-02T19:33:08.370+0000"}, {"author": "John Fung", "body": "ProducerPerformance in 0.7 has quite a few nested if-else conditional logics in addition to while & for loops. In order to keep it simple, this patch contains the change to send sequential MessageID only when the following arguments are specified: --initial-message-id <n> --vary-message-size --async Otherwise, it will preserve the original 0.7 behavior.", "created": "2013-07-02T19:40:03.196+0000"}, {"author": "Jun Rao", "body": "Thanks for the patch. Committed to 0.7 so that we can get the system tests in 0.8 working.", "created": "2013-07-23T17:30:19.331+0000"}], "num_comments": 3, "text": "Issue: KAFKA-882\nSummary: Enhance 0.7 ProducerPerformance to send sequential MessageID as in 0.8\nDescription: Also update related files in the following in Kafka 0.8: 1. system_test/migration_tool_testsuite/config 2. system_test/migration_tool_testsuite/0.7/lib/kafka-perf-0.7.0.jar (after this JIRA is completed) 3. system_test/migration_tool_testsuite/testcase_900x/testcase_900x_properties.json\n\nComments (3):\n1. John Fung: uploaded kafka-882-v1.patch\n2. John Fung: ProducerPerformance in 0.7 has quite a few nested if-else conditional logics in addition to while & for loops. In order to keep it simple, this patch contains the change to send sequential MessageID only when the following arguments are specified: --initial-message-id <n> --vary-message-size --async Otherwise, it will preserve the original 0.7 behavior.\n3. Jun Rao: Thanks for the patch. Committed to 0.7 so that we can get the system tests in 0.8 working.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.878538"}}
{"id": "c3de54c37800cbf6f341973f06c9b607", "issue_key": "SPARK-668", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors", "description": "As described in https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion, calls to JavaRDdLike.flatMap(PairFlatMapFunction) may be falsely rejected by the compiler with \"cannot find symbol; method: flatMap\" errors. Here's a complete standalone example that reproduces the problem: https://gist.github.com/4640356 I tried implementing a similar example in pure-Java (no Spark code) and was able to get the proper typechecking, so I suspect that this might be a Scala compiler bug.", "reporter": "Josh Rosen", "assignee": "Josh Rosen", "created": "2013-01-26T14:40:56.000+0000", "updated": "2014-11-18T20:12:56.000+0000", "resolved": "2013-01-26T16:34:09.000+0000", "labels": [], "components": ["Java API"], "comments": [{"author": "Josh Rosen", "body": "Fixed in https://github.com/mesos/spark/pull/417", "created": "2013-01-26T16:34:09.631+0000"}, {"author": "Josh Rosen", "body": "For reference, I think that this was caused by https://issues.scala-lang.org/browse/SI-6057", "created": "2014-11-18T20:12:56.799+0000"}], "num_comments": 2, "text": "Issue: SPARK-668\nSummary: JavaRDDLike.flatMap(PairFlatMapFunction) may fail with typechecking errors\nDescription: As described in https://groups.google.com/d/topic/spark-users/KrVIf-DHg60/discussion, calls to JavaRDdLike.flatMap(PairFlatMapFunction) may be falsely rejected by the compiler with \"cannot find symbol; method: flatMap\" errors. Here's a complete standalone example that reproduces the problem: https://gist.github.com/4640356 I tried implementing a similar example in pure-Java (no Spark code) and was able to get the proper typechecking, so I suspect that this might be a Scala compiler bug.\n\nComments (2):\n1. Josh Rosen: Fixed in https://github.com/mesos/spark/pull/417\n2. Josh Rosen: For reference, I think that this was caused by https://issues.scala-lang.org/browse/SI-6057", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.026564"}}
{"id": "5e97fe2bd93912f410e70c9e3e49fbb5", "issue_key": "KAFKA-419", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Updated PHP client library to support kafka 0.7+", "description": "I updated the PHP client library to fully work with the kafka code in trunk, with the following improvements: - added support for GZIP compression (both in the producer and the consumer); - implemented stream-based iteration of messages (rather than loading the entire response in memory); - added base Kafka_Exception class (extending RuntimeException) and some child classes to catch specific errors; - added validation and error checking in several parts of the code (like CRC validation of messages, stream connection/EOF checks, response code verification); - added example producer for compressed messages. The code is fully tested (vast improvements to the existing test suite are provided with the patch), and already incorporates a proper fix for KAFKA-319 (which can be rejected). The API hasn't changed and is 100% backwards compatible.", "reporter": "Lorenzo Alberton", "assignee": null, "created": "2012-07-26T16:20:03.000+0000", "updated": "2013-04-04T20:42:12.000+0000", "resolved": "2013-04-04T20:42:03.000+0000", "labels": ["php", "phpunit"], "components": ["clients"], "comments": [{"author": "Lorenzo Alberton", "body": "Patch to update the PHP client library to fully work with the kafka code in trunk", "created": "2012-07-26T16:22:16.117+0000"}, {"author": "Neha Narkhede", "body": "Can't really see the patch here. Could you please upload it using the 'Attach files' option ?", "created": "2012-07-26T16:30:00.644+0000"}, {"author": "Lorenzo Alberton", "body": "Uploading patch again.", "created": "2012-07-26T17:02:49.180+0000"}, {"author": "Lorenzo Alberton", "body": "New patch, supercedes phpclient.patch", "created": "2012-09-15T15:17:27.366+0000"}, {"author": "Lorenzo Alberton", "body": "I updated the PHP client library again to support additional functionality and make it more robust. The new patch (phpclient.new.patch) completely replaces the old one (phpclient.patch). List of improvements on the previous patch: - completely refactored socket handling, to be more robust, with better error checking and handling of edge-cases. - added support for 64bit offsets - better checks for responses from Kafka (fixed connection hanging) - added Zookeeper-based consumer, with support for multiple consumer groups, and for manual offset commit action (so it's possible to wait for an ACK from the message processor before advancing the offsets), and example code - added support for OffsetRequest and getOffsetsBefore() in the SimpleConsumer class to query the state of the queue - new, more specific exception classes (socket errors, empty queue, invalid topic, etc.) - support for connection timeouts in microseconds - vastly improved test suite", "created": "2012-09-15T15:31:55.523+0000"}, {"author": "Lorenzo Alberton", "body": "Replaces phpclient.patch", "created": "2012-09-16T18:12:32.829+0000"}, {"author": "Colin B.", "body": "Old bug. Client libraries, such as this PHP library, are no longer being maintained in the main project. Please close. Won't Fix.", "created": "2013-04-04T19:32:43.217+0000"}], "num_comments": 7, "text": "Issue: KAFKA-419\nSummary: Updated PHP client library to support kafka 0.7+\nDescription: I updated the PHP client library to fully work with the kafka code in trunk, with the following improvements: - added support for GZIP compression (both in the producer and the consumer); - implemented stream-based iteration of messages (rather than loading the entire response in memory); - added base Kafka_Exception class (extending RuntimeException) and some child classes to catch specific errors; - added validation and error checking in several parts of the code (like CRC validation of messages, stream connection/EOF checks, response code verification); - added example producer for compressed messages. The code is fully tested (vast improvements to the existing test suite are provided with the patch), and already incorporates a proper fix for KAFKA-319 (which can be rejected). The API hasn't changed and is 100% backwards compatible.\n\nComments (7):\n1. Lorenzo Alberton: Patch to update the PHP client library to fully work with the kafka code in trunk\n2. Neha Narkhede: Can't really see the patch here. Could you please upload it using the 'Attach files' option ?\n3. Lorenzo Alberton: Uploading patch again.\n4. Lorenzo Alberton: New patch, supercedes phpclient.patch\n5. Lorenzo Alberton: I updated the PHP client library again to support additional functionality and make it more robust. The new patch (phpclient.new.patch) completely replaces the old one (phpclient.patch). List of improvements on the previous patch: - completely refactored socket handling, to be more robust, with better error checking and handling of edge-cases. - added support for 64bit offsets - better checks for responses from Kafka (fixed connection hanging) - added Zookeeper-based consumer, with support for multiple consumer groups, and for manual offset commit action (so it's possible to wait for an ACK from the message processor before advancing the offsets), and example code - added support for OffsetRequest and getOffsetsBefore() in the SimpleConsumer class to query the state of the queue - new, more specific exception classes (socket errors, empty queue, invalid topic, etc.) - support for connection timeouts in microseconds - vastly improved test suite\n6. Lorenzo Alberton: Replaces phpclient.patch\n7. Colin B.: Old bug. Client libraries, such as this PHP library, are no longer being maintained in the main project. Please close. Won't Fix.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.653140"}}
{"id": "b88a797c59560b94be4ae1bf01de5ad5", "issue_key": "KAFKA-380", "issue_type": "Task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Enhance single_host_multi_brokers test with failure to trigger leader re-election in replication", "description": "", "reporter": "John Fung", "assignee": "John Fung", "created": "2012-06-28T21:33:44.000+0000", "updated": "2012-08-06T17:28:12.000+0000", "resolved": "2012-08-06T17:28:12.000+0000", "labels": [], "components": [], "comments": [{"author": "John Fung", "body": "Uploaded kafka-380-v1.patch with the following changes: A. Introduce failure to the brokers in a round robin fashion (as described in README): 1. Start the Kafka cluster 2. Create topic 3. Find the leader 4. Stop the broker in Step 3 5. Send n messages 6. Consume the messages 7. Start the broker in Step 3 8. Goto Step 3 for all servers in the cluster 9. Validate test results B. Keep track of the leader re-election latency (the difference between the broker shutdown and leader re-elected timestamp). C. Report the max and min latency values", "created": "2012-06-28T21:58:22.855+0000"}, {"author": "Joel Jacob Koshy", "body": "- Looks good overall, but will revisit after KAFKA-350 is done. - Instead of sleeping 30s after shutting down a server, pgrep for it. - We should get rid of as many sleeps as possible. E.g., with producer acks we don't need to sleep after producer-performance. Likewise for all other sleeps, let see why they are needed and provide tooling (if necessary) to eliminate/reduce them.", "created": "2012-07-03T21:32:42.816+0000"}, {"author": "Joel Jacob Koshy", "body": "Filed KAFKA-392 which includes the two points above.", "created": "2012-07-03T21:53:28.218+0000"}, {"author": "John Fung", "body": "Thanks Joel for reviewing kafka-380-v1.patch. I have uploaded kafka-380-v2.patch (not the changes you suggested for KAFKA-392) with additional minor changes: The changes made in kafka-380-v2.patch is to fix the problem to run this test in MacOS. The problem is due to the different argument options syntax between Linux and Darwin (MacOS) for the shell command \"date\".", "created": "2012-07-09T19:35:24.173+0000"}, {"author": "Jun Rao", "body": "Patch v2 doesn't apply after kafka-306 is committed. Could you rebase?", "created": "2012-07-10T18:12:28.892+0000"}, {"author": "John Fung", "body": "Thanks Jun. Uploaded kafka-380-v3.patch with the following changes: 1. Rebased from 0.8 branch after KAFKA-306 is checked in. 2. Fixed running issue in MacOS", "created": "2012-07-10T22:59:53.095+0000"}, {"author": "Jun Rao", "body": "Thanks for patch v3. Shouldn't we set invoke_failures to true by default? However, when I do that, the test seems to hang after the following: 2012-07-11 09:57:54 --------------------------------------- 2012-07-11 09:57:54 leader re-election latency : 36 ms 2012-07-11 09:57:54 --------------------------------------- 2012-07-11 09:57:54 starting console consumer 2012-07-11 09:57:54 sleeping for 5s 2012-07-11 09:57:59 starting producer performance", "created": "2012-07-11T17:01:12.491+0000"}, {"author": "John Fung", "body": "Thanks Jun for reviewing. The test is hanging due to the following exception thrown by ProducerPerformance: [2012-07-10 15:38:31,510] INFO Beging shutting down ProducerSendThread (kafka.producer.async.ProducerSendThread) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,495] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread) java.net.SocketTimeoutException at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:201) at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:86) at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:221) at kafka.utils.Utils$.read(Utils.scala:603) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.network.BlockingChannel.receive(BlockingChannel.scala:92) at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:78) at kafka.producer.SyncProducer.doSend(SyncProducer.scala:76) at kafka.producer.SyncProducer.send(SyncProducer.scala:115) at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:76) at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:45) at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:129) at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:95) at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:94) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61) at scala.collection.immutable.List.foreach(List.scala:45) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:44) at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:42) at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:94) at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:65) at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:49) at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:96) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:82) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:60) at scala.collection.immutable.Stream.foreach(Stream.scala:254) at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:59) at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:37)", "created": "2012-07-11T20:11:35.752+0000"}, {"author": "John Fung", "body": "Uploaded kafka-380-v4.patch with the following changes: 1. Rebased with the latest 0.8 branch 2. Fixed an issue running in MacOS 3. Added validation for re-election latency", "created": "2012-07-25T16:58:54.528+0000"}, {"author": "Neha Narkhede", "body": "1. We can probably get rid of the following information from the test output - 2012-07-26 09:21:47 server_shutdown_unix_timestamp : 1343319697 2012-07-26 09:21:47 server_shutdown_unix_timestamp_ms : 743 2012-07-26 09:21:47 elected_leader_unix_timestamp : 1343319697 2012-07-26 09:21:47 elected_leader_unix_timestamp_ms : 794 2012-07-26 09:21:47 full_elected_leader_unix_timestamp : 1343319697.794 2012-07-26 09:21:47 full_server_shutdown_unix_timestamp : 1343319697.743 2. Can you make the test output also get logged to some file called test-output.log ? This will be helpful for debugging. Other than that, this patch looks good.", "created": "2012-07-26T16:28:08.177+0000"}, {"author": "John Fung", "body": "Hi Neha, Thanks for reviewing. Uploaded kafka-380-v5.patch which has the changes suggested.", "created": "2012-07-27T20:38:43.244+0000"}, {"author": "Neha Narkhede", "body": "+1.", "created": "2012-08-06T17:27:55.680+0000"}, {"author": "Neha Narkhede", "body": "Thanks for the patch John !", "created": "2012-08-06T17:28:12.711+0000"}], "num_comments": 13, "text": "Issue: KAFKA-380\nSummary: Enhance single_host_multi_brokers test with failure to trigger leader re-election in replication\n\nComments (13):\n1. John Fung: Uploaded kafka-380-v1.patch with the following changes: A. Introduce failure to the brokers in a round robin fashion (as described in README): 1. Start the Kafka cluster 2. Create topic 3. Find the leader 4. Stop the broker in Step 3 5. Send n messages 6. Consume the messages 7. Start the broker in Step 3 8. Goto Step 3 for all servers in the cluster 9. Validate test results B. Keep track of the leader re-election latency (the difference between the broker shutdown and leader re-elected timestamp). C. Report the max and min latency values\n2. Joel Jacob Koshy: - Looks good overall, but will revisit after KAFKA-350 is done. - Instead of sleeping 30s after shutting down a server, pgrep for it. - We should get rid of as many sleeps as possible. E.g., with producer acks we don't need to sleep after producer-performance. Likewise for all other sleeps, let see why they are needed and provide tooling (if necessary) to eliminate/reduce them.\n3. Joel Jacob Koshy: Filed KAFKA-392 which includes the two points above.\n4. John Fung: Thanks Joel for reviewing kafka-380-v1.patch. I have uploaded kafka-380-v2.patch (not the changes you suggested for KAFKA-392) with additional minor changes: The changes made in kafka-380-v2.patch is to fix the problem to run this test in MacOS. The problem is due to the different argument options syntax between Linux and Darwin (MacOS) for the shell command \"date\".\n5. Jun Rao: Patch v2 doesn't apply after kafka-306 is committed. Could you rebase?\n6. John Fung: Thanks Jun. Uploaded kafka-380-v3.patch with the following changes: 1. Rebased from 0.8 branch after KAFKA-306 is checked in. 2. Fixed running issue in MacOS\n7. Jun Rao: Thanks for patch v3. Shouldn't we set invoke_failures to true by default? However, when I do that, the test seems to hang after the following: 2012-07-11 09:57:54 --------------------------------------- 2012-07-11 09:57:54 leader re-election latency : 36 ms 2012-07-11 09:57:54 --------------------------------------- 2012-07-11 09:57:54 starting console consumer 2012-07-11 09:57:54 sleeping for 5s 2012-07-11 09:57:59 starting producer performance\n8. John Fung: Thanks Jun for reviewing. The test is hanging due to the following exception thrown by ProducerPerformance: [2012-07-10 15:38:31,510] INFO Beging shutting down ProducerSendThread (kafka.producer.async.ProducerSendThread) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,491] INFO Disconnecting from 127.0.0.1:9093 (kafka.producer.SyncProducer) [2012-07-10 15:39:01,495] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread) java.net.SocketTimeoutException at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:201) at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:86) at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:221) at kafka.utils.Utils$.read(Utils.scala:603) at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54) at kafka.network.Receive$class.readCompletely(Transmission.scala:55) at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) at kafka.network.BlockingChannel.receive(BlockingChannel.scala:92) at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:78) at kafka.producer.SyncProducer.doSend(SyncProducer.scala:76) at kafka.producer.SyncProducer.send(SyncProducer.scala:115) at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:76) at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:45) at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:129) at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:95) at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:94) at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61) at scala.collection.immutable.List.foreach(List.scala:45) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:44) at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:42) at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:94) at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:65) at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:49) at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:96) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:82) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:60) at scala.collection.immutable.Stream.foreach(Stream.scala:254) at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:59) at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:37)\n9. John Fung: Uploaded kafka-380-v4.patch with the following changes: 1. Rebased with the latest 0.8 branch 2. Fixed an issue running in MacOS 3. Added validation for re-election latency\n10. Neha Narkhede: 1. We can probably get rid of the following information from the test output - 2012-07-26 09:21:47 server_shutdown_unix_timestamp : 1343319697 2012-07-26 09:21:47 server_shutdown_unix_timestamp_ms : 743 2012-07-26 09:21:47 elected_leader_unix_timestamp : 1343319697 2012-07-26 09:21:47 elected_leader_unix_timestamp_ms : 794 2012-07-26 09:21:47 full_elected_leader_unix_timestamp : 1343319697.794 2012-07-26 09:21:47 full_server_shutdown_unix_timestamp : 1343319697.743 2. Can you make the test output also get logged to some file called test-output.log ? This will be helpful for debugging. Other than that, this patch looks good.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.609433"}}
{"id": "6314428fcbd94b4e0c445ffd60509fbf", "issue_key": "SPARK-842", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Maven assembly is including examples libs and dependencies", "description": "According to this [email exchange|http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201307.mbox/%3C62DB7E7A-0547-4090-BB9A-0182829A0D19%40gmail.com%3E] final assembly has to include \"...libraries that users' client programs need to run. These are core, repl (needed if they use the shell), and likely bagel and streaming and mllib,\" Hence, current Maven assembly needs to be fixed accordinly to exclude examples/. This fix will also affect BIGTOP-715.", "reporter": "Konstantin I Boudnik", "assignee": "Matei Alexandru Zaharia", "created": "2013-07-31T14:32:51.000+0000", "updated": "2013-10-10T18:05:02.000+0000", "resolved": "2013-10-10T18:05:02.000+0000", "labels": [], "components": ["Build"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-842\nSummary: Maven assembly is including examples libs and dependencies\nDescription: According to this [email exchange|http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201307.mbox/%3C62DB7E7A-0547-4090-BB9A-0182829A0D19%40gmail.com%3E] final assembly has to include \"...libraries that users' client programs need to run. These are core, repl (needed if they use the shell), and likely bagel and streaming and mllib,\" Hence, current Maven assembly needs to be fixed accordinly to exclude examples/. This fix will also affect BIGTOP-715.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "6d11fbe933a0d0aeab82741aac6fe82d", "issue_key": "KAFKA-397", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "kafka.common.InvalidMessageSizeException: null", "description": "I've just gotten the following error while running the zookeeper consumer. I made a backup of the kafka log directory and wiped the logs. I restarting kafka and the consumer. After processing a few hundred messages successfully I got the same error again. I restarted the consumer again and got the same error immediately. I wiped the logs yet again and reproduced the error again. I will attach the logs from this final run. I'm running Kafka 0.7.1 I have the following message size configurations: producer config: max.message.size: 1000000 consumer config: fetch.size: 2072000 Does it matter that I'm only wiping the logs and not wiping the zookeeper offsets? 2012-07-10 02:31:21,998 ERROR [Consumer1] c.k.h.c.k.KafkaConsumerServiceWorker: Failed to get next student event kafka.common.InvalidMessageSizeException: null at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.6.0_30] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) ~[na:1.6.0_30] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) ~[na:1.6.0_30] at java.lang.reflect.Constructor.newInstance(Constructor.java:513) ~[na:1.6.0_30] at java.lang.Class.newInstance0(Class.java:355) ~[na:1.6.0_30] at java.lang.Class.newInstance(Class.java:308) ~[na:1.6.0_30] at kafka.common.ErrorMapping$.maybeThrowException(ErrorMapping.scala:53) ~[KPIP-0.4.birdy.jar:na] at kafka.message.ByteBufferMessageSet.kafka$message$ByteBufferMessageSet$$internalIterator(ByteBufferMessageSet.scala:99) ~[KPIP-0.4.birdy.jar:na] at kafka.message.ByteBufferMessageSet.iterator(ByteBufferMessageSet.scala:82) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:81) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:32) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:36) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:43) ~[KPIP-0.4.birdy.jar:na] at java.lang.Thread.run(Thread.java:662) [na:1.6.0_30] 2012-07-10 02:31:21,998 ERROR [Consumer1] c.k.h.c.k.KafkaConsumerServiceWorker: Iterator got into bad state. Thread exiting java.lang.IllegalStateException: Iterator is in failed state at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:47) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:36) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:43) ~[KPIP-0.4.birdy.jar:na] at java.lang.Thread.run(Thread.java:662) [na:1.6.0_30]", "reporter": "David Siegel", "assignee": null, "created": "2012-07-10T17:21:39.000+0000", "updated": "2013-07-12T14:29:47.000+0000", "resolved": "2013-07-12T14:29:47.000+0000", "labels": [], "components": [], "comments": [{"author": "David Siegel", "body": "When we set the number of partitions per topic to 100 and set the consumer to use 100 threads, we don't see this error. When we set the number of partitions per topic to 100 and set the consumers to use 2 threads, we see the error consistently.", "created": "2012-07-10T18:25:22.458+0000"}, {"author": "Jun Rao", "body": "Yes, if you wipe off the log. You need to clean up offsets in ZK too. Could you also check if the broker has any error log with the following text? \"error when processing request \"", "created": "2012-07-10T18:29:50.359+0000"}, {"author": "David Siegel", "body": "2012-07-10T18:07:27.27635 [2012-07-10 18:07:27,275] ERROR error when processing request FetchRequest(topic:tmp, part:0 offset:2411 maxSize:2072000) (kafka.server .KafkaRequestHandlers) 2012-07-10T18:07:27.27638 kafka.common.OffsetOutOfRangeException: offset 2411 is out of range 2012-07-10T18:07:27.27639 at kafka.log.Log$.findRange(Log.scala:46) 2012-07-10T18:07:27.27640 at kafka.log.Log.read(Log.scala:247) 2012-07-10T18:07:27.27640 at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$readMessageSet(KafkaRequestHandlers.scala:108) 2012-07-10T18:07:27.27641 at kafka.server.KafkaRequestHandlers$$anonfun$2.apply(KafkaRequestHandlers.scala:97) 2012-07-10T18:07:27.27641 at kafka.server.KafkaRequestHandlers$$anonfun$2.apply(KafkaRequestHandlers.scala:96) 2012-07-10T18:07:27.27642 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 2012-07-10T18:07:27.27642 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 2012-07-10T18:07:27.27643 at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) 2012-07-10T18:07:27.27644 at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34) 2012-07-10T18:07:27.27644 at scala.collection.TraversableLike$class.map(TraversableLike.scala:206) 2012-07-10T18:07:27.27645 at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34) 2012-07-10T18:07:27.27646 at kafka.server.KafkaRequestHandlers.handleMultiFetchRequest(KafkaRequestHandlers.scala:96) 2012-07-10T18:07:27.27647 at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$3.apply(KafkaRequestHandlers.scala:40) 2012-07-10T18:07:27.27648 at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$3.apply(KafkaRequestHandlers.scala:40) 2012-07-10T18:07:27.27648 at kafka.network.Processor.handle(SocketServer.scala:296) 2012-07-10T18:07:27.27649 at kafka.network.Processor.read(SocketServer.scala:319) 2012-07-10T18:07:27.27649 at kafka.network.Processor.run(SocketServer.scala:214) 2012-07-10T18:07:27.27650 at java.lang.Thread.run(Thread.java:662)", "created": "2012-07-10T18:44:40.168+0000"}, {"author": "David Arthur", "body": "Try cleaning out ZooKeeper as well as the Kafka logs (as Jun suggested). Based on your exception, I'd guess that your consumer is using the last offset stored in ZooKeeper which exceeds the current size of your logs. I have seen exceptions like this when screwing around with the log files.", "created": "2012-10-29T20:00:07.018+0000"}, {"author": "Jay Kreps", "body": "So is there anything we should be fixing here?", "created": "2013-07-11T22:27:36.955+0000"}, {"author": "David Siegel", "body": "I'm pretty sure this was my fault not clearing zookeeper properly.", "created": "2013-07-12T14:29:05.890+0000"}], "num_comments": 6, "text": "Issue: KAFKA-397\nSummary: kafka.common.InvalidMessageSizeException: null\nDescription: I've just gotten the following error while running the zookeeper consumer. I made a backup of the kafka log directory and wiped the logs. I restarting kafka and the consumer. After processing a few hundred messages successfully I got the same error again. I restarted the consumer again and got the same error immediately. I wiped the logs yet again and reproduced the error again. I will attach the logs from this final run. I'm running Kafka 0.7.1 I have the following message size configurations: producer config: max.message.size: 1000000 consumer config: fetch.size: 2072000 Does it matter that I'm only wiping the logs and not wiping the zookeeper offsets? 2012-07-10 02:31:21,998 ERROR [Consumer1] c.k.h.c.k.KafkaConsumerServiceWorker: Failed to get next student event kafka.common.InvalidMessageSizeException: null at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.6.0_30] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) ~[na:1.6.0_30] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) ~[na:1.6.0_30] at java.lang.reflect.Constructor.newInstance(Constructor.java:513) ~[na:1.6.0_30] at java.lang.Class.newInstance0(Class.java:355) ~[na:1.6.0_30] at java.lang.Class.newInstance(Class.java:308) ~[na:1.6.0_30] at kafka.common.ErrorMapping$.maybeThrowException(ErrorMapping.scala:53) ~[KPIP-0.4.birdy.jar:na] at kafka.message.ByteBufferMessageSet.kafka$message$ByteBufferMessageSet$$internalIterator(ByteBufferMessageSet.scala:99) ~[KPIP-0.4.birdy.jar:na] at kafka.message.ByteBufferMessageSet.iterator(ByteBufferMessageSet.scala:82) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:81) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:32) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:36) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:43) ~[KPIP-0.4.birdy.jar:na] at java.lang.Thread.run(Thread.java:662) [na:1.6.0_30] 2012-07-10 02:31:21,998 ERROR [Consumer1] c.k.h.c.k.KafkaConsumerServiceWorker: Iterator got into bad state. Thread exiting java.lang.IllegalStateException: Iterator is in failed state at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:47) ~[KPIP-0.4.birdy.jar:na] at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:36) ~[KPIP-0.4.birdy.jar:na] at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:43) ~[KPIP-0.4.birdy.jar:na] at java.lang.Thread.run(Thread.java:662) [na:1.6.0_30]\n\nComments (6):\n1. David Siegel: When we set the number of partitions per topic to 100 and set the consumer to use 100 threads, we don't see this error. When we set the number of partitions per topic to 100 and set the consumers to use 2 threads, we see the error consistently.\n2. Jun Rao: Yes, if you wipe off the log. You need to clean up offsets in ZK too. Could you also check if the broker has any error log with the following text? \"error when processing request \"\n3. David Siegel: 2012-07-10T18:07:27.27635 [2012-07-10 18:07:27,275] ERROR error when processing request FetchRequest(topic:tmp, part:0 offset:2411 maxSize:2072000) (kafka.server .KafkaRequestHandlers) 2012-07-10T18:07:27.27638 kafka.common.OffsetOutOfRangeException: offset 2411 is out of range 2012-07-10T18:07:27.27639 at kafka.log.Log$.findRange(Log.scala:46) 2012-07-10T18:07:27.27640 at kafka.log.Log.read(Log.scala:247) 2012-07-10T18:07:27.27640 at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$readMessageSet(KafkaRequestHandlers.scala:108) 2012-07-10T18:07:27.27641 at kafka.server.KafkaRequestHandlers$$anonfun$2.apply(KafkaRequestHandlers.scala:97) 2012-07-10T18:07:27.27641 at kafka.server.KafkaRequestHandlers$$anonfun$2.apply(KafkaRequestHandlers.scala:96) 2012-07-10T18:07:27.27642 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 2012-07-10T18:07:27.27642 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 2012-07-10T18:07:27.27643 at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) 2012-07-10T18:07:27.27644 at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34) 2012-07-10T18:07:27.27644 at scala.collection.TraversableLike$class.map(TraversableLike.scala:206) 2012-07-10T18:07:27.27645 at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34) 2012-07-10T18:07:27.27646 at kafka.server.KafkaRequestHandlers.handleMultiFetchRequest(KafkaRequestHandlers.scala:96) 2012-07-10T18:07:27.27647 at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$3.apply(KafkaRequestHandlers.scala:40) 2012-07-10T18:07:27.27648 at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$3.apply(KafkaRequestHandlers.scala:40) 2012-07-10T18:07:27.27648 at kafka.network.Processor.handle(SocketServer.scala:296) 2012-07-10T18:07:27.27649 at kafka.network.Processor.read(SocketServer.scala:319) 2012-07-10T18:07:27.27649 at kafka.network.Processor.run(SocketServer.scala:214) 2012-07-10T18:07:27.27650 at java.lang.Thread.run(Thread.java:662)\n4. David Arthur: Try cleaning out ZooKeeper as well as the Kafka logs (as Jun suggested). Based on your exception, I'd guess that your consumer is using the last offset stored in ZooKeeper which exceeds the current size of your logs. I have seen exceptions like this when screwing around with the log files.\n5. Jay Kreps: So is there anything we should be fixing here?\n6. David Siegel: I'm pretty sure this was my fault not clearing zookeeper properly.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.631291"}}
{"id": "9944b1cbb88f234cb1eca27ab36a0cf5", "issue_key": "SPARK-760", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Include a simple PageRank example in Spark", "description": "Lots of people ask about it. It would be nice to have one in Scala, Java and Python.", "reporter": "Matei Alexandru Zaharia", "assignee": "Chu Tong", "created": "2013-06-05T11:20:33.000+0000", "updated": "2013-08-12T21:27:28.000+0000", "resolved": "2013-08-12T21:27:28.000+0000", "labels": ["Starter"], "components": [], "comments": [{"author": "Nicholas Pentreath", "body": "Is anyone looking at this? I can try to take a crack at it (seems Spark examples are my thing :-)", "created": "2013-06-07T02:08:46.698+0000"}, {"author": "Chu Tong", "body": "Hi Nick, are you still working on this?", "created": "2013-07-28T23:41:58.446+0000"}, {"author": "Nicholas Pentreath", "body": "Hi, yes I still intend to to put something together, I just haven't had a chance just yet.", "created": "2013-07-30T22:32:35.645+0000"}, {"author": "Chu Tong", "body": "Why don't you do Scala and I do Java then?", "created": "2013-07-30T22:35:07.747+0000"}, {"author": "Nicholas Pentreath", "body": "Fair enough - looks like there is a quite simple version here in Scala that I will adapt: http://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-part-2-amp-camp-2012-standalone-programs.pdf Could also be used as a base for the Java version", "created": "2013-07-30T23:21:49.091+0000"}, {"author": "Chu Tong", "body": "thank you Nick.", "created": "2013-07-30T23:24:09.840+0000"}, {"author": "Nicholas Pentreath", "body": "Also Scalding has some PageRank examples that should be fairly straightforward to port over: https://github.com/twitter/scalding/tree/develop/scalding-core/src/main/scala/com/twitter/scalding/examples", "created": "2013-07-30T23:28:07.427+0000"}, {"author": "Chu Tong", "body": "Java version of PageRank is merged and I am working on the Python part.", "created": "2013-08-06T15:26:04.071+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Cool, a Python version would be great.", "created": "2013-08-06T17:07:29.979+0000"}, {"author": "Nicholas Pentreath", "body": "Added the Scala version (based on the Java version PR): https://github.com/mesos/spark/pull/789", "created": "2013-08-07T07:42:45.640+0000"}, {"author": "Chu Tong", "body": "Added Python version at: https://github.com/mesos/spark/pull/802", "created": "2013-08-10T23:24:39.106+0000"}], "num_comments": 11, "text": "Issue: SPARK-760\nSummary: Include a simple PageRank example in Spark\nDescription: Lots of people ask about it. It would be nice to have one in Scala, Java and Python.\n\nComments (11):\n1. Nicholas Pentreath: Is anyone looking at this? I can try to take a crack at it (seems Spark examples are my thing :-)\n2. Chu Tong: Hi Nick, are you still working on this?\n3. Nicholas Pentreath: Hi, yes I still intend to to put something together, I just haven't had a chance just yet.\n4. Chu Tong: Why don't you do Scala and I do Java then?\n5. Nicholas Pentreath: Fair enough - looks like there is a quite simple version here in Scala that I will adapt: http://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-part-2-amp-camp-2012-standalone-programs.pdf Could also be used as a base for the Java version\n6. Chu Tong: thank you Nick.\n7. Nicholas Pentreath: Also Scalding has some PageRank examples that should be fairly straightforward to port over: https://github.com/twitter/scalding/tree/develop/scalding-core/src/main/scala/com/twitter/scalding/examples\n8. Chu Tong: Java version of PageRank is merged and I am working on the Python part.\n9. Matei Alexandru Zaharia: Cool, a Python version would be great.\n10. Nicholas Pentreath: Added the Scala version (based on the Java version PR): https://github.com/mesos/spark/pull/789", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.050736"}}
{"id": "1e871fa4266d2a68aadfd7f05279700e", "issue_key": "SPARK-698", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Spark Standalone Mode is leaving a java process \"spark.executor.StandaloneExecutorBackend\" open on Windows", "description": "The java process runnig \"spark.executor.StandaloneExecutorBackend\" fails to end after a task is finished. Under Max OS X and Unix, there is a single shell script \"run\" to start Spark master, worker, and executor. Under Windows, there is a cascade: \"run.cmd\" calls \"run2.cmd\" which calls java. So when the spark.deploy.worker.ExecutorRunner (which runs in the worker process) wants to kill the executor process via process.destroy(), it actually only kills the process of \"run.cmd\", and the process of \"run2.cmd\" (=> java running the executor) stays alive. See this thread on spark-users for all details: https://groups.google.com/forum/#!topic/spark-users/NrdhVlrUDtU/discussion", "reporter": "Christoph Grothaus", "assignee": "Christoph Grothaus", "created": "2013-02-13T02:11:31.000+0000", "updated": "2013-06-25T19:15:03.000+0000", "resolved": "2013-06-25T19:15:03.000+0000", "labels": [], "components": ["Deploy"], "comments": [{"author": "Christoph Grothaus", "body": "Did some internet search on this: there is a bug open at Oracle: \"4770092 : (process) Process.destroy does not kill multiple child processes\" http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4770092 It affects Windows platforms. All solutions discussed on StackOverflow that I found involve JNA, like this one: http://stackoverflow.com/questions/4912282/java-tool-method-to-force-kill-a-child-process/6032734#6032734 Another suggestion: maybe it is possible to send a stop message to the StandaloneExecutorBackend, so that it can stop the actor system and initiate its own shutdown properly.", "created": "2013-02-14T08:14:42.710+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Thanks for looking into it. The stop message would work in many cases, but unfortunately it would fail if the StandaloneExecutorBackend somehow freezes. Another option I'd consider is to execute the Java child process directly by running {{java}} instead of {{run.cmd}}. We would need to replicate the code for setting environment variables and classpaths that's there, but it shouldn't be too bad (in fact we can consider exporting the same variables that were used to launch the Worker).", "created": "2013-02-14T23:31:41.879+0000"}, {"author": "Christoph Grothaus", "body": "Well, that would indeed work. Maybe it's the best solution in the face of the aforementioned Bug on Windows.", "created": "2013-02-14T23:34:58.843+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Merged Christoph's commit to fix this in 0.8.", "created": "2013-06-25T19:15:03.115+0000"}], "num_comments": 4, "text": "Issue: SPARK-698\nSummary: Spark Standalone Mode is leaving a java process \"spark.executor.StandaloneExecutorBackend\" open on Windows\nDescription: The java process runnig \"spark.executor.StandaloneExecutorBackend\" fails to end after a task is finished. Under Max OS X and Unix, there is a single shell script \"run\" to start Spark master, worker, and executor. Under Windows, there is a cascade: \"run.cmd\" calls \"run2.cmd\" which calls java. So when the spark.deploy.worker.ExecutorRunner (which runs in the worker process) wants to kill the executor process via process.destroy(), it actually only kills the process of \"run.cmd\", and the process of \"run2.cmd\" (=> java running the executor) stays alive. See this thread on spark-users for all details: https://groups.google.com/forum/#!topic/spark-users/NrdhVlrUDtU/discussion\n\nComments (4):\n1. Christoph Grothaus: Did some internet search on this: there is a bug open at Oracle: \"4770092 : (process) Process.destroy does not kill multiple child processes\" http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4770092 It affects Windows platforms. All solutions discussed on StackOverflow that I found involve JNA, like this one: http://stackoverflow.com/questions/4912282/java-tool-method-to-force-kill-a-child-process/6032734#6032734 Another suggestion: maybe it is possible to send a stop message to the StandaloneExecutorBackend, so that it can stop the actor system and initiate its own shutdown properly.\n2. Matei Alexandru Zaharia: Thanks for looking into it. The stop message would work in many cases, but unfortunately it would fail if the StandaloneExecutorBackend somehow freezes. Another option I'd consider is to execute the Java child process directly by running {{java}} instead of {{run.cmd}}. We would need to replicate the code for setting environment variables and classpaths that's there, but it shouldn't be too bad (in fact we can consider exporting the same variables that were used to launch the Worker).\n3. Christoph Grothaus: Well, that would indeed work. Maybe it's the best solution in the face of the aforementioned Bug on Windows.\n4. Matei Alexandru Zaharia: Merged Christoph's commit to fix this in 0.8.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.028998"}}
{"id": "dae94e76f76010af306f75ba58999c7a", "issue_key": "HADOOP-511", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "mapred.reduce.tasks not used", "description": "After some testing with nutch I found problem with ArithmeticException in partition function because numReduceTasks came in 0. After seting mapred.reduce.tasks in hadoop-site.xml to same value as default everything works. This bug disappear in SVN version of hadoop I this known issue. Please check also NUTCH-361 for detail explanation.", "reporter": "Uros Gruber", "assignee": "Owen O'Malley", "created": "2006-09-06T17:36:06.000+0000", "updated": "2009-07-08T16:51:56.000+0000", "resolved": "2007-05-24T06:42:07.000+0000", "labels": [], "components": [], "comments": [{"author": "Owen O'Malley", "body": "This was mostly misconfiguration by having the site file override the job's wishes. Note that now reduces = 0 is not an error, but rather specifying that you don't need a sort.", "created": "2007-05-24T06:42:07.926+0000"}], "num_comments": 1, "text": "Issue: HADOOP-511\nSummary: mapred.reduce.tasks not used\nDescription: After some testing with nutch I found problem with ArithmeticException in partition function because numReduceTasks came in 0. After seting mapred.reduce.tasks in hadoop-site.xml to same value as default everything works. This bug disappear in SVN version of hadoop I this known issue. Please check also NUTCH-361 for detail explanation.\n\nComments (1):\n1. Owen O'Malley: This was mostly misconfiguration by having the site file override the job's wishes. Note that now reduces = 0 is not an error, but rather specifying that you don't need a sort.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.381405"}}
{"id": "729bb976f40fba4219dbc43939d9ae57", "issue_key": "SPARK-453", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Report exceptions in tasks back to master to simplify debugging", "description": "", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0011-10-14T09:50:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from tjhunter: +1", "created": "2011-10-16T16:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This is fixed now.", "created": "2012-06-09T13:59:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-84, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 3, "text": "Issue: SPARK-453\nSummary: Report exceptions in tasks back to master to simplify debugging\n\nComments (3):\n1. Patrick McFadin: Github comment from tjhunter: +1\n2. Patrick McFadin: Github comment from mateiz: This is fixed now.\n3. Patrick McFadin: Imported from Github issue spark-84, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.946169"}}
{"id": "8a1be4ab77d5e8963e58730eb3430a40", "issue_key": "KAFKA-178", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Simplify the brokerinfo argument to the producer-perf-test.sh", "description": "Currently: jkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh Missing required argument \"[brokerinfo]\" Option Description ------ ----------- ... --brokerinfo <broker.list=brokerid: REQUIRED: broker info (either from hostname:port or zk.connect=host: zookeeper or a list. port> ... This is kind of confusing and doesn't match the other scripts. I would like to change it to --zookeeper zk_connect_string --broker-list id1:host1:port1,id2:host2:port2,... and require that one of these be specified.", "reporter": "Jay Kreps", "assignee": null, "created": "2011-10-31T05:26:35.000+0000", "updated": "2015-02-07T23:47:12.000+0000", "resolved": "2015-02-07T23:47:12.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: KAFKA-178\nSummary: Simplify the brokerinfo argument to the producer-perf-test.sh\nDescription: Currently: jkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh Missing required argument \"[brokerinfo]\" Option Description ------ ----------- ... --brokerinfo <broker.list=brokerid: REQUIRED: broker info (either from hostname:port or zk.connect=host: zookeeper or a list. port> ... This is kind of confusing and doesn't match the other scripts. I would like to change it to --zookeeper zk_connect_string --broker-list id1:host1:port1,id2:host2:port2,... and require that one of these be specified.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.543130"}}
{"id": "3ab989e1f6b8bf9e7ded691c6b0bd708", "issue_key": "HADOOP-1099", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "NullPointerException in JobInProgress.getTaskInProgress", "description": "Saw this in the JobTracker log when a user was trying to reload a page for a running job that since finished. 2007-03-09 00:23:44,029 WARN /: /taskdetails.jsp?jobid=job_0003&taskid=tip_0003_m_001379: java.lang.NullPointerException at org.apache.hadoop.mapred.JobInProgress.getTaskInProgress(JobInProgress.java:916) at org.apache.hadoop.mapred.JobTracker.getTip(JobTracker.java:1551) at org.apache.hadoop.mapred.JobTracker.getTaskStatuses(JobTracker.java:1526) at org.apache.hadoop.mapred.taskdetails_jsp._jspService(taskdetails_jsp.java:59) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94) at javax.servlet.http.HttpServlet.service(HttpServlet.java:802) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427) at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567) at org.mortbay.http.HttpContext.handle(HttpContext.java:1565) at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635) at org.mortbay.http.HttpContext.handle(HttpContext.java:1517) at org.mortbay.http.HttpServer.service(HttpServer.java:954) at org.mortbay.http.HttpConnection.service(HttpConnection.java:814) at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981) at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831) at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244) at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357) at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)", "reporter": "Nigel Daley", "assignee": "Gautam Kowshik", "created": "2007-03-09T02:18:38.000+0000", "updated": "2009-07-08T16:52:11.000+0000", "resolved": "2007-03-13T21:12:51.000+0000", "labels": [], "components": [], "comments": [{"author": "Gautam Kowshik", "body": "jobfailures_jsp.java contructs the taskdetails.jsp link with the wrong GET params viz. taskid instead of tipid which the taskdetails.jsp expects. Changed that.", "created": "2007-03-13T18:17:15.376+0000"}, {"author": "David Bowen", "body": "+1. My mistake I think.", "created": "2007-03-13T18:30:22.959+0000"}, {"author": "Owen O'Malley", "body": "+1", "created": "2007-03-13T20:01:33.651+0000"}, {"author": "Thomas White", "body": "I've just committed this. Thanks Gautam!", "created": "2007-03-13T21:12:51.200+0000"}], "num_comments": 4, "text": "Issue: HADOOP-1099\nSummary: NullPointerException in JobInProgress.getTaskInProgress\nDescription: Saw this in the JobTracker log when a user was trying to reload a page for a running job that since finished. 2007-03-09 00:23:44,029 WARN /: /taskdetails.jsp?jobid=job_0003&taskid=tip_0003_m_001379: java.lang.NullPointerException at org.apache.hadoop.mapred.JobInProgress.getTaskInProgress(JobInProgress.java:916) at org.apache.hadoop.mapred.JobTracker.getTip(JobTracker.java:1551) at org.apache.hadoop.mapred.JobTracker.getTaskStatuses(JobTracker.java:1526) at org.apache.hadoop.mapred.taskdetails_jsp._jspService(taskdetails_jsp.java:59) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94) at javax.servlet.http.HttpServlet.service(HttpServlet.java:802) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427) at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567) at org.mortbay.http.HttpContext.handle(HttpContext.java:1565) at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635) at org.mortbay.http.HttpContext.handle(HttpContext.java:1517) at org.mortbay.http.HttpServer.service(HttpServer.java:954) at org.mortbay.http.HttpConnection.service(HttpConnection.java:814) at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981) at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831) at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244) at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357) at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)\n\nComments (4):\n1. Gautam Kowshik: jobfailures_jsp.java contructs the taskdetails.jsp link with the wrong GET params viz. taskid instead of tipid which the taskdetails.jsp expects. Changed that.\n2. David Bowen: +1. My mistake I think.\n3. Owen O'Malley: +1\n4. Thomas White: I've just committed this. Thanks Gautam!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.563478"}}
{"id": "586404afd056ee35744ae131659b50a9", "issue_key": "SPARK-1155", "issue_type": "New Feature", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Clean up and document use of SparkEnv", "description": "We should provide better documentation explaining what SparkEnv is and why it needs to be thread local (basically, to allow it to be accessed inside of closures on executors). Also, in cases where SparkEnv is being accessed on the driver we should access it through the associated SparkContext rather than through the thread local. Finally, we should see if it's possible to just remove this as a thread local and instead make it a static singleton that the exeucutor sets once. This last thing might not be possible if, under certain code paths, this is used on the driver.", "reporter": "Patrick Wendell", "assignee": "Andrew Or", "created": "2014-02-28T15:36:54.000+0000", "updated": "2016-01-05T21:18:42.000+0000", "resolved": "2016-01-05T21:18:42.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Josh Rosen", "body": "This issue is no longer relevant now that SparkEnv is no longer a thread-local.", "created": "2016-01-05T21:18:42.297+0000"}], "num_comments": 1, "text": "Issue: SPARK-1155\nSummary: Clean up and document use of SparkEnv\nDescription: We should provide better documentation explaining what SparkEnv is and why it needs to be thread local (basically, to allow it to be accessed inside of closures on executors). Also, in cases where SparkEnv is being accessed on the driver we should access it through the associated SparkContext rather than through the thread local. Finally, we should see if it's possible to just remove this as a thread local and instead make it a static singleton that the exeucutor sets once. This last thing might not be possible if, under certain code paths, this is used on the driver.\n\nComments (1):\n1. Josh Rosen: This issue is no longer relevant now that SparkEnv is no longer a thread-local.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "592312253925bd0fb5dee1023dd591c0", "issue_key": "HADOOP-208", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "add failure page to webapp", "description": "I'd like a webapp page that just lists the failures in a job so that I can find them more easily. I also want to put back the job detail page.", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "created": "2006-05-11T05:31:52.000+0000", "updated": "2009-07-08T16:51:46.000+0000", "resolved": "2006-05-13T04:16:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Owen O'Malley", "body": "This patch: 1. pulls the task tracker list off the root page and puts it on its own page 2. creates a job failure page that lists the task attempts that have failed 3. puts back a generic job page that covers both map and reduce", "created": "2006-05-11T05:36:05.000+0000"}, {"author": "Owen O'Malley", "body": "Don't commit this one yet, I messed up the links for the next/prev. I'll generate a new patch.", "created": "2006-05-11T23:46:12.000+0000"}, {"author": "Owen O'Malley", "body": "This patch replaces the other one and fixes the targets of the links for next/prev on the task lists.", "created": "2006-05-12T01:15:42.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. It looks great! Thanks, Owen!", "created": "2006-05-13T04:16:03.000+0000"}], "num_comments": 4, "text": "Issue: HADOOP-208\nSummary: add failure page to webapp\nDescription: I'd like a webapp page that just lists the failures in a job so that I can find them more easily. I also want to put back the job detail page.\n\nComments (4):\n1. Owen O'Malley: This patch: 1. pulls the task tracker list off the root page and puts it on its own page 2. creates a job failure page that lists the task attempts that have failed 3. puts back a generic job page that covers both map and reduce\n2. Owen O'Malley: Don't commit this one yet, I messed up the links for the next/prev. I'll generate a new patch.\n3. Owen O'Malley: This patch replaces the other one and fixes the targets of the links for next/prev on the task lists.\n4. Doug Cutting: I just committed this. It looks great! Thanks, Owen!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.244201"}}
{"id": "c309aea58a3a96cf2e904d8d151557c7", "issue_key": "KAFKA-512", "issue_type": "Bug", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "Remove checksum from ByteBufferMessageSet.iterator", "description": "Messages are explicitly checksumed in Log.append. But there is also a checksum computed and checked automatically in ByteBufferMessageSet.iterator as we iterate. This iterator is used quite a lot and as a result we compute this checksum 39 times on a single message produce. It turns out the default crc32 implementation in java is quite expensive so this is not good. The proposed fix is to remove the automatic checksum from the iterator and add explicit isValid() checks in the consumer as well as retaining the existing check in Log.append(). If folks are in agreement I will probably include this in the KAFKA-506 patch as that already contains a lot of ByteBufferMessageSet changes.", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "created": "2012-09-14T16:24:03.000+0000", "updated": "2012-10-09T15:44:04.000+0000", "resolved": "2012-10-09T15:43:55.000+0000", "labels": ["bugs"], "components": [], "comments": [{"author": "Jun Rao", "body": "Fixed in KAFKA-506.", "created": "2012-10-09T15:43:55.491+0000"}], "num_comments": 1, "text": "Issue: KAFKA-512\nSummary: Remove checksum from ByteBufferMessageSet.iterator\nDescription: Messages are explicitly checksumed in Log.append. But there is also a checksum computed and checked automatically in ByteBufferMessageSet.iterator as we iterate. This iterator is used quite a lot and as a result we compute this checksum 39 times on a single message produce. It turns out the default crc32 implementation in java is quite expensive so this is not good. The proposed fix is to remove the automatic checksum from the iterator and add explicit isValid() checks in the consumer as well as retaining the existing check in Log.append(). If folks are in agreement I will probably include this in the KAFKA-506 patch as that already contains a lot of ByteBufferMessageSet changes.\n\nComments (1):\n1. Jun Rao: Fixed in KAFKA-506.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.688210"}}
{"id": "e58acc83cf907d6c757301d98c2e7911", "issue_key": "SPARK-836", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ResultTask's serialization forget to handle generation", "description": "", "reporter": "Andy Huang", "assignee": null, "created": "2013-07-30T07:14:16.000+0000", "updated": "2013-12-07T14:37:30.000+0000", "resolved": "2013-12-07T14:37:30.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Aaron Davidson", "body": "Could you provide elaboration on this issue? If not, I think we'll have to close it.", "created": "2013-11-14T18:23:43.551+0000"}], "num_comments": 1, "text": "Issue: SPARK-836\nSummary: ResultTask's serialization forget to handle generation\n\nComments (1):\n1. Aaron Davidson: Could you provide elaboration on this issue? If not, I think we'll have to close it.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "f7cdcf42df41623833519958ae40bbe4", "issue_key": "KAFKA-323", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Add the ability to use the async producer in the Log4j appender", "description": "I needed the log4j appender to use the async producer, so I added a couple of configuration methods to the log4j appender. I only added methods for the configuration fields that I needed. There are several in in the various ProducerConfigs that still cannot be set in the appender. Sample use: KafkaLog4jAppender kafkaAppender = new KafkaLog4jAppender(); kafkaAppender.setZkConnect( \"localhost:2181/kafka\" ); kafkaAppender.setTopic( \"webapp\" ); kafkaAppender.setProducerType( \"async\" ); kafkaAppender.setEnqueueTimeout( Integer.toString( Integer.MIN_VALUE ) ); kafkaAppender.activateOptions();", "reporter": "Jose Quinteiro", "assignee": "Jay Kreps", "created": "2012-03-29T19:51:52.000+0000", "updated": "2012-08-14T20:36:30.000+0000", "resolved": "2012-08-14T20:36:30.000+0000", "labels": ["appender", "log4j"], "components": ["core"], "comments": [{"author": "Jose Quinteiro", "body": "Index: core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala =================================================================== --- core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala (revision 1307067) +++ core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala (working copy) @@ -22,9 +22,7 @@ import org.apache.log4j.AppenderSkeleton import org.apache.log4j.helpers.LogLog import kafka.utils.Logging -import kafka.serializer.Encoder import java.util.{Properties, Date} -import kafka.message.Message import scala.collection._ class KafkaLog4jAppender extends AppenderSkeleton with Logging { @@ -34,7 +32,11 @@ var serializerClass:String = null var zkConnect:String = null var brokerList:String = null - + var producerType:String = null + var compressionCodec:String = null + var enqueueTimeout:String = null + var queueSize:String = null + private var producer: Producer[String, String] = null def getTopic:String = topic @@ -49,6 +51,18 @@ def getSerializerClass:String = serializerClass def setSerializerClass(serializerClass:String) { this.serializerClass = serializerClass } + def getProducerType:String = producerType + def setProducerType(producerType:String) { this.producerType = producerType } + + def getCompressionCodec:String = compressionCodec + def setCompressionCodec(compressionCodec:String) { this.compressionCodec = compressionCodec } + + def getEnqueueTimeout:String = enqueueTimeout + def setEnqueueTimeout(enqueueTimeout:String) { this.enqueueTimeout = enqueueTimeout } + + def getQueueSize:String = queueSize + def setQueueSize(queueSize:String) { this.queueSize = queueSize } + override def activateOptions() { val connectDiagnostic : mutable.ListBuffer[String] = mutable.ListBuffer(); // check for config parameter validity @@ -68,6 +82,11 @@ LogLog.warn(\"Using default encoder - kafka.serializer.StringEncoder\") } props.put(\"serializer.class\", serializerClass) + //These have default values in ProducerConfig and AsyncProducerConfig. We don't care if they're not specified + if(producerType != null) props.put(\"producer.type\", producerType) + if(compressionCodec != null) props.put(\"compression.codec\", compressionCodec) + if(enqueueTimeout != null) props.put(\"queue.enqueueTimeout.ms\", enqueueTimeout) + if(queueSize != null) props.put(\"queue.size\", queueSize) val config : ProducerConfig = new ProducerConfig(props) producer = new Producer[String, String](config) LogLog.debug(\"Kafka producer connected to \" + (if(config.zkConnect == null) config.brokerList else config.zkConnect))", "created": "2012-03-29T19:53:50.209+0000"}, {"author": "Neha Narkhede", "body": "Thanks for the patch ! Would you mind attaching it as a file, and granting it to Apache ?", "created": "2012-03-29T20:02:50.814+0000"}], "num_comments": 2, "text": "Issue: KAFKA-323\nSummary: Add the ability to use the async producer in the Log4j appender\nDescription: I needed the log4j appender to use the async producer, so I added a couple of configuration methods to the log4j appender. I only added methods for the configuration fields that I needed. There are several in in the various ProducerConfigs that still cannot be set in the appender. Sample use: KafkaLog4jAppender kafkaAppender = new KafkaLog4jAppender(); kafkaAppender.setZkConnect( \"localhost:2181/kafka\" ); kafkaAppender.setTopic( \"webapp\" ); kafkaAppender.setProducerType( \"async\" ); kafkaAppender.setEnqueueTimeout( Integer.toString( Integer.MIN_VALUE ) ); kafkaAppender.activateOptions();\n\nComments (2):\n1. Jose Quinteiro: Index: core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala =================================================================== --- core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala (revision 1307067) +++ core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala (working copy) @@ -22,9 +22,7 @@ import org.apache.log4j.AppenderSkeleton import org.apache.log4j.helpers.LogLog import kafka.utils.Logging -import kafka.serializer.Encoder import java.util.{Properties, Date} -import kafka.message.Message import scala.collection._ class KafkaLog4jAppender extends AppenderSkeleton with Logging { @@ -34,7 +32,11 @@ var serializerClass:String = null var zkConnect:String = null var brokerList:String = null - + var producerType:String = null + var compressionCodec:String = null + var enqueueTimeout:String = null + var queueSize:String = null + private var producer: Producer[String, String] = null def getTopic:String = topic @@ -49,6 +51,18 @@ def getSerializerClass:String = serializerClass def setSerializerClass(serializerClass:String) { this.serializerClass = serializerClass } + def getProducerType:String = producerType + def setProducerType(producerType:String) { this.producerType = producerType } + + def getCompressionCodec:String = compressionCodec + def setCompressionCodec(compressionCodec:String) { this.compressionCodec = compressionCodec } + + def getEnqueueTimeout:String = enqueueTimeout + def setEnqueueTimeout(enqueueTimeout:String) { this.enqueueTimeout = enqueueTimeout } + + def getQueueSize:String = queueSize + def setQueueSize(queueSize:String) { this.queueSize = queueSize } + override def activateOptions() { val connectDiagnostic : mutable.ListBuffer[String] = mutable.ListBuffer(); // check for config parameter validity @@ -68,6 +82,11 @@ LogLog.warn(\"Using default encoder - kafka.serializer.StringEncoder\") } props.put(\"serializer.class\", serializerClass) + //These have default values in ProducerConfig and AsyncProducerConfig. We don't care if they're not specified + if(producerType != null) props.put(\"producer.type\", producerType) + if(compressionCodec != null) props.put(\"compression.codec\", compressionCodec) + if(enqueueTimeout != null) props.put(\"queue.enqueueTimeout.ms\", enqueueTimeout) + if(queueSize != null) props.put(\"queue.size\", queueSize) val config : ProducerConfig = new ProducerConfig(props) producer = new Producer[String, String](config) LogLog.debug(\"Kafka producer connected to \" + (if(config.zkConnect == null) config.brokerList else config.zkConnect))\n2. Neha Narkhede: Thanks for the patch ! Would you mind attaching it as a file, and granting it to Apache ?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.593720"}}
{"id": "086a92a97165f9839db4d5287a3763e2", "issue_key": "HADOOP-277", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Race condition in Configuration.getLocalPath()", "description": "(attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out ... 060605 142531 task_0001_r_000009_1 0.31808624% reduce > append > /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142725 task_0001_r_000009_1 java.io.FileNotFoundException: /data2/tmp/mapred/local/task_0001_r_000009_1/map_15.out", "reporter": "Peter Sutter", "assignee": "Sameer Paranjpye", "created": "2006-06-06T08:11:33.000+0000", "updated": "2006-08-03T17:46:45.000+0000", "resolved": "2006-06-08T02:35:46.000+0000", "labels": [], "components": [], "comments": [{"author": "Owen O'Malley", "body": "This patch is closer to what we did for the routine above it last week. (Sorry about not fixing this one too at the same time. It wasn't biting us, but that was no reason not to fix the obviously parallel code.) Is there some reason that you need the synchronized block around the mkdirs? File.mkdirs does a File.exists internally, so you don't need to call it yourself.", "created": "2006-06-06T10:11:55.000+0000"}, {"author": "Owen O'Malley", "body": "By the way, if there is a need for the sync block around the mkdirs, we should go ahead and change the function above it so that next week we don't get a third bug. *smile*", "created": "2006-06-06T11:15:07.000+0000"}, {"author": "Naveen Nalam", "body": "If we take out the sync block, the code would look like: if (fs.exists(dir)) { return file; } fs.mkdirs(dir); if (fs.exists(dir)) { return file; } we added the sync block around the mkdirs because we were not sure exactly how mkdirs() handles race cases. for example if one thread is creating the directory hierarchy, can the other thread return error while the first thread is still creating the hierarchy? if so when the second thread executes fs.exists(), it would return false since the directories are still be created. but perhaps this isn't the behavior mkdirs() exhibits. it could return false only if it can't create the final leaf directory in the directory hierarchy (because the other thread just created the leaf). if that's the case then the sync would not be needed. the sun java api doc isn't clear on this.", "created": "2006-06-06T13:07:16.000+0000"}, {"author": "Owen O'Malley", "body": "The File.mkdirs (based on what I see in eclipse) looks like: public boolean mkdirs() { if (exists()) { return false; } if (mkdir()) { return true; } ... <handle recursive mkdirs>... } in any case, the final mkdir would need to be the last thing done. Without the sync block, I believe your code is functionally identical to my proposal of: if (fs.mkdirs(dir) || fs.exists(dir)) { return file; } Or am I missing something? If we need to synchronize, we really need to do it everywhere and do it consistently. On a side note, the Configuration's getFile roll-over between local directories is problematic. The problem is that readers need to find the file regardless of where it was written. So if the writer can spill over to other directories, there should be a findFile(?) that looks in all of the directories (in the right order) until it finds it. That way readers can find the file regardless of which directory the writer was spilled in to.", "created": "2006-06-06T23:46:28.000+0000"}, {"author": "Sameer Paranjpye", "body": "Not sure I understand that, a sync block would still not protect againt multiple processes trying to create the same hierarchy, and the races would still bite you... Maybe the safest thing to do is to traverse the hierarchy in getLocalPath(), creating each directory and checking for it's existence.", "created": "2006-06-06T23:56:12.000+0000"}, {"author": "Peter Sutter", "body": "Sameer -> I think you're right. Ultimately, the synchronize is not good enough, and it seems that you've got the right solution. Owen -> here's the case Naveen described: lets say five directory levels are created by the call to mkdirs(), and there are two threads entering mkrdirs() at about the same time. Will one of the two threads exit before the fifth directory is created? I dont have the source code, but it seems to me that it could. If that would happen, that thread will step ahead to the next directory because when it does the exists() check, the directory will not exist. Which leads to Sameer's point that even the synchronize is not good enough.", "created": "2006-06-07T00:31:38.000+0000"}, {"author": "Naveen Nalam", "body": "Below is what mkdirs looks like according to the jad decompiler (File.class from is from Sun JDK 1.5). It looks like to me if two processes/threads are trying to create \"/a/b/c/d/e/\" and nothing yet exists, they both try to create \"/a\". One will fail, while the other succeeds. The failing process will return failure early, while the other process continues to create \"b/c/d/e/\". If the failing process after returning from mkdirs() now calls exists(\"/a/b/c/d/e\"), exists() could return false because the other process is still creating the directories along the path. So probably Sameer's suggestion of traversing in getLocalPath() is the best solution. public boolean mkdirs() { if(exists()) return false; if(mkdir()) return true; File file = null; try { file = getCanonicalFile(); } catch(IOException ioexception) { return false; } String s = file.getParent(); return s != null && (new File(s, fs.prefixLength(s))).mkdirs() && file.mkdir(); }", "created": "2006-06-07T01:00:51.000+0000"}, {"author": "Owen O'Malley", "body": "Ok, I see your point. Let's go with the traversing, but I think it should be done in the FileSystem.mkdirs rather than in Configuration.", "created": "2006-06-07T01:12:47.000+0000"}, {"author": "Naveen Nalam", "body": "FileSystem.mkdirs is an abstract method. It would have been nice to put the traversal logic there. FSDirectory.mkdirs() already does a traversal like we need, so perhaps that code can be copied and modified for LocalFileSystem.mkdirs() ?", "created": "2006-06-07T02:58:54.000+0000"}, {"author": "Owen O'Malley", "body": "Sameer is working on a fix for this in LocalFileSystem.mkdirs()", "created": "2006-06-07T03:10:33.000+0000"}, {"author": "Sameer Paranjpye", "body": "Attached patch for mkdir issue. LocalFileSystem.mkdirs() now traverses the hierarchy creating each directory along the way. This patch changes the semantics of 'mkdirs' in the FileSystem interface. The semantics are now those of 'mkdir -p', in that existence of the specified directory or any ancestor of it is no longer an error. Also updates mkdirs in dfs.FSDirectory so that it has the same behavior.", "created": "2006-06-07T04:29:27.000+0000"}, {"author": "Doug Cutting", "body": "This patch conflicts with HADOOP-240, which I just committed. Can you please resolve this? Thanks.", "created": "2006-06-08T01:21:49.000+0000"}, {"author": "Sameer Paranjpye", "body": "Attached new patch.", "created": "2006-06-08T02:18:43.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Sameer!", "created": "2006-06-08T02:35:46.000+0000"}], "num_comments": 14, "text": "Issue: HADOOP-277\nSummary: Race condition in Configuration.getLocalPath()\nDescription: (attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out ... 060605 142531 task_0001_r_000009_1 0.31808624% reduce > append > /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142725 task_0001_r_000009_1 java.io.FileNotFoundException: /data2/tmp/mapred/local/task_0001_r_000009_1/map_15.out\n\nComments (14):\n1. Owen O'Malley: This patch is closer to what we did for the routine above it last week. (Sorry about not fixing this one too at the same time. It wasn't biting us, but that was no reason not to fix the obviously parallel code.) Is there some reason that you need the synchronized block around the mkdirs? File.mkdirs does a File.exists internally, so you don't need to call it yourself.\n2. Owen O'Malley: By the way, if there is a need for the sync block around the mkdirs, we should go ahead and change the function above it so that next week we don't get a third bug. *smile*\n3. Naveen Nalam: If we take out the sync block, the code would look like: if (fs.exists(dir)) { return file; } fs.mkdirs(dir); if (fs.exists(dir)) { return file; } we added the sync block around the mkdirs because we were not sure exactly how mkdirs() handles race cases. for example if one thread is creating the directory hierarchy, can the other thread return error while the first thread is still creating the hierarchy? if so when the second thread executes fs.exists(), it would return false since the directories are still be created. but perhaps this isn't the behavior mkdirs() exhibits. it could return false only if it can't create the final leaf directory in the directory hierarchy (because the other thread just created the leaf). if that's the case then the sync would not be needed. the sun java api doc isn't clear on this.\n4. Owen O'Malley: The File.mkdirs (based on what I see in eclipse) looks like: public boolean mkdirs() { if (exists()) { return false; } if (mkdir()) { return true; } ... <handle recursive mkdirs>... } in any case, the final mkdir would need to be the last thing done. Without the sync block, I believe your code is functionally identical to my proposal of: if (fs.mkdirs(dir) || fs.exists(dir)) { return file; } Or am I missing something? If we need to synchronize, we really need to do it everywhere and do it consistently. On a side note, the Configuration's getFile roll-over between local directories is problematic. The problem is that readers need to find the file regardless of where it was written. So if the writer can spill over to other directories, there should be a findFile(?) that looks in all of the directories (in the right order) until it finds it. That way readers can find the file regardless of which directory the writer was spilled in to.\n5. Sameer Paranjpye: Not sure I understand that, a sync block would still not protect againt multiple processes trying to create the same hierarchy, and the races would still bite you... Maybe the safest thing to do is to traverse the hierarchy in getLocalPath(), creating each directory and checking for it's existence.\n6. Peter Sutter: Sameer -> I think you're right. Ultimately, the synchronize is not good enough, and it seems that you've got the right solution. Owen -> here's the case Naveen described: lets say five directory levels are created by the call to mkdirs(), and there are two threads entering mkrdirs() at about the same time. Will one of the two threads exit before the fifth directory is created? I dont have the source code, but it seems to me that it could. If that would happen, that thread will step ahead to the next directory because when it does the exists() check, the directory will not exist. Which leads to Sameer's point that even the synchronize is not good enough.\n7. Naveen Nalam: Below is what mkdirs looks like according to the jad decompiler (File.class from is from Sun JDK 1.5). It looks like to me if two processes/threads are trying to create \"/a/b/c/d/e/\" and nothing yet exists, they both try to create \"/a\". One will fail, while the other succeeds. The failing process will return failure early, while the other process continues to create \"b/c/d/e/\". If the failing process after returning from mkdirs() now calls exists(\"/a/b/c/d/e\"), exists() could return false because the other process is still creating the directories along the path. So probably Sameer's suggestion of traversing in getLocalPath() is the best solution. public boolean mkdirs() { if(exists()) return false; if(mkdir()) return true; File file = null; try { file = getCanonicalFile(); } catch(IOException ioexception) { return false; } String s = file.getParent(); return s != null && (new File(s, fs.prefixLength(s))).mkdirs() && file.mkdir(); }\n8. Owen O'Malley: Ok, I see your point. Let's go with the traversing, but I think it should be done in the FileSystem.mkdirs rather than in Configuration.\n9. Naveen Nalam: FileSystem.mkdirs is an abstract method. It would have been nice to put the traversal logic there. FSDirectory.mkdirs() already does a traversal like we need, so perhaps that code can be copied and modified for LocalFileSystem.mkdirs() ?\n10. Owen O'Malley: Sameer is working on a fix for this in LocalFileSystem.mkdirs()", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.271334"}}
{"id": "8fec38f17186bbe502c2c6a3e89e07bd", "issue_key": "SPARK-535", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Error with technique to find hostname in bin/start-slaves.sh in dev branch", "description": "I get an error when I run the new shell script to start slaves on my OS X Mountain Lion laptop (named Lucifer): $ bin/start-slaves.sh cd /Users/andyk/Development/spark/bin/.. ; /Users/andyk/Development/spark/bin/spark-daemon.sh start spark.deploy.worker.Worker spark://found::7077 It Looks like the lines getting hostname aren't working for me: ~~~~~~~~~~~~ Lucifer:spark andyk$ hostname=`hostname` Lucifer:spark andyk$ ip=`host \"$hostname\" | cut -d \" \" -f 4` Lucifer:spark andyk$ echo $ip found: Lucifer:spark andyk$ hostname Lucifer.local Lucifer:spark andyk$ host Lucifer.local Host Lucifer.local not found: 3(NXDOMAIN) ~~~~~~~~~~~~", "reporter": "Andy Konwinski", "assignee": null, "created": "0012-09-16T13:11:00.000+0000", "updated": "2012-10-22T15:09:39.000+0000", "resolved": "2012-10-22T15:09:39.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from andyk: I poked around [1, 2], and one thing that worked for me that is probably somewhat portable but depends on having python is: <code>ip=\"\\`python -c \"import socket;print(socket.gethostbyname(socket.gethostname()))\"\\`\"</code> [1] http://stackoverflow.com/questions/2361709/efficient-way-to-get-your-ip-address-in-shell-scripts [2] http://stackoverflow.com/questions/166506/finding-local-ip-addresses-using-pythons-stdlib", "created": "2012-09-16T13:35:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: I couldn't get this to happen on my Mac, so it might have to do with the way you've asked Mac OS X to set your hostname. Since our scripts are just based on Hadoop's though, my guess is that the same thing would happen for Hadoop, so it can't be super common. Maybe one thing you can try to fix it is to add export SPARK_MASTER_IP=<numeric IP> in your conf/spark-env.sh. Then you will force it to bind to a specific IP.", "created": "2012-09-26T22:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-201, originally reported by andyk", "created": "2012-10-19T18:03:00.000+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Reynold fixed this now.", "created": "2012-10-22T15:09:24.231+0000"}], "num_comments": 4, "text": "Issue: SPARK-535\nSummary: Error with technique to find hostname in bin/start-slaves.sh in dev branch\nDescription: I get an error when I run the new shell script to start slaves on my OS X Mountain Lion laptop (named Lucifer): $ bin/start-slaves.sh cd /Users/andyk/Development/spark/bin/.. ; /Users/andyk/Development/spark/bin/spark-daemon.sh start spark.deploy.worker.Worker spark://found::7077 It Looks like the lines getting hostname aren't working for me: ~~~~~~~~~~~~ Lucifer:spark andyk$ hostname=`hostname` Lucifer:spark andyk$ ip=`host \"$hostname\" | cut -d \" \" -f 4` Lucifer:spark andyk$ echo $ip found: Lucifer:spark andyk$ hostname Lucifer.local Lucifer:spark andyk$ host Lucifer.local Host Lucifer.local not found: 3(NXDOMAIN) ~~~~~~~~~~~~\n\nComments (4):\n1. Patrick McFadin: Github comment from andyk: I poked around [1, 2], and one thing that worked for me that is probably somewhat portable but depends on having python is: <code>ip=\"\\`python -c \"import socket;print(socket.gethostbyname(socket.gethostname()))\"\\`\"</code> [1] http://stackoverflow.com/questions/2361709/efficient-way-to-get-your-ip-address-in-shell-scripts [2] http://stackoverflow.com/questions/166506/finding-local-ip-addresses-using-pythons-stdlib\n2. Patrick McFadin: Github comment from mateiz: I couldn't get this to happen on my Mac, so it might have to do with the way you've asked Mac OS X to set your hostname. Since our scripts are just based on Hadoop's though, my guess is that the same thing would happen for Hadoop, so it can't be super common. Maybe one thing you can try to fix it is to add export SPARK_MASTER_IP=<numeric IP> in your conf/spark-env.sh. Then you will force it to bind to a specific IP.\n3. Patrick McFadin: Imported from Github issue spark-201, originally reported by andyk\n4. Matei Alexandru Zaharia: Reynold fixed this now.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.980378"}}
{"id": "f23f3111660e5682af32b6cf456b24cc", "issue_key": "HADOOP-627", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "MiniMRCluster missing synchronization", "description": "org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner contains (at least) 2 instance variables that are read by another thread: isInitialized and isDead. These should be declared volatile or proper synchronization should be used for their access.", "reporter": "Nigel Daley", "assignee": "Nigel Daley", "created": "2006-10-24T01:02:15.000+0000", "updated": "2009-07-08T16:51:58.000+0000", "resolved": "2006-10-24T21:53:07.000+0000", "labels": [], "components": [], "comments": [{"author": "Nigel Daley", "body": "Note that by declaring isInitialized, isDead, and tt variables volatile, I have had many more successful build/test cycles than before this change.", "created": "2006-10-24T18:02:12.000+0000"}, {"author": "Nigel Daley", "body": "Owen, please review patch.", "created": "2006-10-24T18:13:13.000+0000"}, {"author": "Owen O'Malley", "body": "+1", "created": "2006-10-24T20:29:41.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Nigel!", "created": "2006-10-24T21:53:07.000+0000"}], "num_comments": 4, "text": "Issue: HADOOP-627\nSummary: MiniMRCluster missing synchronization\nDescription: org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner contains (at least) 2 instance variables that are read by another thread: isInitialized and isDead. These should be declared volatile or proper synchronization should be used for their access.\n\nComments (4):\n1. Nigel Daley: Note that by declaring isInitialized, isDead, and tt variables volatile, I have had many more successful build/test cycles than before this change.\n2. Nigel Daley: Owen, please review patch.\n3. Owen O'Malley: +1\n4. Doug Cutting: I just committed this. Thanks, Nigel!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.408498"}}
{"id": "2ef9fde1536895fcd8f9ab235eae432b", "issue_key": "HADOOP-577", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Job tracker should collect detailed information about tasks and return it to the client", "description": "Mapreduce driver should get detailed information about task execution -- at least at the end of the job so that it can be repoted to the user (stored in a \"normal\" or HDFS file, printed to the stdout, whatever). This information should include, at least: -- task execution start and end -- the number of record read and written -- the number of butes read and written for each task (for each attempt) It would also be nice to have this infromation while the job is running, updated at least when a task is started or ended. However, this is just \"nice to have\", while the final report is quite essential.", "reporter": "arkady borkovsky", "assignee": "Owen O'Malley", "created": "2006-10-05T16:46:15.000+0000", "updated": "2009-07-08T16:51:57.000+0000", "resolved": "2007-05-24T07:00:59.000+0000", "labels": [], "components": [], "comments": [{"author": "Sanjay Dahiya", "body": "Does it make sense to use JobTracker history mechanism ( HADOOP-239) to include this information as well ? I think it does.", "created": "2006-10-05T17:30:12.000+0000"}, {"author": "Owen O'Malley", "body": "This was fixed by HADOOP-492, which implemented per-task and per-job counters. The default counters include bytes and records input and output. They are printed on the console when the job completes. They are available as the job runs via the http server. The times are available from the time stamps on the progress messages and are available on the web/ui.", "created": "2007-05-24T07:00:58.245+0000"}], "num_comments": 2, "text": "Issue: HADOOP-577\nSummary: Job tracker should collect detailed information about tasks and return it to the client\nDescription: Mapreduce driver should get detailed information about task execution -- at least at the end of the job so that it can be repoted to the user (stored in a \"normal\" or HDFS file, printed to the stdout, whatever). This information should include, at least: -- task execution start and end -- the number of record read and written -- the number of butes read and written for each task (for each attempt) It would also be nice to have this infromation while the job is running, updated at least when a task is started or ended. However, this is just \"nice to have\", while the final report is quite essential.\n\nComments (2):\n1. Sanjay Dahiya: Does it make sense to use JobTracker history mechanism ( HADOOP-239) to include this information as well ? I think it does.\n2. Owen O'Malley: This was fixed by HADOOP-492, which implemented per-task and per-job counters. The default counters include bytes and records input and output. They are printed on the console when the job completes. They are available as the job runs via the http server. The times are available from the time stamps on the progress messages and are available on the web/ui.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.395913"}}
{"id": "4dc45170eadaf07eac7a9078633b5239", "issue_key": "SPARK-771", "issue_type": "Sub-task", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Throw a more meaning message when SparkContext cannot connect to the master", "description": "The current error message is bash-4.1$ Exception in thread \"Thread-24\" java.util.concurrent.TimeoutException: Futures timed out after [10000] milliseconds at akka.dispatch.DefaultPromise.ready(Future.scala:870) at akka.dispatch.DefaultPromise.result(Future.scala:874) at akka.dispatch.Await$.result(Future.scala:74) at spark.deploy.client.Client.stop(Client.scala:118) at spark.scheduler.cluster.SparkDeploySchedulerBackend.stop(SparkDeploySchedulerBackend.scala:43) at spark.scheduler.cluster.ClusterScheduler.stop(ClusterScheduler.scala:396) at spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:740) at spark.SparkContext.stop(SparkContext.scala:565) at shark.SharkEnv$.stop(SharkEnv.scala:126) at shark.SharkServer$$anon$2.run(SharkServer.scala:107) We should say something more meaningful than that. E.g. \"Cannot connect to ...\" or \"Connection to ... timed out\"", "reporter": "Reynold Xin", "assignee": "Patrick McFadin", "created": "2013-06-13T17:22:12.000+0000", "updated": "2013-07-29T14:17:00.000+0000", "resolved": "2013-07-29T14:17:00.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Hey [~rxin] I can't reproduce this. What was the exact scenario under which this happened? In master if I try from Shark or Spark to connect to a disconnected master I get this appropriate message:  13/07/13 16:49:46 ERROR client.Client$ClientActor: Connection to master failed; stopping client  It's possible this is triggered in some other scenario, such as when the master and SC have different versions, I'm not sure.", "created": "2013-07-13T16:51:40.315+0000"}, {"author": "Patrick McFadin", "body": "I also confirmed things work if the Context connects to a master which is killed.", "created": "2013-07-13T16:54:06.090+0000"}, {"author": "Reynold Xin", "body": "Looks like the exception stack was from stopping a client? In client.scala:  def stop() { if (actor != null) { try { val timeout = Duration.create(System.getProperty(\"spark.akka.askTimeout\", \"10\").toLong, \"seconds\") val future = actor.ask(StopClient)(timeout) Await.result(future, timeout) } catch { case e: AskTimeoutException => // Ignore it, maybe master went away } actor = null } }", "created": "2013-07-13T17:49:37.566+0000"}, {"author": "Patrick McFadin", "body": "Okay [~rxin], my guess is that this means we should be catching {{TimeoutException}} rather than {{AskTimeoutException}} since it seems like the more general of the two types can be thrown (the latter is extends the former).", "created": "2013-07-14T18:32:03.139+0000"}], "num_comments": 4, "text": "Issue: SPARK-771\nSummary: Throw a more meaning message when SparkContext cannot connect to the master\nDescription: The current error message is bash-4.1$ Exception in thread \"Thread-24\" java.util.concurrent.TimeoutException: Futures timed out after [10000] milliseconds at akka.dispatch.DefaultPromise.ready(Future.scala:870) at akka.dispatch.DefaultPromise.result(Future.scala:874) at akka.dispatch.Await$.result(Future.scala:74) at spark.deploy.client.Client.stop(Client.scala:118) at spark.scheduler.cluster.SparkDeploySchedulerBackend.stop(SparkDeploySchedulerBackend.scala:43) at spark.scheduler.cluster.ClusterScheduler.stop(ClusterScheduler.scala:396) at spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:740) at spark.SparkContext.stop(SparkContext.scala:565) at shark.SharkEnv$.stop(SharkEnv.scala:126) at shark.SharkServer$$anon$2.run(SharkServer.scala:107) We should say something more meaningful than that. E.g. \"Cannot connect to ...\" or \"Connection to ... timed out\"\n\nComments (4):\n1. Patrick McFadin: Hey [~rxin] I can't reproduce this. What was the exact scenario under which this happened? In master if I try from Shark or Spark to connect to a disconnected master I get this appropriate message:  13/07/13 16:49:46 ERROR client.Client$ClientActor: Connection to master failed; stopping client  It's possible this is triggered in some other scenario, such as when the master and SC have different versions, I'm not sure.\n2. Patrick McFadin: I also confirmed things work if the Context connects to a master which is killed.\n3. Reynold Xin: Looks like the exception stack was from stopping a client? In client.scala:  def stop() { if (actor != null) { try { val timeout = Duration.create(System.getProperty(\"spark.akka.askTimeout\", \"10\").toLong, \"seconds\") val future = actor.ask(StopClient)(timeout) Await.result(future, timeout) } catch { case e: AskTimeoutException => // Ignore it, maybe master went away } actor = null } }\n4. Patrick McFadin: Okay [~rxin], my guess is that this means we should be catching {{TimeoutException}} rather than {{AskTimeoutException}} since it seems like the more general of the two types can be thrown (the latter is extends the former).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.052740"}}
{"id": "fa627d753772f00b118dd135d4214f22", "issue_key": "HADOOP-387", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "LocalJobRunner assigns duplicate mapid's", "description": "While hunting down nutch issue NUTCH-266 i discovered that id's are generated with following fragment of code: private String newId() { return Integer.toString(Math.abs(new Random().nextInt()),36); } and the related Javadoc: \" public Random() Creates a new random number generator. Its seed is initialized to a value based on the current time: public Random() { this(System.currentTimeMillis()); } Two Random objects created within the same millisecond will have the same sequence of random numbers. \" it appears that in this case there are more than one Random pobject generated at the same millisecond and id's are no longer unique.", "reporter": "Sami Siren", "assignee": null, "created": "2006-07-25T18:39:30.000+0000", "updated": "2009-07-08T16:51:50.000+0000", "resolved": "2006-07-26T07:29:34.000+0000", "labels": [], "components": [], "comments": [{"author": "Sami Siren", "body": "fix for hadoop 0.4", "created": "2006-07-25T18:40:41.000+0000"}, {"author": "Sami Siren", "body": "fix for trunk", "created": "2006-07-25T18:41:20.000+0000"}, {"author": "Sami Siren", "body": "One more note, there were couiple of other places in code where the random was used in the same way, perhaps someone with better understanding of the internals should check them out too.", "created": "2006-07-25T18:42:29.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Sami!", "created": "2006-07-26T07:29:34.000+0000"}], "num_comments": 4, "text": "Issue: HADOOP-387\nSummary: LocalJobRunner assigns duplicate mapid's\nDescription: While hunting down nutch issue NUTCH-266 i discovered that id's are generated with following fragment of code: private String newId() { return Integer.toString(Math.abs(new Random().nextInt()),36); } and the related Javadoc: \" public Random() Creates a new random number generator. Its seed is initialized to a value based on the current time: public Random() { this(System.currentTimeMillis()); } Two Random objects created within the same millisecond will have the same sequence of random numbers. \" it appears that in this case there are more than one Random pobject generated at the same millisecond and id's are no longer unique.\n\nComments (4):\n1. Sami Siren: fix for hadoop 0.4\n2. Sami Siren: fix for trunk\n3. Sami Siren: One more note, there were couiple of other places in code where the random was used in the same way, perhaps someone with better understanding of the internals should check them out too.\n4. Doug Cutting: I just committed this. Thanks, Sami!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.330761"}}
{"id": "72f929a1bc2bb217c43e822126ba3800", "issue_key": "KAFKA-788", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Periodic refresh of topic metadata on the producer doesn't include all topics", "description": "We added a patch to the producer to refresh the metadata for all topics periodically. However, the producer only does this for the topics in the last batch. But some topics sent by the producer could be low throughput and might not be present in every batch. If we bounce the cluster or if brokers fail and leaders change, the metadata for those low throughput topic is not refreshed by this periodic topic metadata request. The next produce request for those topics have to fail and then a separate metadata request needs to be reissued to handle the produce request. This is especially a problem for the migration tool. So even if the producer had a chance to refresh the metadata when the leader changed, it throws LeaderNotAvailableExceptions much later when it sends a request for that topic. I propose we just fetch data for all topics sent by the producer in the periodic refresh of topic metadata", "reporter": "Neha Narkhede", "assignee": "Neha Narkhede", "created": "2013-03-05T19:11:01.000+0000", "updated": "2017-10-30T09:34:20.000+0000", "resolved": "2017-10-30T09:34:20.000+0000", "labels": ["kafka-0.8", "p2"], "components": ["producer "], "comments": [{"author": "Manikumar", "body": "Closing inactive issue. The old producer is no longer supported.", "created": "2017-10-30T09:34:20.057+0000"}], "num_comments": 1, "text": "Issue: KAFKA-788\nSummary: Periodic refresh of topic metadata on the producer doesn't include all topics\nDescription: We added a patch to the producer to refresh the metadata for all topics periodically. However, the producer only does this for the topics in the last batch. But some topics sent by the producer could be low throughput and might not be present in every batch. If we bounce the cluster or if brokers fail and leaders change, the metadata for those low throughput topic is not refreshed by this periodic topic metadata request. The next produce request for those topics have to fail and then a separate metadata request needs to be reissued to handle the produce request. This is especially a problem for the migration tool. So even if the producer had a chance to refresh the metadata when the leader changed, it throws LeaderNotAvailableExceptions much later when it sends a request for that topic. I propose we just fetch data for all topics sent by the producer in the periodic refresh of topic metadata\n\nComments (1):\n1. Manikumar: Closing inactive issue. The old producer is no longer supported.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.830731"}}
{"id": "325ec90dffc4ecb52cd2bee431ef85b4", "issue_key": "HADOOP-985", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Namenode should identify DataNodes as ip:port instead of hostname:port", "description": "Right now NameNode keeps track of DataNodes with \"hostname:port\". One proposal is to keep track of datanodes with \"ip:port\". There are various concerns expressed regd hostnames and ip. Please add your experiences here so that we have better idea on what we should fix etc. How should be calculate datanode ip: 1) Just like how we calculate hostname currently with \"dfs.datanode.dns.interface\" and \"dfs.datanode.dns.nameserver\". So if interface specified wrong, it could report ip like 127.0.0.1 which might or might not be intended. 2) Namenode can use the remove socket address when the datanode registers. Not sure how easy it to get this address in RPC or if this is desirable. 3) Namenode could just resolve the hostname when a datanode registers. It could print of a warning if the resolved ip and reported ip don't match. One advantage of using IPs is that DFSClient does not need to resolve them when it connects to datanode. This could save few milliseconds for each block. Also, DFSClient should check all its ips to see if a given ip is local or not. As far I see namenode does not resolve any DNS in normal operations since it does not actively contact datanodes. In that sense not sure if this have any change in Namenode performance. Thoughts?", "reporter": "Raghu Angadi", "assignee": "Raghu Angadi", "created": "2007-02-06T21:25:17.000+0000", "updated": "2010-07-20T04:52:53.000+0000", "resolved": "2007-02-22T19:50:41.000+0000", "labels": [], "components": [], "comments": [{"author": "Marco Nicosia", "body": "I support option #2 (determining remote IP from the socket). From my comment on HADOOP-685: I know it's not trivial, but I'd prefer that the nameNode record the IP address of a connection. That way there's no DNS involved at any level in the transaction, and we know exactly which interface/IP address is being used. Additionally, there's no worrying about /etc/hosts, or dhcp, or whatnot. It works for the entire time the dataNode's up, and making network connections. Regarding option #1: On the dataNode's side, determining which IP address to use is even harder than determining administrative hostname, since you don't know what route packets will take to get to the nameNode (and on some OS's (solaris) if you have interface IPs and VIPs on that interface, you can't control which IP address will be used). Regarding option #3: On startup, massive clusters really pound on the nameNode, delaying startup. The nameNode's already very busy. Worse, I'd hate if the cluster had extended difficulties coming up because DNS lookups were either slow or busted entirely.", "created": "2007-02-07T02:30:11.055+0000"}, {"author": "Raghu Angadi", "body": "I prefer #2 as well. This could be the default behavior and if dfs.datanode.dns.interface is specified, then we can use the ip of the specific interface (this might be required for some special cases). Instead of modifying RPC so that namenode sees remote ip for this case, datanode can report the ip and hostname. Datanode can open a UDP socket to namenode and check the local ip of the socket. I think it does not even need to send any packets. Either case, it does not need namenode to be up or wait for namenode response. Datanode can resolve the ip for hostname. This won't always match 'hostname -f'.. I will check how exactly we currently get the hostname.", "created": "2007-02-07T15:44:36.876+0000"}, {"author": "Owen O'Malley", "body": "I think the best way to support rpc calls being able to find the IP address of the caller would be to have a static method in RPC that uses a thread-local variable to return the IP address of the caller. Clearly the RPC framework would set the variable before calling the method on the server and clear it when it was done. Something like: /** * Get the host ip address of the caller. Only valid on the server while running the remote procedure. * @return the dotted ip address of the caller or NULL if not in an RPC call */ public static String getHostAddress() { ... }", "created": "2007-02-07T17:14:24.855+0000"}, {"author": "Doug Cutting", "body": "I think Owen's design is good: a static method that references a thread local. I'd put the static method on Server, though, not RPC, and call it getClientAddress().", "created": "2007-02-07T17:25:41.407+0000"}, {"author": "Raghu Angadi", "body": "I was thinking of thread local as well.. but was not sure if it was normal practice or not. will do that. Regd hostname, should we just let Datanode behave pretty much how it does now and not bother resolving it Namenode?", "created": "2007-02-07T17:52:00.414+0000"}, {"author": "Raghu Angadi", "body": "With this fix, what we displace on dfs front page changes. The href for datanode now will have ip address. See attached dfshealth.html. Following comment in dfshealth.jsp describes what we display: /* Say the datanode is dn1.hadoop.apache.org with ip 192.168.0.5 we use: 1) d.getHostName():d.getPort() to display. Domain and port are stipped if they are common across the nodes. i.e. \"dn1\" 2) d.getHostName():d.Port() for \"title\". i.e. \"dn1.hadoop.apache.org:50010\" 3) d.getHost():d.getInfoPort() for url. i.e. \"http://192.168.0.5:50075/...\" Note that \"d.getHost():d.getPort()\" is what DFS clients use to interact with datanodes. */ Yes, the datanode hrefs don't looks good. But one advantage is that we can easily see what namenode and clients see.", "created": "2007-02-09T21:40:01.514+0000"}, {"author": "Raghu Angadi", "body": "Ok, I switched (2) and (3) above. \"title\" (hover) shows 192.168.0.5:50010 and href will have hostname.", "created": "2007-02-09T22:26:02.949+0000"}, {"author": "Raghu Angadi", "body": "Attached patch for using ips in namenode. Added extra field hostName in DatanodeID but it is not serialized. I tested with a deliberately wrong config so that each datanode gets \"localhost\" as its hostname. Namenode web page lists \"localhost\" for all the nodes but the cluster just-works :).", "created": "2007-02-10T01:58:36.207+0000"}, {"author": "Raghu Angadi", "body": "2.patch : minor typo fix.", "created": "2007-02-10T02:17:22.167+0000"}, {"author": "Hairong Kuang", "body": "The open request takes the client host name as a parameter. Upon receiving an open request, the name node searches the datanode map to find the descriptor of the data node that runs on the client machine. Now that DatanodeDescriptor contains its ip address not its host name. This search always returns null.", "created": "2007-02-15T23:23:29.996+0000"}, {"author": "Raghu Angadi", "body": "attached 3.patch. Updated patch removed 'clientMachine' argument from ClientProtocol's open() and create(). This argument was part of rack-aware patch.", "created": "2007-02-15T23:27:38.627+0000"}, {"author": "Raghu Angadi", "body": "Thanks Hairong. minor change in 4.patch.", "created": "2007-02-15T23:59:24.785+0000"}, {"author": "Hairong Kuang", "body": "The patch looks good. I have two comments: 1. ClientProtocolVersionNumber should be bumped since the syntax of the open & create requests is changed. 2. DatanodeID contains the fields that need to be saved to the disk. Since the new field hostName does not need to serialized, it might better be put in DatanodeDescriptor.", "created": "2007-02-16T00:23:01.863+0000"}, {"author": "Raghu Angadi", "body": "Thanks Hairong. I will include both in a new patch. This changes the what DFS returns for getDatanodeHints(), which is ultimately used by mapreduce. Two options for handling this: a) we can modify getDatanodeHints() to return what it used return before this patch. i.e. return descriptor.getHostName() instead of descriptor.getHost(). Advantage is that no changes are necessary in mapreduce. But does not confirm to 'ip every where' policy. b) Make Job and task tracker also deal in ips. I am not sure yet how intrusive this change is. My preference is (a). comments?", "created": "2007-02-16T01:48:42.257+0000"}, {"author": "Hairong Kuang", "body": "I also prefer option (a). I would open another jira issue to investigate the use of ip in mapred.", "created": "2007-02-16T06:49:21.790+0000"}, {"author": "Raghu Angadi", "body": "5.patch : includes the changes Hairong suggested. We now send hostname for hints. Thanks Ownen, verified that job tracker correctly assigns the jobs.", "created": "2007-02-17T00:33:12.240+0000"}, {"author": "Doug Cutting", "body": "This patch no longer applies to the current trunk. Can you please update it? Thanks!", "created": "2007-02-21T21:07:56.129+0000"}, {"author": "Raghu Angadi", "body": "Fixed conflict with HADOOP-442 in ClientProtocol.java.", "created": "2007-02-22T00:27:53.864+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Raghu!", "created": "2007-02-22T19:50:41.560+0000"}], "num_comments": 19, "text": "Issue: HADOOP-985\nSummary: Namenode should identify DataNodes as ip:port instead of hostname:port\nDescription: Right now NameNode keeps track of DataNodes with \"hostname:port\". One proposal is to keep track of datanodes with \"ip:port\". There are various concerns expressed regd hostnames and ip. Please add your experiences here so that we have better idea on what we should fix etc. How should be calculate datanode ip: 1) Just like how we calculate hostname currently with \"dfs.datanode.dns.interface\" and \"dfs.datanode.dns.nameserver\". So if interface specified wrong, it could report ip like 127.0.0.1 which might or might not be intended. 2) Namenode can use the remove socket address when the datanode registers. Not sure how easy it to get this address in RPC or if this is desirable. 3) Namenode could just resolve the hostname when a datanode registers. It could print of a warning if the resolved ip and reported ip don't match. One advantage of using IPs is that DFSClient does not need to resolve them when it connects to datanode. This could save few milliseconds for each block. Also, DFSClient should check all its ips to see if a given ip is local or not. As far I see namenode does not resolve any DNS in normal operations since it does not actively contact datanodes. In that sense not sure if this have any change in Namenode performance. Thoughts?\n\nComments (19):\n1. Marco Nicosia: I support option #2 (determining remote IP from the socket). From my comment on HADOOP-685: I know it's not trivial, but I'd prefer that the nameNode record the IP address of a connection. That way there's no DNS involved at any level in the transaction, and we know exactly which interface/IP address is being used. Additionally, there's no worrying about /etc/hosts, or dhcp, or whatnot. It works for the entire time the dataNode's up, and making network connections. Regarding option #1: On the dataNode's side, determining which IP address to use is even harder than determining administrative hostname, since you don't know what route packets will take to get to the nameNode (and on some OS's (solaris) if you have interface IPs and VIPs on that interface, you can't control which IP address will be used). Regarding option #3: On startup, massive clusters really pound on the nameNode, delaying startup. The nameNode's already very busy. Worse, I'd hate if the cluster had extended difficulties coming up because DNS lookups were either slow or busted entirely.\n2. Raghu Angadi: I prefer #2 as well. This could be the default behavior and if dfs.datanode.dns.interface is specified, then we can use the ip of the specific interface (this might be required for some special cases). Instead of modifying RPC so that namenode sees remote ip for this case, datanode can report the ip and hostname. Datanode can open a UDP socket to namenode and check the local ip of the socket. I think it does not even need to send any packets. Either case, it does not need namenode to be up or wait for namenode response. Datanode can resolve the ip for hostname. This won't always match 'hostname -f'.. I will check how exactly we currently get the hostname.\n3. Owen O'Malley: I think the best way to support rpc calls being able to find the IP address of the caller would be to have a static method in RPC that uses a thread-local variable to return the IP address of the caller. Clearly the RPC framework would set the variable before calling the method on the server and clear it when it was done. Something like: /** * Get the host ip address of the caller. Only valid on the server while running the remote procedure. * @return the dotted ip address of the caller or NULL if not in an RPC call */ public static String getHostAddress() { ... }\n4. Doug Cutting: I think Owen's design is good: a static method that references a thread local. I'd put the static method on Server, though, not RPC, and call it getClientAddress().\n5. Raghu Angadi: I was thinking of thread local as well.. but was not sure if it was normal practice or not. will do that. Regd hostname, should we just let Datanode behave pretty much how it does now and not bother resolving it Namenode?\n6. Raghu Angadi: With this fix, what we displace on dfs front page changes. The href for datanode now will have ip address. See attached dfshealth.html. Following comment in dfshealth.jsp describes what we display: /* Say the datanode is dn1.hadoop.apache.org with ip 192.168.0.5 we use: 1) d.getHostName():d.getPort() to display. Domain and port are stipped if they are common across the nodes. i.e. \"dn1\" 2) d.getHostName():d.Port() for \"title\". i.e. \"dn1.hadoop.apache.org:50010\" 3) d.getHost():d.getInfoPort() for url. i.e. \"http://192.168.0.5:50075/...\" Note that \"d.getHost():d.getPort()\" is what DFS clients use to interact with datanodes. */ Yes, the datanode hrefs don't looks good. But one advantage is that we can easily see what namenode and clients see.\n7. Raghu Angadi: Ok, I switched (2) and (3) above. \"title\" (hover) shows 192.168.0.5:50010 and href will have hostname.\n8. Raghu Angadi: Attached patch for using ips in namenode. Added extra field hostName in DatanodeID but it is not serialized. I tested with a deliberately wrong config so that each datanode gets \"localhost\" as its hostname. Namenode web page lists \"localhost\" for all the nodes but the cluster just-works :).\n9. Raghu Angadi: 2.patch : minor typo fix.\n10. Hairong Kuang: The open request takes the client host name as a parameter. Upon receiving an open request, the name node searches the datanode map to find the descriptor of the data node that runs on the client machine. Now that DatanodeDescriptor contains its ip address not its host name. This search always returns null.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.533913"}}
{"id": "6a43d2f41e1af8e2c9772229464405ca", "issue_key": "KAFKA-285", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Increase maximum value of log.retention.size", "description": "The log.retention.size property was retrieved as an Int, which means a maximum of Int.MaxValue (2 GB). This patch gets the property as a long.", "reporter": "Elben Shira", "assignee": null, "created": "2012-02-24T16:18:12.000+0000", "updated": "2012-09-04T17:25:52.000+0000", "resolved": "2012-02-28T01:51:33.000+0000", "labels": ["patch"], "components": [], "comments": [{"author": "Elben Shira", "body": "This is a clean patch on branches/0.7. The other one (KAFKA-285.patch) applies cleanly on trunk.", "created": "2012-02-25T21:06:26.576+0000"}, {"author": "Elben Shira", "body": "Applies cleanly to trunk.", "created": "2012-02-25T21:07:07.988+0000"}, {"author": "Jun Rao", "body": "Thanks for the patch, Elben. Just committed the patch to trunk.", "created": "2012-02-28T01:51:33.185+0000"}], "num_comments": 3, "text": "Issue: KAFKA-285\nSummary: Increase maximum value of log.retention.size\nDescription: The log.retention.size property was retrieved as an Int, which means a maximum of Int.MaxValue (2 GB). This patch gets the property as a long.\n\nComments (3):\n1. Elben Shira: This is a clean patch on branches/0.7. The other one (KAFKA-285.patch) applies cleanly on trunk.\n2. Elben Shira: Applies cleanly to trunk.\n3. Jun Rao: Thanks for the patch, Elben. Just committed the patch to trunk.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.582100"}}
{"id": "21e795dea33581e2fe29e4e499222760", "issue_key": "KAFKA-188", "issue_type": "New Feature", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Support multiple data directories", "description": "Currently we allow only a single data directory. This means that a multi-disk configuration needs to be a RAID array or LVM volume or something like that to be mounted as a single directory. For a high-throughput low-reliability configuration this would mean RAID0 striping. Common wisdom in Hadoop land has it that a JBOD setup that just mounts each disk as a separate directory and does application-level balancing over these results in about 30% write-improvement. For example see this claim here: http://old.nabble.com/Re%3A-RAID-vs.-JBOD-p21466110.html It is not clear to me why this would be the case--it seems the RAID controller should be able to balance writes as well as the application so it may depend on the details of the setup. Nonetheless this would be really easy to implement, all you need to do is add multiple data directories and balance partition creation over these disks. One problem this might cause is if a particular topic is much larger than the others it might unbalance the load across the disks. The partition->disk assignment policy should probably attempt to evenly spread each topic to avoid this, rather than just trying keep the number of partitions balanced between disks.", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "created": "2011-11-05T23:39:11.000+0000", "updated": "2021-06-06T00:09:12.000+0000", "resolved": "2012-11-02T19:02:47.000+0000", "labels": [], "components": [], "comments": [{"author": "Jay Kreps", "body": "Here are a few additional thoughts on this: 1. This is actually a lot more valuable after kafka 0.8 is out since we will already allow replication at a higher level so the raid is less desirable. Patch should definitely be on 0.8 branch, though it will likely be the same for 0.7. 2. It is worth deciding if we want to support unbalanced disk sizes and speeds. E.g. if you have a 7.2k RPM drive and a 10k rpm drive will we allow you to balance over these? I recommend we skip this for now, we can always do it later. So like with RAID, we will treat all drives equally. 3. I think the only change will be in LogManager. Instead of a single config.logDir parameter we will need logDirs, an array of directories. In createLog() we will need a policy that chooses the best disk on which to place the new log-partition. 4. There may be a few other places that assume a single log directory, may have to grep around and check for that. I don't think their is to much else, as everything else should interact through LogManager and once the Log instance is created it doesn't care where it's home directory is. One approach to placement would be to always create new logs on the \"least loaded\" directory. The definition of \"least loaded\" is likely to be a heuristic. There are two things we want to balance (1) data size, (2) i/o throughput. If the retention policy is based on time, then size is a good proxy for throughput. However you could imagine having one log with very small retention size but very high throughput. Another problem is that the usage may change over time, and migration is not feasable. For example a new feature going through a ramped rollout might produce almost no data at first and then later produce gobs of data. Furthermore you might get odd results in the case where you manually pre-create many topics all at once as they would all end up on whichever directory had the least data. I think a better strategy would be to not try to estimate the least-loaded partition and instead just do round-robin assignment (e.g. logDirs(counter.getAndIncrement() % logDirs.length)). The assumption would be that the number of partitions is large enough that each topic has one partition on each disk. Either of these strategies has a corner case if all data goes to a single topic (or one topic dominates the load distribution), and that topic has (say) 5 local partitions and 4 data directories. In this case one directory will get 2x the others. However this corner case could be worked around by carefully aligning the partition count and the total number of data directories, so I don't think we need to handle it here.", "created": "2012-07-18T18:03:22.358+0000"}, {"author": "Jonathan Creasy", "body": "I started the implementation and my code looks much like you have described. I am now to the point of determining which data location to use. I am planning on doing a round-robin assignment for each partition. So, with 4 data dirs and the following topic/partition scheme: topic1 - 2 partitions topic2 - 4 partitions topic3 - 1 partition topic4 - 2 partitions disk1 = topic1/1, topic2/3, topic4/2 disc2 = topic1/2, topic2/4 disc3 = topic2/1, topic3/1 disc4 = topic2/2, topic4/1 This is a good first step, we may want to later add re-balancing code based on metrics so that the \"produced/consumed messages per second\" are roughly balanced per disk. This may or may not be feasible and valuable and isn't really that important in this initial implementation.", "created": "2012-07-18T19:47:26.602+0000"}, {"author": "Jonathan Creasy", "body": "Sorry, I meant to have this patch in sooner but got quite busy, I expect to have it soon.", "created": "2012-07-31T22:24:30.413+0000"}, {"author": "Jay Kreps", "body": "Hey Jonathan, I am working on some log stuff, mind if I take a shot at this too while I am in there?", "created": "2012-09-29T23:30:01.900+0000"}, {"author": "Jonathan Creasy", "body": "Yes please, I have the code written, and I even tested it a little, but I've been able to spend no time on it in a few weeks. It's probably not done yet, but it's close.", "created": "2012-10-02T02:31:41.968+0000"}, {"author": "Jay Kreps", "body": "Do you want to just upload whatever you've got in whatever state its in and I can use that as a starting point?", "created": "2012-10-02T15:53:41.804+0000"}, {"author": "Jay Kreps", "body": "Okay, took a shot at this. Attached is a draft patch. It needs some tests specific to the log selection. There is a hitch, which I discuss below. The basic change is 1. Change KafkaConfig.logDir to logDirectories and take a CSV of paths (I still support the old setting as a fallback) 2. Move the whole clean shutdown marker file thing into the LogManager. I feel it should have always been inside LogManager since it is an implementation detail of log recovery. Now there is one such file per log directory. 3. Create new logs in the directories in a round-robin fashion. To do this I keep a counter in LogManager that controls the index of the dir in which we will next create a new log. Initialize this to a random dir index. Each time we create a new index use the dir this points at and then increment it. The hitch is the high watermark file. Currently we keep it in log.dir. But what directory should we keep it in when there are multiple log directories? I hackily just use the first. However this creates a dependency on the order of the log dirs in the config, which is not ideal. If we sort them and use the dir that is alphabetically first then if we add new directories that will mess it up (whereas if we hadn't sorted we could have tacked them on the end). Some options: 0. What I currently do, just use the first directory for the hwmark file. 1. Add a new configuration property, metadataDir and use this for the highwatermark file and the clean shutdown marker and any future persistent thing. Downside of this is that it requires a new config parameter the user has to set up. 2. Require a top level directory for all log directories. e.g. all_logs/ log_dir_1/ my_topic-1/ my_topic-2 log_dir_2/ ... Obviously since the goal is to support multiple independently mounted disks and you might not control where they mount to you might have to use soft links. From a previous life I am remembering lots of pain related to java and softlinks. I would like to get people's feedback on this.", "created": "2012-10-12T05:09:03.145+0000"}, {"author": "Jun Rao", "body": "Thanks for the patch. A couple of comments: 1. It's probably better to have a separate high watermark file per dir for partitions assigned to it. That way, if a disk is damaged, we only lose the high watermarks and the data for partitions on that disk. 2. About data balancing, in the normal case, assigning partitions in a round-robin way to log dirs is fine. However, if a disk is damaged and is replaced, initially there is no data on it. The round-robin approach would mean that the newly replaced disk will always have fewer partitions that other disks. The same issue can occur if a new dir is added in the configuration. An alternative approach is to assign a new partition to the dir with the fewest partitions, which would alleviate this issue.", "created": "2012-10-14T00:25:10.492+0000"}, {"author": "Jay Kreps", "body": "Yeah, I think I agree. It's a little more complex, be we can do a \"least loaded\" thing.", "created": "2012-10-14T22:02:12.711+0000"}, {"author": "Neha Narkhede", "body": "+1 on maintaining one highwatermark file per log directory. This file can contain highwatermarks for partitions that live in that log directory. One way of doing this is by maintaining the partition->highwatermark mapping per log directory inside the ReplicaManager and then just dumping that to the respective log directory's .highwatermark file by the checkpointing thread.", "created": "2012-10-15T21:23:54.057+0000"}, {"author": "Jay Kreps", "body": "Updated patch: 1. Split HWM file per data directory 2. Move to a \"least partitions\" partition assignment strategy 3. Add a unit test for the assignment strategy I think I may have also fixed the transient failure in LogManager.testTimeBasedFlush, though it remains a time-bomb due to its reliance on the scheduler and wall-clock time. One thing to think about is that the use of \"least loaded\" does have a few corner cases of its own. In general it won't differ much from round robin. The case where it will differ is the case where we add a new data directory to an existing server or lose a single data directory on a server. In this case ALL new partitions will be created in the empty data directory until it becomes full. The problem this could create is that any new topics created during this time period will have all partitions assigned to the empty data dir. This may lead to imbalance of load. I think despite this, this strategy is better than (1) round robin, (2) RAID, or (3) something more complicated we might think of now. This patch is ready for review.", "created": "2012-10-25T17:53:21.376+0000"}, {"author": "Jay Kreps", "body": "Wups, missed two minor changes in that last patch.", "created": "2012-10-25T18:01:20.572+0000"}, {"author": "Jay Kreps", "body": "Attached an updated patch with a few very minor changes: 1. If there is only a single log directory I skip the least loaded calculation. This calculation iterates over all logs so it could be a bit expensive in cases where we have very many logs (though it should be rare). This makes it so that the case where we have only one directory is no more expensive then it is now. 2. Fixed a deprecation warning 3. Fixed an outdated scaladoc comment", "created": "2012-10-25T23:31:18.889+0000"}, {"author": "Neha Narkhede", "body": "Took a quick look at patch v4. Here are few review comments - 1. KafkaConfig We should probably raise an error if the same log directory name was specified more than once. 2. LogManager 2.1. I see you have a check in createLogIfNotExists to return if the log is already created. I guess this will happen if two threads execute the following at the same time and enter createLogIfNotExists one after the other. logs.get(topicAndPartition) match { case null => createLogIfNotExists(topicAndPartition) I wonder if it is useful to move the lock to getOrCreateLog instead ? Also, shouldn't we use the same lock to protect other accesses to the \"logs\" data structure (getLog(), allLogs() and topics()) ? 2.2. Fix typo on nextLogDir \"chose the\" -> \"choose the\" 3. ReplicaManager 3.1 Does it make sense to handle the absence of a matching Log for a topic partition correctly, instead of assuming the presence of one through the get API on an Option ? 3.2 Nit pick -> \"highwater marks\" -> \"high watermarks\" or \"highwatermarks\" ? :) 4. HighwatermarkCheckpoint 4.1 While you're in there, do you mind changing the following API to take in a map of TopicAndPartition->Long instead ? We've been bitten by scala bugs that don't handle equality on tuples very well. def write(highwaterMarksPerPartition: Map[(String, Int), Long])", "created": "2012-10-26T02:25:05.439+0000"}, {"author": "Jay Kreps", "body": "New patch, rebased and addresses Neha's comments: 1. Good thought. Added a check in LogManager to detect duplicate data directories. This is not the only bad possibility though. It is possible to have another kafka process that has opened using the same data directory. I am not sure what would happen, but something bad. I added a per data-directory file lock to check for this. This adds a new file .lock to each data directory and uses it to do the equivalent of flock/funlock. This will lock access across processes or within a process. 2.1. I agree this is a bit roundabout. The reason is that the logs are in a Pool, which is really a ConcurrentHashMap. These are nice as they don't lock the whole hash table on each lookup. So I think although it is a little more verbose it is better how it is because in the common case (fetching a log) there is no global lock needed. This should also make the other accesses threadsafe. 2.2. Learned to spell Choose. :-) 3.1. I don't really understand this code that well, so I am not sure. If it is a programming error to for there not to be a log present then I would rather leave it (I think you would get the NoSuchElementException and it would be clear what happened). The reason is that adding a match/case statement in the middle of that groupby is going to make it awfully hard to understand. 3.2. Fixed, nice catch. 4.1. Done. Also: 1. Re-arranged methods in LogManager to make a little more sense.", "created": "2012-10-29T03:38:19.150+0000"}, {"author": "Jun Rao", "body": "Thanks for patch v5. Some more comments: 50. LogManager.nextLogDir(): zeros should only include dirs not already used, right? Currently, it seems to include all log dirs. 51. ReplicaManager.checkpointHighWatermarks(): When handling a leaderAndIsr request, we first create a partition and then create a local replica (which creates the local log). So, there is a slight possibility that a partition in allPartitions may not have a local log. The simplest way is to ignore such partition when checkpointing HW. 52. VerifiableProperties: The following constructor doesn't seem to be used. def this() = this(new Properties)", "created": "2012-10-29T16:56:24.118+0000"}, {"author": "Jay Kreps", "body": "Okay, you and Neha are giving conflicting advice on ReplicaManager. Can I omit the case where there is no replica? That is what is complicating this method, because if there is no replica then will there be a log to get the parent directory from? If so then I am left with: def checkpointHighWatermarks() { val replicas = allPartitions.values.map(_.getReplica(config.brokerId)).collect{case Some(replica) => replica} val replicasByDir = replicas.filter(_.log.isDefined).groupBy(_.log.get.dir.getParent) for((dir, reps) <- replicasByDir) { val hwms = reps.map(r => (TopicAndPartition(r.topic, r.partitionId) -> r.highWatermark)).toMap highWatermarkCheckpoints(dir).write(hwms) } } which is much simpler. lmk.", "created": "2012-10-29T18:06:40.031+0000"}, {"author": "Jay Kreps", "body": "Okay this patch addresses Jun's comments: 50. The zeros are actually correct, basically I am initializing to 0 and ten overwriting with the count if there is one. The goal is to ensure that there is an entry for each directory even if it has no logs (otherwise it would never get any logs assigned). It is possible to do this with some kind of case statement, but I think this is more readable. 51. Okay I used the logic above. The logic is now slightly different from what was there before. Now I filter any partition which has no replica from the file. I also filter any replica which has no log, though my understanding is that that shouldn't happen. 52. Left this. The idea is that previously for tests you could do new Properties so it makes sense to be able to do new VerifiableProperties. Not essential so happy either way.", "created": "2012-10-29T22:11:54.911+0000"}, {"author": "Jun Rao", "body": "Our system tests fail with the latest patch. python -B system_test_runner.py 2>&1 | tee test.out Saw the following in broker log. [2012-10-30 07:40:19,682] FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable) java.io.IOException: No such file or directory at java.io.UnixFileSystem.createFileExclusively(Native Method) at java.io.File.createNewFile(File.java:883) at kafka.utils.FileLock.<init>(FileLock.scala:12) at kafka.log.LogManager$$anonfun$10.apply(LogManager.scala:64) at kafka.log.LogManager$$anonfun$10.apply(LogManager.scala:64) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34) at scala.collection.TraversableLike$class.map(TraversableLike.scala:206) at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34) at kafka.log.LogManager.<init>(LogManager.scala:64) at kafka.server.KafkaServer.startup(KafkaServer.scala:60) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34) at kafka.Kafka$.main(Kafka.scala:46) at kafka.Kafka.main(Kafka.scala) [2012-10-30 07:40:19,683] INFO [Kafka Server 1], shutting down (kafka.server.KafkaServer)", "created": "2012-10-30T14:53:37.769+0000"}, {"author": "Jay Kreps", "body": "Looks like previously if you configured a data directory that didn't exist we created it for you. This behavior was broken since now we try to acquire a lock first. This patch addresses that problem with the following changes: 0. Rebased 1. Cleaned up the LogManager initialization so that we create and validate directories before locking them. This fixes the issue. 2. It looks like the nio FileLock.tryLock has different behvaiors depending on whether the lock is held by your process or another process. This could lead to getting a bad error message if we failed to lock the data directory. Fixed this in our wrapper class.", "created": "2012-10-30T18:02:02.015+0000"}, {"author": "Jun Rao", "body": "Now, some unit tests fail with exceptions like the following. Most of them seem to be transient, but they show up more frequently now. \u001b[0m[\u001b[31merror\u001b[0m] \u001b[0mTest Failed: testCleanShutdown(kafka.server.ServerShutdownTest)\u001b[0m kafka.common.KafkaException: Failed to acquire lock on file .lock in /tmp/kafka-246675. A Kafka instance in another process or thread is using this directory.", "created": "2012-10-30T22:23:49.723+0000"}, {"author": "Jay Kreps", "body": "This failing test was due to a real bug in LogManager.shutdown that lead to the locks not being released. I fixed this bug. In addition: 1. Rebased again 2. Cleaned up ServerShutdownTest and added comments since I spent a lot of time puzzling over this test to figure out what it was doing. 3. Add Utils.swallow to each shutdown call in KafkaServer.shutdown--otherwise any failure prevents the rest of the shutdown.", "created": "2012-10-31T18:31:09.527+0000"}, {"author": "Jun Rao", "body": "+1 on patch v8. Thanks,", "created": "2012-11-02T00:51:35.868+0000"}, {"author": "Jay Kreps", "body": "Checked in rebased version of v8. I did see one failure in the system tests, but chased it down and it was due to KAFKA-593.", "created": "2012-11-02T19:02:47.453+0000"}, {"author": "chenshangan", "body": "assign a new partition to the dir with the fewest partitions, it works fine if all of the partitions have most or less the same size. But if partition size varies quite a lot, it will cause disk usage imbalance. So it's better to take the disk usage into account.", "created": "2014-11-07T09:15:59.643+0000"}, {"author": "Jay Kreps", "body": "[~chenshangan521@163.com] see my comment earlier on this ticket on why assigning to the partitions with the smallest data size has a couple of really bad gotchas. That was why we went with fewest partitions.", "created": "2014-11-07T18:00:05.323+0000"}, {"author": "chenshangan", "body": "[~jkreps] I think we could provide an alternative, user can choose either one: Partitions determined or segments determined.", "created": "2015-05-29T11:42:50.601+0000"}, {"author": "Jay Kreps", "body": "@chenshangan The issue with using data size was that it is very very common to create a bunch of topics as once. When you do this all new partitions will be put on the same least full partition. Then when data starts being written that partition will be totally overloaded. We can make this configurable, but I think almost anyone who chooses that option will get bit by it. I recommend we instead leave this as it is for initial placement and implement \"rebalancing\" option that actively migrates partitions to balance data between directories. This is harder to implement but I think it is what you actually want.", "created": "2015-05-29T18:28:21.000+0000"}, {"author": "chenshangan", "body": "[~jkreps] \"I recommend we instead leave this as it is for initial placement and implement \"rebalancing\" option that actively migrates partitions to balance data between directories. This is harder to implement but I think it is what you actually want.\" Exactly, this is what I really want, but it's pretty hard to implement. And in our use case, we seldom create a bunch of topics at the same time, topics are increasing day by day. Common use case: 1. a new kafka cluster setup, lots of topics from other kafka cluster or system dump data into this new cluster. segments determined policy works well as all topics are started from zero, so segments are consistent with partitions. 2. an existing kafka cluster, topics are added day by day. This is the ideal case, segments policy will work well. 3. an existing kafka cluster, topics are added in bunch. It might cause all new topics being put on the same least directory, of course it will cause bad consequence. But if the cluster is big enough and disk counts and capacity of a broker is big enough, and this is not a common use case, the consequence will not be so serious. Users use this option should consider how to avoid such situation. Above all, it's worthy providing such an option. But If we can implement a \"rebalancing\" option, it would be perfect.", "created": "2015-05-30T00:45:14.129+0000"}, {"author": "Georgy", "body": "[~jkreps] please look at KAFKA-12900 It seems that we do not have fair data/IO spread now.", "created": "2021-06-06T00:09:12.624+0000"}], "num_comments": 30, "text": "Issue: KAFKA-188\nSummary: Support multiple data directories\nDescription: Currently we allow only a single data directory. This means that a multi-disk configuration needs to be a RAID array or LVM volume or something like that to be mounted as a single directory. For a high-throughput low-reliability configuration this would mean RAID0 striping. Common wisdom in Hadoop land has it that a JBOD setup that just mounts each disk as a separate directory and does application-level balancing over these results in about 30% write-improvement. For example see this claim here: http://old.nabble.com/Re%3A-RAID-vs.-JBOD-p21466110.html It is not clear to me why this would be the case--it seems the RAID controller should be able to balance writes as well as the application so it may depend on the details of the setup. Nonetheless this would be really easy to implement, all you need to do is add multiple data directories and balance partition creation over these disks. One problem this might cause is if a particular topic is much larger than the others it might unbalance the load across the disks. The partition->disk assignment policy should probably attempt to evenly spread each topic to avoid this, rather than just trying keep the number of partitions balanced between disks.\n\nComments (30):\n1. Jay Kreps: Here are a few additional thoughts on this: 1. This is actually a lot more valuable after kafka 0.8 is out since we will already allow replication at a higher level so the raid is less desirable. Patch should definitely be on 0.8 branch, though it will likely be the same for 0.7. 2. It is worth deciding if we want to support unbalanced disk sizes and speeds. E.g. if you have a 7.2k RPM drive and a 10k rpm drive will we allow you to balance over these? I recommend we skip this for now, we can always do it later. So like with RAID, we will treat all drives equally. 3. I think the only change will be in LogManager. Instead of a single config.logDir parameter we will need logDirs, an array of directories. In createLog() we will need a policy that chooses the best disk on which to place the new log-partition. 4. There may be a few other places that assume a single log directory, may have to grep around and check for that. I don't think their is to much else, as everything else should interact through LogManager and once the Log instance is created it doesn't care where it's home directory is. One approach to placement would be to always create new logs on the \"least loaded\" directory. The definition of \"least loaded\" is likely to be a heuristic. There are two things we want to balance (1) data size, (2) i/o throughput. If the retention policy is based on time, then size is a good proxy for throughput. However you could imagine having one log with very small retention size but very high throughput. Another problem is that the usage may change over time, and migration is not feasable. For example a new feature going through a ramped rollout might produce almost no data at first and then later produce gobs of data. Furthermore you might get odd results in the case where you manually pre-create many topics all at once as they would all end up on whichever directory had the least data. I think a better strategy would be to not try to estimate the least-loaded partition and instead just do round-robin assignment (e.g. logDirs(counter.getAndIncrement() % logDirs.length)). The assumption would be that the number of partitions is large enough that each topic has one partition on each disk. Either of these strategies has a corner case if all data goes to a single topic (or one topic dominates the load distribution), and that topic has (say) 5 local partitions and 4 data directories. In this case one directory will get 2x the others. However this corner case could be worked around by carefully aligning the partition count and the total number of data directories, so I don't think we need to handle it here.\n2. Jonathan Creasy: I started the implementation and my code looks much like you have described. I am now to the point of determining which data location to use. I am planning on doing a round-robin assignment for each partition. So, with 4 data dirs and the following topic/partition scheme: topic1 - 2 partitions topic2 - 4 partitions topic3 - 1 partition topic4 - 2 partitions disk1 = topic1/1, topic2/3, topic4/2 disc2 = topic1/2, topic2/4 disc3 = topic2/1, topic3/1 disc4 = topic2/2, topic4/1 This is a good first step, we may want to later add re-balancing code based on metrics so that the \"produced/consumed messages per second\" are roughly balanced per disk. This may or may not be feasible and valuable and isn't really that important in this initial implementation.\n3. Jonathan Creasy: Sorry, I meant to have this patch in sooner but got quite busy, I expect to have it soon.\n4. Jay Kreps: Hey Jonathan, I am working on some log stuff, mind if I take a shot at this too while I am in there?\n5. Jonathan Creasy: Yes please, I have the code written, and I even tested it a little, but I've been able to spend no time on it in a few weeks. It's probably not done yet, but it's close.\n6. Jay Kreps: Do you want to just upload whatever you've got in whatever state its in and I can use that as a starting point?\n7. Jay Kreps: Okay, took a shot at this. Attached is a draft patch. It needs some tests specific to the log selection. There is a hitch, which I discuss below. The basic change is 1. Change KafkaConfig.logDir to logDirectories and take a CSV of paths (I still support the old setting as a fallback) 2. Move the whole clean shutdown marker file thing into the LogManager. I feel it should have always been inside LogManager since it is an implementation detail of log recovery. Now there is one such file per log directory. 3. Create new logs in the directories in a round-robin fashion. To do this I keep a counter in LogManager that controls the index of the dir in which we will next create a new log. Initialize this to a random dir index. Each time we create a new index use the dir this points at and then increment it. The hitch is the high watermark file. Currently we keep it in log.dir. But what directory should we keep it in when there are multiple log directories? I hackily just use the first. However this creates a dependency on the order of the log dirs in the config, which is not ideal. If we sort them and use the dir that is alphabetically first then if we add new directories that will mess it up (whereas if we hadn't sorted we could have tacked them on the end). Some options: 0. What I currently do, just use the first directory for the hwmark file. 1. Add a new configuration property, metadataDir and use this for the highwatermark file and the clean shutdown marker and any future persistent thing. Downside of this is that it requires a new config parameter the user has to set up. 2. Require a top level directory for all log directories. e.g. all_logs/ log_dir_1/ my_topic-1/ my_topic-2 log_dir_2/ ... Obviously since the goal is to support multiple independently mounted disks and you might not control where they mount to you might have to use soft links. From a previous life I am remembering lots of pain related to java and softlinks. I would like to get people's feedback on this.\n8. Jun Rao: Thanks for the patch. A couple of comments: 1. It's probably better to have a separate high watermark file per dir for partitions assigned to it. That way, if a disk is damaged, we only lose the high watermarks and the data for partitions on that disk. 2. About data balancing, in the normal case, assigning partitions in a round-robin way to log dirs is fine. However, if a disk is damaged and is replaced, initially there is no data on it. The round-robin approach would mean that the newly replaced disk will always have fewer partitions that other disks. The same issue can occur if a new dir is added in the configuration. An alternative approach is to assign a new partition to the dir with the fewest partitions, which would alleviate this issue.\n9. Jay Kreps: Yeah, I think I agree. It's a little more complex, be we can do a \"least loaded\" thing.\n10. Neha Narkhede: +1 on maintaining one highwatermark file per log directory. This file can contain highwatermarks for partitions that live in that log directory. One way of doing this is by maintaining the partition->highwatermark mapping per log directory inside the ReplicaManager and then just dumping that to the respective log directory's .highwatermark file by the checkpointing thread.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.546690"}}
{"id": "4b098ec0a9594b04f237d70234545fcd", "issue_key": "HADOOP-779", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Hadoop streaming does not work with gzipped input", "description": "When input files are gzipped, StreamLineRecordReader does not take the corect OutputStream to fetch the next record. Instead of using a GzipOutputStream, it uses a FSOutputStream. So input files are read as uncompressed plain text.", "reporter": "Hairong Kuang", "assignee": "Hairong Kuang", "created": "2006-12-04T23:42:11.000+0000", "updated": "2009-07-08T17:05:32.000+0000", "resolved": "2006-12-06T23:56:31.000+0000", "labels": [], "components": [], "comments": [{"author": "Hairong Kuang", "body": "The patch fixes the described bug. In addition, it does the following: 1. clean up \"next\" function of StreamLineRecordReader 2. add a junit test for gzipped input 3. restructure the junit test TestStreaming 4. turn on the debug option for streaming test cases in build-contrib.xml", "created": "2006-12-04T23:48:43.000+0000"}, {"author": "Hairong Kuang", "body": "Here is the patch", "created": "2006-12-04T23:50:10.000+0000"}, {"author": "Hadoop QA", "body": "+1, http://issues.apache.org/jira/secure/attachment/12346382/GzipIn.patch applied and successfully tested against trunk revision 482415", "created": "2006-12-05T00:11:18.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Hairong!", "created": "2006-12-06T23:56:31.000+0000"}], "num_comments": 4, "text": "Issue: HADOOP-779\nSummary: Hadoop streaming does not work with gzipped input\nDescription: When input files are gzipped, StreamLineRecordReader does not take the corect OutputStream to fetch the next record. Instead of using a GzipOutputStream, it uses a FSOutputStream. So input files are read as uncompressed plain text.\n\nComments (4):\n1. Hairong Kuang: The patch fixes the described bug. In addition, it does the following: 1. clean up \"next\" function of StreamLineRecordReader 2. add a junit test for gzipped input 3. restructure the junit test TestStreaming 4. turn on the debug option for streaming test cases in build-contrib.xml\n2. Hairong Kuang: Here is the patch\n3. Hadoop QA: +1, http://issues.apache.org/jira/secure/attachment/12346382/GzipIn.patch applied and successfully tested against trunk revision 482415\n4. Doug Cutting: I just committed this. Thanks, Hairong!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.472985"}}
{"id": "27e327d7dc7a319fbd754ce67c32bf95", "issue_key": "KAFKA-934", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "kafka hadoop consumer and producer use older 0.19.2 hadoop api's", "description": "New hadoop api present in 0.20.1 especially package org.apache.hadoop.mapredude.lib is not used code affected is both consumer and producer in kafka in the contrib package [amilkowski@localhost contrib]$ pwd /opt/local/git/kafka/contrib [amilkowski@localhost contrib]$ ls -lt total 12 drwxrwxr-x 8 amilkowski amilkowski 4096 May 30 11:14 hadoop-consumer drwxrwxr-x 6 amilkowski amilkowski 4096 May 29 19:31 hadoop-producer drwxrwxr-x 6 amilkowski amilkowski 4096 May 29 16:43 target [amilkowski@localhost contrib]$ in example import org.apache.hadoop.mapred.JobClient; import org.apache.hadoop.mapred.JobConf; import org.apache.hadoop.mapred.RunningJob; import org.apache.hadoop.mapred.TextOutputFormat; use 0.19.2 hadoop api format, this prevents merging of hadoop feature into more modern hadoop implementation instead of drawing from 0.20.1 api set in import org.apache.hadoop.mapreduce", "reporter": "Andrew Milkowski", "assignee": "Grant Henke", "created": "2013-06-05T16:58:49.000+0000", "updated": "2016-01-26T04:24:20.000+0000", "resolved": "2016-01-26T04:24:20.000+0000", "labels": ["hadoop", "hadoop-2.0", "newbie"], "components": ["contrib"], "comments": [{"author": "Gwen Shapira", "body": "Are we still maintaining those? I thought these were no longer maintained since the community's preference is to use Camus. If these are maintained, I have a long list of issues to add :)", "created": "2014-08-27T17:29:35.704+0000"}, {"author": "Jun Rao", "body": "Gwen, Yes, we probably need to make a decision on whether to keep maintaining the Kafka hadoop package here. Could you provide your list of issues here in any case so that we don't forget about them?", "created": "2014-09-15T03:44:28.272+0000"}, {"author": "Harsha", "body": "[~gwenshap] are we still planning on maintaining these in light of copycat.", "created": "2015-08-19T15:42:38.419+0000"}, {"author": "Gwen Shapira", "body": "I'm really not sure. I removed it from 0.8.3 since it is not one of the 0.8.3 targets, I don't think there are specific future plans, although I agree that once Copycat has an HDFS connector, it may be silly to maintain both.", "created": "2015-08-19T15:59:42.149+0000"}, {"author": "Grant Henke", "body": "This code has been removed from the project.", "created": "2016-01-26T04:24:20.409+0000"}], "num_comments": 5, "text": "Issue: KAFKA-934\nSummary: kafka hadoop consumer and producer use older 0.19.2 hadoop api's\nDescription: New hadoop api present in 0.20.1 especially package org.apache.hadoop.mapredude.lib is not used code affected is both consumer and producer in kafka in the contrib package [amilkowski@localhost contrib]$ pwd /opt/local/git/kafka/contrib [amilkowski@localhost contrib]$ ls -lt total 12 drwxrwxr-x 8 amilkowski amilkowski 4096 May 30 11:14 hadoop-consumer drwxrwxr-x 6 amilkowski amilkowski 4096 May 29 19:31 hadoop-producer drwxrwxr-x 6 amilkowski amilkowski 4096 May 29 16:43 target [amilkowski@localhost contrib]$ in example import org.apache.hadoop.mapred.JobClient; import org.apache.hadoop.mapred.JobConf; import org.apache.hadoop.mapred.RunningJob; import org.apache.hadoop.mapred.TextOutputFormat; use 0.19.2 hadoop api format, this prevents merging of hadoop feature into more modern hadoop implementation instead of drawing from 0.20.1 api set in import org.apache.hadoop.mapreduce\n\nComments (5):\n1. Gwen Shapira: Are we still maintaining those? I thought these were no longer maintained since the community's preference is to use Camus. If these are maintained, I have a long list of issues to add :)\n2. Jun Rao: Gwen, Yes, we probably need to make a decision on whether to keep maintaining the Kafka hadoop package here. Could you provide your list of issues here in any case so that we don't forget about them?\n3. Harsha: [~gwenshap] are we still planning on maintaining these in light of copycat.\n4. Gwen Shapira: I'm really not sure. I removed it from 0.8.3 since it is not one of the 0.8.3 targets, I don't think there are specific future plans, although I agree that once Copycat has an HDFS connector, it may be silly to maintain both.\n5. Grant Henke: This code has been removed from the project.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.894267"}}
{"id": "97fd1d38dcd2e63ae5fd563b24db4f7f", "issue_key": "SPARK-370", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Spark HTTP FileServer", "description": "A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver. - Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing. - Test with standalone mode locally and local mode, but not on a mesos cluster.", "reporter": "Denny Britz", "assignee": null, "created": "0012-08-30T10:09:00.000+0000", "updated": "2012-10-19T22:50:37.000+0000", "resolved": "2012-10-19T22:50:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: I pointed out a few small formatting issues but I think there is also a problem with ShuffleMapTask's special serialization -- check that out and add a test for it. Otherwise it looks pretty good.", "created": "2012-08-30T16:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I made the changes you suggested and added the functionality of dynamically adding JAR files. Like you said, I had to modify the ShuffleMapTask serialization to make it work.", "created": "2012-09-04T18:01:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from mateiz: This looks okay except for one big thing: creating a new ClassLoader for each task in the Executor is going to be very expensive, and is going to result in many copies of each task being loaded. Can you instead reuse the same ClassLoader, but give it more paths to search? One easy way would be to have a custom subclass of ClassLoader that holds an array of URLClassLoaders, and queries them to find the first one that contains a file. As a more minor suggestion, in ShuffleMapTask, can you cache the serialized versions of the JAR and file HashMaps the same way we cache the RDDs? That serialization code becomes a bottleneck when you have a lot of tasks, and these maps will not change for subsequent runs of the task (or at least we shouldn't let a user use a JAR *before* they add it to the system). You may need to change the Executor to ignore older JARs as well in case it gets an old task. Also, in the serialization part, you may be better off doing a toArray on these and serializing that instead of serializing a HashMap; then, just do a toMap on the Executor.", "created": "2012-09-04T21:11:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: Yep, that make sense. I'll make the changes.", "created": "2012-09-04T21:21:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dennybritz: I am closing this pull request here because I want to submit the request from another, clean, branch.", "created": "2012-09-10T14:54:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-186, originally reported by dennybritz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 6, "text": "Issue: SPARK-370\nSummary: Spark HTTP FileServer\nDescription: A general fileserver for serving both regular as well as JAR files. - I modified the existing JAR file server code to put everything into one fileserver. - Modified the LocalScheduler to mimic the ClusterScheduler/Executor in terms of getting files. That's useful for testing. - Test with standalone mode locally and local mode, but not on a mesos cluster.\n\nComments (6):\n1. Patrick McFadin: Github comment from mateiz: I pointed out a few small formatting issues but I think there is also a problem with ShuffleMapTask's special serialization -- check that out and add a test for it. Otherwise it looks pretty good.\n2. Patrick McFadin: Github comment from dennybritz: I made the changes you suggested and added the functionality of dynamically adding JAR files. Like you said, I had to modify the ShuffleMapTask serialization to make it work.\n3. Patrick McFadin: Github comment from mateiz: This looks okay except for one big thing: creating a new ClassLoader for each task in the Executor is going to be very expensive, and is going to result in many copies of each task being loaded. Can you instead reuse the same ClassLoader, but give it more paths to search? One easy way would be to have a custom subclass of ClassLoader that holds an array of URLClassLoaders, and queries them to find the first one that contains a file. As a more minor suggestion, in ShuffleMapTask, can you cache the serialized versions of the JAR and file HashMaps the same way we cache the RDDs? That serialization code becomes a bottleneck when you have a lot of tasks, and these maps will not change for subsequent runs of the task (or at least we shouldn't let a user use a JAR *before* they add it to the system). You may need to change the Executor to ignore older JARs as well in case it gets an old task. Also, in the serialization part, you may be better off doing a toArray on these and serializing that instead of serializing a HashMap; then, just do a toMap on the Executor.\n4. Patrick McFadin: Github comment from dennybritz: Yep, that make sense. I'll make the changes.\n5. Patrick McFadin: Github comment from dennybritz: I am closing this pull request here because I want to submit the request from another, clean, branch.\n6. Patrick McFadin: Imported from Github issue spark-186, originally reported by dennybritz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.979738"}}
{"id": "46195045452408a03bf50239058e80a0", "issue_key": "KAFKA-327", "issue_type": "Improvement", "status": "Resolved", "priority": "Critical", "resolution": null, "summary": "Monitoring and tooling for Kafka replication", "description": "This is an umbrella JIRA to hold all monitoring and tooling related items required for Kafka replication.", "reporter": "Neha Narkhede", "assignee": null, "created": "2012-04-06T17:51:56.000+0000", "updated": "2014-03-20T21:57:22.000+0000", "resolved": "2014-03-20T21:57:22.000+0000", "labels": ["replication", "tools"], "components": [], "comments": [{"author": "Jay Kreps", "body": "Seems done.", "created": "2014-03-20T21:57:22.081+0000"}], "num_comments": 1, "text": "Issue: KAFKA-327\nSummary: Monitoring and tooling for Kafka replication\nDescription: This is an umbrella JIRA to hold all monitoring and tooling related items required for Kafka replication.\n\nComments (1):\n1. Jay Kreps: Seems done.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.593720"}}
{"id": "7c0becbaf7029a589f3859935e2676a5", "issue_key": "KAFKA-954", "issue_type": "Bug", "status": "Closed", "priority": "Blocker", "resolution": null, "summary": "tidy up README file for better general availability", "description": "e.g. how to start server after building and all would be good too", "reporter": "Joe Stein", "assignee": "Jun Rao", "created": "2013-06-24T11:28:35.000+0000", "updated": "2013-10-08T20:05:00.000+0000", "resolved": "2013-10-08T20:04:53.000+0000", "labels": ["0.8.0-beta1"], "components": [], "comments": [{"author": "Jun Rao", "body": "Created reviewboard https://reviews.apache.org/r/14538/", "created": "2013-10-08T16:50:31.169+0000"}, {"author": "Jun Rao", "body": "Thanks for the review. Committed to 0.8.", "created": "2013-10-08T20:04:53.581+0000"}], "num_comments": 2, "text": "Issue: KAFKA-954\nSummary: tidy up README file for better general availability\nDescription: e.g. how to start server after building and all would be good too\n\nComments (2):\n1. Jun Rao: Created reviewboard https://reviews.apache.org/r/14538/\n2. Jun Rao: Thanks for the review. Committed to 0.8.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.894267"}}
{"id": "adb2f3ad05e48b0de6265be804e30a1d", "issue_key": "KAFKA-414", "issue_type": "New Feature", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Evaluate mmap-based writes for Log implementation", "description": "Working on another project I noticed that small write performance for FileChannel is really very bad. This likely effects Kafka in the case where messages are produced one at a time or in small batches. I wrote a quick program to evaluate the following options: raf = RandomAccessFile mmap = MappedByteBuffer channel = FileChannel For both of the later two I tried both direct-allocated and non-direct allocated buffers (direct allocation is supposed to be faster). Here are the results I saw: [jkreps@jkreps-ld valencia]$ java -XX:+UseConcMarkSweepGC -cp target/test-classes -server -Xmx1G -Xms1G valencia.TestLinearWritePerformance $((256*1024)) $((1*1024*1024*1024)) 2 file_length size (bytes) raf (mb/sec) channel_direct (mb/sec) mmap_direct (mb/sec) channel_heap (mb/sec) mmap_heap (mb/sec) 1000000 1 0.60 0.52 28.66 0.55 50.40 2000000 2 1.18 1.16 67.84 1.13 74.17 4000000 4 2.33 2.26 121.52 2.23 122.14 8000000 8 4.72 4.51 228.39 4.41 175.20 16000000 16 9.25 8.96 393.24 8.88 314.11 32000000 32 18.43 17.93 601.83 17.28 482.25 64000000 64 36.25 35.21 799.98 34.39 680.39 128000000 128 69.80 67.52 963.30 66.21 870.82 256000000 256 134.24 129.25 1064.13 129.01 1014.00 512000000 512 247.38 238.24 1124.71 235.57 1091.81 1024000000 1024 420.42 411.43 1170.94 406.57 1138.80 1073741824 2048 671.93 658.96 1133.63 650.39 1151.81 1073741824 4096 1007.84 989.88 1165.60 976.10 1158.49 1073741824 8192 1137.12 1145.01 1189.38 1128.30 1174.66 1073741824 16384 1172.63 1228.33 1192.19 1206.58 1156.37 1073741824 32768 1221.13 1295.37 1170.96 1262.28 1156.65 1073741824 65536 1255.23 1306.33 1160.22 1268.24 1142.52 1073741824 131072 1240.65 1292.06 1101.90 1269.00 1119.14 The size column gives the size of the write, and the length column gives the total length of the file written. Now over a period of time the 1GB/sec performance is unsustainable because the disk on my machine would not be able to keep up. Nonetheless it is worth noting that even up to 256 byte writes that is not the bottleneck, the bottleneck is the write overhead. This would indicate that a better strategy for the log would be to pre-allocate the segment and mmap it. Then use the memory map for writes and continue to use the filechannel for reads.", "reporter": "Jay Kreps", "assignee": null, "created": "2012-07-25T00:13:36.000+0000", "updated": "2017-11-16T16:45:41.000+0000", "resolved": "2017-11-16T16:45:41.000+0000", "labels": [], "components": [], "comments": [{"author": "Jay Kreps", "body": "Attached test code to generate writes and a better-formatted version of the data.", "created": "2012-07-25T00:16:10.019+0000"}, {"author": "Snke Liebau", "body": "The switch to using MappedByteBuffers was made as part of KAFKA-506 as far as I can tell, so we can probably close this issue.", "created": "2017-11-16T10:04:09.624+0000"}, {"author": "Ismael Juma", "body": "We are still using `FileChannel` for log appends (see `FileRecords.append`)", "created": "2017-11-16T10:30:54.109+0000"}, {"author": "Snke Liebau", "body": "True, I was looking at the index file code, sorry for the mixup! Is this still relevant and should be reopened then, or has there been a decision to not go down this road at some point in time? I couldn't find anything in jira or the wiki on this topic when researching..", "created": "2017-11-16T15:52:12.596+0000"}, {"author": "Ismael Juma", "body": "I don't know to be honest. The JIRA is more than 5 years old. :)", "created": "2017-11-16T15:53:57.942+0000"}, {"author": "Jay Kreps", "body": "This was meant as more of a \"memo to myself\". No reason to leave it open, the core observation remains true but I don't think it is really the biggest bottleneck and pre-allocation has other downsides.", "created": "2017-11-16T16:45:23.333+0000"}], "num_comments": 6, "text": "Issue: KAFKA-414\nSummary: Evaluate mmap-based writes for Log implementation\nDescription: Working on another project I noticed that small write performance for FileChannel is really very bad. This likely effects Kafka in the case where messages are produced one at a time or in small batches. I wrote a quick program to evaluate the following options: raf = RandomAccessFile mmap = MappedByteBuffer channel = FileChannel For both of the later two I tried both direct-allocated and non-direct allocated buffers (direct allocation is supposed to be faster). Here are the results I saw: [jkreps@jkreps-ld valencia]$ java -XX:+UseConcMarkSweepGC -cp target/test-classes -server -Xmx1G -Xms1G valencia.TestLinearWritePerformance $((256*1024)) $((1*1024*1024*1024)) 2 file_length size (bytes) raf (mb/sec) channel_direct (mb/sec) mmap_direct (mb/sec) channel_heap (mb/sec) mmap_heap (mb/sec) 1000000 1 0.60 0.52 28.66 0.55 50.40 2000000 2 1.18 1.16 67.84 1.13 74.17 4000000 4 2.33 2.26 121.52 2.23 122.14 8000000 8 4.72 4.51 228.39 4.41 175.20 16000000 16 9.25 8.96 393.24 8.88 314.11 32000000 32 18.43 17.93 601.83 17.28 482.25 64000000 64 36.25 35.21 799.98 34.39 680.39 128000000 128 69.80 67.52 963.30 66.21 870.82 256000000 256 134.24 129.25 1064.13 129.01 1014.00 512000000 512 247.38 238.24 1124.71 235.57 1091.81 1024000000 1024 420.42 411.43 1170.94 406.57 1138.80 1073741824 2048 671.93 658.96 1133.63 650.39 1151.81 1073741824 4096 1007.84 989.88 1165.60 976.10 1158.49 1073741824 8192 1137.12 1145.01 1189.38 1128.30 1174.66 1073741824 16384 1172.63 1228.33 1192.19 1206.58 1156.37 1073741824 32768 1221.13 1295.37 1170.96 1262.28 1156.65 1073741824 65536 1255.23 1306.33 1160.22 1268.24 1142.52 1073741824 131072 1240.65 1292.06 1101.90 1269.00 1119.14 The size column gives the size of the write, and the length column gives the total length of the file written. Now over a period of time the 1GB/sec performance is unsustainable because the disk on my machine would not be able to keep up. Nonetheless it is worth noting that even up to 256 byte writes that is not the bottleneck, the bottleneck is the write overhead. This would indicate that a better strategy for the log would be to pre-allocate the segment and mmap it. Then use the memory map for writes and continue to use the filechannel for reads.\n\nComments (6):\n1. Jay Kreps: Attached test code to generate writes and a better-formatted version of the data.\n2. Snke Liebau: The switch to using MappedByteBuffers was made as part of KAFKA-506 as far as I can tell, so we can probably close this issue.\n3. Ismael Juma: We are still using `FileChannel` for log appends (see `FileRecords.append`)\n4. Snke Liebau: True, I was looking at the index file code, sorry for the mixup! Is this still relevant and should be reopened then, or has there been a decision to not go down this road at some point in time? I couldn't find anything in jira or the wiki on this topic when researching..\n5. Ismael Juma: I don't know to be honest. The JIRA is more than 5 years old. :)\n6. Jay Kreps: This was meant as more of a \"memo to myself\". No reason to leave it open, the core observation remains true but I don't think it is really the biggest bottleneck and pre-allocation has other downsides.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.651129"}}
{"id": "00bb77f143e067cd5471e95ad46f5e40", "issue_key": "SPARK-1130", "issue_type": "Task", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Clean up project documentation navigation menu", "description": "1. Create a top level Configuration menu 2. Move most stuff in More into Deployment 3. Rename More to Development", "reporter": "Reynold Xin", "assignee": "Reynold Xin", "created": "2014-02-24T18:44:39.000+0000", "updated": "2014-03-18T10:01:07.000+0000", "resolved": "2014-03-18T10:01:07.000+0000", "labels": [], "components": [], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1130\nSummary: Clean up project documentation navigation menu\nDescription: 1. Create a top level Configuration menu 2. Move most stuff in More into Deployment 3. Rename More to Development", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "ffe0e8a235eef33140de2f53d55475eb", "issue_key": "HADOOP-660", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "Format of junit output should be configurable", "description": "It would be helpful if the junit output type (text or xml) could be configured by a property.", "reporter": "Nigel Daley", "assignee": "Nigel Daley", "created": "2006-10-31T22:02:03.000+0000", "updated": "2006-11-03T22:40:30.000+0000", "resolved": "2006-10-31T22:45:48.000+0000", "labels": [], "components": [], "comments": [{"author": "Nigel Daley", "body": "Allows the junit output format to be set by the test.junit.output.format property.", "created": "2006-10-31T22:02:59.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Nigel!", "created": "2006-10-31T22:45:48.000+0000"}], "num_comments": 2, "text": "Issue: HADOOP-660\nSummary: Format of junit output should be configurable\nDescription: It would be helpful if the junit output type (text or xml) could be configured by a property.\n\nComments (2):\n1. Nigel Daley: Allows the junit output format to be set by the test.junit.output.format property.\n2. Doug Cutting: I just committed this. Thanks, Nigel!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.430376"}}
{"id": "9ce095897968fe10d2f2fbe2583259ee", "issue_key": "SPARK-785", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ClosureCleaner not invoked on most PairRDDFunctions", "description": "It's pretty weird that we've missed this so far, but it seems to be the case. Unfortunately it may not be good to fix this in 0.7.3 because it could change behavior in unexpected ways; I haven't decided yet. But we should definitely do it for 0.8, and add tests.", "reporter": "Matei Alexandru Zaharia", "assignee": "Sean R. Owen", "created": "2013-06-23T16:21:05.000+0000", "updated": "2014-12-17T20:21:52.000+0000", "resolved": "2014-12-17T20:21:32.000+0000", "labels": [], "components": ["Spark Core"], "comments": [{"author": "Aaron Davidson", "body": "Is this still an open issue?", "created": "2013-11-14T18:25:56.586+0000"}, {"author": "Matei Alexandru Zaharia", "body": "[~adav] it still seems to be, weirdly enough: for instance combineByKey doesn't do it. Unless I'm missing something.", "created": "2014-11-06T07:04:34.648+0000"}, {"author": "Apache Spark", "body": "User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3690", "created": "2014-12-14T01:10:17.144+0000"}, {"author": "Josh Rosen", "body": "I've merged https://github.com/apache/spark/pull/3690 to fix this in the maintenance branches and have tagged this for a 1.2.1 backport.", "created": "2014-12-16T00:10:03.678+0000"}, {"author": "Josh Rosen", "body": "I've merged this into {{branch-1.2}}, so it will be included in Spark 1.2.1. Since this was the last backport, I'm marking this as Fixed.", "created": "2014-12-17T20:21:32.047+0000"}], "num_comments": 5, "text": "Issue: SPARK-785\nSummary: ClosureCleaner not invoked on most PairRDDFunctions\nDescription: It's pretty weird that we've missed this so far, but it seems to be the case. Unfortunately it may not be good to fix this in 0.7.3 because it could change behavior in unexpected ways; I haven't decided yet. But we should definitely do it for 0.8, and add tests.\n\nComments (5):\n1. Aaron Davidson: Is this still an open issue?\n2. Matei Alexandru Zaharia: [~adav] it still seems to be, weirdly enough: for instance combineByKey doesn't do it. Unless I'm missing something.\n3. Apache Spark: User 'srowen' has created a pull request for this issue: https://github.com/apache/spark/pull/3690\n4. Josh Rosen: I've merged https://github.com/apache/spark/pull/3690 to fix this in the maintenance branches and have tagged this for a 1.2.1 backport.\n5. Josh Rosen: I've merged this into {{branch-1.2}}, so it will be included in Spark 1.2.1. Since this was the last backport, I'm marking this as Fixed.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.054746"}}
{"id": "2a974c6cf986895b13581b30773638ca", "issue_key": "KAFKA-667", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Rename .highwatermark file", "description": "The 0.8 branch currently has a file in each log directory called .highwatermark Soon we hope to add two more files in the same format. One will hold the cleaner position for log deduplication, and the other will hold the flusher position for log flush. Each of these is sort of a \"highwater mark\". It would be good to rename .highwatermark to be a little bit more intuitive when we add these other files. I propose: replication-offset-checkpoint flusher-offset-checkpoint cleaner-offset-checkpoint replication-offset-checkpoint would replace the .highwatermark file. I am not making them dot files since they represent an important part of the persistent state and so the user should see it. Also shell * doesn't match hidden files, so if you did something like cp my_log/* to my_backup_log/* you would not get corresponding .highwatermark file. I am filing this bug now because it might be nice to just make this trivial change now and avoid having to handle backwards compatibility later.", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "created": "2012-12-10T22:59:45.000+0000", "updated": "2012-12-11T19:36:16.000+0000", "resolved": "2012-12-11T19:36:16.000+0000", "labels": [], "components": [], "comments": [{"author": "Jay Kreps", "body": "Trivial patch to change the name of the highwatermark file. Ready for review.", "created": "2012-12-11T15:48:15.738+0000"}, {"author": "Jun Rao", "body": "Thanks for the patch. +1.", "created": "2012-12-11T17:23:22.899+0000"}, {"author": "Neha Narkhede", "body": "+1", "created": "2012-12-11T18:16:03.437+0000"}], "num_comments": 3, "text": "Issue: KAFKA-667\nSummary: Rename .highwatermark file\nDescription: The 0.8 branch currently has a file in each log directory called .highwatermark Soon we hope to add two more files in the same format. One will hold the cleaner position for log deduplication, and the other will hold the flusher position for log flush. Each of these is sort of a \"highwater mark\". It would be good to rename .highwatermark to be a little bit more intuitive when we add these other files. I propose: replication-offset-checkpoint flusher-offset-checkpoint cleaner-offset-checkpoint replication-offset-checkpoint would replace the .highwatermark file. I am not making them dot files since they represent an important part of the persistent state and so the user should see it. Also shell * doesn't match hidden files, so if you did something like cp my_log/* to my_backup_log/* you would not get corresponding .highwatermark file. I am filing this bug now because it might be nice to just make this trivial change now and avoid having to handle backwards compatibility later.\n\nComments (3):\n1. Jay Kreps: Trivial patch to change the name of the highwatermark file. Ready for review.\n2. Jun Rao: Thanks for the patch. +1.\n3. Neha Narkhede: +1", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.792940"}}
{"id": "4458a5b1412719c379a7b069f56ae788", "issue_key": "SPARK-807", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Show totals for shuffle data and CPU time in Stage pages of UI", "description": "Might also be good to show it on the homepage overview of Stages.", "reporter": "Matei Alexandru Zaharia", "assignee": "Karen Feng", "created": "2013-07-13T17:42:18.000+0000", "updated": "2013-07-29T17:03:00.000+0000", "resolved": "2013-07-29T17:03:00.000+0000", "labels": [], "components": ["Web UI"], "comments": [{"author": "Patrick McFadin", "body": "The reason it doesn't do this now is that computing this is `O(tasks)` and you could imagine a user with a very large number of tasks accounted for in the UI. For finished stages, we should just compute this once and keep it around.", "created": "2013-07-13T18:20:42.705+0000"}, {"author": "Matei Alexandru Zaharia", "body": "Yes, I think we should save it for finished stages and compute it on the fly for running ones. Another option is to keep a mutable variable for each stage that counts these (it's tougher for time if we want to sum the CPU-seconds used by all tasks so far I guess).", "created": "2013-07-13T18:49:06.672+0000"}], "num_comments": 2, "text": "Issue: SPARK-807\nSummary: Show totals for shuffle data and CPU time in Stage pages of UI\nDescription: Might also be good to show it on the homepage overview of Stages.\n\nComments (2):\n1. Patrick McFadin: The reason it doesn't do this now is that computing this is `O(tasks)` and you could imagine a user with a very large number of tasks accounted for in the UI. For finished stages, we should just compute this once and keep it around.\n2. Matei Alexandru Zaharia: Yes, I think we should save it for finished stages and compute it on the fly for running ones. Another option is to keep a mutable variable for each stage that counts these (it's tougher for time if we want to sum the CPU-seconds used by all tasks so far I guess).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.058756"}}
{"id": "5ccdc707ecb03089cce47fc1fba34b4f", "issue_key": "SPARK-745", "issue_type": "Improvement", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "Document Scala environment configuration when Scala is installed from RPM", "description": "As https://github.com/mesos/spark/pull/292 points out, the Typesafe Scala RPM installs the {{scala}} executable in {{/usr/bin/}} and places the Scala library JARs in {{/usr/share/java/}}. That pull request added a SCALA_LIBRARY_PATH setting to support this method of installing Scala. We should document proper configuration of SCALA_LIBRARY_PATH and SCALA_HOME; right now, that pull request is the only documentation and it's implicit that the proper Debian / CentOS settings are SCALA_HOME=/usr/ and SCALA_LIBRARY_PATH=/usr/share/java/. I propose adding extra comments to the spark-conf.sh template that describe typical values for these settings. Maybe we also want to mention this in the docs somewhere?", "reporter": "Josh Rosen", "assignee": "Matei Alexandru Zaharia", "created": "2013-05-07T18:20:45.000+0000", "updated": "2013-06-30T17:14:43.000+0000", "resolved": "2013-06-30T17:14:43.000+0000", "labels": [], "components": ["Documentation"], "comments": [{"author": "Matei Alexandru Zaharia", "body": "I've now documented these: https://github.com/mesos/spark/commit/5bbd0eec84867937713ceb8438f25a943765a084", "created": "2013-06-30T17:14:43.692+0000"}], "num_comments": 1, "text": "Issue: SPARK-745\nSummary: Document Scala environment configuration when Scala is installed from RPM\nDescription: As https://github.com/mesos/spark/pull/292 points out, the Typesafe Scala RPM installs the {{scala}} executable in {{/usr/bin/}} and places the Scala library JARs in {{/usr/share/java/}}. That pull request added a SCALA_LIBRARY_PATH setting to support this method of installing Scala. We should document proper configuration of SCALA_LIBRARY_PATH and SCALA_HOME; right now, that pull request is the only documentation and it's implicit that the proper Debian / CentOS settings are SCALA_HOME=/usr/ and SCALA_LIBRARY_PATH=/usr/share/java/. I propose adding extra comments to the spark-conf.sh template that describe typical values for these settings. Maybe we also want to mention this in the docs somewhere?\n\nComments (1):\n1. Matei Alexandru Zaharia: I've now documented these: https://github.com/mesos/spark/commit/5bbd0eec84867937713ceb8438f25a943765a084", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.044716"}}
{"id": "9616108f516adce9f9a093c3f289799d", "issue_key": "KAFKA-859", "issue_type": "Improvement", "status": "Patch Available", "priority": "Major", "resolution": null, "summary": "support basic auth protection of mx4j console", "description": "Support configuration of a basic auth username and password to protect the mx4j console.", "reporter": "Scott Clasen", "assignee": null, "created": "2013-04-09T20:06:32.000+0000", "updated": "2020-05-28T15:38:57.000+0000", "resolved": null, "labels": [], "components": ["core"], "comments": [{"author": "Scott Clasen", "body": "--- core/src/main/scala/kafka/utils/Mx4jLoader.scala | 23 +++++++++++++++++++---- 1 file changed, 19 insertions(+), 4 deletions(-) diff --git a/core/src/main/scala/kafka/utils/Mx4jLoader.scala b/core/src/main/scala/kafka/utils/Mx4jLoader.scala index 64d84cc..539433d 100644 --- a/core/src/main/scala/kafka/utils/Mx4jLoader.scala +++ b/core/src/main/scala/kafka/utils/Mx4jLoader.scala @@ -5,8 +5,8 @@ * The ASF licenses this file to You under the Apache License, Version 2.0 * (the \"License\"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at - * - * http://www.apache.org/licenses/LICENSE-2.0 + * + * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, @@ -20,16 +20,18 @@ package kafka.utils import java.lang.management.ManagementFactory import javax.management.ObjectName +import util.Properties /** * If mx4j-tools is in the classpath call maybeLoad to load the HTTP interface of mx4j. * * The default port is 8082. To override that provide e.g. -Dmx4jport=8083 * The default listen address is 0.0.0.0. To override that provide -Dmx4jaddress=127.0.0.1 + * To set a basic auth username and password, specify -Dmx4jcredentials=user:pass * This feature must be enabled with -Dmx4jenable=true * * This is a Scala port of org.apache.cassandra.utils.Mx4jTool written by Ran Tavory for CASSANDRA-1068 - * */ + **/ object Mx4jLoader extends Logging { def maybeLoad(): Boolean = { @@ -38,9 +40,16 @@ object Mx4jLoader extends Logging { false val address = props.getString(\"mx4jaddress\", \"0.0.0.0\") val port = props.getInt(\"mx4jport\", 8082) + + try { debug(\"Will try to load MX4j now, if it's in the classpath\"); + val creds = Properties.propOrNone(\"mx4jcredentials\").map(_.split(\":\", 2)).flatMap { + case Array(user, pass) => Some((user, pass)) + case _ => throw new RuntimeException(\"Couldn't parse mx4jcredentials, please format correctly. user:password\") + } + val mbs = ManagementFactory.getPlatformMBeanServer() val processorName = new ObjectName(\"Server:name=XSLTProcessor\") @@ -49,6 +58,12 @@ object Mx4jLoader extends Logging { httpAdaptorClass.getMethod(\"setHost\", classOf[String]).invoke(httpAdaptor, address.asInstanceOf[AnyRef]) httpAdaptorClass.getMethod(\"setPort\", Integer.TYPE).invoke(httpAdaptor, port.asInstanceOf[AnyRef]) + creds.foreach { + case (user, pass) => + httpAdaptorClass.getMethod(\"setAuthenticationMethod\", classOf[String]).invoke(httpAdaptor, \"basic\".asInstanceOf[AnyRef]) + httpAdaptorClass.getMethod(\"addAuthorization\", classOf[String], classOf[String]).invoke(httpAdaptor, user.asInstanceOf[AnyRef], pass.asInstanceOf[AnyRef]) + } + val httpName = new ObjectName(\"system:name=http\") mbs.registerMBean(httpAdaptor, httpName) @@ -61,7 +76,7 @@ object Mx4jLoader extends Logging { true } catch { - case e: ClassNotFoundException => { + case e: ClassNotFoundException => { info(\"Will not load MX4J, mx4j-tools.jar is not in the classpath\"); } case e => { -- 1.8.0.1", "created": "2013-04-09T20:08:51.322+0000"}, {"author": "Neha Narkhede", "body": "Is it possible to add a unit test for this ?", "created": "2013-04-09T23:19:48.133+0000"}, {"author": "Scott Clasen", "body": "Not sure its possible/desirable. Since current kafka practice is to downolad mx4j-tools.jar and put it on the classpath for this code to be invoked, that jar would have to be included in the kafka codebase wouldn't it? Would you want that jar checked in?", "created": "2013-04-10T00:07:08.473+0000"}, {"author": "Snke Liebau", "body": "Is this still something we would want to do? The issue has not seen any updates in five and a half years, but apparently people are still using mx4j in recent-ish versions (KAFKA-4946)? Looking at the mx4j project it seems to have died sometime around 2012, should we maybe consider replacing it with something like [Jolokia|https://jolokia.org/] if we really want jmx over http?", "created": "2018-11-29T22:05:16.882+0000"}, {"author": "Matthias J. Sax", "body": "Is this still a valid ticket? \\cc [~rsivaram]", "created": "2020-05-27T21:05:45.094+0000"}, {"author": "Rajini Sivaram", "body": "We should decide what to do with `Mx4jLoader`. If we want to continue to support it, then adding security makes sense. Otherwise we should remove that class altogether, not sure anyone actually uses it. Either way, I guess it needs a KIP.", "created": "2020-05-28T15:38:57.189+0000"}], "num_comments": 6, "text": "Issue: KAFKA-859\nSummary: support basic auth protection of mx4j console\nDescription: Support configuration of a basic auth username and password to protect the mx4j console.\n\nComments (6):\n1. Scott Clasen: --- core/src/main/scala/kafka/utils/Mx4jLoader.scala | 23 +++++++++++++++++++---- 1 file changed, 19 insertions(+), 4 deletions(-) diff --git a/core/src/main/scala/kafka/utils/Mx4jLoader.scala b/core/src/main/scala/kafka/utils/Mx4jLoader.scala index 64d84cc..539433d 100644 --- a/core/src/main/scala/kafka/utils/Mx4jLoader.scala +++ b/core/src/main/scala/kafka/utils/Mx4jLoader.scala @@ -5,8 +5,8 @@ * The ASF licenses this file to You under the Apache License, Version 2.0 * (the \"License\"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at - * - * http://www.apache.org/licenses/LICENSE-2.0 + * + * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, @@ -20,16 +20,18 @@ package kafka.utils import java.lang.management.ManagementFactory import javax.management.ObjectName +import util.Properties /** * If mx4j-tools is in the classpath call maybeLoad to load the HTTP interface of mx4j. * * The default port is 8082. To override that provide e.g. -Dmx4jport=8083 * The default listen address is 0.0.0.0. To override that provide -Dmx4jaddress=127.0.0.1 + * To set a basic auth username and password, specify -Dmx4jcredentials=user:pass * This feature must be enabled with -Dmx4jenable=true * * This is a Scala port of org.apache.cassandra.utils.Mx4jTool written by Ran Tavory for CASSANDRA-1068 - * */ + **/ object Mx4jLoader extends Logging { def maybeLoad(): Boolean = { @@ -38,9 +40,16 @@ object Mx4jLoader extends Logging { false val address = props.getString(\"mx4jaddress\", \"0.0.0.0\") val port = props.getInt(\"mx4jport\", 8082) + + try { debug(\"Will try to load MX4j now, if it's in the classpath\"); + val creds = Properties.propOrNone(\"mx4jcredentials\").map(_.split(\":\", 2)).flatMap { + case Array(user, pass) => Some((user, pass)) + case _ => throw new RuntimeException(\"Couldn't parse mx4jcredentials, please format correctly. user:password\") + } + val mbs = ManagementFactory.getPlatformMBeanServer() val processorName = new ObjectName(\"Server:name=XSLTProcessor\") @@ -49,6 +58,12 @@ object Mx4jLoader extends Logging { httpAdaptorClass.getMethod(\"setHost\", classOf[String]).invoke(httpAdaptor, address.asInstanceOf[AnyRef]) httpAdaptorClass.getMethod(\"setPort\", Integer.TYPE).invoke(httpAdaptor, port.asInstanceOf[AnyRef]) + creds.foreach { + case (user, pass) => + httpAdaptorClass.getMethod(\"setAuthenticationMethod\", classOf[String]).invoke(httpAdaptor, \"basic\".asInstanceOf[AnyRef]) + httpAdaptorClass.getMethod(\"addAuthorization\", classOf[String], classOf[String]).invoke(httpAdaptor, user.asInstanceOf[AnyRef], pass.asInstanceOf[AnyRef]) + } + val httpName = new ObjectName(\"system:name=http\") mbs.registerMBean(httpAdaptor, httpName) @@ -61,7 +76,7 @@ object Mx4jLoader extends Logging { true } catch { - case e: ClassNotFoundException => { + case e: ClassNotFoundException => { info(\"Will not load MX4J, mx4j-tools.jar is not in the classpath\"); } case e => { -- 1.8.0.1\n2. Neha Narkhede: Is it possible to add a unit test for this ?\n3. Scott Clasen: Not sure its possible/desirable. Since current kafka practice is to downolad mx4j-tools.jar and put it on the classpath for this code to be invoked, that jar would have to be included in the kafka codebase wouldn't it? Would you want that jar checked in?\n4. Snke Liebau: Is this still something we would want to do? The issue has not seen any updates in five and a half years, but apparently people are still using mx4j in recent-ish versions (KAFKA-4946)? Looking at the mx4j project it seems to have died sometime around 2012, should we maybe consider replacing it with something like [Jolokia|https://jolokia.org/] if we really want jmx over http?\n5. Matthias J. Sax: Is this still a valid ticket? \\cc [~rsivaram]\n6. Rajini Sivaram: We should decide what to do with `Mx4jLoader`. If we want to continue to support it, then adding security makes sense. Otherwise we should remove that class altogether, not sure anyone actually uses it. Either way, I guess it needs a KIP.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.862650"}}
{"id": "36d0eb95b32dd0b3ca95f4ba2402efe8", "issue_key": "HADOOP-819", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "LineRecordWriter should not always insert tab char between key and value", "description": "With the current implementation of LineRecordWriter in TextOutputFormat, the client cannot pass null key/or value to the write function, and a tab char is always inserted between the key and value. This works fine most time. However, in some cases, one just does not want to have the extra tab char. A common example is that, if I need to implement a utility similar to the unix sort with some fields in the lines as the sort key, I can have my map to extract the sort key from each line and pass the whole line as the value. The reducer just outputs the values and ignore the keys. However, if I use TextOutputFormat, my output will have an extra tab key in each of the lines, which is annoying. A simple solution is that let the write function of LineRecordWriter accept null key argument, and write out the value only if the key is null.", "reporter": "Runping Qi", "assignee": "Runping Qi", "created": "2006-12-12T22:33:02.000+0000", "updated": "2009-07-08T16:52:03.000+0000", "resolved": "2007-04-11T19:38:57.000+0000", "labels": [], "components": [], "comments": [{"author": "Owen O'Malley", "body": "I'd propose something like: if (key != null) { out.write(key.toString()); } if (key !=null && value != null) { out.write(\"\\t\"); } if (value != null) { out.write(value.toString()); }", "created": "2007-03-22T23:12:37.457+0000"}, {"author": "Runping Qi", "body": "+1", "created": "2007-03-23T00:07:24.687+0000"}, {"author": "Runping Qi", "body": "A quick/safe fix", "created": "2007-04-04T22:17:03.256+0000"}, {"author": "Owen O'Malley", "body": "You have a spurious whitespace change at the end of the patch and the patch does not have a unit test case.", "created": "2007-04-04T23:02:00.409+0000"}, {"author": "Hadoop QA", "body": "+1, because http://issues.apache.org/jira/secure/attachment/12354956/patch-819.txt applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/525596. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch", "created": "2007-04-05T01:27:59.242+0000"}, {"author": "Runping Qi", "body": "Added unit test for the cases calling write method with null key and/or value", "created": "2007-04-05T06:02:30.550+0000"}, {"author": "Runping Qi", "body": "a new patch with unit test code included", "created": "2007-04-05T06:03:20.382+0000"}, {"author": "Owen O'Malley", "body": "Thanks for the unit test Runping! I think it would be a much stronger test if you checked the results of what was put out on the stream.", "created": "2007-04-05T15:48:43.495+0000"}, {"author": "Runping Qi", "body": "Of course. And we need to have unit tests for many other aspects of TextOutputFormat/LineRecordWriters.", "created": "2007-04-05T16:12:17.590+0000"}, {"author": "Owen O'Malley", "body": "You can easily read the file into a String with TestMiniMRWithDFS.readOutput(Path,JobConf) and use assertEquals on the output to the expected value.", "created": "2007-04-06T17:01:21.024+0000"}, {"author": "Runping Qi", "body": "re-load a new patch", "created": "2007-04-10T17:42:13.369+0000"}, {"author": "Runping Qi", "body": "A new patch with updated unit tests", "created": "2007-04-10T17:43:40.885+0000"}, {"author": "Runping Qi", "body": "A new patch with updated unit test", "created": "2007-04-10T17:49:24.125+0000"}, {"author": "Hadoop QA", "body": "+1 http://issues.apache.org/jira/secure/attachment/12355260/patch-819.txt applied and successfully tested against trunk revision r527100. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/20/console", "created": "2007-04-10T19:46:56.300+0000"}, {"author": "Owen O'Malley", "body": "+1", "created": "2007-04-10T21:31:32.467+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Runping!", "created": "2007-04-11T19:38:57.333+0000"}, {"author": "Hadoop QA", "body": "Integrated in Hadoop-Nightly #55 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/55/)", "created": "2007-04-12T11:21:38.626+0000"}], "num_comments": 17, "text": "Issue: HADOOP-819\nSummary: LineRecordWriter should not always insert tab char between key and value\nDescription: With the current implementation of LineRecordWriter in TextOutputFormat, the client cannot pass null key/or value to the write function, and a tab char is always inserted between the key and value. This works fine most time. However, in some cases, one just does not want to have the extra tab char. A common example is that, if I need to implement a utility similar to the unix sort with some fields in the lines as the sort key, I can have my map to extract the sort key from each line and pass the whole line as the value. The reducer just outputs the values and ignore the keys. However, if I use TextOutputFormat, my output will have an extra tab key in each of the lines, which is annoying. A simple solution is that let the write function of LineRecordWriter accept null key argument, and write out the value only if the key is null.\n\nComments (17):\n1. Owen O'Malley: I'd propose something like: if (key != null) { out.write(key.toString()); } if (key !=null && value != null) { out.write(\"\\t\"); } if (value != null) { out.write(value.toString()); }\n2. Runping Qi: +1\n3. Runping Qi: A quick/safe fix\n4. Owen O'Malley: You have a spurious whitespace change at the end of the patch and the patch does not have a unit test case.\n5. Hadoop QA: +1, because http://issues.apache.org/jira/secure/attachment/12354956/patch-819.txt applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/525596. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch\n6. Runping Qi: Added unit test for the cases calling write method with null key and/or value\n7. Runping Qi: a new patch with unit test code included\n8. Owen O'Malley: Thanks for the unit test Runping! I think it would be a much stronger test if you checked the results of what was put out on the stream.\n9. Runping Qi: Of course. And we need to have unit tests for many other aspects of TextOutputFormat/LineRecordWriters.\n10. Owen O'Malley: You can easily read the file into a String with TestMiniMRWithDFS.readOutput(Path,JobConf) and use assertEquals on the output to the expected value.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.489749"}}
{"id": "5a2efeed1262897447b03c586c8de823", "issue_key": "SPARK-556", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Results from tasks should be returned through our own sockets rather than Mesos framework messages", "description": "With large results, the Mesos library might not like sending large framework messages. We can avoid this by sending the results to the master through a socket rather than through sendFrameworkMessage. Another thing that would help is warning the user in this case.", "reporter": "Matei Alexandru Zaharia", "assignee": "Kay Ousterhout", "created": "0011-11-19T14:56:00.000+0000", "updated": "2013-10-10T18:07:14.000+0000", "resolved": "2013-10-10T18:07:14.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Imported from Github issue spark-94, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 1, "text": "Issue: SPARK-556\nSummary: Results from tasks should be returned through our own sockets rather than Mesos framework messages\nDescription: With large results, the Mesos library might not like sending large framework messages. We can avoid this by sending the results to the master through a socket rather than through sendFrameworkMessage. Another thing that would help is warning the user in this case.\n\nComments (1):\n1. Patrick McFadin: Imported from Github issue spark-94, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.948183"}}
{"id": "8094c2451c7852c2d3f1637c6e6dadc5", "issue_key": "HADOOP-970", "issue_type": "Bug", "status": "Open", "priority": "Major", "resolution": null, "summary": "Reduce CPU usage of hadoop ipc package", "description": "There are a couple of optimizations that could be done to reduce CPU consumption. 1. The method Server.cleanupConnections() could be invoked less often. 2. The method Server.cleanupConnections() uses a List to manage all active connections and uses connectionList.get(i) to iterate. Locating the ith element essentially translates to traversing the list from the beginning to the ith position. 3. The current DFS heartbeattime is 3 seconds whereas ipc.client.connection.maxidletime is set to 1 second. The proposal is to change the default value of ipc.client.connection.maxidletime to something larger than the heartbeat interval. This also has to suit the heartbeat periodicity of map-reduce software. 4. Evaluate epoll() added in JDK 1.5.10 (this is a java cmd line option) http://java.sun.com/j2se/1.5.0/ReleaseNotes.html#150_10", "reporter": "Dhruba Borthakur", "assignee": null, "created": "2007-02-02T19:02:29.000+0000", "updated": "2011-07-16T18:26:47.000+0000", "resolved": null, "labels": [], "components": ["ipc"], "comments": [{"author": "Harsh J", "body": "Dhruba, Looking at the blocker linked to this ticket, is the problem still valid on trunk or a maintained release today with Java 1.6 being the bare minimum requirement?", "created": "2011-07-16T18:26:47.332+0000"}], "num_comments": 1, "text": "Issue: HADOOP-970\nSummary: Reduce CPU usage of hadoop ipc package\nDescription: There are a couple of optimizations that could be done to reduce CPU consumption. 1. The method Server.cleanupConnections() could be invoked less often. 2. The method Server.cleanupConnections() uses a List to manage all active connections and uses connectionList.get(i) to iterate. Locating the ith element essentially translates to traversing the list from the beginning to the ith position. 3. The current DFS heartbeattime is 3 seconds whereas ipc.client.connection.maxidletime is set to 1 second. The proposal is to change the default value of ipc.client.connection.maxidletime to something larger than the heartbeat interval. This also has to suit the heartbeat periodicity of map-reduce software. 4. Evaluate epoll() added in JDK 1.5.10 (this is a java cmd line option) http://java.sun.com/j2se/1.5.0/ReleaseNotes.html#150_10\n\nComments (1):\n1. Harsh J: Dhruba, Looking at the blocker linked to this ticket, is the problem still valid on trunk or a maintained release today with Java 1.6 being the bare minimum requirement?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.531908"}}
{"id": "aedbd4034fb44fa8cb27e8fb1d3f7c2f", "issue_key": "HADOOP-882", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "S3FileSystem should retry if there is a communication problem with S3", "description": "File system operations currently fail if there is a communication problem (IOException) with S3. All operations that communicate with S3 should retry a fixed number of times before failing.", "reporter": "Thomas White", "assignee": "Thomas White", "created": "2007-01-11T09:16:43.000+0000", "updated": "2007-03-02T23:02:07.000+0000", "resolved": "2007-02-09T20:49:54.000+0000", "labels": [], "components": ["fs"], "comments": [{"author": "Bryan Pendleton", "body": "Argh. Jira just ate my comment, this will be a terser version. -- Retry levels should be configurable, up to the point of infinite retry. Long running stream operations are better not dying, even if they have to wait while S3 fixes hardware shortages/failures. Not sure if it's a separate issue, but failed writes aren't cleaned-up very well right now. In DFS, a file that isn't closed doesn't exist for other operations - if possible, there should at least be a way to find out if a file in S3 is \"done\", preferably it should be invisible to \"normal\" operations while its state is not final.", "created": "2007-01-11T19:23:59.234+0000"}, {"author": "Mike Smith", "body": "I've been working on a patch to handle these exceptions. There are three major exceptions that need to be retried. InternalError RequestTimeout OperationAborted InternalError exception has reasonably high rate for PUT request! I have finished the patch for Jets3tFileSystem.java which exponentially increases the waiting time. But, I've been dealing with a very strange problem. When I get the internalError (500) response, and I want to retry the request I keep getting RequestTimeout response from S3. This is a client exception and it shows that jets3t closes the connection after InternalError Exception! Even when I try to restablish the connection I still get the same RequestTimeout response. Following is the changed put() method in Jets3tFileSystem.java, I have done similar changes for other methods as well, let me know if you see something wrong: private void put(String key, InputStream in, long length) throws IOException { int attempts = 0; while(true){ try{ S3Object object = new S3Object(key); object.setDataInputStream(in); object.setContentType(\"binary/octet-stream\"); object.setContentLength(length); s3Service.putObject(bucket, object); break; } catch (S3ServiceException e) { if(!retry(e,++attempts)){ if (e.getCause() instanceof IOException) { throw (IOException) e.getCause(); } throw new S3Exception(e); } } } } private boolean retry(S3ServiceException e,int attempts){ if(attempts > maxRetry) return false; // for internal exception (500), retry is allowed if(e.getErrorCode().equals(S3_INTERNAL_ERROR_CODE) || e.getErrorCode().equals(S3_OPERATION_ABORTED_CODE)){ LOG.info(\"retrying failed s3Service [\"+e.getErrorCode()+\"]. Delay: \" + retryDelay*attempts+\" msec. Attempts: \"+attempts); try{ Thread.sleep(retryDelay*attempts); } catch(Exception ee){} return true; } // allows retry for the socket timeout exception. // connection needs to be restablished. if(e.getErrorCode().equals(S3_REQUEST_TIMEOUT_CODE)){ try{ AWSCredentials awsCredentials = new AWSCredentials(accessKey, secretAccessKey); this.s3Service = new RestS3Service(awsCredentials); } catch (S3ServiceException ee) { // this exception will be taken care of later } LOG.info(\"retrying failed s3Service [\"+e.getErrorCode()+\"]. Attempts: \"+attempts); return true; } // for all other exceptions retrying is not allowed // Maybe it would be better to keep retrying for all sorts of exceptions!? return false; }", "created": "2007-01-20T02:17:14.721+0000"}, {"author": "Thomas White", "body": "As you say, the immediate RequestTimeout after an InternalError looks like a jets3t issue. Probably worth looking at the jets3t source or sending a message to the jets3t list? To implement retries, I was imagining creating a wrapper implementation of FileSystemStore that only had retry functionality in it and delegated to another FileSystemStore for actual storage. This would make testing easier as well as isolating the retry code. I think the InputStreams in FileSystemStore would need to be changed to be Files, but this should be fine.", "created": "2007-01-21T22:18:28.620+0000"}, {"author": "Doug Cutting", "body": "> To implement retries, I was imagining creating a wrapper implementation of FileSystemStore that only had retry functionality This reminds me of HADOOP-601. That's not directly applicable, since S3 doesn't use Hadoop's RPC. But it makes me wonder if perhaps we ought to pursue a generic retry mechanism. Perhaps we could have a RetryProxy that uses java.lang.reflect.Proxy to implement retries for all methods in an interface. One could perhaps specify per-method retry policies as a ctor parameter, e.g.: MethodRetry[] retries = new MethodRetry[] { new MethodRetry(\"myFirstMethod\", ONCE), new MethodRetry(\"mySecondMethod, FOREVER) } MyInterface impl = new MyInterfaceImpl(); MyInterface proxy = RetryProxy.create(MyInterface.class, impl, retries); or somesuch. Could this work?", "created": "2007-01-22T19:50:38.300+0000"}, {"author": "James P. White", "body": "I like the idea of a general approach to retries, but using an existing AOP mechanism seems like a better way to go to me. Although AspectJ is a strong tool, Spring 2.0 AOP is closer to what you propose. In addition to Spring's autoproxies, Spring 2.0 fully supports AspectJ syntax including annotations. That provides the greatest flexibility and avoids the invention of yet-another-syntax for advice. http://static.springframework.org/spring/docs/2.0.x/reference/new-in-2.html#new-in-2-aop", "created": "2007-01-22T21:08:08.135+0000"}, {"author": "Mike Smith", "body": "Instead of the released version of jets3t which is 5 months old, I built it from their CVS source codes. In this version unlike the released version, you will have the retry mechanism for S3 exceptions. The CVS version seems to be much more stable than the released version. Here are the two most important new features from their release notes: - Requests that fail due to S3 Internal Server error are retried a configurable number of times, with an increasing delay between each retry attempt. - Sends an upload object's MD5 data hash to S3 in the header Content-MD5, to confirm no data corruption has taken place on the wire.", "created": "2007-01-23T18:25:55.705+0000"}, {"author": "Thomas White", "body": "I was actually just writing the following response: If we throw a different type of exception (TransientS3Exception) for recoverable error codes (there's a complete list of error codes here BTW: http://docs.amazonwebservices.com/AmazonS3/2006-03-01/ErrorCodeList.html) then we can make the proxy only retry for that exception. This is in line with HADOOP-601, but until that's done we could build an S3RetryProxy just for S3. We still might want to follow this more general approach (if it gives us more control), but for the time being if jets3t's retry mechanism is adequate then let's try that.", "created": "2007-01-23T18:58:05.538+0000"}, {"author": "Thomas White", "body": "Looks like jets3t version 0.5.0 was released yesterday: http://jets3t.s3.amazonaws.com/downloads.html. Configuration notes are here: http://jets3t.s3.amazonaws.com/toolkit/configuration.html - s3service.internal-error-retry-max is the relevant one.", "created": "2007-01-23T19:20:45.159+0000"}, {"author": "Michael Stack", "body": "I updated jets3t to the 0.5.0 release. I had to make the below edits. The API has probably changed in other ways but I've not spent the time verifying. Unless someone else is working on a patch that includes the new version of the jets3t lib and complimentary changes to s3 fs, I can give it a go (retries are necessary it seems if you're trying to upload anything more than a few kilobytes). Related, after adding the new lib and making below changes, uploads ('puts') would fail with below complaint. 07/01/31 00:47:13 WARN service.S3Service: Encountered 1 S3 Internal Server error(s), will retry in 50ms put: Input stream is not repeatable as 1048576 bytes have been written, exceeding the available buffer size of 131072 I found the 131072 buffer in jets3t. Turns out the buffer size is configurable. Dropping a jets3t.properties file into ${HADOOP_HOME}/conf directory (so a lookup on CLASSPATH succeeds) with amended s3service.stream-retry-buffer-size got me over the 'put: Input stream...' hump. I set it to the value of dfs.block.size so it could replay a full-block if it had to. Then I noticed that the blocks written to S3 were of 1MB in size. I'm uploading tens of Gs so that made for tens of thousands of blocks. No harm I suppose but I was a little stumped that block size in S3 wasn't the value of dfs.block.size. I found the fs.s3.block.size property in the S3 fs code. Shouldn't this setting be bubbled up into hadoop-default with a default of value of ${dfs.block.size}? (Setting this in my config. made for 64MB S3 blocks). I can add the latter items to the wiki on S3 or can include an jets3t.properties and s3.block.size in patch. What do others think? Index: src/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java =================================================================== --- src/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java (revision 501895) +++ src/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java (working copy) @@ -133,7 +133,7 @@ S3Object object = s3Service.getObject(bucket, key); return object.getDataInputStream(); } catch (S3ServiceException e) { - if (e.getErrorCode().equals(\"NoSuchKey\")) { + if (e.getS3ErrorCode().equals(\"NoSuchKey\")) { return null; } if (e.getCause() instanceof IOException) { @@ -149,7 +149,7 @@ null, byteRangeStart, null); return object.getDataInputStream(); } catch (S3ServiceException e) { - if (e.getErrorCode().equals(\"NoSuchKey\")) { + if (e.getS3ErrorCode().equals(\"NoSuchKey\")) { return null; } if (e.getCause() instanceof IOException) {", "created": "2007-01-31T17:27:14.244+0000"}, {"author": "Thomas White", "body": "The best way to check the new jets3t library is to run the Jets3tS3FileSystemTest unit test. You will need to set your S3 credentials in the hadoop-site.xml file in the test directory. If this passes you can be confident that the upgrade has worked. Changing the buffer size to be as big as the block size sounds good (however, I worry a little whether there could be a memory issue if jets3t buffers in memory, as seems likely). The 1MB block size was a fairly arbitrary value I selected during testing. I agree that 64MB would be better. The property fs.s3.block.size property needs adding to hadoop-default.xml, and the DEFAULT_BLOCK_SIZE constant in S3FileSystem needs changing too. A patch for all this stuff would be very welcome! As for whether completing all these items would mean the issue is closed I'm not sure. The jets3t retry mechanism is for S3-level exceptions, if there is a traffic-level communication problem then there is nothing to handle it. For this, we could use the more general Hadoop-level mechanism, described above (or possibly use the retry-mechanism in HttpClient, if that's sufficient). I think this work would belong in another Jira issue. Thoughts?", "created": "2007-01-31T20:50:20.827+0000"}, {"author": "Michael Stack", "body": "Here's a patch that makes the minor changes necessary so the s3 implementation can use the new 0.5.0 jets3t 'retrying' lib. It also exposes fs.s3.block.size in hadoop-default.xml with a note about how to set the jets3t RepeatableInputStream buffer size by adding a jets3t.properties to ${HADOOP_HOME}/conf. Setting this latter buffer to the same as the s3 block size avoids failures of the kind 'Input stream is not repeatable as 1048576 bytes have been written, exceeding the available buffer size of 131072'. Downside to this patch's approach is that if you want to match block and buffer size, you need to set the same value in two places: once in hadoop-site and again in jets3t.properties. This seemed to be me better than the alternative, a tighter coupling bubbling the main jets3t properties up into hadoop-*.xml filesystem section as fs.s3.jets3t.XXX properties with the init of the s3 filesystem setting the values into the org.jets3t.service.Jets3tProperties. I didn't change the default S3 block size from 1MB. Setting it to 64MB seems too far afield from the default jets3t RepeatableInputStream size of 100k only. I've included the 0.5.0 jets3t lib as part of the upload (There doesn't seem to be a way to include binaries using svn diff). Its license is apache 2. Tom White, thanks for pointing me at the unit test. Also, I'd go along with closing this issue with the update of jets3t lib opening another issue for tracking the S3 filesystems implementing a general, 'traffic-level' hadoop retry mechanism.", "created": "2007-02-08T02:24:39.934+0000"}, {"author": "Thomas White", "body": "+1 I have tested the patch successfully using Jets3tS3FileSystemTest and think it is ready for inclusion. This patch will certainly improve the reliability of S3 \"metadata\" operations since they fit into the jets3t buffer limit. Block read and write failures aren't retried effectively, so for these cases I have created HADOOP-997 as a follow on issue.", "created": "2007-02-08T21:40:42.771+0000"}, {"author": "Hadoop QA", "body": "-1, because 3 attempts failed to build and test the latest attachment (http://issues.apache.org/jira/secure/attachment/12350614/jets3t-upgrade.patch) against trunk revision r504682. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch.", "created": "2007-02-08T21:47:03.287+0000"}, {"author": "Michael Stack", "body": "Nigel, I'm guessing the patch application failed because it doesn't incorporate removal of the jar ${HADOOP_HOME}/lib/jets3t.jar and replacement with the attached jets3t-0.5.0.jar. Without the latter lib, the patch won't work. Thanks.", "created": "2007-02-08T22:26:18.672+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Michael!", "created": "2007-02-09T20:49:54.561+0000"}], "num_comments": 15, "text": "Issue: HADOOP-882\nSummary: S3FileSystem should retry if there is a communication problem with S3\nDescription: File system operations currently fail if there is a communication problem (IOException) with S3. All operations that communicate with S3 should retry a fixed number of times before failing.\n\nComments (15):\n1. Bryan Pendleton: Argh. Jira just ate my comment, this will be a terser version. -- Retry levels should be configurable, up to the point of infinite retry. Long running stream operations are better not dying, even if they have to wait while S3 fixes hardware shortages/failures. Not sure if it's a separate issue, but failed writes aren't cleaned-up very well right now. In DFS, a file that isn't closed doesn't exist for other operations - if possible, there should at least be a way to find out if a file in S3 is \"done\", preferably it should be invisible to \"normal\" operations while its state is not final.\n2. Mike Smith: I've been working on a patch to handle these exceptions. There are three major exceptions that need to be retried. InternalError RequestTimeout OperationAborted InternalError exception has reasonably high rate for PUT request! I have finished the patch for Jets3tFileSystem.java which exponentially increases the waiting time. But, I've been dealing with a very strange problem. When I get the internalError (500) response, and I want to retry the request I keep getting RequestTimeout response from S3. This is a client exception and it shows that jets3t closes the connection after InternalError Exception! Even when I try to restablish the connection I still get the same RequestTimeout response. Following is the changed put() method in Jets3tFileSystem.java, I have done similar changes for other methods as well, let me know if you see something wrong: private void put(String key, InputStream in, long length) throws IOException { int attempts = 0; while(true){ try{ S3Object object = new S3Object(key); object.setDataInputStream(in); object.setContentType(\"binary/octet-stream\"); object.setContentLength(length); s3Service.putObject(bucket, object); break; } catch (S3ServiceException e) { if(!retry(e,++attempts)){ if (e.getCause() instanceof IOException) { throw (IOException) e.getCause(); } throw new S3Exception(e); } } } } private boolean retry(S3ServiceException e,int attempts){ if(attempts > maxRetry) return false; // for internal exception (500), retry is allowed if(e.getErrorCode().equals(S3_INTERNAL_ERROR_CODE) || e.getErrorCode().equals(S3_OPERATION_ABORTED_CODE)){ LOG.info(\"retrying failed s3Service [\"+e.getErrorCode()+\"]. Delay: \" + retryDelay*attempts+\" msec. Attempts: \"+attempts); try{ Thread.sleep(retryDelay*attempts); } catch(Exception ee){} return true; } // allows retry for the socket timeout exception. // connection needs to be restablished. if(e.getErrorCode().equals(S3_REQUEST_TIMEOUT_CODE)){ try{ AWSCredentials awsCredentials = new AWSCredentials(accessKey, secretAccessKey); this.s3Service = new RestS3Service(awsCredentials); } catch (S3ServiceException ee) { // this exception will be taken care of later } LOG.info(\"retrying failed s3Service [\"+e.getErrorCode()+\"]. Attempts: \"+attempts); return true; } // for all other exceptions retrying is not allowed // Maybe it would be better to keep retrying for all sorts of exceptions!? return false; }\n3. Thomas White: As you say, the immediate RequestTimeout after an InternalError looks like a jets3t issue. Probably worth looking at the jets3t source or sending a message to the jets3t list? To implement retries, I was imagining creating a wrapper implementation of FileSystemStore that only had retry functionality in it and delegated to another FileSystemStore for actual storage. This would make testing easier as well as isolating the retry code. I think the InputStreams in FileSystemStore would need to be changed to be Files, but this should be fine.\n4. Doug Cutting: > To implement retries, I was imagining creating a wrapper implementation of FileSystemStore that only had retry functionality This reminds me of HADOOP-601. That's not directly applicable, since S3 doesn't use Hadoop's RPC. But it makes me wonder if perhaps we ought to pursue a generic retry mechanism. Perhaps we could have a RetryProxy that uses java.lang.reflect.Proxy to implement retries for all methods in an interface. One could perhaps specify per-method retry policies as a ctor parameter, e.g.: MethodRetry[] retries = new MethodRetry[] { new MethodRetry(\"myFirstMethod\", ONCE), new MethodRetry(\"mySecondMethod, FOREVER) } MyInterface impl = new MyInterfaceImpl(); MyInterface proxy = RetryProxy.create(MyInterface.class, impl, retries); or somesuch. Could this work?\n5. James P. White: I like the idea of a general approach to retries, but using an existing AOP mechanism seems like a better way to go to me. Although AspectJ is a strong tool, Spring 2.0 AOP is closer to what you propose. In addition to Spring's autoproxies, Spring 2.0 fully supports AspectJ syntax including annotations. That provides the greatest flexibility and avoids the invention of yet-another-syntax for advice. http://static.springframework.org/spring/docs/2.0.x/reference/new-in-2.html#new-in-2-aop\n6. Mike Smith: Instead of the released version of jets3t which is 5 months old, I built it from their CVS source codes. In this version unlike the released version, you will have the retry mechanism for S3 exceptions. The CVS version seems to be much more stable than the released version. Here are the two most important new features from their release notes: - Requests that fail due to S3 Internal Server error are retried a configurable number of times, with an increasing delay between each retry attempt. - Sends an upload object's MD5 data hash to S3 in the header Content-MD5, to confirm no data corruption has taken place on the wire.\n7. Thomas White: I was actually just writing the following response: If we throw a different type of exception (TransientS3Exception) for recoverable error codes (there's a complete list of error codes here BTW: http://docs.amazonwebservices.com/AmazonS3/2006-03-01/ErrorCodeList.html) then we can make the proxy only retry for that exception. This is in line with HADOOP-601, but until that's done we could build an S3RetryProxy just for S3. We still might want to follow this more general approach (if it gives us more control), but for the time being if jets3t's retry mechanism is adequate then let's try that.\n8. Thomas White: Looks like jets3t version 0.5.0 was released yesterday: http://jets3t.s3.amazonaws.com/downloads.html. Configuration notes are here: http://jets3t.s3.amazonaws.com/toolkit/configuration.html - s3service.internal-error-retry-max is the relevant one.\n9. Michael Stack: I updated jets3t to the 0.5.0 release. I had to make the below edits. The API has probably changed in other ways but I've not spent the time verifying. Unless someone else is working on a patch that includes the new version of the jets3t lib and complimentary changes to s3 fs, I can give it a go (retries are necessary it seems if you're trying to upload anything more than a few kilobytes). Related, after adding the new lib and making below changes, uploads ('puts') would fail with below complaint. 07/01/31 00:47:13 WARN service.S3Service: Encountered 1 S3 Internal Server error(s), will retry in 50ms put: Input stream is not repeatable as 1048576 bytes have been written, exceeding the available buffer size of 131072 I found the 131072 buffer in jets3t. Turns out the buffer size is configurable. Dropping a jets3t.properties file into ${HADOOP_HOME}/conf directory (so a lookup on CLASSPATH succeeds) with amended s3service.stream-retry-buffer-size got me over the 'put: Input stream...' hump. I set it to the value of dfs.block.size so it could replay a full-block if it had to. Then I noticed that the blocks written to S3 were of 1MB in size. I'm uploading tens of Gs so that made for tens of thousands of blocks. No harm I suppose but I was a little stumped that block size in S3 wasn't the value of dfs.block.size. I found the fs.s3.block.size property in the S3 fs code. Shouldn't this setting be bubbled up into hadoop-default with a default of value of ${dfs.block.size}? (Setting this in my config. made for 64MB S3 blocks). I can add the latter items to the wiki on S3 or can include an jets3t.properties and s3.block.size in patch. What do others think? Index: src/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java =================================================================== --- src/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java (revision 501895) +++ src/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java (working copy) @@ -133,7 +133,7 @@ S3Object object = s3Service.getObject(bucket, key); return object.getDataInputStream(); } catch (S3ServiceException e) { - if (e.getErrorCode().equals(\"NoSuchKey\")) { + if (e.getS3ErrorCode().equals(\"NoSuchKey\")) { return null; } if (e.getCause() instanceof IOException) { @@ -149,7 +149,7 @@ null, byteRangeStart, null); return object.getDataInputStream(); } catch (S3ServiceException e) { - if (e.getErrorCode().equals(\"NoSuchKey\")) { + if (e.getS3ErrorCode().equals(\"NoSuchKey\")) { return null; } if (e.getCause() instanceof IOException) {\n10. Thomas White: The best way to check the new jets3t library is to run the Jets3tS3FileSystemTest unit test. You will need to set your S3 credentials in the hadoop-site.xml file in the test directory. If this passes you can be confident that the upgrade has worked. Changing the buffer size to be as big as the block size sounds good (however, I worry a little whether there could be a memory issue if jets3t buffers in memory, as seems likely). The 1MB block size was a fairly arbitrary value I selected during testing. I agree that 64MB would be better. The property fs.s3.block.size property needs adding to hadoop-default.xml, and the DEFAULT_BLOCK_SIZE constant in S3FileSystem needs changing too. A patch for all this stuff would be very welcome! As for whether completing all these items would mean the issue is closed I'm not sure. The jets3t retry mechanism is for S3-level exceptions, if there is a traffic-level communication problem then there is nothing to handle it. For this, we could use the more general Hadoop-level mechanism, described above (or possibly use the retry-mechanism in HttpClient, if that's sufficient). I think this work would belong in another Jira issue. Thoughts?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.503855"}}
{"id": "0b4d9cae3555e0c76bab2bee2c810b68", "issue_key": "HADOOP-717", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "When there are few reducers, sorting should be done by mappers", "description": "If I understand correctly, currently, sort happens on the reducer side. So if few hundred mappers produce few (or many) Gig of data, and there is just ONE reduce to consume it, copying and sorting takes forever. It may make sense to have a special case optimization for a single reducer. (E.g. \"when there is only reducer and many mappers, sort is done by the mappers, and reducer does only a merge\") Or to have some smarter policy that makes sure that sorting uses as many CPUs as it makes sense. If the map step has produced data on all the nodes of the cluster, it makes sense to use all the nodes for sorting.", "reporter": "arkady borkovsky", "assignee": "Owen O'Malley", "created": "2006-11-14T06:47:47.000+0000", "updated": "2009-07-08T16:52:00.000+0000", "resolved": "2006-12-15T22:29:49.000+0000", "labels": [], "components": [], "comments": [{"author": "Devaraj Das", "body": "This is handled by Hadoop-331 (work in progress)", "created": "2006-11-14T06:51:58.000+0000"}, {"author": "Owen O'Malley", "body": "This was fixed by HADOOP-331.", "created": "2006-12-15T22:29:49.000+0000"}], "num_comments": 2, "text": "Issue: HADOOP-717\nSummary: When there are few reducers, sorting should be done by mappers\nDescription: If I understand correctly, currently, sort happens on the reducer side. So if few hundred mappers produce few (or many) Gig of data, and there is just ONE reduce to consume it, copying and sorting takes forever. It may make sense to have a special case optimization for a single reducer. (E.g. \"when there is only reducer and many mappers, sort is done by the mappers, and reducer does only a merge\") Or to have some smarter policy that makes sure that sorting uses as many CPUs as it makes sense. If the map step has produced data on all the nodes of the cluster, it makes sense to use all the nodes for sorting.\n\nComments (2):\n1. Devaraj Das: This is handled by Hadoop-331 (work in progress)\n2. Owen O'Malley: This was fixed by HADOOP-331.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.446904"}}
{"id": "fb3c839238a95e8aaae14c90ae49242a", "issue_key": "KAFKA-293", "issue_type": "Improvement", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "Allow to configure all broker ids at once similar to how zookeeper handles server ids", "description": "Zookeeper allows to specify all server ids in the same configuration (https://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_configuration) which has the benefit that the configuration file is the same for all zookeeper instances. A similar approach for Kafka would be quite useful, e.g. brokerid.1=<host 1> brokerid.2=<host 2> etc. It'd still require per-instance configuration (myid file in the zookeeper case) but that can be created separately (e.g. by the deployment tool used).", "reporter": "Thomas Dudziak", "assignee": null, "created": "2012-03-05T21:58:38.000+0000", "updated": "2015-02-07T23:52:03.000+0000", "resolved": "2015-02-07T23:52:03.000+0000", "labels": [], "components": [], "comments": [{"author": "Jay Kreps", "body": "The current approach is, admittedly, hacky; but the zookeeper approach is equally hacky if you ask me. I don't think it is good to put any host information in your server config. If we want to fix this issue I think the right way to go would be to create a zk sequence of ids. Servers would store the broker id in their data directory (maybe we could add a ${data.dir}/meta.properties file to hold this kind of stuff). If a server starts and has no meta.properties file then it would increment the zk id counter and take a new id and store it persistently with its logs. On startup the broker would take its id from this file. The reason this approach is better is because the node.id is not really configuration. That is if you swap the node.ids for two brokers you effectively corrupt the logs in the eyes of consumers. So the node.id is pinned to a particular set of logs. This would make any manual borking of your node id impossible and takes the node id out of the things the user has to configure. Thoughts?", "created": "2012-03-09T22:34:01.543+0000"}, {"author": "Neha Narkhede", "body": "I like the meta.properties approach, better than Zookeeper's approach to its configuration. It is certainly more involved, but has some advantages, like Jay mentioned. Just one thing to be careful about if we use the ephemeral sequential node feature in ZK - A kafka broker asks zookeeper to create an ephemeral sequential node, and the create() succeeds on the server but the server crashes before returning the name of the node to kafka. When the kafka broker reconnects, its session is still valid and, thus, the node is not removed. The implication is that it is difficult for the Kafka broker to know if its node was created or not. One way of handling recoverable errors while using sequential ephemeral nodes is that we can simply assume that the node wasn't created and create another one. This means that the ids will not be contiguous.", "created": "2012-03-13T00:04:48.433+0000"}, {"author": "Thomas Dudziak", "body": "I agree that the zookeeper approach is hacky, but at least in the zookeeper case they don't have an (obvious) choice because zookeeper is the source of truth (though admittedly they could use DNS or other means to discover the other servers on startup). For Kafka, I like your idea because it basically means I don't have to care about the id except if I want to move the data to a different physical server (and even then I only need to know that I have to move the meta.properties file as well). The only problem is that it relies on the zookeeper sequence maintaining integrity (i.e. it can be restarted as that can lead to duplicate ids). Maybe a better way might be if the kafka nodes generate uuids and use them as node names in zookeeper. If the path already exist, then they could simply create a new uuid and try again.", "created": "2012-03-15T05:52:17.572+0000"}], "num_comments": 3, "text": "Issue: KAFKA-293\nSummary: Allow to configure all broker ids at once similar to how zookeeper handles server ids\nDescription: Zookeeper allows to specify all server ids in the same configuration (https://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_configuration) which has the benefit that the configuration file is the same for all zookeeper instances. A similar approach for Kafka would be quite useful, e.g. brokerid.1=<host 1> brokerid.2=<host 2> etc. It'd still require per-instance configuration (myid file in the zookeeper case) but that can be created separately (e.g. by the deployment tool used).\n\nComments (3):\n1. Jay Kreps: The current approach is, admittedly, hacky; but the zookeeper approach is equally hacky if you ask me. I don't think it is good to put any host information in your server config. If we want to fix this issue I think the right way to go would be to create a zk sequence of ids. Servers would store the broker id in their data directory (maybe we could add a ${data.dir}/meta.properties file to hold this kind of stuff). If a server starts and has no meta.properties file then it would increment the zk id counter and take a new id and store it persistently with its logs. On startup the broker would take its id from this file. The reason this approach is better is because the node.id is not really configuration. That is if you swap the node.ids for two brokers you effectively corrupt the logs in the eyes of consumers. So the node.id is pinned to a particular set of logs. This would make any manual borking of your node id impossible and takes the node id out of the things the user has to configure. Thoughts?\n2. Neha Narkhede: I like the meta.properties approach, better than Zookeeper's approach to its configuration. It is certainly more involved, but has some advantages, like Jay mentioned. Just one thing to be careful about if we use the ephemeral sequential node feature in ZK - A kafka broker asks zookeeper to create an ephemeral sequential node, and the create() succeeds on the server but the server crashes before returning the name of the node to kafka. When the kafka broker reconnects, its session is still valid and, thus, the node is not removed. The implication is that it is difficult for the Kafka broker to know if its node was created or not. One way of handling recoverable errors while using sequential ephemeral nodes is that we can simply assume that the node wasn't created and create another one. This means that the ids will not be contiguous.\n3. Thomas Dudziak: I agree that the zookeeper approach is hacky, but at least in the zookeeper case they don't have an (obvious) choice because zookeeper is the source of truth (though admittedly they could use DNS or other means to discover the other servers on startup). For Kafka, I like your idea because it basically means I don't have to care about the id except if I want to move the data to a different physical server (and even then I only need to know that I have to move the meta.properties file as well). The only problem is that it relies on the zookeeper sequence maintaining integrity (i.e. it can be restarted as that can lead to duplicate ids). Maybe a better way might be if the kafka nodes generate uuids and use them as node names in zookeeper. If the path already exist, then they could simply create a new uuid and try again.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.582100"}}
{"id": "0de6d99acd203cea0b15725c7fd253b5", "issue_key": "HADOOP-535", "issue_type": "Test", "status": "Open", "priority": "Major", "resolution": null, "summary": "back to back testing of codecs", "description": "We should write some unit tests that use codecs back to back doing writing and then reading. compressed block1, compressed block 2, compressed block3, ... that will check that the compression codecs are consuming the entire block when they read.", "reporter": "Owen O'Malley", "assignee": "Arun Murthy", "created": "2006-09-14T23:04:26.000+0000", "updated": "2014-07-17T16:03:38.000+0000", "resolved": null, "labels": [], "components": ["io"], "comments": [{"author": "Allen Wittenauer", "body": "This was done years ago, wasn't it?", "created": "2014-07-17T16:03:38.854+0000"}], "num_comments": 1, "text": "Issue: HADOOP-535\nSummary: back to back testing of codecs\nDescription: We should write some unit tests that use codecs back to back doing writing and then reading. compressed block1, compressed block 2, compressed block3, ... that will check that the compression codecs are consuming the entire block when they read.\n\nComments (1):\n1. Allen Wittenauer: This was done years ago, wasn't it?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.389518"}}
{"id": "3b5cc7e5d76c31b475d492bfb3c6c9bd", "issue_key": "HADOOP-657", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Free temporary space should be modelled better", "description": "Currently, there is a configurable size that must be free for a task tracker to accept a new task. However, that isn't a very good model of what the task is likely to take. I'd like to propose: Map tasks: totalInputSize * conf.getFloat(\"map.output.growth.factor\", 1.0) / numMaps Reduce tasks: totalInputSize * 2 * conf.getFloat(\"map.output.growth.factor\", 1.0) / numReduces where totalInputSize is the size of all the maps inputs for the given job. To start a new task, newTaskAllocation + (sum over running tasks of (1.0 - done) * allocation) >= free disk * conf.getFloat(\"mapred.max.scratch.allocation\", 0.90); So in English, we will model the expected sizes of tasks and only task tasks that should leave us a 10% margin. With: map.output.growth.factor -- the relative size of the transient data relative to the map inputs mapred.max.scratch.allocation -- the maximum amount of our disk we want to allocate to tasks.", "reporter": "Owen O'Malley", "assignee": "Ariel Shemaiah Rabkin", "created": "2006-10-31T00:24:29.000+0000", "updated": "2009-07-08T16:51:59.000+0000", "resolved": "2008-08-12T20:16:49.000+0000", "labels": [], "components": [], "comments": [{"author": "Arun Murthy", "body": "Looks like Owen's got it nailed pretty good... anyone wants to shout on this one? What do you guys think is a reasonable default for \"map.output.growth.factor\" ? 1.0? Do we need more leeway?", "created": "2006-11-08T20:08:34.000+0000"}, {"author": "Arun Murthy", "body": "Current flow relevant to this discussion: TaskTracker.offerService() -> TaskTracker.checkForNewTasks() -> if (TaskTracker.enoughFreeSpace()) then poll/startNewTask We could put the above checks (infact we can do better by checking if we have assigned fileSplit's size * conf.getFloat(\"map.output.growth.factor\", 1.0)) in TaskTracker.enoughFreeSpace()... ... alternatively we could make '(sum over running tasks of (1.0 - done) * allocation)' part of TaskTrackerStatus i.e. a 'availableDiskSpace' member, check to ensure that 'sufficient' free space is available on the tasktracker before assigning it the task itself in JobInProgress.findNewTask - this ensures that a task isn't allocated in the first place to a tasktracker if it can't handle it. What do you guys think? Am I missing out on something which prevents option #2 from working?", "created": "2006-11-09T16:59:44.000+0000"}, {"author": "Owen O'Malley", "body": "option #2 with putting the \"unallocated\" disk space into the TaskTrackerStatus should work well.", "created": "2006-11-15T17:46:43.000+0000"}, {"author": "Doug Cutting", "body": "We should also log something when tasks are not accepted due to space limitations. At present I think nothing is displayed in this case.", "created": "2006-11-15T17:57:55.000+0000"}, {"author": "Arun Murthy", "body": "It should be very useful to include the 'unallocated disk space' in the global and per-host jsps so as to provide an easy way for operator to diagnose if and when tasks can't be allocated due to lack of diskspace... I think it should be a part of this same bug.", "created": "2006-11-15T18:12:38.000+0000"}, {"author": "Doug Cutting", "body": "> include the 'unallocated disk space' in the global and per-host jsps Yes, I agree this is another metric that should be displayed in the central web ui. It should be reported through the metrics API, as discussed in HADOOP-481 and HADOOP-481. The MapReduce framework should implement a MetricsContext, and system (and perhaps user) code can use this to route statisitics to central locations. I don't think we want to fix that as a part of this issue, but I also don't think we should hack in an alternate mechanism just for this statistic.", "created": "2006-11-15T18:24:17.000+0000"}, {"author": "Arun Murthy", "body": "I see the value in Doug's suggestion... for e.g. at some point in the future we might also put in metrics like CPU load, VM stats etc. and this would let the JobTracker make 'smarter' decisions about which task to assign to which TaskTrackers i.e. CPU-bound tasks to IO-laden TTs and vice-versa. I do agree that it might be a very futuristic scenario, but the point is to keep the infrastructure robust when we can...", "created": "2006-11-16T17:45:33.000+0000"}, {"author": "Owen O'Malley", "body": "Actually, I'd rather have the unallocated disk space in the heartbeat, because when HADOOP-639 is implemented, the JobTracker should be given fresh information to decide which tasks should be launched there.", "created": "2006-11-16T17:51:33.000+0000"}, {"author": "Doug Cutting", "body": "> I'd rather have the unallocated disk space in the heartbeat [ ...] I agree, but I think we should use a general mechanism to route metrics to the jobtracker through heartbeats, rather than hack things in one-by-one.", "created": "2006-11-16T17:58:23.000+0000"}, {"author": "Arun Murthy", "body": "Taking things forward, looks like both Owen/Doug agree that we need to send metrics through the heartbeat... ... given this we are looking at implementing a MapReduceMetricsContext which sends over heartbeat (with metrics) via RPC to the JT and TaskTracker.offerService() becoming a callback for the timer. Does that make sense or do folks prefer something different?", "created": "2006-11-28T18:19:18.000+0000"}, {"author": "Arun Murthy", "body": "It also necessiates a 'Writable' MetricsRecordImpl (for RPC) and some apis for 'reading' the metrics i.e. getMetric/getTag apis which the JobTracker can use to retrieve information.", "created": "2006-11-30T17:47:48.000+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "Here's my proposed fix: 1) Add a \"free space on compute node\" field to TaskTrackerStatus. This is the real physical space available, plus the sum of (commitment - reservation) for each running map task. 2) Add a \"space used by this task\" and \"space reserved for task\" to TaskStatus as well. 3) Add a \"space to reserve\" to either Task or MapTask. This is computed by the JobTracker, and used by the TaskTracker 4) Create a new ResourceConsumptionEstimator class, and have an instance of that type for each JobInProgress. This will have, at a minimum, reportCompletedMapTask(MapTaskStatus t) and estimateSpaceForMapTask(MapTask mt) The implementation would probably be a thread that processes asynchronously, and updates an atomic value that'll be either the estimated space requirement, or else the estimated ratio between input size and output size. Until sufficiently many maps have completed (10%, say) the size estimate would just be the size of each map's input. Afterwards, we'll take the 75th percentile of the measured blowup in task size. 5) Modify obtainNewMapTask to return null if the space available on the given task tracker is less than the estimate of available space. 6) To avoid deadlocks if there are multiple jobs contending for space, abort the job if too many trackers are rejected as having insufficient space. ----- Thoughts?", "created": "2008-05-21T22:26:32.305+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "Here's a stab at solving the issue. I've tested this locally, and it doesn't break anything, when space IS available. I haven't yet tested this in the low-disk space case.", "created": "2008-05-28T22:03:21.719+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "The associated patch.", "created": "2008-05-28T22:04:03.691+0000"}, {"author": "Devaraj Das", "body": "bq. 4) Create a new ResourceConsumptionEstimator class, and have an instance of that type for each JobInProgress. This will have, at a minimum, reportCompletedMapTask(MapTaskStatus t) and estimateSpaceForMapTask(MapTask mt) The implementation would probably be a thread that processes asynchronously, and updates an atomic value that'll be either the estimated space requirement, or else the estimated ratio between input size and output size. Until sufficiently many maps have completed (10%, say) the size estimate would just be the size of each map's input. Afterwards, we'll take the 75th percentile of the measured blowup in task size. Ari, I haven't looked at the patch yet, but it'd help if could you please give an example for this one with some numbers.", "created": "2008-05-29T06:33:22.825+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "Here's what we have currently. ResourceEstimator keeps an estimate of how big the average map's output is. As Map tasks complete, we update this. If a node has less than twice the average outputsize in free disk space, we don't assign tasks to it. Haven't implemented the percentile aspect; average is computationally much easier. So if a job has 10 GB of input, split across ten map tasks, tasks will only be started on nodes with at least two gigabytes free. It's been tested locally, and indeed, jobs only go to a task tracker with sufficient space. Next step is testing at scale, on a cluster,", "created": "2008-05-29T21:30:26.336+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "Current patch. Limitations: Not confident we're making a sound estimate for Reduce space consumption. Doesn't detect deadlock or starvation.", "created": "2008-05-29T21:33:06.465+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "temporarily canceling patch; expect final version within a few days.", "created": "2008-05-30T00:24:19.784+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "Tested on a small cluster, seems to work. One limitation is that we don't automatically resolve deadlocks. This could be done, e.g., by failing tasks that can't be placed for a long period.", "created": "2008-06-06T21:26:08.086+0000"}, {"author": "Hadoop QA", "body": "-1 overall. Here are the results of testing the latest attachment http://issues.apache.org/jira/secure/attachment/12383592/diskspaceest_v3.patch against trunk revision 664208. +1 @author. The patch does not contain any @author tags. -1 tests included. The patch doesn't appear to include any new or modified tests. Please justify why no tests are needed for this patch. -1 javadoc. The javadoc tool appears to have generated 1 warning messages. +1 javac. The applied patch does not increase the total number of javac compiler warnings. -1 findbugs. The patch appears to introduce 3 new Findbugs warnings. -1 release audit. The applied patch generated 196 release audit warnings (more than the trunk's current 195 warnings). -1 core tests. The patch failed core unit tests. +1 contrib tests. The patch passed contrib unit tests. Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2613/testReport/ Release audit warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2613/artifact/trunk/current/releaseAuditDiffWarnings.txt Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2613/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2613/artifact/trunk/build/test/checkstyle-errors.html Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2613/console This message is automatically generated.", "created": "2008-06-07T01:39:19.025+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "Fix pedantic fixbugs warnings; add license at top of file. Notes: No tests supplied, since there's no simple or painless way to run unit tests across multiple filesystems. TestHarFileSystem error is believed unrelated to patch.", "created": "2008-06-11T18:49:12.246+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "cancelling to resubmit", "created": "2008-06-17T23:37:51.928+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "revised, should no longer trigger findbugs", "created": "2008-06-17T23:38:35.829+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "here's the patch", "created": "2008-06-17T23:43:48.663+0000"}, {"author": "Hadoop QA", "body": "-1 overall. Here are the results of testing the latest attachment http://issues.apache.org/jira/secure/attachment/12384165/diskspaceest_v4.patch against trunk revision 668867. +1 @author. The patch does not contain any @author tags. -1 tests included. The patch doesn't appear to include any new or modified tests. Please justify why no tests are needed for this patch. +1 javadoc. The javadoc tool did not generate any warning messages. +1 javac. The applied patch does not increase the total number of javac compiler warnings. +1 findbugs. The patch does not introduce any new Findbugs warnings. +1 release audit. The applied patch does not increase the total number of release audit warnings. +1 core tests. The patch passed core unit tests. +1 contrib tests. The patch passed contrib unit tests. Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2677/testReport/ Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2677/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2677/artifact/trunk/build/test/checkstyle-errors.html Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2677/console This message is automatically generated.", "created": "2008-06-18T04:43:09.903+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "clean up format of code", "created": "2008-07-14T17:19:54.835+0000"}, {"author": "Hadoop QA", "body": "-1 overall. Here are the results of testing the latest attachment http://issues.apache.org/jira/secure/attachment/12385993/clean_spaceest.patch against trunk revision 676772. +1 @author. The patch does not contain any @author tags. -1 tests included. The patch doesn't appear to include any new or modified tests. Please justify why no tests are needed for this patch. +1 javadoc. The javadoc tool did not generate any warning messages. +1 javac. The applied patch does not increase the total number of javac compiler warnings. +1 findbugs. The patch does not introduce any new Findbugs warnings. +1 release audit. The applied patch does not increase the total number of release audit warnings. +1 core tests. The patch passed core unit tests. +1 contrib tests. The patch passed contrib unit tests. Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2861/testReport/ Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2861/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2861/artifact/trunk/build/test/checkstyle-errors.html Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2861/console This message is automatically generated.", "created": "2008-07-15T20:27:59.933+0000"}, {"author": "Vinod Kumar Vavilapalli", "body": "HADOOP-3581 tries to manage memory used by tasks. I am trying to follow the approach of this JIRA, and have a couple of comments. - I see that you are having free-space computation inside the task. Instead, why can't we do it in the tasktracker itself? In this JIRA, we are caring only about mapOutputFiles and for watching them, we just need the JOB ID and TIP ID. Memory tracking HAS to be done in TT and not task, to shield the tracking business itself from any rogue tasks. I think it would be good if we can manage both these resources in TT itself, ultimately moving all of these into a single resource management class in TT. Unless I am missing something else here. Thoughts? - I also see in this patch that availableSpace is sent to JT via TaskTrackerStatus. What happened to Doug's idea of \"using a general mechanism to route metrics to the jobtracker through heartbeats, rather than hack things in one-by-one\". A general mechanism like the one Arun proposed (MetricsContext) would also help HADOOP-3759(which intends to use freeMemory information for scheduling decisions).", "created": "2008-07-17T07:34:13.345+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "I don't have strong feelings about whether to do space-consumed measurement in the TaskTracker or the Task. I figured it made more sense to fill out the whole TaskStatus in one place.Otherwise you get confused in the TaskTracker code, whether or not the space-consumed has been filled in yet. I'm open to doing this the other way 'round, and having TaskTracker responsible for it. Certainly if there were other similar resource counters being filled in in TaskTracker, this one ought to be. I was tempted to use metrics for this, and looked at piggybacking of this sort of thing more generally on heartbeats. I was promptly shot down. There was a strong sentiment, notably from Owen and Arun, that Hadoop's core functionality shouldn't depend on Metrics, and that Metrics should just be for analytics.", "created": "2008-07-17T16:14:37.906+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "no more need to modify Task", "created": "2008-07-17T21:12:11.682+0000"}, {"author": "Ariel Shemaiah Rabkin", "body": "As per Vinod's suggestion, move lookup into TaskTracker.", "created": "2008-07-17T21:12:58.854+0000"}, {"author": "Hadoop QA", "body": "-1 overall. Here are the results of testing the latest attachment http://issues.apache.org/jira/secure/attachment/12386347/spaceest_717.patch against trunk revision 677781. +1 @author. The patch does not contain any @author tags. -1 tests included. The patch doesn't appear to include any new or modified tests. Please justify why no tests are needed for this patch. +1 javadoc. The javadoc tool did not generate any warning messages. +1 javac. The applied patch does not increase the total number of javac compiler warnings. +1 findbugs. The patch does not introduce any new Findbugs warnings. +1 release audit. The applied patch does not increase the total number of release audit warnings. +1 core tests. The patch passed core unit tests. +1 contrib tests. The patch passed contrib unit tests. Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2896/testReport/ Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2896/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2896/artifact/trunk/build/test/checkstyle-errors.html Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2896/console This message is automatically generated.", "created": "2008-07-18T04:35:33.632+0000"}, {"author": "Owen O'Malley", "body": "I just committed this. Thanks, Ari!", "created": "2008-08-12T20:16:49.684+0000"}, {"author": "Jothi Padmanabhan", "body": "The reduce task for sortvalidator , 500 nodes, seem to get stuck with the following message (several of them), even though the sort itself succeeded fine. Could there be a bug in the estimation of the reduce input size? 2008-08-14 07:56:16,507 WARN org.apache.hadoop.mapred.JobInProgress: No room for reduce task. Node tracker_xxx.com:xxx..com/<IPADDR>:58251 has 204889718784 bytes free; but we expect reduce input to take 1004644589190 2008-08-14 07:56:16,508 INFO org.apache.hadoop.mapred.ResourceEstimator: estimate map will take 150463470 bytes. (blowup = 2*0.04748320346406185) 346406185)", "created": "2008-08-14T08:04:49.653+0000"}, {"author": "Hudson", "body": "Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])", "created": "2008-08-22T12:34:11.398+0000"}], "num_comments": 35, "text": "Issue: HADOOP-657\nSummary: Free temporary space should be modelled better\nDescription: Currently, there is a configurable size that must be free for a task tracker to accept a new task. However, that isn't a very good model of what the task is likely to take. I'd like to propose: Map tasks: totalInputSize * conf.getFloat(\"map.output.growth.factor\", 1.0) / numMaps Reduce tasks: totalInputSize * 2 * conf.getFloat(\"map.output.growth.factor\", 1.0) / numReduces where totalInputSize is the size of all the maps inputs for the given job. To start a new task, newTaskAllocation + (sum over running tasks of (1.0 - done) * allocation) >= free disk * conf.getFloat(\"mapred.max.scratch.allocation\", 0.90); So in English, we will model the expected sizes of tasks and only task tasks that should leave us a 10% margin. With: map.output.growth.factor -- the relative size of the transient data relative to the map inputs mapred.max.scratch.allocation -- the maximum amount of our disk we want to allocate to tasks.\n\nComments (35):\n1. Arun Murthy: Looks like Owen's got it nailed pretty good... anyone wants to shout on this one? What do you guys think is a reasonable default for \"map.output.growth.factor\" ? 1.0? Do we need more leeway?\n2. Arun Murthy: Current flow relevant to this discussion: TaskTracker.offerService() -> TaskTracker.checkForNewTasks() -> if (TaskTracker.enoughFreeSpace()) then poll/startNewTask We could put the above checks (infact we can do better by checking if we have assigned fileSplit's size * conf.getFloat(\"map.output.growth.factor\", 1.0)) in TaskTracker.enoughFreeSpace()... ... alternatively we could make '(sum over running tasks of (1.0 - done) * allocation)' part of TaskTrackerStatus i.e. a 'availableDiskSpace' member, check to ensure that 'sufficient' free space is available on the tasktracker before assigning it the task itself in JobInProgress.findNewTask - this ensures that a task isn't allocated in the first place to a tasktracker if it can't handle it. What do you guys think? Am I missing out on something which prevents option #2 from working?\n3. Owen O'Malley: option #2 with putting the \"unallocated\" disk space into the TaskTrackerStatus should work well.\n4. Doug Cutting: We should also log something when tasks are not accepted due to space limitations. At present I think nothing is displayed in this case.\n5. Arun Murthy: It should be very useful to include the 'unallocated disk space' in the global and per-host jsps so as to provide an easy way for operator to diagnose if and when tasks can't be allocated due to lack of diskspace... I think it should be a part of this same bug.\n6. Doug Cutting: > include the 'unallocated disk space' in the global and per-host jsps Yes, I agree this is another metric that should be displayed in the central web ui. It should be reported through the metrics API, as discussed in HADOOP-481 and HADOOP-481. The MapReduce framework should implement a MetricsContext, and system (and perhaps user) code can use this to route statisitics to central locations. I don't think we want to fix that as a part of this issue, but I also don't think we should hack in an alternate mechanism just for this statistic.\n7. Arun Murthy: I see the value in Doug's suggestion... for e.g. at some point in the future we might also put in metrics like CPU load, VM stats etc. and this would let the JobTracker make 'smarter' decisions about which task to assign to which TaskTrackers i.e. CPU-bound tasks to IO-laden TTs and vice-versa. I do agree that it might be a very futuristic scenario, but the point is to keep the infrastructure robust when we can...\n8. Owen O'Malley: Actually, I'd rather have the unallocated disk space in the heartbeat, because when HADOOP-639 is implemented, the JobTracker should be given fresh information to decide which tasks should be launched there.\n9. Doug Cutting: > I'd rather have the unallocated disk space in the heartbeat [ ...] I agree, but I think we should use a general mechanism to route metrics to the jobtracker through heartbeats, rather than hack things in one-by-one.\n10. Arun Murthy: Taking things forward, looks like both Owen/Doug agree that we need to send metrics through the heartbeat... ... given this we are looking at implementing a MapReduceMetricsContext which sends over heartbeat (with metrics) via RPC to the JT and TaskTracker.offerService() becoming a callback for the timer. Does that make sense or do folks prefer something different?", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.428881"}}
{"id": "edc827a1a83780aba15f09e851ffa5b4", "issue_key": "KAFKA-701", "issue_type": "Bug", "status": "Resolved", "priority": "Minor", "resolution": null, "summary": "ConsoleProducer does not exit correctly and fix some config properties following KAFKA-648", "description": "Just added a proper try/catch around the ConsoleProducer so that when an exception is thrown, the system exits (with error code 1) In addition, KAFKA-648 broker some configs like request.enqueue.timeout.ms and zk.connection.timeout.ms that I fixed", "reporter": "Maxime Brugidou", "assignee": "Maxime Brugidou", "created": "2013-01-14T16:14:12.000+0000", "updated": "2013-01-14T17:22:44.000+0000", "resolved": "2013-01-14T17:22:44.000+0000", "labels": [], "components": ["config", "core"], "comments": [{"author": "Jun Rao", "body": "Thanks for the patch. Committed to 0.8 after reverting the change in system_test/ (the consumer property file there is used for an 0.7 consumer).", "created": "2013-01-14T17:22:44.323+0000"}], "num_comments": 1, "text": "Issue: KAFKA-701\nSummary: ConsoleProducer does not exit correctly and fix some config properties following KAFKA-648\nDescription: Just added a proper try/catch around the ConsoleProducer so that when an exception is thrown, the system exits (with error code 1) In addition, KAFKA-648 broker some configs like request.enqueue.timeout.ms and zk.connection.timeout.ms that I fixed\n\nComments (1):\n1. Jun Rao: Thanks for the patch. Committed to 0.8 after reverting the change in system_test/ (the consumer property file there is used for an 0.7 consumer).", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.814620"}}
{"id": "6be549c50f1e68884636e2afdb9ee075", "issue_key": "HADOOP-198", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "adding owen's examples to exampledriver", "description": "owen's sorter and randomwriter are not added to the examples.jar file", "reporter": "Mahadev Konar", "assignee": "Mahadev Konar", "created": "2006-05-05T07:33:24.000+0000", "updated": "2006-08-03T17:46:40.000+0000", "resolved": "2006-05-05T23:29:51.000+0000", "labels": [], "components": [], "comments": [{"author": "Mahadev Konar", "body": "Added owen's examples to exampledriver. Also, made the ClusterStatus contructors public since the WritableFactories throw an exception if non public fields are accessed out of the package.", "created": "2006-05-05T07:37:05.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. I changed your fix to the WritableFactory problem slightly, as I'd rather avoid making more things public to work around this. Please confirm that things work for you the way I fixed it. Thanks!", "created": "2006-05-05T23:29:51.000+0000"}], "num_comments": 2, "text": "Issue: HADOOP-198\nSummary: adding owen's examples to exampledriver\nDescription: owen's sorter and randomwriter are not added to the examples.jar file\n\nComments (2):\n1. Mahadev Konar: Added owen's examples to exampledriver. Also, made the ClusterStatus contructors public since the WritableFactories throw an exception if non public fields are accessed out of the package.\n2. Doug Cutting: I just committed this. I changed your fix to the WritableFactory problem slightly, as I'd rather avoid making more things public to work around this. Please confirm that things work for you the way I fixed it. Thanks!", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.242196"}}
{"id": "7385a4b8a9b06ad4806fdc57eed0ec53", "issue_key": "HADOOP-1097", "issue_type": "Improvement", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Bug in XML serialization of strings in record I/O", "description": "XML serialization of strings in record I/O has a bug whicha makes the XmlInputArchive crash with SAXParseExcpetion (generated XML contains invalid Unicode characters http://www.w3.org/TR/REC-xml/#charsets).", "reporter": "Milind Barve", "assignee": "Milind Barve", "created": "2007-03-09T00:29:51.000+0000", "updated": "2007-07-17T21:22:34.000+0000", "resolved": "2007-05-07T22:37:06.000+0000", "labels": [], "components": ["record"], "comments": [{"author": "Milind Barve", "body": "Fixes XML serialization of Java strings.", "created": "2007-03-09T00:49:25.319+0000"}, {"author": "Milind Barve", "body": "This patch needs to also fix c++ version.", "created": "2007-03-09T18:26:27.297+0000"}, {"author": "Milind Barve", "body": "Unable to attach ne patch (or remove old one). Looks like a Jira configuration issue: Exception trying to establish attachment directory. Check that the application server and JIRA have permissions to write to it: com.atlassian.jira.web.util.AttachmentException: Cannot write to attachment directory. Check that the application server and JIRA have permissions to write to: /var/local/jira/attachments/jira/HADOOP/HADOOP-1097", "created": "2007-03-09T19:26:29.098+0000"}, {"author": "Milind Barve", "body": "There is a severe bug in the binary serialization of strings as well. (It does not work for non-ascii characters.) This bug was introduced when we moved from Text class to Java String class. I will submit a patch by the end of the day.", "created": "2007-03-15T16:48:06.521+0000"}, {"author": "Milind Barve", "body": "Patch for this bug was submitted as part of another patch for HADOOP-1096", "created": "2007-05-07T22:37:06.813+0000"}], "num_comments": 5, "text": "Issue: HADOOP-1097\nSummary: Bug in XML serialization of strings in record I/O\nDescription: XML serialization of strings in record I/O has a bug whicha makes the XmlInputArchive crash with SAXParseExcpetion (generated XML contains invalid Unicode characters http://www.w3.org/TR/REC-xml/#charsets).\n\nComments (5):\n1. Milind Barve: Fixes XML serialization of Java strings.\n2. Milind Barve: This patch needs to also fix c++ version.\n3. Milind Barve: Unable to attach ne patch (or remove old one). Looks like a Jira configuration issue: Exception trying to establish attachment directory. Check that the application server and JIRA have permissions to write to it: com.atlassian.jira.web.util.AttachmentException: Cannot write to attachment directory. Check that the application server and JIRA have permissions to write to: /var/local/jira/attachments/jira/HADOOP/HADOOP-1097\n4. Milind Barve: There is a severe bug in the binary serialization of strings as well. (It does not work for non-ascii characters.) This bug was introduced when we moved from Text class to Java String class. I will submit a patch by the end of the day.\n5. Milind Barve: Patch for this bug was submitted as part of another patch for HADOOP-1096", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.563478"}}
{"id": "f7d1d488c5ef769d1dbb3d5ef979f3f6", "issue_key": "SPARK-513", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "Make NexusScheduler more efficient by keeping a list of tasks for each node", "description": "Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.", "reporter": "Matei Alexandru Zaharia", "assignee": null, "created": "0010-04-03T23:06:00.000+0000", "updated": "2012-10-22T14:55:32.000+0000", "resolved": "2012-10-19T22:50:22.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: Fixed in master.", "created": "2010-10-15T15:20:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-3, originally reported by mateiz", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 2, "text": "Issue: SPARK-513\nSummary: Make NexusScheduler more efficient by keeping a list of tasks for each node\nDescription: Right now, it performs an O(N) scan to find local tasks on each node. We could make this O(1) (amortized) by having a separate queue per node with the preferred tasks for that node, plus a global queue in case the node queue is empty.\n\nComments (2):\n1. Patrick McFadin: Github comment from mateiz: Fixed in master.\n2. Patrick McFadin: Imported from Github issue spark-3, originally reported by mateiz", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.929776"}}
{"id": "6db4a96ce5e4ff32e62fada2fe154292", "issue_key": "KAFKA-670", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Cleanup spurious .index files if present in the log directory on initialization", "description": "It is possible that an index file could somehow be left on the filesystem with no corresponding log file. This is not currently handled well. If the .index file happens to fall on the same offset as a new log segment, then when that segment is created terrible things will happen. We should check this condition on initialization and add some unit tests against it.", "reporter": "Jay Kreps", "assignee": "Jay Kreps", "created": "2012-12-12T17:50:05.000+0000", "updated": "2012-12-13T21:22:43.000+0000", "resolved": "2012-12-13T21:22:43.000+0000", "labels": [], "components": [], "comments": [{"author": "Jay Kreps", "body": "Cleanup any bogus index files (those without a corresponding .log file) during log initialization. Also adds a unit test to cover this case.", "created": "2012-12-12T20:17:07.364+0000"}, {"author": "Neha Narkhede", "body": "+1. Do you think this would resolve KAFKA-669 as well ?", "created": "2012-12-13T00:15:10.095+0000"}, {"author": "Neha Narkhede", "body": "I checked in v1 as part of testing if the git repo works.", "created": "2012-12-13T20:56:47.766+0000"}, {"author": "Jay Kreps", "body": "Closing since Neha checked in the patch.", "created": "2012-12-13T21:22:43.818+0000"}], "num_comments": 4, "text": "Issue: KAFKA-670\nSummary: Cleanup spurious .index files if present in the log directory on initialization\nDescription: It is possible that an index file could somehow be left on the filesystem with no corresponding log file. This is not currently handled well. If the .index file happens to fall on the same offset as a new log segment, then when that segment is created terrible things will happen. We should check this condition on initialization and add some unit tests against it.\n\nComments (4):\n1. Jay Kreps: Cleanup any bogus index files (those without a corresponding .log file) during log initialization. Also adds a unit test to cover this case.\n2. Neha Narkhede: +1. Do you think this would resolve KAFKA-669 as well ?\n3. Neha Narkhede: I checked in v1 as part of testing if the git repo works.\n4. Jay Kreps: Closing since Neha checked in the patch.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.794949"}}
{"id": "8da4f87a4d11b3b322bce63da2b0b4be", "issue_key": "SPARK-1167", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Remove metrics-ganglia from default build due to LGPL issue", "description": "The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here: https://groups.google.com/forum/#!searchin/metrics-user/lgpl/metrics-user/1tQd_qZHQNE/TqAfXYwh7OUJ We should isolate this code in a separate module inside of an `/extras` folder and have a build flag in Maven/SBT (e.g. `-Pganglia`) that will pull this in if desired. That way users can still use it if they do a special build.", "reporter": "Patrick Wendell", "assignee": "Patrick McFadin", "created": "2014-03-02T20:16:28.000+0000", "updated": "2014-03-30T04:13:39.000+0000", "resolved": "2014-03-11T11:55:05.000+0000", "labels": [], "components": ["Spark Core"], "comments": [], "num_comments": 0, "text": "Issue: SPARK-1167\nSummary: Remove metrics-ganglia from default build due to LGPL issue\nDescription: The metrics ganglia code depends on an LGPL library which we can't distribute with Spark. More information can be found here: https://groups.google.com/forum/#!searchin/metrics-user/lgpl/metrics-user/1tQd_qZHQNE/TqAfXYwh7OUJ We should isolate this code in a separate module inside of an `/extras` folder and have a build flag in Maven/SBT (e.g. `-Pganglia`) that will pull this in if desired. That way users can still use it if they do a special build.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.140276"}}
{"id": "2a469838677321a589e0dc1e3c8a5426", "issue_key": "SPARK-840", "issue_type": "Bug", "status": "Resolved", "priority": "Blocker", "resolution": null, "summary": "Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.", "description": "Even though the distribution has everything it needs invoking `spark-shell` now errors with: SCALA_HOME is not set and scala is not in PATH It looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1.", "reporter": "Benjamin Hindman", "assignee": "Benjamin Hindman", "created": "2013-07-31T10:45:41.000+0000", "updated": "2013-07-31T16:40:12.000+0000", "resolved": "2013-07-31T16:40:12.000+0000", "labels": [], "components": [], "comments": [{"author": "Benjamin Hindman", "body": "Add a fix for this into the pull request at https://github.com/mesos/spark/pull/749.", "created": "2013-07-31T12:52:27.748+0000"}, {"author": "Benjamin Hindman", "body": "This can be marked as resolved, it was included in https://github.com/mesos/spark/pull/749.", "created": "2013-07-31T16:32:15.787+0000"}], "num_comments": 2, "text": "Issue: SPARK-840\nSummary: Exporting 'SPARK_LAUNCH_WITH_SCALA=1' by default in 'spark-shell' causes 'run' in distribution to fail.\nDescription: Even though the distribution has everything it needs invoking `spark-shell` now errors with: SCALA_HOME is not set and scala is not in PATH It looks like the offending commit is f4d514810e6fd9f42868ebb9a89390c62c3b42e1.\n\nComments (2):\n1. Benjamin Hindman: Add a fix for this into the pull request at https://github.com/mesos/spark/pull/749.\n2. Benjamin Hindman: This can be marked as resolved, it was included in https://github.com/mesos/spark/pull/749.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.066520"}}
{"id": "f9465806271ff542fde559692c428eed", "issue_key": "SPARK-551", "issue_type": "Bug", "status": "Resolved", "priority": null, "resolution": null, "summary": "slave disconnected", "description": "I am a new mesos user. I run mesos on single machine. If I start the master and slave via the script deploy/start-mesos and run the example, it ends successfully but the slaves would be disconnected. However, if I start the master and slave as foreground process, it does work. What's wrong?", "reporter": "dukeecnu", "assignee": null, "created": "0011-12-26T00:37:00.000+0000", "updated": "2013-12-07T14:12:13.000+0000", "resolved": "2013-12-07T14:12:13.000+0000", "labels": [], "components": [], "comments": [{"author": "Patrick McFadin", "body": "Github comment from mateiz: If you're not the one who posted about this on the mailing list, let me know what output you see from the slave. Look in the logs directory of Mesos for something called mesos-slave.xxx.INFO.", "created": "2011-12-30T08:26:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dukeecnu: I1229 11:17:20.043180 2678 process_based_isolation_module.cpp:114] Forked executor at = 2701 I1229 11:17:20.106433 2678 slave.cpp:725] Got registration for executor 'default' of framework 201112291117-0-0000 I1229 11:17:20.106547 2678 slave.cpp:779] Flushing queued tasks for framework 201112291117-0-0000 I1229 11:17:20.231096 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:20.231884 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.234006 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:21.234973 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.236644 2678 slave.cpp:398] Got assigned task 1 for framework 201112291117-0-0000 I1229 11:17:21.239364 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:21.240097 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.239802 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:22.240749 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.242307 2678 slave.cpp:398] Got assigned task 2 for framework 201112291117-0-0000 I1229 11:17:22.244925 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:22.245684 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.245420 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:23.246287 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.248219 2678 slave.cpp:398] Got assigned task 3 for framework 201112291117-0-0000 I1229 11:17:23.251009 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:23.251930 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.251518 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:24.252471 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.254112 2678 slave.cpp:398] Got assigned task 4 for framework 201112291117-0-0000 I1229 11:17:24.256762 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:24.257613 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.257258 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:25.259258 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.259996 2678 slave.cpp:568] Asked to shut down framework 201112291117-0-0000 I1229 11:17:25.260043 2678 slave.cpp:572] Shutting down framework 201112291117-0-0000 I1229 11:17:25.260066 2678 slave.cpp:1306] Shutting down executor 'default' of framework 201112291117-0-0000 I1229 11:17:26.216948 2678 process_based_isolation_module.cpp:217] Telling slave of lost executor default of framework 201112291117-0-0000 F1229 11:17:26.217048 2678 utils.hpp:130] Expecting 'MESOS_HOME' in environment variables", "created": "2012-01-05T22:03:00.000+0000"}, {"author": "Patrick McFadin", "body": "Github comment from dukeecnu: when I use one machine as master and another as slave, I run them as foreground process. Unfortunately, I fail too. At the end of the console terminal, there are *** Check failure stack trace: *** @ 0x59240d google::LogMessage::Fail() @ 0x5980c7 google::LogMessage::SendToLog() @ 0x593cc4 google::LogMessage::Flush() @ 0x593f26 google::LogMessageFatal::~LogMessageFatal() @ 0x473605 mesos::internal::utils::process::killtree() @ 0x46dc0c mesos::internal::slave::ProcessBasedIsolationModule::killExecutor() @ 0x46c5d4 mesos::internal::slave::ProcessBasedIsolationModule::processExited() @ 0x473fc9 std::tr1::_Function_handler<>::_M_invoke() @ 0x47453e std::tr1::_Function_handler<>::_M_invoke() @ 0x5adbc5 process::ProcessBase::serve() @ 0x44c541 process::ProcessBase::operator()() @ 0x5b45f2 process::ProcessManager::run() @ 0x5b4760 process::trampoline() @ 0x3cbca41820 (unknown)", "created": "2012-01-05T22:12:00.000+0000"}, {"author": "Patrick McFadin", "body": "Imported from Github issue spark-106, originally reported by dukeecnu", "created": "2012-10-19T18:03:00.000+0000"}], "num_comments": 4, "text": "Issue: SPARK-551\nSummary: slave disconnected\nDescription: I am a new mesos user. I run mesos on single machine. If I start the master and slave via the script deploy/start-mesos and run the example, it ends successfully but the slaves would be disconnected. However, if I start the master and slave as foreground process, it does work. What's wrong?\n\nComments (4):\n1. Patrick McFadin: Github comment from mateiz: If you're not the one who posted about this on the mailing list, let me know what output you see from the slave. Look in the logs directory of Mesos for something called mesos-slave.xxx.INFO.\n2. Patrick McFadin: Github comment from dukeecnu: I1229 11:17:20.043180 2678 process_based_isolation_module.cpp:114] Forked executor at = 2701 I1229 11:17:20.106433 2678 slave.cpp:725] Got registration for executor 'default' of framework 201112291117-0-0000 I1229 11:17:20.106547 2678 slave.cpp:779] Flushing queued tasks for framework 201112291117-0-0000 I1229 11:17:20.231096 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:20.231884 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.234006 2678 slave.cpp:920] Status update: task 0 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:21.234973 2678 slave.cpp:642] Got acknowledgement of status update for task 0 of framework 201112291117-0-0000 I1229 11:17:21.236644 2678 slave.cpp:398] Got assigned task 1 for framework 201112291117-0-0000 I1229 11:17:21.239364 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:21.240097 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.239802 2678 slave.cpp:920] Status update: task 1 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:22.240749 2678 slave.cpp:642] Got acknowledgement of status update for task 1 of framework 201112291117-0-0000 I1229 11:17:22.242307 2678 slave.cpp:398] Got assigned task 2 for framework 201112291117-0-0000 I1229 11:17:22.244925 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:22.245684 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.245420 2678 slave.cpp:920] Status update: task 2 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:23.246287 2678 slave.cpp:642] Got acknowledgement of status update for task 2 of framework 201112291117-0-0000 I1229 11:17:23.248219 2678 slave.cpp:398] Got assigned task 3 for framework 201112291117-0-0000 I1229 11:17:23.251009 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:23.251930 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.251518 2678 slave.cpp:920] Status update: task 3 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:24.252471 2678 slave.cpp:642] Got acknowledgement of status update for task 3 of framework 201112291117-0-0000 I1229 11:17:24.254112 2678 slave.cpp:398] Got assigned task 4 for framework 201112291117-0-0000 I1229 11:17:24.256762 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_RUNNING I1229 11:17:24.257613 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.257258 2678 slave.cpp:920] Status update: task 4 of framework 201112291117-0-0000 is now in state TASK_FINISHED I1229 11:17:25.259258 2678 slave.cpp:642] Got acknowledgement of status update for task 4 of framework 201112291117-0-0000 I1229 11:17:25.259996 2678 slave.cpp:568] Asked to shut down framework 201112291117-0-0000 I1229 11:17:25.260043 2678 slave.cpp:572] Shutting down framework 201112291117-0-0000 I1229 11:17:25.260066 2678 slave.cpp:1306] Shutting down executor 'default' of framework 201112291117-0-0000 I1229 11:17:26.216948 2678 process_based_isolation_module.cpp:217] Telling slave of lost executor default of framework 201112291117-0-0000 F1229 11:17:26.217048 2678 utils.hpp:130] Expecting 'MESOS_HOME' in environment variables\n3. Patrick McFadin: Github comment from dukeecnu: when I use one machine as master and another as slave, I run them as foreground process. Unfortunately, I fail too. At the end of the console terminal, there are *** Check failure stack trace: *** @ 0x59240d google::LogMessage::Fail() @ 0x5980c7 google::LogMessage::SendToLog() @ 0x593cc4 google::LogMessage::Flush() @ 0x593f26 google::LogMessageFatal::~LogMessageFatal() @ 0x473605 mesos::internal::utils::process::killtree() @ 0x46dc0c mesos::internal::slave::ProcessBasedIsolationModule::killExecutor() @ 0x46c5d4 mesos::internal::slave::ProcessBasedIsolationModule::processExited() @ 0x473fc9 std::tr1::_Function_handler<>::_M_invoke() @ 0x47453e std::tr1::_Function_handler<>::_M_invoke() @ 0x5adbc5 process::ProcessBase::serve() @ 0x44c541 process::ProcessBase::operator()() @ 0x5b45f2 process::ProcessManager::run() @ 0x5b4760 process::trampoline() @ 0x3cbca41820 (unknown)\n4. Patrick McFadin: Imported from Github issue spark-106, originally reported by dukeecnu", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.951086"}}
{"id": "8a7d665e597003854878eb12f31cd096", "issue_key": "HADOOP-678", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Investigate direct buffer leaks and fix them.", "description": "\"direct memory\" leaks are suspected in NameNode running out of memory. At this point I don't have much more information. Please add any info you have if you have looked in to NameNode memory. I am taking a look at Server.java in hadoop.ipc.", "reporter": "Raghu Angadi", "assignee": "Raghu Angadi", "created": "2006-11-03T02:58:27.000+0000", "updated": "2006-12-15T23:02:31.000+0000", "resolved": "2006-11-03T06:54:37.000+0000", "labels": [], "components": [], "comments": [{"author": "Christian Kunz", "body": "I started an issue regarding this kind of name node server memory leak: HADOOP-637", "created": "2006-11-03T03:10:20.000+0000"}, {"author": "Raghu Angadi", "body": "Duplicate of HADOOP-637", "created": "2006-11-03T06:54:37.000+0000"}], "num_comments": 2, "text": "Issue: HADOOP-678\nSummary: Investigate direct buffer leaks and fix them.\nDescription: \"direct memory\" leaks are suspected in NameNode running out of memory. At this point I don't have much more information. Please add any info you have if you have looked in to NameNode memory. I am taking a look at Server.java in hadoop.ipc.\n\nComments (2):\n1. Christian Kunz: I started an issue regarding this kind of name node server memory leak: HADOOP-637\n2. Raghu Angadi: Duplicate of HADOOP-637", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.434402"}}
{"id": "a26c06b3d95c4ceff935516520ca6464", "issue_key": "HADOOP-905", "issue_type": "Bug", "status": "Closed", "priority": "Major", "resolution": null, "summary": "Code to qualify inputDirs doesn't affect path validation", "description": "This code, at JobClient:306, doesn't seem to validate the fully qualified inputDirs, since inputDirs is a newly created arrray: Path[] inputDirs = job.getInputPaths(); // make sure directories are fully qualified before checking them for(int i=0; i < inputDirs.length; ++i) { if (inputDirs[i].toUri().getScheme() == null) { inputDirs[i] = userFileSys.makeQualified(inputDirs[i]); } } // input paths should exist. job.getInputFormat().validateInput(job);", "reporter": "Kenji Matsuoka", "assignee": null, "created": "2007-01-18T19:22:29.000+0000", "updated": "2007-02-03T03:23:26.000+0000", "resolved": "2007-01-18T21:30:30.000+0000", "labels": [], "components": ["fs"], "comments": [{"author": "Doug Cutting", "body": "Yes, this looks like dead code, left over from a prior version. It should be removed, as the paths are now fully-qualified in InputFormatBase. Thanks for noticing this!", "created": "2007-01-18T19:45:36.217+0000"}, {"author": "Doug Cutting", "body": "I fixed this, removing the dead code.", "created": "2007-01-18T21:30:30.921+0000"}], "num_comments": 2, "text": "Issue: HADOOP-905\nSummary: Code to qualify inputDirs doesn't affect path validation\nDescription: This code, at JobClient:306, doesn't seem to validate the fully qualified inputDirs, since inputDirs is a newly created arrray: Path[] inputDirs = job.getInputPaths(); // make sure directories are fully qualified before checking them for(int i=0; i < inputDirs.length; ++i) { if (inputDirs[i].toUri().getScheme() == null) { inputDirs[i] = userFileSys.makeQualified(inputDirs[i]); } } // input paths should exist. job.getInputFormat().validateInput(job);\n\nComments (2):\n1. Doug Cutting: Yes, this looks like dead code, left over from a prior version. It should be removed, as the paths are now fully-qualified in InputFormatBase. Thanks for noticing this!\n2. Doug Cutting: I fixed this, removing the dead code.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.509854"}}
{"id": "0d08f09b014e387d8617b5ee01117051", "issue_key": "HADOOP-298", "issue_type": "Improvement", "status": "Closed", "priority": "Minor", "resolution": null, "summary": "nicer reports of progress for distcp", "description": "The unformatted number of bytes in distcp is difficult to read.", "reporter": "Owen O'Malley", "assignee": "Owen O'Malley", "created": "2006-06-14T02:31:52.000+0000", "updated": "2006-08-03T17:46:47.000+0000", "resolved": "2006-06-14T04:56:06.000+0000", "labels": [], "components": ["util"], "comments": [{"author": "Owen O'Malley", "body": "This patch adds a percentage complete and a human readable format of the number of bytes copied so far.", "created": "2006-06-14T02:32:38.000+0000"}, {"author": "Doug Cutting", "body": "I just committed this. Thanks, Owen.", "created": "2006-06-14T04:56:06.000+0000"}], "num_comments": 2, "text": "Issue: HADOOP-298\nSummary: nicer reports of progress for distcp\nDescription: The unformatted number of bytes in distcp is difficult to read.\n\nComments (2):\n1. Owen O'Malley: This patch adds a percentage complete and a human readable format of the number of bytes copied so far.\n2. Doug Cutting: I just committed this. Thanks, Owen.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.280307"}}
{"id": "514544ffa72800b94505873febf865c0", "issue_key": "KAFKA-900", "issue_type": "Bug", "status": "Resolved", "priority": "Major", "resolution": null, "summary": "ClosedByInterruptException when high-level consumer shutdown normally", "description": "I'm porting some unit tests from 0.7.2 to 0.8.0. The test does the following, all embedded in the same java process: -- spins up a zk instance -- spins up a kafka server using a fresh log directory -- creates a producer and sends a message -- creates a high-level consumer and verifies that it can consume the message -- shuts down the consumer -- stops the kafka server -- stops zk The test seems to be working fine now, however, I consistently see the following exception, when the consumer connector is shutdown: 1699 [ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683] WARN kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683], Error in fetch Name: FetchRequest; Version: 0; CorrelationId: 1; ClientId: group1-ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683; ReplicaId: -1; MaxWait: 100 ms; MinBytes: 1 bytes; RequestInfo: [test-topic,0] -> PartitionFetchInfo(1,1048576) java.nio.channels.ClosedByInterruptException at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:543) at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57) at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:47) at kafka.consumer.SimpleConsumer.reconnect(SimpleConsumer.scala:60) at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:81) at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:73) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:112) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:112) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:112) at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:111) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:111) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:111) at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:110) at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:96) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51) 1721 [Thread-12] INFO com.squareup.kafka.server.KafkaServer - Shutting down KafkaServer 2030 [main] INFO com.squareup.kafka.server.KafkaServer - Shut down complete for KafkaServer Disconnected from the target VM, address: '127.0.0.1:49243', transport: 'socket' It would be great if instead, something meaningful was logged, like: \"Consumer connector has been shutdown\"", "reporter": "Jason Rosenberg", "assignee": "Jun Rao", "created": "2013-05-09T05:21:26.000+0000", "updated": "2013-05-29T16:56:58.000+0000", "resolved": "2013-05-29T16:56:58.000+0000", "labels": [], "components": ["consumer"], "comments": [{"author": "Jun Rao", "body": "Attach a patch. Jason, could you give it a try?", "created": "2013-05-09T16:35:38.743+0000"}, {"author": "Neha Narkhede", "body": "+1, thanks for the patch!", "created": "2013-05-29T16:39:44.193+0000"}, {"author": "Jun Rao", "body": "Thanks for the review. Committed to 0.8.", "created": "2013-05-29T16:56:58.234+0000"}], "num_comments": 3, "text": "Issue: KAFKA-900\nSummary: ClosedByInterruptException when high-level consumer shutdown normally\nDescription: I'm porting some unit tests from 0.7.2 to 0.8.0. The test does the following, all embedded in the same java process: -- spins up a zk instance -- spins up a kafka server using a fresh log directory -- creates a producer and sends a message -- creates a high-level consumer and verifies that it can consume the message -- shuts down the consumer -- stops the kafka server -- stops zk The test seems to be working fine now, however, I consistently see the following exception, when the consumer connector is shutdown: 1699 [ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683] WARN kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683], Error in fetch Name: FetchRequest; Version: 0; CorrelationId: 1; ClientId: group1-ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683; ReplicaId: -1; MaxWait: 100 ms; MinBytes: 1 bytes; RequestInfo: [test-topic,0] -> PartitionFetchInfo(1,1048576) java.nio.channels.ClosedByInterruptException at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184) at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:543) at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57) at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:47) at kafka.consumer.SimpleConsumer.reconnect(SimpleConsumer.scala:60) at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:81) at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:73) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:112) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:112) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:112) at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:111) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:111) at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:111) at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:110) at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:96) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51) 1721 [Thread-12] INFO com.squareup.kafka.server.KafkaServer - Shutting down KafkaServer 2030 [main] INFO com.squareup.kafka.server.KafkaServer - Shut down complete for KafkaServer Disconnected from the target VM, address: '127.0.0.1:49243', transport: 'socket' It would be great if instead, something meaningful was logged, like: \"Consumer connector has been shutdown\"\n\nComments (3):\n1. Jun Rao: Attach a patch. Jason, could you give it a try?\n2. Neha Narkhede: +1, thanks for the patch!\n3. Jun Rao: Thanks for the review. Committed to 0.8.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:36.878538"}}
{"id": "cffbf907b4473b6d6cb71fd94ff534ea", "issue_key": "HADOOP-887", "issue_type": "Improvement", "status": "Open", "priority": "Minor", "resolution": null, "summary": "metrics API should enforce some restrictions on tag and metric names", "description": "The metrics API currently allows any string to be a metric name or tag name (see org.apache.hadoop.metrics.Metrics.createRecord(...) and org.apache.hadoop.metrics .Metrics.report(...)). Such unrestricted names makes it difficult to implement metrics providers that generate source code to manipulate these metrics, since many characters are invalid as method or variable names. I'd like to propose that metric names be restricted to letters, digits, and underscore (A-Za-z0-9_) and this restriction be documented and enforced by the API.", "reporter": "Nigel Daley", "assignee": null, "created": "2007-01-12T21:48:00.000+0000", "updated": "2007-01-12T22:28:20.000+0000", "resolved": null, "labels": [], "components": ["metrics"], "comments": [{"author": "Raghu Angadi", "body": "and '.'? datanode.reads, datanode.writes is more human friendly than namenode_reads and consistent with hadoop config names.", "created": "2007-01-12T22:11:38.714+0000"}, {"author": "Milind Barve", "body": "'.' is not allowed in most languages in variable and method names. Also hadoop record I/O does not allow this character to be part of a record definition. If Metrics API is used to collect Job stats with JobMetricsContext with IPC as a transport mechanism, one would need the metrics record to be writable. It would be good to use Hadoop record I/O to define such Metrics record, which means disallowing such special charracters in metric names.", "created": "2007-01-12T22:28:20.309+0000"}], "num_comments": 2, "text": "Issue: HADOOP-887\nSummary: metrics API should enforce some restrictions on tag and metric names\nDescription: The metrics API currently allows any string to be a metric name or tag name (see org.apache.hadoop.metrics.Metrics.createRecord(...) and org.apache.hadoop.metrics .Metrics.report(...)). Such unrestricted names makes it difficult to implement metrics providers that generate source code to manipulate these metrics, since many characters are invalid as method or variable names. I'd like to propose that metric names be restricted to letters, digits, and underscore (A-Za-z0-9_) and this restriction be documented and enforced by the API.\n\nComments (2):\n1. Raghu Angadi: and '.'? datanode.reads, datanode.writes is more human friendly than namenode_reads and consistent with hadoop config names.\n2. Milind Barve: '.' is not allowed in most languages in variable and method names. Also hadoop record I/O does not allow this character to be part of a record definition. If Metrics API is used to collect Job stats with JobMetricsContext with IPC as a transport mechanism, one would need the metrics record to be writable. It would be good to use Hadoop record I/O to define such Metrics record, which means disallowing such special charracters in metric names.", "metadata": {"source": "apache_jira", "scraped_at": "2025-11-25T00:31:37.505854"}}
